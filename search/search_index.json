{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#unilu-hpc-technical-documentation","title":"Uni.lu HPC Technical Documentation","text":"<p>hpc-docs.uni.lu is a resource with the technical details for users to make effective use of the Uni.lu High Performance Computing (ULHPC) Facility's resources.</p> <p> ULHPC Supercomputers  Getting Started</p> <p>New: monthly HPC trainings for beginners, see our dedicated page.</p>"},{"location":"#ulhpc-web-portals","title":"ULHPC Web Portals","text":"<ul> <li>ULHPC Tutorials - tutorials for many HPC topics</li> <li>Helpdesk / Ticket Portal - open tickets, make requests</li> <li>ULHPC Discourse - forum-like community portal</li> <li>ULHPC Home page - center news and information: hpc.uni.lu</li> </ul>"},{"location":"#popular-documentation-pages","title":"Popular documentation pages","text":"<ul> <li>SSH Management on ULHPC Identity Management Portal (IdM/IPA)</li> <li>Usage Charging Policy</li> <li>Job Status and Reason Codes</li> <li>Job Prioritization Factors</li> <li>Example of Job Launchers - currated example of job launcher scripts</li> <li>Slurm overview - Slurm commands, job script basics, submitting, updating jobs</li> <li>Join and Monitor Jobs</li> <li>File permissions - Unix file permissions</li> <li>ULHPC Software/Modules Environment</li> <li>Compiling/Building your own software</li> </ul> About this site <p>The ULHPC Technical Documentation is based on MkDocs and the mkdocs-material theme, and inspired by the (excellent) NERSC documentation site. These pages are hosted from a git repository and contributions are welcome!</p>"},{"location":"getting-started/","title":"Getting Started on ULHPC Facilities","text":"<p>Welcome to the High Performance Computing (HPC) Facility of the University of Luxembourg (ULHPC)!</p> <p>This page will guide you through the basics of using ULHPC's supercomputers, storage systems, and services.</p>"},{"location":"getting-started/#what-is-ulhpc","title":"What is ULHPC ?","text":"<p>HPC is crucial in academic environments to achieve high-quality results in all application areas. All world-class universities require this type of facility to accelerate its research and ensure cutting-edge results in time to face the global competition.</p> What is High Performance Computing? <p>If you're new to all of this, this is probably the first question you have in mind. Here is a possible definition:</p> <p>\"High Performance Computing (HPC) most generally refers to the practice of aggregating computing power in a way that delivers much higher performance than one could get out of a typical desktop computer or workstation in order to solve large problems in science, engineering, or business.\"</p> <p>Indeed, with the advent of the technological revolution and the digital transformation that made all scientific disciplines becoming computational nowadays, High-Performance Computing (HPC) is increasingly identified as a strategic asset and enabler to accelerate the research performed in all areas requiring intensive computing and large-scale Big Data analytic capabilities. Tasks which would typically require several years or centuries to be computed on a typical desktop computer may only require a couple of hours, days or weeks over an HPC system.</p> <p>For more details, you may want to refer to this Inside HPC article.</p> <p>Since 2007, the University of Luxembourg (UL) has invested tens of millions of euros into its own HPC facilities to responds to the growing needs for increased computing and storage. ULHPC (sometimes referred to as Uni.lu HPC) is the entity  providing High Performance Computing and Big Data Storage services and support for UL researchers and its external partners.</p> <p>The University manages several research computing facilities located on the Belval campus, offering a cutting-edge research infrastructure to Luxembourg public research while serving as edge access to bigger systems from PRACE or EuroHPC, such as the Euro-HPC Luxembourg supercomputer \"MeluXina\".</p> <p>Warning</p> <p>In particular, the ULHPC is NOT the national HPC center of Luxembourg, but simply one of its strategic partner operating the second largest HPC facility of the country.</p> <p>The HPC facility is one element of the extensive digital research infrastructure and expertise developed by the University over the last years. It also supports the University\u2019s ambitious digital strategy and in particular the creation of a Facility for Data and HPC Sciences. This facility aims to provide a world-class user-driven digital infrastructure and services for fostering the development of collaborative activities related to frontier research and teaching in the fields of Computational and Data Sciences, including High Performance Computing, Data Analytics, Big Data Applications, Artificial Intelligence and Machine Learning.</p> <p>Reference ULHPC Article to cite</p> <p>If you want to get a good overview of the way our facility is setup, managed and evaluated, you can refer to the reference article you are in all cases entitled to refer to when crediting the ULHPC facility as per AUP.</p> <p>ACM Reference Format | ORBilu entry | slides : Sebastien Varrette, Hyacinthe Cartiaux, Sarah Peter, Emmanuel Kieffer, Teddy Valette, and Abatcha Olloh. 2022. Management of an Academic HPC &amp; Research Computing Facility: The ULHPC Experience 2.0. In 6<sup>th</sup> High Performance Computing and Cluster Technologies Conference (HPCCT 2022), July 08-10, 2022, Fuzhou, China. ACM, New York, NY, USA, 14 pages. https://doi.org/10.1145/3560442.3560445</p>"},{"location":"getting-started/#supercomputing-and-storage-resources-at-a-glance","title":"Supercomputing and Storage Resources at a glance","text":"<p>ULHPC is a strategic asset of the university and an important factor for the scientific and therefore also economic competitiveness of the Grand Duchy of Luxembourg. We provide a key research infrastructure featuring state-of-the-art computing and storage resources serving the UL HPC community primarily composed by UL researchers.</p> <p>The UL HPC platform has kept growing over time thanks to the continuous efforts of the core HPC / Digital Platform team - contact: hpc-team@uni.lu, recently completed with the EuroHPC Competence Center Task force (A. Vandeventer (Project Manager), L. Koutsantonis).</p> <p>ULHPC Computing and Storage Capacity (2022)</p> <p>Installed in the premises of the University\u2019s Centre de Calcul (CDC), the UL HPC facilities provides a total computing capacity of 2.76 PetaFlops and a shared storage capacity of around 10 PetaBytes.</p> How big is 1 PetaFlops? 1 PetaByte? <ul> <li>1 PetaFlops = 10<sup>15</sup> floating-point operations per second (PFlops or PF for short), corresponds to the cumulative performance of more than 3510 Macbook Pro 13\" laptops <sup>1</sup>, or 7420 iPhone XS <sup>2</sup></li> <li>1 PetaByte = 10<sup>15</sup> bytes = 8*10<sup>15</sup> bits, corresponding to the cumulative raw capacity of more than 1950 SSDs 512GB.</li> </ul> <p> </p> <p>This places the HPC center of the University of Luxembourg as one of the major actors in HPC and Big Data for the Greater Region Saar-Lor-Lux.</p> <p>In practice, the UL HPC Facility features 3 types of computing resources:</p> <ul> <li>\"regular\" nodes: Dual CPU, no accelerators, 128 to 256 GB of RAM</li> <li>\"gpu\" nodes:     Dual CPU, 4 Nvidia accelerators, 768 GB RAM</li> <li>\"bigmem\" nodes:  Quad-CPU, no accelerators, 3072 GB RAM</li> </ul> <p>These resources can be reserved and allocated for the execution of jobs scheduled on the platform thanks to a Resource and Job Management Systems (RJMS) - Slurm in practice. This tool allows for a fine-grain analysis and accounting of the used resources, facilitating the generation of activity reports for a given time period.</p>"},{"location":"getting-started/#iris","title":"Iris","text":"<p><code>iris</code>, in production since June 2017, is a Dell/Intel supercomputer with a theoretical peak performance of 1082 TFlop/s, featuring 196 computing nodes (totalling 5824 computing cores) and 96 GPU accelerators (NVidia V100).</p> <p> Iris Detailed system specifications</p>"},{"location":"getting-started/#aion","title":"Aion","text":"<p><code>aion</code>, in production since October 2020, is a Bull Sequana XH2000/AMD supercomputer offering a peak performance of 1692 TFlop/s, featuring 318 compute nodes (totalling 40704 computing cores).</p> <p> Aion Detailed system specifications</p>"},{"location":"getting-started/#gpfsspectrumscale-file-system-home-project","title":"GPFS/SpectrumScale File System (<code>$HOME</code>, project)","text":"<p>IBM Spectrum Scale, formerly known as the General Parallel File System (GPFS), is global high-performance clustered file system available on all ULHPC computational systems. It is deployed over Dell-based storage hardware.</p> <p>It allows sharing homedirs and project data between users, systems, and eventually (i.e. if needed) with the \"outside world\".</p> <p> GPFS/Spectrumscale Detailed specifications</p>"},{"location":"getting-started/#lustre-file-system-scratch","title":"Lustre File System (<code>$SCRATCH</code>)","text":"<p>The Lustre file system is an open-source, parallel file system that supports many requirements of leadership class HPC simulation environments. It is available as a global high-performance file system on all ULHPC computational systems through a DDN ExaScaler and is meant to host temporary scratch data.</p> <p> Lustre Detailed specifications</p>"},{"location":"getting-started/#onefs-file-system-project-backup-archival","title":"OneFS File System (project, backup, archival)","text":"<p>In 2014, the SIU, the UL HPC and the LCSB join their forces (and their funding) to acquire a scalable and modular NAS solution able to sustain the need for an internal big data storage, i.e. provides space for centralized data and backups of all devices used by the UL staff and all research-related data, including the one proceed on the UL HPC platform. A global low-performance Dell/EMC Isilon system is available on all ULHPC computational systems. It is intended for long term storage of data that is not frequently accessed. For more details, see Isilon specifications.</p>"},{"location":"getting-started/#fast-infiniband-network","title":"Fast Infiniband Network","text":"<p>High Performance Computing (HPC) encompasses advanced computation over parallel processing, enabling faster execution of highly compute intensive tasks. The execution time of a given simulation depends upon many factors, such as the number of CPU/GPU cores and their utilisation factor and the interconnect performance, efficiency, and scalability. InfiniBand is the fast interconnect technology implemented within all ULHPC supercomputers, more specifically:</p> <ul> <li>Iris relies on a EDR Infiniband (IB) Fabric in a Fat-Tree Topology</li> <li>Aion relies on a HDR100 Infiniband (IB) Fabric in a Fat-Tree Topology</li> </ul> <p>For more details, see ULHPC IB Network Detailed specifications.</p>"},{"location":"getting-started/#acceptable-use-policy-aup","title":"Acceptable Use Policy (AUP)","text":"<p>There are a number of policies which apply to ULHPC users.</p> <p> UL HPC Acceptable Use Policy (AUP) [pdf] </p> <p>Important</p> <p>All users of UL HPC resources and PIs must abide by the UL HPC Acceptable Use Policy (AUP). You should read and keep a signed copy of this document before using the facility.</p> <p>Access and/or usage of any ULHPC system assumes the tacit acknowledgement to this policy.</p>"},{"location":"getting-started/#ulhpc-accounts","title":"ULHPC Accounts","text":"<p>In order to use the ULHPC facilities, you need to have a user account with an associated user login name (also called username) placed under an account hierarchy.</p> <ul> <li>Get a ULHPC account</li> <li>Understanding Slurm account hierarchy and accounting rules</li> <li>ULHPC Identity Management (IPA portal)</li> <li>Password policy</li> <li>Usage Charging Policy</li> </ul>"},{"location":"getting-started/#connecting-to-ulhpc-supercomputers","title":"Connecting to ULHPC supercomputers","text":"<p>MFA is strongly encouraged for all ULHPC users</p> <p>It will be soon become mandatory - detailed instructions will be provided soon.</p> <ul> <li>SSH</li> <li>Open On Demand Portal</li> <li>ULHPC Login/Access servers</li> <li>Troubleshooting connection problems</li> </ul>"},{"location":"getting-started/#data-management","title":"Data Management","text":"<ul> <li>Global Directory Structure</li> <li>Transferring data: Tools and recommendations to transfer data both inside and outside of ULHPC.</li> <li>Quotas</li> <li>Understanding Unix File Permissions</li> </ul>"},{"location":"getting-started/#user-environment","title":"User Environment","text":"<p>Info</p> <p><code>$HOME</code>, Project and <code>$SCRATCH</code> directories are shared across all ULHPC systems, meaning that</p> <ul> <li>every file/directory pushed or created on the front-end is available on the computing nodes</li> <li>every file/directory pushed or created on the computing nodes is available on the front-end</li> </ul> <p> ULHPC User Environment</p>"},{"location":"getting-started/#computing-software-environment","title":"Computing Software Environment","text":"<p>The ULHPC Team supplies a large variety of HPC utilities, scientific applications and programming libraries to its user community. The user software environment is generated using Easybuild (EB) and is made available as environment modules through LMod. </p> <ul> <li>ULHPC Modules Environment</li> <li>ULHPC Supported Software List.<ul> <li>Available modules are reachable from the compute nodes only via <code>module avail</code></li> </ul> </li> <li>ULHPC Easybuild Configuration</li> </ul> <ul> <li>Running Containers</li> </ul> <p>Software building support</p> <p>If you need help to build / develop software, we encourage you to first try using Easybuild as a recipe probably exist for the software you consider. You can then open a ticket on HPC Help Desk Portal and we will evaluate the cost and effort required. You may also ask the help of other ULHPC users using the HPC User community mailing list: (moderated): `hpc-users@uni.lu.</p>"},{"location":"getting-started/#running-jobs","title":"Running Jobs","text":"<p>Typical usage of the ULHPC supercomputers involves the reservation and allocation of computing resources for the execution of jobs (submitted via launcher scripts) and scheduled on the platform thanks to a Resource and Job Management Systems (RJMS) - Slurm in our case.</p> <p> Slurm on ULHPC clusters  Convenient Slurm Commands</p> <ul> <li>Rich set of launcher scripts examples</li> <li>Fairshare</li> <li>Job Priority and Backfilling</li> <li>Job Accounting and Billing</li> </ul>"},{"location":"getting-started/#interactive-computing","title":"Interactive Computing","text":"<p>ULHPC also supports interactive computing.</p> <ul> <li>Interactive jobs</li> <li>Jupyter Notebook</li> </ul>"},{"location":"getting-started/#getting-help","title":"Getting Help","text":"<p>ULHPC places a very strong emphasis on enabling science and providing user-oriented systems and services.</p>"},{"location":"getting-started/#documentation","title":"Documentation","text":"<p>We have always maintained an extensive documentation and HPC tutorials available online, which aims at being the most up-to-date and comprehensive while covering many (many) topics.</p> <p> ULHPC Technical Documentation  ULHPC Tutorials</p> <p>The ULHPC Team welcomes your contributions</p> <p>These pages are hosted from a git repository and contributions are welcome! Fork this repo</p>"},{"location":"getting-started/#support","title":"Support","text":"<p> ULHPC Support Overview  Service Now HPC Support Portal</p> <p>Availability and Response Time</p> <p>HPC support is provided on a volunteer basis by UL HPC staff and associated UL experts working at normal business hours. We offer no guarantee on response time except with paid support contracts.</p> <ol> <li> <p>The best MacBook Pro 13\" in 2020 is equiped with Ice Lake 2 GHz Intel Quad-Core i5 processors with an estimated computing performance of 284.3 Gflops as measured by the Geekbench 4 multi-core benchmarks platform, with SGEMM \u21a9</p> </li> <li> <p>Apple A12 Bionic, the 64-bit ARM-based system on a chip (SoC) proposed on the iPhone XS has an estimated performance of 134.7 GFlops as measured by the Geekbench 4 multi-core benchmarks platform, with SGEMM \u21a9</p> </li> </ol>"},{"location":"hpc-schools/","title":"On-site HPC trainings and tutorials","text":"<p>We propose periodical on-site events for our users. They are free of charge and can be attended by anyone from the University of Luxembourg faculties and interdisciplinary centers.  Additionally, we also accept users from LIST, LISER and LIH. If you are part of another public research center, please contact us.</p>"},{"location":"hpc-schools/#forthcoming-events","title":"Forthcoming events","text":"<ul> <li>Introduction to HPC and Machine Learning, eligible for ECTS credits: 2-4 of June 2025, Belval Campus</li> </ul>"},{"location":"hpc-schools/#hpc-school-for-beginners","title":"HPC School for beginners","text":"<p>This event aims to equip you with essential skills and knowledge to embark on your High-Performance Computing journey. The event is organized monthly and is composed of two half days (usually 9am-12pm).</p> <p>Feel free to only attend the second day session if:</p> <ul> <li>You can connect to the ULHPC</li> <li>You are comfortable with the command line interface</li> </ul> <p>Limited spots available per session (usually 30 max).</p>"},{"location":"hpc-schools/#upcoming-sessions","title":"Upcoming sessions","text":"<ul> <li>Date: September 2025, 11<sup>th</sup>-12<sup>th</sup></li> <li>Time: 9am to 12pm (both days).</li> <li>Location: 2.380, MSA - Belval Campus.</li> </ul>"},{"location":"hpc-schools/#morning-1-accessing-the-cluster-and-command-line-introduction","title":"Morning 1 - Accessing the Cluster and Command Line Introduction","text":"<p>Learn how to access the HPC cluster, set up your machine, and navigate the command line interface effectively. Gain confidence in interacting with the cluster environment.</p>"},{"location":"hpc-schools/#morning-2-understanding-hpc-workflow-job-submission-and-monitoring","title":"Morning 2 - Understanding HPC Workflow: Job Submission and Monitoring","text":"<p>Explore the inner workings of HPC systems. Discover the process of submitting and managing computational tasks. Learn how to monitor and optimize job performance.</p>"},{"location":"hpc-schools/#introduction-to-hpc-and-machine-learning","title":"Introduction to HPC and Machine Learning","text":"<p>A combination of \"Introduction to HPC\" and \"Machine Learning for beginners\" courses that is eligible for ECTS credits.</p>"},{"location":"hpc-schools/#upcoming-sessions_1","title":"Upcoming sessions","text":"<p>No sessions are planned at the moment. Future sessions will be announced here, please wait for announcements or contact the HPC team via email to express your interest.</p>"},{"location":"hpc-schools/#introduction-to-hpc","title":"Introduction to HPC","text":"<p>This event is an extended version of the \"HPC school for beginners\" and provides users with the essential skills required to use HPC facilities and to compose and deploy efficient programs in an HPC environment. The event is spread in 4 sessions across 2 days.</p> <p>Limited spots available per session (20 max).</p>"},{"location":"hpc-schools/#upcoming-sessions_2","title":"Upcoming sessions","text":"<ul> <li>Dates: 2<sup>nd</sup> and 3<sup>rd</sup> of June 2025</li> <li>Time: 9am to 5pm (both days)</li> <li>Location: MNO 1.040, Belval Campus</li> </ul>"},{"location":"hpc-schools/#session-1-accessing-the-cluster-and-command-line-introduction","title":"Session 1 - Accessing the Cluster and Command Line Introduction","text":"<p>Timeslot: Day 1 09:00-12:00, Location: MNO 1.040, Belval Campus </p> <p>Learn how to access the HPC cluster, set up your machine, and navigate the command line interface effectively. Gain confidence in interacting with the cluster environment.</p> <p>Feel free to skip session 1 if:</p> <ul> <li>you can connect to the ULHPC, and</li> <li>you are comfortable with the command line interface.</li> </ul>"},{"location":"hpc-schools/#session-2-understanding-hpc-workflow-job-submission-and-monitoring","title":"Session 2 - Understanding HPC Workflow: Job Submission and Monitoring","text":"<p>Timeslot: Day 1 13:00-17:00, Location: MNO 1.040, Belval Campus</p> <p>Explore the inner workings of HPC systems. Discover the process of submitting and managing computational tasks. Learn how to monitor and optimize job performance.</p>"},{"location":"hpc-schools/#session-3-working-with-software-environments-and-containers","title":"Session 3 - Working with software environments and containers","text":"<p>Timeslot: Day 2 09:00-12:00, Location: MNO 1.040, Belval Campus</p> <p>Discover how you can setup isolated software environments and containers in the HPC systems. Improve the reproducibility of you workflows by creating reproducible setups.</p>"},{"location":"hpc-schools/#session-4-using-resources-efficiently","title":"Session 4 - Using resources efficiently","text":"<p>Timeslot: Day 2 13:00-17:00, Location: MNO 1.040, Belval Campus</p> <p>Understand the allocation of resources in HPC systems. Configure you code to access cores, memory channels, and GPUs efficiently and prevent over-subscription.</p>"},{"location":"hpc-schools/#resources","title":"Resources","text":"<ul> <li>Setup<ul> <li>Request an account</li> <li>Access the HPC - Linux and Mac</li> <li>Access the HPC - Windows</li> </ul> </li> <li>Basic shell and cluster skills<ul> <li>Introduction to the shell</li> <li>Introduction to the job scheduler</li> </ul> </li> <li>CLI Cheat Sheet</li> </ul>"},{"location":"hpc-schools/#requirements","title":"Requirements","text":"<ul> <li>Having an HPC account to access the cluster. Request an account following the instructions in our system documentation.</li> </ul>"},{"location":"hpc-schools/#machine-learning-for-beginners","title":"Machine Learning for beginners","text":"<p>This two-days course introduces participants to Machine Learning (ML) and Deep Learning (DL) on HPC. During the course, we will cover the fundamentals of ML and DL, work through practical exercises on model training, and explore how to speed up computations using HPC resources, distributed computing, and GPU acceleration. The course combines theory, coding exercises, and HPC applications to give participants both a solid foundation and practical skills.</p> <p>Limited spots available per session (20 max).</p>"},{"location":"hpc-schools/#upcoming-sessions_3","title":"Upcoming sessions","text":"<ul> <li>Date: 4<sup>th</sup> and 5<sup>th</sup> of June 2025</li> <li>Time: 9am to 5pm (both days)</li> <li>Location: MNO 1.040 and 1.050, Belval Campus</li> </ul>"},{"location":"hpc-schools/#training-outcomes","title":"Training outcomes","text":"<p>By the end of the course, participants will:</p> <ul> <li>Understand key ML and DL concepts and techniques;</li> <li>Gain hands-on experience with data preprocessing, model training, and evaluation;</li> <li>Learn how to use HPC resources for accelerated ML workloads;</li> <li>Explore distributed computing and GPU acceleration tools;</li> </ul>"},{"location":"hpc-schools/#course-structure","title":"Course structure","text":""},{"location":"hpc-schools/#day-1-ml-foundations","title":"Day 1 - ML Foundations","text":"<p>Location: MNO 1.050, Belval Campus</p> <ul> <li>Introduction to ML - AI &amp; ML, types of ML, key concepts;</li> <li>Exploratory Data Analysis (EDA) in Jupyter Notebook - Loading, preprocessing, and visualizing;</li> <li>Supervised Learning - Regression vs. Classification, model evaluation, hands-on exercises;</li> <li>Introduction to Neural Networks.</li> </ul>"},{"location":"hpc-schools/#day-2-dl-hpc-acceleration","title":"Day 2 - DL &amp; HPC Acceleration","text":"<p>Location: MNO 1.040, Belval Campus</p> <ul> <li>DL &amp; CNNs - Building and training DL models;</li> <li>Distributed computing on HPC;</li> <li>Accelerated ML &amp; DL.</li> </ul>"},{"location":"hpc-schools/#requirements_1","title":"Requirements","text":"<ul> <li>Having an HPC account to access the cluster.</li> <li>Basic knowledge on SLURM (beginners HPC school).</li> <li>A basic understanding of Python programming.</li> <li>Familiarity with Jupyter Notebook (installed and configured).</li> <li>A basic understanding of Numpy and linear algebra.</li> </ul>"},{"location":"hpc-schools/#python-hpc-school","title":"Python HPC School","text":"<p>In this workshop, we will explore the process of improving Python code for efficient execution. Chances are, you 're already familiar with Python and Numpy. However, we will start by mastering profiling and efficient NumPy usage as these are crucial steps before venturing into parallelization. Once your code is fine-tuned with Numpy we will explore the utilization of Python's parallel libraries to unlock the potential of using multiple CPU cores. By the end, you will be well equipped to harness Python's potential for high-performance tasks on the HPC infrastructure. </p>"},{"location":"hpc-schools/#target-audience-description","title":"Target Audience Description","text":"<p>The workshop is designed for individuals who are interested in advancing their skills and knowledge in Python-based scientific and data computing. The ideal participants would typically possess basic to intermediate Python and Numpy skills, along with some familiarity with parallel programming. This workshop will give a good starting point to leverage the usage of the HPC computing power to speed up your Python programs. </p>"},{"location":"hpc-schools/#upcoming-sessions_4","title":"Upcoming sessions","text":"<p>No sessions are planned at the moment. Future sessions will be announced here, please wait for announcements or contact the HPC team via email to express your interest.</p>"},{"location":"hpc-schools/#first-day-jupyter-notebook-on-ulhpc-profiling-efficient-usage-of-numpy","title":"First day \u2013 Jupyter notebook on ULHPC / profiling efficient usage of Numpy","text":""},{"location":"hpc-schools/#program","title":"Program","text":"<ul> <li>Setting up a Jupyter notebook on an HPC node - 10am to 11am</li> <li>Taking time and profiling python code - 11am to 12pm</li> <li>Lunch break - 12pm to 2pm</li> <li>Numpy basics for replacing python loops for efficient computations - 2pm to 4pm</li> </ul>"},{"location":"hpc-schools/#requirements_2","title":"Requirements","text":"<ul> <li>Having an HPC account to access the cluster. </li> <li>Basic knowledge on SLURM (beginners HPC school). </li> <li>A basic understanding of Python programming. </li> <li>Familiarity with Jupyter Notebook (installed and configured). </li> <li>A basic understanding of Numpy and linear algebra. </li> </ul>"},{"location":"hpc-schools/#second-day-improving-performance-with-python-parallel-packages","title":"Second day \u2013 Improving performance with python parallel packages","text":""},{"location":"hpc-schools/#program_1","title":"Program","text":"<ul> <li>Use case understanding and Python implementation - 10am to 10:30am</li> <li>Numpy implementation - 10:30am to 11am</li> <li>Python\u2019s Multiprocessing - 11am to 12pm</li> <li>Lunch break - 12pm to 2pm</li> <li>PyMP - 2pm to 2:30pm</li> <li>Cython - 2:30pm to 3pm</li> <li>Numba and final remarks- 3pm to 4pm</li> </ul>"},{"location":"hpc-schools/#requirements_3","title":"Requirements","text":"<ul> <li>Having an HPC account to access the cluster.</li> <li>Basic knowledge on SLURM (beginners HPC school). </li> <li>A basic understanding of Python programming. </li> <li>Familiarity with Jupyter Notebook (installed and configured). </li> <li>A basic understanding of Numpy and linear algebra. </li> <li>Familiarity with parallel programming. </li> </ul>"},{"location":"hpc-schools/#conda-environment-management-for-python-and-r","title":"Conda environment management for Python and R","text":"<p>The creation of Conda environments is supported in the University of Luxembourg HPC systems. But when Conda environments are needed and what tools are available to create Conda environments? Attend this tutorial if your projects involve R or Python and you need support with installing packages.</p> <p>The topics that will be covered include:</p> <ul> <li>how to install packages using the facilities available in R and Python,</li> <li>how to document and exchange environment setups,</li> <li>when a Conda environment is required for a project, and</li> <li>what tools are available for the creation of Conda environments.</li> </ul>"},{"location":"hpc-schools/#upcoming-sessions_5","title":"Upcoming sessions","text":"<p>No sessions are planned at the moment. Future sessions will be announced here, please wait for announcements or contact the HPC team via email to express your interest.</p>"},{"location":"hpc-schools/#introduction-to-numerical-methods-with-blas","title":"Introduction to numerical methods with BLAS","text":"<p>This seminar covers basic principles of numerical library usage with BLAS as an example. The library mechanisms for organizing software are studied in detail, covering topics such as the differences between static and dynamic libraries. The practical sessions will demonstrate the generation of library files from source code, and how programs can use library functions.</p> <p>After an overview of software libraries, the BLAS library is presented, including the available operations and the organization of the code. The attendees will have the opportunity to use functions of BLAS in a few practical examples. The effects of caches in numerical library performance are then studied in detail. In the practical sessions the attendees will have the opportunity to try cache aware programming techniques that better exploit the performance of the available hardware.</p> <p>Overall in this seminar you learn how to:</p> <ul> <li>compile libraries from source code,</li> <li>compile and link code that uses numerical libraries,</li> <li>understand the effects of caches in numerical library performance, and</li> <li>exploit caches to leverage better performance.</li> </ul>"},{"location":"hpc-schools/#upcoming-sessions_6","title":"Upcoming sessions","text":"<p>No sessions are planned at the moment. Future sessions will be announced here, please wait for announcements or contact the HPC team via email to express your interest.</p>"},{"location":"hpc-schools/#an-overview-of-hpc-systems-and-applications","title":"An overview of HPC systems and applications","text":"<p>This introductory presentation for HPC users with previous computing experience. It's a quite condensed course with minimal practical sections. The topics covered are</p> <ul> <li>the architecture of HPC systems,</li> <li>methods to extract architectural information about nodes,</li> <li>advanced scheduler directives such as process pinning and job dependencies,</li> <li>software distribution and containerization,</li> <li>message passing programming with MPI,</li> <li>shared memory programming with OpenMP, and</li> <li>optimizing programs execution for specific architectures.</li> </ul>"},{"location":"hpc-schools/#upcoming-sessions_7","title":"Upcoming sessions","text":"<p>The course is given only at special events.</p>"},{"location":"hpc-schools/#resources_1","title":"Resources","text":"<ul> <li>An overview of HPC systems and applications</li> </ul>"},{"location":"layout/","title":"Layout","text":"<p>This repository is organized as follows (use <code>tree -L 2</code> to complete):</p> <pre><code>.\n\u251c\u2500\u2500 Makefile        # GNU Make configuration\n\u251c\u2500\u2500 README.md       # Project README\n\u251c\u2500\u2500 VERSION         # /!\\ DO NOT EDIT. Current repository version\n\u251c\u2500\u2500 docs/           # [MkDocs](mkdocs.org) main directory\n\u251c\u2500\u2500 mkdocs.yml      # [MkDocs](mkdocs.org) configuration\n\u251c\u2500\u2500 .envrc           # Local direnv configuration -- see https://direnv.net/\n\u2502                    # Assumes you have installed in ~/.config/direnv/direnvrc\n\u2502                    # the version proposed on\n\u2502                    #    https://raw.githubusercontent.com/Falkor/dotfiles/master/direnv/direnvrc\n\u251c\u2500\u2500 .python-{version,virtualenv}  # Pyenv/Virtualenv configuration\n</code></pre>"},{"location":"setup/","title":"Pre-Requisites and Laptop Setup","text":"<p>You should follow the instructions provided on the ULHPC Tutorials: Pre-requisites page. </p> <p>For those not familiar with Linux Shell, kindly refer to the \"Introducing the UNIX/Linux Shell\" tutorial. </p>"},{"location":"teaching-with-the-ulhpc/","title":"Teaching with the ULHPC","text":"<p>If you plan to use the ULHPC to teach for groups of students, we highly recommend that you contact us (the HPC team) for the following reasons:</p> <ul> <li>When possible, we can plan our maintenance sessions outside of your planned teaching / training dates.</li> <li>We can help with the reservation of HPC ressources (e.g., GPU or big memory nodes) as some are highly booked and may not be available on-demand the day of your teaching or training session.</li> <li>We can provide temporary ULHPC account for your students / attendees.</li> </ul>"},{"location":"teaching-with-the-ulhpc/#resource-reservation","title":"Resource reservation","text":"<p>The ULHPC offers different types of computing nodes and their availability can vary greatly throughout the year. In particular, GPU and big memory nodes are rare and intensively used. If you plan to use them for a teaching session, please contact our team at hpc-team@uni.lu.</p>"},{"location":"teaching-with-the-ulhpc/#temporary-student-accounts","title":"Temporary student accounts","text":"<p>For hands-on sessions involving students or trainees who don't necessarily have an ULHPC account, we can provide temporary accesses. As a teacher / trainer, your account will also have access to all the students / trainees accounts to simplify interactions and troubleshooting during your sessions. </p> <p>Please contact our team at hpc-team@uni.lu to help you in the preparation of your teaching / training session.</p>"},{"location":"accounts/","title":"Get an Account","text":"<p>In order to use the ULHPC facilities, you need to have a user account with an associated user login name (also called username) placed under an account hierarchy.</p>"},{"location":"accounts/#conditions-of-acceptance","title":"Conditions of acceptance","text":""},{"location":"accounts/#acceptable-use-policy-aup","title":"Acceptable Use Policy (AUP)","text":"<p>There are a number of policies which apply to ULHPC users.</p> <p> UL HPC Acceptable Use Policy (AUP) [pdf] </p> <p>Important</p> <p>All users of UL HPC resources and PIs must abide by the UL HPC Acceptable Use Policy (AUP). You should read and keep a signed copy of this document before using the facility.</p> <p>Access and/or usage of any ULHPC system assumes the tacit acknowledgement to this policy.</p> <p>Remember that you are expected to acknowledge ULHPC in your publications. See Acceptable Use Policy for more details.</p> ULHPC Platforms are meant ONLY for R&amp;D! <p>The ULHPC facility is made for Research and Development and it is NOT a full production computing center -- for such needs, consider using the National HPC center.</p> <p>In particular, we cannot make any guarantees of cluster availability or timely job completion even if we target a minimum compute node availability above 95% which is typically met - for instance, past KPI statistics in 2019 report a computing node availability above 97%.</p>"},{"location":"accounts/#resource-allocation-policies","title":"Resource allocation policies","text":"<p> ULHPC Usage Charging and Resource allocation policy</p>"},{"location":"accounts/#ul-internal-rd-and-training","title":"UL internal R&amp;D and training","text":"<p>ULHPC resources are free of charge for UL staff for their internal work and training activities. Principal Investigators (PI) will nevertheless receive on a regular basis a usage report of their team activities on the UL HPC platform. The corresponding accumulated price will be provided even if this amount is purely indicative and won't be charged back.</p> <p>Any other activities will be reviewed with the rectorate and are a priori subjected to be billed.</p>"},{"location":"accounts/#research-projects","title":"Research Projects","text":""},{"location":"accounts/#ulhpc-usage-charging-policy","title":"ULHPC Usage Charging Policy","text":"<p>The advertised prices are for internal partners only</p> <p>The price list and all other information of this page are meant for internal partners, i.e., not for external companies.  If you are not an internal partner, please contact us at hpc-partnership@uni.lu. Alternatively, you can contact LuxProvide, the national HPC center which aims at serving the private sector for HPC needs.</p>"},{"location":"accounts/#how-to-estimate-hpc-costs-for-projects","title":"How to estimate HPC costs for projects?","text":"<p>You can use the following excel document to estimate the cost of your HPC usage:</p> <p> UL HPC Cost Estimates for Project Proposals [xlsx] </p> <p>Note that there are two sheets offering two ways to estimate based on your specific situation. Please read the red sections to ensure that you are using the correct estimation sheet.</p> <p>Note that even if you plan for large-scale experiments on PRACE/EuroHPC supercomputers through computing credits granted by Call for Proposals for Project Access, you should plan for ULHPC costs since you will have to demonstrate the scalability of your code -- the University's facility is ideal for that. You can contact hpc-partnership@uni.lu for more details about this.</p>"},{"location":"accounts/#hpc-price-list-2022-10-01","title":"HPC price list - 2022-10-01","text":"<p>Note that ULHPC price list has been updated, see below.</p>"},{"location":"accounts/#compute","title":"Compute","text":"Compute type Description \u20ac (excl. VAT) / node-hour CPU - small 28 cores, 128 GB RAM 0.25\u20ac CPU - regular 128 cores, 256 GB RAM 1.25\u20ac CPU - big mem 112 cores, 3 TB RAM 6.00\u20ac GPU 4 V100, 28 cores, 768 GB RAM 5.00\u20ac <p>The prices above correspond to a full-node cost. However, jobs can use a fraction of a node and the price of the job will be computed based on that fraction. Please find below the core-hour / GPU-hour costs and how we compute how much to charge:</p> Compute type Unit \u20ac (excl. VAT) CPU - small Core-hour 0.0089\u20ac CPU - regular Core-hour 0.0097\u20ac CPU - big mem Core-hour 0.0535\u20ac GPU GPU-hour 1.25\u20ac <p>For CPU nodes, the fraction correspond to the number of requested cores, e.g. 64 cores on a CPU - regular node corresponds to 50% of the available cores and thus will be charged 50% of 1.25\u20ac. </p> <p>Regarding the RAM of a job, if you do not override the default behaviour, you will receive a percentage of the RAM corresponding to the amount of requested cores, e.g, 128G of RAM for the 64 cores example from above (50% of a CPU - regular node). If you override the default behaviour and request more RAM, we will re-compute the equivalent number of cores, e.g. if you request 256G of RAM and 64 cores, we will charge 128 cores.</p> <p>For GPU nodes, the fraction considers the number of GPUs. There are 4 GPUs, 28 cores and 768G of RAM on one machine. This means that for each GPU, you can have up to 7 cores and 192G of RAM. If you request more than those default, we will re-compute the GPU equivalent, e.g. if you request 1 GPU and 8 cores, we will charge 2 GPUs.</p>"},{"location":"accounts/#storage","title":"Storage","text":"Storage type \u20ac (excl. VAT) / GB / Month Additional information Home Free 500 GB Project 0.02\u20ac 1 TB free Scratch Free 10 TB <p>Note that for project storage, we charge the quota and not the used storage.</p>"},{"location":"accounts/#hpc-resource-allocation-for-ul-internal-rd-and-training","title":"HPC Resource allocation for UL internal R&amp;D and training","text":"<p>ULHPC resources are free of charge for UL staff for their internal work and training activities. Principal Investigators (PI) will nevertheless receive on a regular basis a usage report of their team activities on the UL HPC platform. The corresponding accumulated price will be provided even if this amount is purely indicative and won't be charged back.</p> <p>Any other activities will be reviewed with the rectorate and are a priori subjected to be billed.</p>"},{"location":"accounts/#submit-project-related-jobs","title":"Submit project related jobs","text":"<p>To allow the ULHPC team to keep track of the jobs related to a project, use the <code>-A &lt;projectname&gt;</code> flag in Slurm, either in the Slurm directives preamble of your script, e.g.,</p> <pre><code>#SBATCH -A myproject\n</code></pre> <p>or on the command line when you submit your job, e.g., <code>sbatch -A myproject /path/to/launcher.sh</code></p>"},{"location":"accounts/#externals-and-private-partners","title":"Externals and private partners","text":""},{"location":"accounts/#ulhpc-usage-charging-policy_1","title":"ULHPC Usage Charging Policy","text":"<p>The advertised prices are for internal partners only</p> <p>The price list and all other information of this page are meant for internal partners, i.e., not for external companies.  If you are not an internal partner, please contact us at hpc-partnership@uni.lu. Alternatively, you can contact LuxProvide, the national HPC center which aims at serving the private sector for HPC needs.</p>"},{"location":"accounts/#how-to-estimate-hpc-costs-for-projects_1","title":"How to estimate HPC costs for projects?","text":"<p>You can use the following excel document to estimate the cost of your HPC usage:</p> <p> UL HPC Cost Estimates for Project Proposals [xlsx] </p> <p>Note that there are two sheets offering two ways to estimate based on your specific situation. Please read the red sections to ensure that you are using the correct estimation sheet.</p> <p>Note that even if you plan for large-scale experiments on PRACE/EuroHPC supercomputers through computing credits granted by Call for Proposals for Project Access, you should plan for ULHPC costs since you will have to demonstrate the scalability of your code -- the University's facility is ideal for that. You can contact hpc-partnership@uni.lu for more details about this.</p>"},{"location":"accounts/#hpc-price-list-2022-10-01_1","title":"HPC price list - 2022-10-01","text":"<p>Note that ULHPC price list has been updated, see below.</p>"},{"location":"accounts/#compute_1","title":"Compute","text":"Compute type Description \u20ac (excl. VAT) / node-hour CPU - small 28 cores, 128 GB RAM 0.25\u20ac CPU - regular 128 cores, 256 GB RAM 1.25\u20ac CPU - big mem 112 cores, 3 TB RAM 6.00\u20ac GPU 4 V100, 28 cores, 768 GB RAM 5.00\u20ac <p>The prices above correspond to a full-node cost. However, jobs can use a fraction of a node and the price of the job will be computed based on that fraction. Please find below the core-hour / GPU-hour costs and how we compute how much to charge:</p> Compute type Unit \u20ac (excl. VAT) CPU - small Core-hour 0.0089\u20ac CPU - regular Core-hour 0.0097\u20ac CPU - big mem Core-hour 0.0535\u20ac GPU GPU-hour 1.25\u20ac <p>For CPU nodes, the fraction correspond to the number of requested cores, e.g. 64 cores on a CPU - regular node corresponds to 50% of the available cores and thus will be charged 50% of 1.25\u20ac. </p> <p>Regarding the RAM of a job, if you do not override the default behaviour, you will receive a percentage of the RAM corresponding to the amount of requested cores, e.g, 128G of RAM for the 64 cores example from above (50% of a CPU - regular node). If you override the default behaviour and request more RAM, we will re-compute the equivalent number of cores, e.g. if you request 256G of RAM and 64 cores, we will charge 128 cores.</p> <p>For GPU nodes, the fraction considers the number of GPUs. There are 4 GPUs, 28 cores and 768G of RAM on one machine. This means that for each GPU, you can have up to 7 cores and 192G of RAM. If you request more than those default, we will re-compute the GPU equivalent, e.g. if you request 1 GPU and 8 cores, we will charge 2 GPUs.</p>"},{"location":"accounts/#storage_1","title":"Storage","text":"Storage type \u20ac (excl. VAT) / GB / Month Additional information Home Free 500 GB Project 0.02\u20ac 1 TB free Scratch Free 10 TB <p>Note that for project storage, we charge the quota and not the used storage.</p>"},{"location":"accounts/#hpc-resource-allocation-for-ul-internal-rd-and-training_1","title":"HPC Resource allocation for UL internal R&amp;D and training","text":"<p>ULHPC resources are free of charge for UL staff for their internal work and training activities. Principal Investigators (PI) will nevertheless receive on a regular basis a usage report of their team activities on the UL HPC platform. The corresponding accumulated price will be provided even if this amount is purely indicative and won't be charged back.</p> <p>Any other activities will be reviewed with the rectorate and are a priori subjected to be billed.</p>"},{"location":"accounts/#submit-project-related-jobs_1","title":"Submit project related jobs","text":"<p>To allow the ULHPC team to keep track of the jobs related to a project, use the <code>-A &lt;projectname&gt;</code> flag in Slurm, either in the Slurm directives preamble of your script, e.g.,</p> <pre><code>#SBATCH -A myproject\n</code></pre> <p>or on the command line when you submit your job, e.g., <code>sbatch -A myproject /path/to/launcher.sh</code></p>"},{"location":"accounts/#how-to-get-a-new-user-account","title":"How to Get a New User account?","text":"<p> Account Request Form</p> <ol> <li>University staff - you can submit a request for a new ULHPC account by using the ServiceNow portal (Research &gt; HPC &gt; User access &amp; accounts &gt; New HPC account request). Students - submit your account request on the Student Service Portal. Externals - a University staff member must request the account for you, using the section New HPC account for external. Enter the professional data (organization and institutional email address). Specify the line manager / project PI if needed.</li> <li>If you need to access a specific project directory, ask the project directory owner to open a ticket using the section Add user within project.</li> <li>Your account will undergo user checks, in accordance with ULHPC policies, to verify your identity and the information proposed. Under some circumstances, there could be a delay while this vetting takes place.</li> <li>After vetting has completed, you will receive a welcome email with your login information, and a link to the HPC Identity Management Portal where you can setup your password and a Multi-Factor Authentication (MFA) method for your account.<ul> <li>Your new password must adhere to ULHPC's password requirements<ul> <li>see  Password policy and guidelines</li> </ul> </li> <li>ULHPC Identity Management Portal documentation</li> </ul> </li> </ol> UL HPC credentials are not University credentials <p>Be aware that the source of authentication for the HPC services based on RedHat IdM/IPA DIFFERS from the University credentials (based on UL Active Directory).</p> <ul> <li>ULHPC credentials are maintained by the HPC team; associated portal: https://hpc-account.uni.lu<ul> <li>authentication service for: UL HPC</li> </ul> </li> <li>University credentials are maintained by the IT team of the University<ul> <li>authentication service for Service Now and all other UL services</li> </ul> </li> </ul>"},{"location":"accounts/#managing-user-accounts","title":"Managing User Accounts","text":"<p>ULHPC user accounts are managed in through the HPC Identity Management Portal.</p>"},{"location":"accounts/#security-incidents","title":"Security Incidents","text":"<p>If you think there has been a computer security incident, you should contact the ULHPC Team and the University CISO team as soon as possible:</p> <p>To: hpc-team@uni.lu,iso@uni.lu</p> <p>Subject: Security Incident for HPC account '<code>&lt;login&gt;</code>' (ADAPT accordingly)</p> <p>Please save any evidence of the break-in and include as many details as possible in your communication with us.</p>"},{"location":"accounts/#how-to-get-a-new-project-account","title":"How to Get a New Project account?","text":"<p>Projects are defined for accounting purposes and are associated to a set of user accounts allowed by the project PI to access its data and submit jobs on behalf of the project account. See Slurm Account Hierarchy.</p> <p>You can request (or be automatically added) to project accounts for accounting purposes. For more information, please see the Project Account documentation</p>"},{"location":"accounts/#faq","title":"FAQ","text":""},{"location":"accounts/#can-i-share-an-account-account-security-policies","title":"Can I share an account? \u2013 Account Security Policies","text":"<p>Danger</p> <p>The sharing of passwords or login credentials is not allowed under UL HPC and University information security policies. Please bear in mind that this policy also protects the end-user.</p> <p>Sharing credentials removes the ability to audit and accountability for the account holder in case of account misuse. Accounts which are in violation of this policy may be disabled or otherwise limited. Accounts knowingly skirting this policy may be banned.</p> <p>If you find that you need to share resources among multiple individuals, shared projects are just the way to go, and remember that the University extends access to its HPC resources (i.e., facility and expert HPC consultants) to the scientific staff of national public organizations and external partners for the duration of joint research projects under the conditions defined above.</p> <p>When in doubt, please contact us and we will be happy to assist you with finding a safe and secure way to do so.</p>"},{"location":"accounts/collaboration_accounts/","title":"Collaboration Accounts","text":"<p>All ULHPC login accounts are associated with specific individuals and must not be shared. In some HPC centers, you may be able to request Collaboration Accounts designed to handle the following use cases:</p> <ul> <li>Collaborative Data Management:   Large scale experimental and simulation data are typically read or written by multiple collaborators and are kept on disk for long periods.</li> <li>Collaborative Software Management</li> <li>Collaborative Job Management</li> </ul> <p>Info</p> <p>By default, we DO NOT provide Collaboration Accounts and encourage the usage of shared research projects <code>&lt;name&gt;</code> stored on the Global project directory to enable the group members to manipulate project data with the appropriate use of unix groups and file permissions.</p> <p>For dedicated job billing and accounting purposes, you should also request the creation of a project account (this will be done for all accepted funded projects).</p> <p>For more details, see Project Accounts documentation.</p> <p>We are aware nevertheless that a problem that often arises is that the files are owned by the collaborator who did the work and if that collaborator changes roles the default unix file permissions usually are such that the files cannot be managed (deleted) by other members of the collaboration and system administrators must be contacted. Similarly, for some use cases, Collaboration Accounts would enable members of the team to manipulate jobs submitted by other team members as necessary. Justified and argued use cases can be submitted to the HPC team to find the appropriate solution by opening a ticket on the HPC Helpdesk Portal.</p>"},{"location":"accounts/projects/","title":"Projects Accounts","text":""},{"location":"accounts/projects/#shared-project-in-the-global-project-directory","title":"Shared project in the Global project directory.","text":"<p>We can setup for you a dedicated project directory on the GPFS/SpectrumScale Filesystem for sharing research data with other colleagues.</p> <p>Whether to create a new project directory or to add/remove members to the group set to access the project data, use the Service Now HPC Support Portal.</p> <p> Service Now HPC Support Portal</p>"},{"location":"accounts/projects/#data-storage-charging","title":"Data Storage Charging","text":""},{"location":"accounts/projects/#ulhpc-usage-charging-policy","title":"ULHPC Usage Charging Policy","text":"<p>The advertised prices are for internal partners only</p> <p>The price list and all other information of this page are meant for internal partners, i.e., not for external companies.  If you are not an internal partner, please contact us at hpc-partnership@uni.lu. Alternatively, you can contact LuxProvide, the national HPC center which aims at serving the private sector for HPC needs.</p>"},{"location":"accounts/projects/#how-to-estimate-hpc-costs-for-projects","title":"How to estimate HPC costs for projects?","text":"<p>You can use the following excel document to estimate the cost of your HPC usage:</p> <p> UL HPC Cost Estimates for Project Proposals [xlsx] </p> <p>Note that there are two sheets offering two ways to estimate based on your specific situation. Please read the red sections to ensure that you are using the correct estimation sheet.</p> <p>Note that even if you plan for large-scale experiments on PRACE/EuroHPC supercomputers through computing credits granted by Call for Proposals for Project Access, you should plan for ULHPC costs since you will have to demonstrate the scalability of your code -- the University's facility is ideal for that. You can contact hpc-partnership@uni.lu for more details about this.</p>"},{"location":"accounts/projects/#hpc-price-list-2022-10-01","title":"HPC price list - 2022-10-01","text":"<p>Note that ULHPC price list has been updated, see below.</p>"},{"location":"accounts/projects/#compute","title":"Compute","text":"Compute type Description \u20ac (excl. VAT) / node-hour CPU - small 28 cores, 128 GB RAM 0.25\u20ac CPU - regular 128 cores, 256 GB RAM 1.25\u20ac CPU - big mem 112 cores, 3 TB RAM 6.00\u20ac GPU 4 V100, 28 cores, 768 GB RAM 5.00\u20ac <p>The prices above correspond to a full-node cost. However, jobs can use a fraction of a node and the price of the job will be computed based on that fraction. Please find below the core-hour / GPU-hour costs and how we compute how much to charge:</p> Compute type Unit \u20ac (excl. VAT) CPU - small Core-hour 0.0089\u20ac CPU - regular Core-hour 0.0097\u20ac CPU - big mem Core-hour 0.0535\u20ac GPU GPU-hour 1.25\u20ac <p>For CPU nodes, the fraction correspond to the number of requested cores, e.g. 64 cores on a CPU - regular node corresponds to 50% of the available cores and thus will be charged 50% of 1.25\u20ac. </p> <p>Regarding the RAM of a job, if you do not override the default behaviour, you will receive a percentage of the RAM corresponding to the amount of requested cores, e.g, 128G of RAM for the 64 cores example from above (50% of a CPU - regular node). If you override the default behaviour and request more RAM, we will re-compute the equivalent number of cores, e.g. if you request 256G of RAM and 64 cores, we will charge 128 cores.</p> <p>For GPU nodes, the fraction considers the number of GPUs. There are 4 GPUs, 28 cores and 768G of RAM on one machine. This means that for each GPU, you can have up to 7 cores and 192G of RAM. If you request more than those default, we will re-compute the GPU equivalent, e.g. if you request 1 GPU and 8 cores, we will charge 2 GPUs.</p>"},{"location":"accounts/projects/#storage","title":"Storage","text":"Storage type \u20ac (excl. VAT) / GB / Month Additional information Home Free 500 GB Project 0.02\u20ac 1 TB free Scratch Free 10 TB <p>Note that for project storage, we charge the quota and not the used storage.</p>"},{"location":"accounts/projects/#hpc-resource-allocation-for-ul-internal-rd-and-training","title":"HPC Resource allocation for UL internal R&amp;D and training","text":"<p>ULHPC resources are free of charge for UL staff for their internal work and training activities. Principal Investigators (PI) will nevertheless receive on a regular basis a usage report of their team activities on the UL HPC platform. The corresponding accumulated price will be provided even if this amount is purely indicative and won't be charged back.</p> <p>Any other activities will be reviewed with the rectorate and are a priori subjected to be billed.</p>"},{"location":"accounts/projects/#submit-project-related-jobs","title":"Submit project related jobs","text":"<p>To allow the ULHPC team to keep track of the jobs related to a project, use the <code>-A &lt;projectname&gt;</code> flag in Slurm, either in the Slurm directives preamble of your script, e.g.,</p> <pre><code>#SBATCH -A myproject\n</code></pre> <p>or on the command line when you submit your job, e.g., <code>sbatch -A myproject /path/to/launcher.sh</code></p>"},{"location":"accounts/projects/#slurm-project-account","title":"Slurm Project Account","text":"<p>As explained in the Slurm Account Hierarchy, projects account can be created at the L3 level of the association tree.</p> <p>To quickly list a given project accounts and the users attached to it, you can use the <code>sassoc</code> helper function:</p> <pre><code># /!\\ ADAPT project acronym/name &lt;name&gt;accordingly\nsassoc project_&lt;name&gt;\n</code></pre> <p>Alternatively, you can rely on <code>sacctmgr</code>, typically coupled with the <code>withassoc</code> attribute:</p> <pre><code># /!\\ ADAPT project acronym/name &lt;name&gt;accordingly\nsacctmgr show account where name=project_&lt;name&gt; format=\"account%20,user%20,Share,QOS%50\" withassoc\n</code></pre> <p>As per HPC Resource Allocations for Research Project, creation of such project accounts is mandatory for funded research projects, since usage charging may occur when a detailed reporting will be provided for auditing purposes.</p> <p>With the help of the University Research Support department, we will create automatically project accounts from the list of accepted project which acknowledge the need of computing resources. Feel free nevertheless to use the Service Now HPC Support Portal to request the creation of a new project account or to add/remove members to the group - this might be pertinent for internal research projects or specific collaboration with external partners requiring a separate usage monitoring.</p> <p>Important</p> <p>Project account is a natural way to access the higher priority QOS not granted by default to your personal account on the ULHPC. For instance, the <code>high</code> QOS is automatically granted as soon as a contribution to the HPC budget line is performed by the project.</p>"},{"location":"aws/connection/","title":"Connection to the AWS Cluster","text":""},{"location":"aws/connection/#access-to-the-frontend","title":"Access to the frontend","text":"<p>The ULHPC team will create specific access for the AWS Cluster and send to all project members a ssh key in order to connect to the cluster frontend.</p> <p>Once your account has been enabled, you can connect to the cluster using ssh. Computers based on Linux or Mac usually have ssh installed by default. To create a direct connection, use the command below (using your specific cluster name if it differs from workshop-cluster).</p> <p><pre><code>ssh -i id_rsa username@ec2-52-5-167-162.compute-1.amazonaws.com \n</code></pre> This will open a direct, non-graphical connection in the terminal. To exit the remote terminal session, use the standard Linux command \u201cexit\u201d.</p> <p>Alternatively, you may want to save the configuration of this connection (and create an alias for it) by editing the file <code>~/.ssh/config</code> (create it if it does not already exist) and adding the following entries:</p> <pre><code>Host aws-ulhpc-access\n  User username\n  Hostname ec2-52-5-167-162.compute-1.amazonaws.com \n  IdentityFile ~/.ssh/id_rsa\n  IdentitiesOnly yes\n</code></pre> <p>For additionnal information about ssh connection, please refer to the following page.</p> <p>Data storage</p> <ul> <li>HOME storage is limited to 500GB for all users.</li> <li>The ULHPC team will also create for you a project directory located at <code>/shared/projects/&lt;project_id&gt;</code>. All members of the project will have the possibility to read, write and execute only in this directory.</li> <li>We strongly advise you to use the project directory to store data and install softwares. </li> </ul>"},{"location":"aws/overview/","title":"Context &amp; System Overview","text":""},{"location":"aws/overview/#context","title":"Context","text":"<p>The University of Luxembourg announced a collaboration with Amazon Web Services (AWS) to deploy Amazon Elastic Compute Cloud (Amazon EC2) cloud computing infrastructure in order to accelerate strategic high-performance computing (HPC) research and development in Europe.</p> <p>University of Luxembourg will be among the first European universities to provide research and development communities with access to compute environments that use an architecture similar to the European Processor Initiative (EPI), which will be the basis for Europe\u2019s future exascale computing architecture.</p> <p>Using Amazon EC2 instances powered by AWS Graviton3, the University of Luxembourg will make simulation capacity available to University researchers. This autumn, research projects will be selected from proposals submitted by University R&amp;D teams.</p> <p>As part of this project, AWS will provide cloud computing services to the University that will support the development, design, and testing of numerical codes (i.e., codes that use only digits, such as binary), which traditionally demands a lot of compute power. This will give researchers an accessible, easy-to-use, end-to-end environment in which they can validate their simulation codes on ARM64 architectures, including servers, personal computers, and Internet of Things (IoT).</p> <p>After initial project selection by a steering committee that includes representatives from the University of Luxembourg and AWS, additional projects will be selected each quarter. Selections will be based on the University\u2019s outlined research goals. Priority will be given to research carried out by the University of Luxembourg and its interdisciplinary research centers; however, based on available capacity and project qualifications, the initiative could extend to European industrial partners.</p>"},{"location":"aws/overview/#system-description-and-environment","title":"System description and environment","text":"<p>The AWS Parallel Cluster based on the new HPC-based Graviton3 instances (all instances and storage located in US-EAST-1) will provide cloud computing services to Uni.lu that will support the development, design, and testing of numerical codes, which traditionally demands a lot of compute power. This will give researchers an accessible, easy-to-use, end-to-end environment in which they can validate their simulation codes on ARM64 architectures, including servers, personal computers, and Internet of Things (IoT). The cluster will consist in two main partitions and jobs will be submitted using the Slurm scheduler :</p> <p></p> <p>PIs and their teams of the funded projects under this call will have the possibility to compile their code with the Arm compiler and using the Arm Performance Library(APL). Support will be provided by the ULHPC team as well as training activities.</p>"},{"location":"aws/setup/","title":"Environment Setup","text":"<p>AWS suggest to use Spack to setup your software environment. There is no hard requirement that you must use Spack. However we have included it here, as it is a quick, simple way to setup a development environment. The official ULHPC swsets are not available on the AWS cluster. If you prefer to use EasyBuild or manually compile softwares, please refer to the ULHPC software documentation for this purpose.</p>"},{"location":"aws/setup/#environment-modules-and-lmod","title":"Environment modules and LMod","text":"<p>Like the ULHPC facility, the AWS cluster  relies on the Environment Modules / LMod framework which provided the <code>module</code> utility on Compute nodes to manage nearly all software. There are two main advantages of the <code>module</code> approach:</p> <ol> <li>ULHPC can provide many different versions and/or installations of a    single software package on a given machine, including a default    version as well as several older and newer version.</li> <li>Users can easily switch to different versions or installations    without having to explicitly specify different paths. With modules,    the <code>PATH</code> and related environment variables (<code>LD_LIBRARY_PATH</code>, <code>MANPATH</code>, etc.) are automatically managed.</li> </ol> <p>Environment Modules in itself are a standard and well-established technology across HPC sites, to permit developing and using complex software and libraries build with dependencies, allowing multiple versions of software stacks and combinations thereof to co-exist. It brings the <code>module</code> command which is used to manage environment variables such as <code>PATH</code>, <code>LD_LIBRARY_PATH</code> and <code>MANPATH</code>, enabling the easy loading and unloading of application/library profiles and their dependencies.</p> <p>See https://hpc-docs.uni.lu/environment/modules/ for more details</p> Command Description <code>module avail</code> Lists all the modules which are available to be loaded <code>module spider &lt;pattern&gt;</code> Search for  among available modules (Lmod only) <code>module load &lt;mod1&gt; [mod2...]</code> Load a module <code>module unload &lt;module&gt;</code> Unload a module <code>module list</code> List loaded modules <code>module purge</code> Unload all modules (purge) <code>module display &lt;module&gt;</code> Display what a module does <code>module use &lt;path&gt;</code> Prepend the directory to the MODULEPATH environment variable <code>module unuse &lt;path&gt;</code> Remove the directory from the MODULEPATH environment variable <p>At the heart of environment modules interaction resides the following components:</p> <ul> <li>the <code>MODULEPATH</code> environment variable, which defines the list of searched directories for modulefiles</li> <li><code>modulefile</code></li> </ul> <p>Take a look at the current values:</p> <p><pre><code>$ echo $MODULEPATH\n/shared/apps/easybuild/modules/all:/usr/share/Modules/modulefiles:/etc/modulefiles:/usr/share/modulefiles/Linux:/usr/share/modulefiles/Core:/usr/share/lmod/lmod/modulefiles/Core\n$ module show toolchain/foss\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n   /shared/apps/easybuild/modules/all/toolchain/foss/2022b.lua:\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nhelp([[\nDescription\n===========\nGNU Compiler Collection (GCC) based compiler toolchain, including\n OpenMPI for MPI support, OpenBLAS (BLAS and LAPACK support), FFTW and ScaLAPACK.\n\n\nMore information\n================\n - Homepage: https://easybuild.readthedocs.io/en/master/Common-toolchains.html#foss-toolchain\n]])\nwhatis(\"Description: GNU Compiler Collection (GCC) based compiler toolchain, including\n OpenMPI for MPI support, OpenBLAS (BLAS and LAPACK support), FFTW and ScaLAPACK.\")\nwhatis(\"Homepage: https://easybuild.readthedocs.io/en/master/Common-toolchains.html#foss-toolchain\")\nwhatis(\"URL: https://easybuild.readthedocs.io/en/master/Common-toolchains.html#foss-toolchain\")\nconflict(\"toolchain/foss\")\nload(\"compiler/GCC/12.2.0\")\nload(\"mpi/OpenMPI/4.1.4-GCC-12.2.0\")\nload(\"lib/FlexiBLAS/3.2.1-GCC-12.2.0\")\nload(\"numlib/FFTW/3.3.10-GCC-12.2.0\")\nload(\"numlib/FFTW.MPI/3.3.10-gompi-2022b\")\nload(\"numlib/ScaLAPACK/2.2.0-gompi-2022b-fb\")\nsetenv(\"EBROOTFOSS\",\"/shared/apps/easybuild/software/foss/2022b\")\nsetenv(\"EBVERSIONFOSS\",\"2022b\")\nsetenv(\"EBDEVELFOSS\",\"/shared/apps/easybuild/software/foss/2022b/easybuild/toolchain-foss-2022b-easybuild-devel\")\n</code></pre> Now you can search for a given software using <code>module spider &lt;pattern&gt;</code>:</p> <pre><code>$  module spider lang/Python\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  lang/Python:\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    Description:\n      Python is a programming language that lets you work more quickly and integrate your systems more effectively.\n\n     Versions:\n        lang/Python/3.10.8-GCCcore-12.2.0-bare\n        lang/Python/3.10.8-GCCcore-12.2.0\n\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  For detailed information about a specific \"lang/Python\" module (including how to load the modules) use the module's full name.\n  For example:\n\n     $ module spider lang/Python/3.10.8-GCCcore-12.2.0\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n</code></pre> <p>Let's see the effect of loading/unloading a module</p> <pre><code>$ module list\nNo modules loaded\n$ which python\n/usr/bin/python\n$ python --version       # System level python\nPython 2.7.18\n\n$ module load lang/Python    # use TAB to auto-complete\n$ which python\n/shared/apps/easybuild/software/Python/3.10.8-GCCcore-12.2.0/bin/python\n$ python --version\nPython 3.10.8\n$ module purge\n</code></pre>"},{"location":"aws/setup/#installing-softwares-with-easybuild","title":"Installing softwares with Easybuild","text":"<p>EasyBuild is a tool that allows to perform automated and reproducible compilation and installation of software. A large number of scientific software are supported (2995 supported software packages in the last release 4.8.0) -- see also What is EasyBuild?</p> <p>All builds and installations are performed at user level, so you don't need the admin (i.e. <code>root</code>) rights. The software are installed in your home directory under <code>$EASYBUILD_PREFIX</code> -- see https://hpc-docs.uni.lu/environment/easybuild/</p> Default setting (local) Recommended setting <code>$EASYBUILD_PREFIX</code> <code>$HOME/.local/easybuild</code> <code>/shared/apps/easybuild/</code> <ul> <li>built software are placed under <code>${EASYBUILD_PREFIX}/software/</code></li> <li>modules install path <code>${EASYBUILD_PREFIX}/modules/all</code></li> </ul>"},{"location":"aws/setup/#easybuild-main-concepts","title":"Easybuild main concepts","text":"<p>See also the official Easybuild Tutorial: \"Maintaining a Modern Scientific Software Stack Made Easy with EasyBuild\"</p> <p>EasyBuild relies on two main concepts: Toolchains and EasyConfig files.</p> <p>A toolchain corresponds to a compiler and a set of libraries which are commonly used to build a software. The two main toolchains frequently used on the UL HPC platform are the <code>foss</code> (\"Free and Open Source Software\") and the <code>intel</code> one.</p> <ol> <li><code>foss</code>  is based on the GCC compiler and on open-source libraries (OpenMPI, OpenBLAS, etc.).</li> <li><code>intel</code> is based on the Intel compiler and on Intel libraries (Intel MPI, Intel Math Kernel Library, etc.).</li> </ol> <p>An EasyConfig file is a simple text file that describes the build process of a software. For most software that uses standard procedures (like <code>configure</code>, <code>make</code> and <code>make install</code>), this file is very simple. Many EasyConfig files are already provided with EasyBuild. By default, EasyConfig files and generated modules are named using the following convention: <code>&lt;Software-Name&gt;-&lt;Software-Version&gt;-&lt;Toolchain-Name&gt;-&lt;Toolchain-Version&gt;</code>. However, we use a hierarchical approach where the software are classified under a category (or class) -- see  the <code>CategorizedModuleNamingScheme</code> option for the <code>EASYBUILD_MODULE_NAMING_SCHEME</code> environmental variable), meaning that the layout will respect the following hierarchy: <code>&lt;Software-Class&gt;/&lt;Software-Name&gt;/&lt;Software-Version&gt;-&lt;Toolchain-Name&gt;-&lt;Toolchain-Version&gt;</code></p> <p>Additional details are available on EasyBuild website:</p> <ul> <li>EasyBuild homepage</li> <li>EasyBuild documentation</li> <li>What is EasyBuild?</li> <li>Toolchains</li> <li>EasyConfig files</li> <li>List of supported software packages</li> </ul> <p>Easybuild is provided to you as a software module to complement the existing software set. </p> <pre><code>module load tools/EasyBuild\n</code></pre> <p>In case you cant to install the latest version yourself, please follow the official instructions. Nonetheless, we strongly recommand to use the provided module. Don't forget to setup your local Easybuild configuration first.</p> <p>What is important for the installation of Easybuild are the following variables:</p> <ul> <li><code>EASYBUILD_PREFIX</code>: where to install local modules and software, i.e. <code>$HOME/.local/easybuild</code></li> <li><code>EASYBUILD_MODULES_TOOL</code>: the type of modules tool you are using, i.e. <code>LMod</code> in this case</li> <li><code>EASYBUILD_MODULE_NAMING_SCHEME</code>: the way the software and modules should be organized (flat view or hierarchical) -- we're advising on <code>CategorizedModuleNamingScheme</code></li> </ul> <p>Important</p> <ul> <li>Recall that you should be on a compute node to install Easybuild (otherwise the checks of the <code>module</code> command availability will fail.)</li> </ul>"},{"location":"aws/setup/#install-a-missing-software-by-complementing-the-software-set","title":"Install a missing software by complementing the software set","text":"<p>The current software set contains the toolchain foss-2022b that is necessary to build other softwares. We have build OpenMPI-4.1.4 to take in account the latest AWS EFA and the slurm integration. In order to install missing softwares for your project, you can complement the existing software set located at <code>/shared/apps/easybuild</code> by using the provided EasyBuild module (latest version).  Once Easybuild has been loaded, you can search and install new softwares. By default, these new softwares will be installed at <code>${HOME}/.local/easybuild</code>. Feel free to adapt the environment variable <code>${EASYBUILD_PREFIX}</code> to select a new installation directory. </p> <p>Let's try to install a missing software</p> <pre><code>(heanode)$ srun -p small -N 1 -n1 -c16  --pty bash -i\n(node)$ module spider HPL   # HPL is a software package that solves a (random) dense linear system in double precision (64 bits)\nLmod has detected the following error:  Unable to find: \"HPL\".\n(node)$ module load tools/EasyBuild\n# Search for recipes for the missing software\n(node)$ eb -S HPL\n== found valid index for /shared/apps/easybuild/software/EasyBuild/4.8.0/easybuild/easyconfigs, so using it...\nCFGS1=/shared/apps/easybuild/software/EasyBuild/4.8.0/easybuild/easyconfigs\n * $CFGS1/b/bashplotlib/bashplotlib-0.6.5-GCCcore-10.3.0.eb\n * $CFGS1/h/HPL/HPL-2.1-foss-2016.04.eb\n * $CFGS1/h/HPL/HPL-2.1-foss-2016.06.eb\n * $CFGS1/h/HPL/HPL-2.1-foss-2016a.eb\n * $CFGS1/h/HPL/HPL-2.1-foss-2016b.eb\n ...\n * $CFGS1/h/HPL/HPL-2.3-foss-2022a.eb\n * $CFGS1/h/HPL/HPL-2.3-foss-2022b.eb\n * $CFGS1/h/HPL/HPL-2.3-foss-2023a.eb\n ...\n * $CFGS1/h/HPL/HPL-2.3-intel-2022b.eb\n * $CFGS1/h/HPL/HPL-2.3-intel-2023.03.eb\n * $CFGS1/h/HPL/HPL-2.3-intel-2023a.eb\n * $CFGS1/h/HPL/HPL-2.3-intelcuda-2019b.eb\n * $CFGS1/h/HPL/HPL-2.3-intelcuda-2020a.eb\n * $CFGS1/h/HPL/HPL-2.3-iomkl-2019.01.eb\n * $CFGS1/h/HPL/HPL-2.3-iomkl-2021a.eb\n * $CFGS1/h/HPL/HPL-2.3-iomkl-2021b.eb\n * $CFGS1/h/HPL/HPL_parallel-make.patch\n</code></pre> <p>From this list, you should select the version matching the target toolchain version -- here foss-2022b.</p> <p>Once you pick a given recipy, install it with</p> <pre><code>   eb &lt;name&gt;.eb [-D] -r\n</code></pre> <ul> <li><code>-D</code> enables the dry-run mode to check what's going to be install -- ALWAYS try it first</li> <li><code>-r</code> enables the robot mode to automatically install all dependencies while searching for easyconfigs in a set of pre-defined directories -- you can also prepend new directories to search for eb files (like the current directory <code>$PWD</code>) using the option and syntax <code>--robot-paths=$PWD:</code> (do not forget the ':'). See Controlling the robot search path documentation</li> <li>The <code>$CFGS&lt;n&gt;/</code> prefix should be dropped unless you know what you're doing (and thus have previously defined the variable -- see the first output of the <code>eb -S [...]</code> command).</li> </ul> <p>Let's try to review the missing dependencies from a dry-run :</p> <p><pre><code># Select the one matching the target software set version\n(node)$ eb HPL-2.3-foss-2022b.eb -Dr   # Dry-run\n== Temporary log file in case of crash /tmp/eb-lzv785be/easybuild-ihga94y0.log\n== found valid index for /shared/apps/easybuild/software/EasyBuild/4.8.0/easybuild/easyconfigs, so using it...\n== found valid index for /shared/apps/easybuild/software/EasyBuild/4.8.0/easybuild/easyconfigs, so using it...\nDry run: printing build status of easyconfigs and dependencies\nCFGS=/shared/apps/easybuild/software/EasyBuild/4.8.0/easybuild/easyconfigs\n * [x] $CFGS/m/M4/M4-1.4.19.eb (module: devel/M4/1.4.19)\n * [x] $CFGS/b/Bison/Bison-3.8.2.eb (module: lang/Bison/3.8.2)\n * [x] $CFGS/f/flex/flex-2.6.4.eb (module: lang/flex/2.6.4)\n * [x] $CFGS/z/zlib/zlib-1.2.12.eb (module: lib/zlib/1.2.12)\n * [x] $CFGS/b/binutils/binutils-2.39.eb (module: tools/binutils/2.39)\n * [x] $CFGS/g/GCCcore/GCCcore-12.2.0.eb (module: compiler/GCCcore/12.2.0)\n * [x] $CFGS/z/zlib/zlib-1.2.12-GCCcore-12.2.0.eb (module: lib/zlib/1.2.12-GCCcore-12.2.0)\n * [x] $CFGS/h/help2man/help2man-1.49.2-GCCcore-12.2.0.eb (module: tools/help2man/1.49.2-GCCcore-12.2.0)\n * [x] $CFGS/m/M4/M4-1.4.19-GCCcore-12.2.0.eb (module: devel/M4/1.4.19-GCCcore-12.2.0)\n * [x] $CFGS/b/Bison/Bison-3.8.2-GCCcore-12.2.0.eb (module: lang/Bison/3.8.2-GCCcore-12.2.0)\n * [x] $CFGS/f/flex/flex-2.6.4-GCCcore-12.2.0.eb (module: lang/flex/2.6.4-GCCcore-12.2.0)\n * [x] $CFGS/b/binutils/binutils-2.39-GCCcore-12.2.0.eb (module: tools/binutils/2.39-GCCcore-12.2.0)\n * [x] $CFGS/p/pkgconf/pkgconf-1.9.3-GCCcore-12.2.0.eb (module: devel/pkgconf/1.9.3-GCCcore-12.2.0)\n * [x] $CFGS/g/groff/groff-1.22.4-GCCcore-12.2.0.eb (module: tools/groff/1.22.4-GCCcore-12.2.0)\n * [x] $CFGS/n/ncurses/ncurses-6.3-GCCcore-12.2.0.eb (module: devel/ncurses/6.3-GCCcore-12.2.0)\n * [x] $CFGS/e/expat/expat-2.4.9-GCCcore-12.2.0.eb (module: tools/expat/2.4.9-GCCcore-12.2.0)\n * [x] $CFGS/b/bzip2/bzip2-1.0.8-GCCcore-12.2.0.eb (module: tools/bzip2/1.0.8-GCCcore-12.2.0)\n * [x] $CFGS/g/GCC/GCC-12.2.0.eb (module: compiler/GCC/12.2.0)\n * [x] $CFGS/f/FFTW/FFTW-3.3.10-GCC-12.2.0.eb (module: numlib/FFTW/3.3.10-GCC-12.2.0)\n * [x] $CFGS/u/UnZip/UnZip-6.0-GCCcore-12.2.0.eb (module: tools/UnZip/6.0-GCCcore-12.2.0)\n * [x] $CFGS/l/libreadline/libreadline-8.2-GCCcore-12.2.0.eb (module: lib/libreadline/8.2-GCCcore-12.2.0)\n * [x] $CFGS/l/libtool/libtool-2.4.7-GCCcore-12.2.0.eb (module: lib/libtool/2.4.7-GCCcore-12.2.0)\n * [x] $CFGS/m/make/make-4.3-GCCcore-12.2.0.eb (module: devel/make/4.3-GCCcore-12.2.0)\n * [x] $CFGS/t/Tcl/Tcl-8.6.12-GCCcore-12.2.0.eb (module: lang/Tcl/8.6.12-GCCcore-12.2.0)\n * [x] $CFGS/p/pkgconf/pkgconf-1.8.0.eb (module: devel/pkgconf/1.8.0)\n * [x] $CFGS/s/SQLite/SQLite-3.39.4-GCCcore-12.2.0.eb (module: devel/SQLite/3.39.4-GCCcore-12.2.0)\n * [x] $CFGS/o/OpenSSL/OpenSSL-1.1.eb (module: system/OpenSSL/1.1)\n * [x] $CFGS/l/libevent/libevent-2.1.12-GCCcore-12.2.0.eb (module: lib/libevent/2.1.12-GCCcore-12.2.0)\n * [x] $CFGS/c/cURL/cURL-7.86.0-GCCcore-12.2.0.eb (module: tools/cURL/7.86.0-GCCcore-12.2.0)\n * [x] $CFGS/d/DB/DB-18.1.40-GCCcore-12.2.0.eb (module: tools/DB/18.1.40-GCCcore-12.2.0)\n * [x] $CFGS/p/Perl/Perl-5.36.0-GCCcore-12.2.0.eb (module: lang/Perl/5.36.0-GCCcore-12.2.0)\n * [x] $CFGS/a/Autoconf/Autoconf-2.71-GCCcore-12.2.0.eb (module: devel/Autoconf/2.71-GCCcore-12.2.0)\n * [x] $CFGS/a/Automake/Automake-1.16.5-GCCcore-12.2.0.eb (module: devel/Automake/1.16.5-GCCcore-12.2.0)\n * [x] $CFGS/a/Autotools/Autotools-20220317-GCCcore-12.2.0.eb (module: devel/Autotools/20220317-GCCcore-12.2.0)\n * [x] $CFGS/n/numactl/numactl-2.0.16-GCCcore-12.2.0.eb (module: tools/numactl/2.0.16-GCCcore-12.2.0)\n * [x] $CFGS/u/UCX/UCX-1.13.1-GCCcore-12.2.0.eb (module: lib/UCX/1.13.1-GCCcore-12.2.0)\n * [x] $CFGS/l/libfabric/libfabric-1.16.1-GCCcore-12.2.0.eb (module: lib/libfabric/1.16.1-GCCcore-12.2.0)\n * [x] $CFGS/l/libffi/libffi-3.4.4-GCCcore-12.2.0.eb (module: lib/libffi/3.4.4-GCCcore-12.2.0)\n * [x] $CFGS/x/xorg-macros/xorg-macros-1.19.3-GCCcore-12.2.0.eb (module: devel/xorg-macros/1.19.3-GCCcore-12.2.0)\n * [x] $CFGS/l/libpciaccess/libpciaccess-0.17-GCCcore-12.2.0.eb (module: system/libpciaccess/0.17-GCCcore-12.2.0)\n * [x] $CFGS/u/UCC/UCC-1.1.0-GCCcore-12.2.0.eb (module: lib/UCC/1.1.0-GCCcore-12.2.0)\n * [x] $CFGS/n/ncurses/ncurses-6.3.eb (module: devel/ncurses/6.3)\n * [x] $CFGS/g/gettext/gettext-0.21.1.eb (module: tools/gettext/0.21.1)\n * [x] $CFGS/x/XZ/XZ-5.2.7-GCCcore-12.2.0.eb (module: tools/XZ/5.2.7-GCCcore-12.2.0)\n * [x] $CFGS/p/Python/Python-3.10.8-GCCcore-12.2.0-bare.eb (module: lang/Python/3.10.8-GCCcore-12.2.0-bare)\n * [x] $CFGS/b/BLIS/BLIS-0.9.0-GCC-12.2.0.eb (module: numlib/BLIS/0.9.0-GCC-12.2.0)\n * [x] $CFGS/o/OpenBLAS/OpenBLAS-0.3.21-GCC-12.2.0.eb (module: numlib/OpenBLAS/0.3.21-GCC-12.2.0)\n * [x] $CFGS/l/libarchive/libarchive-3.6.1-GCCcore-12.2.0.eb (module: tools/libarchive/3.6.1-GCCcore-12.2.0)\n * [x] $CFGS/l/libxml2/libxml2-2.10.3-GCCcore-12.2.0.eb (module: lib/libxml2/2.10.3-GCCcore-12.2.0)\n * [x] $CFGS/c/CMake/CMake-3.24.3-GCCcore-12.2.0.eb (module: devel/CMake/3.24.3-GCCcore-12.2.0)\n * [ ] $CFGS/h/hwloc/hwloc-2.8.0-GCCcore-12.2.0.eb (module: system/hwloc/2.8.0-GCCcore-12.2.0)\n * [ ] $CFGS/p/PMIx/PMIx-4.2.2-GCCcore-12.2.0.eb (module: lib/PMIx/4.2.2-GCCcore-12.2.0)\n * [x] $CFGS/o/OpenMPI/OpenMPI-4.1.4-GCC-12.2.0.eb (module: mpi/OpenMPI/4.1.4-GCC-12.2.0)\n * [x] $CFGS/f/FlexiBLAS/FlexiBLAS-3.2.1-GCC-12.2.0.eb (module: lib/FlexiBLAS/3.2.1-GCC-12.2.0)\n * [x] $CFGS/g/gompi/gompi-2022b.eb (module: toolchain/gompi/2022b)\n * [x] $CFGS/f/FFTW.MPI/FFTW.MPI-3.3.10-gompi-2022b.eb (module: numlib/FFTW.MPI/3.3.10-gompi-2022b)\n * [x] $CFGS/s/ScaLAPACK/ScaLAPACK-2.2.0-gompi-2022b-fb.eb (module: numlib/ScaLAPACK/2.2.0-gompi-2022b-fb)\n * [x] $CFGS/f/foss/foss-2022b.eb (module: toolchain/foss/2022b)\n * [ ] $CFGS/h/HPL/HPL-2.3-foss-2022b.eb (module: tools/HPL/2.3-foss-2022b)\n== Temporary log file(s) /tmp/eb-lzv785be/easybuild-ihga94y0.log* have been removed.\n== Temporary directory /tmp/eb-lzv785be has been removed.\n</code></pre> Let's try to install it (remove the <code>-D</code>):</p> <p><pre><code># Select the one matching the target software set version\n(node)$ eb HPL-2.3-foss-2022b.eb -r\n</code></pre> From now on, you should be able to see the new module.</p> <pre><code>(node)$  module spider HPL\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  tools/HPL: tools/HPL/2.3-foss-2022b\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    Description:\n      HPL is a software package that solves a (random) dense linear system in double precision (64 bits) arithmetic on distributed-memory computers. It can thus be regarded as a portable as well as freely available implementation of the High Performance Computing Linpack Benchmark.\n\n\n    This module can be loaded directly: module load tools/HPL/2.3-foss-2022b\n\n    Help:\n\n      Description\n      ===========\n      HPL is a software package that solves a (random) dense linear system in double precision (64 bits)\n       arithmetic on distributed-memory computers. It can thus be regarded as a portable as well as freely available\n       implementation of the High Performance Computing Linpack Benchmark.\n\n\n      More information\n      ================\n       - Homepage: https://www.netlib.org/benchmark/hpl/\n</code></pre> <p>Tips: When you load a module <code>&lt;NAME&gt;</code> generated by Easybuild, it is installed within the directory reported by the <code>$EBROOT&lt;NAME&gt;</code> variable. In the above case, you will find the generated binary in <code>${EBROOTHPL}/</code>.</p>"},{"location":"aws/setup/#installing-softwares-with-spack","title":"Installing softwares with Spack","text":"<ul> <li> <p>To do this, please clone the\u00a0Spack GitHub repository\u00a0into a\u00a0<code>SPACK_ROOT</code>\u00a0which is defined to be on a your project directory, i.e., <code>/shared/project/&lt;project_id&gt;</code>.</p> </li> <li> <p>Then add the configuration to you <code>~/.bashrc</code> file.</p> </li> <li> <p>You may wish to change the location of the<code>SPACK_ROOT</code>\u00a0to fit your specific cluster configuration.</p> </li> <li> <p>Here, we consider the release v0.19 of Spack from the\u00a0releases/v0.19\u00a0branch, however, you may wish to checkout the\u00a0develop\u00a0branch for the latest packages.</p> </li> </ul> <pre><code>git clone -c feature.manyFiles=true -b releases/v0.19 https://github.com/spack/spack $SPACK_ROOT\n</code></pre> <ul> <li>Then, add the following lines in your .bashrc</li> </ul> <pre><code>export PROJECT=\"/shared/projects/&lt;project_id&gt;\"\nexport SPACK_ROOT=\"${PROJECT}/spack\"\nif [[ -f \"${SPACK_ROOT}/share/spack/setup-env.sh\" &amp;&amp; -n ${SLURM_JOB_ID} ]];then\n    source ${SPACK_ROOT}/share/spack/setup-env.sh\" \nfi\n</code></pre> <p>Adapt accordingly</p> <ul> <li>Do NOT forget to replace <code>&lt;project_id&gt;</code> with your project name</li> </ul>"},{"location":"aws/setup/#spack-binary-cache","title":"Spack Binary Cache","text":"<p>At ISC'22, in conjunction with the Spack v0.18 release, AWS announced a collaborative effort to\u00a0host a Binary Cache\u00a0. The binary cache stores prebuilt versions of common HPC packages, meaning that the installation process is reduced to relocation rather than compilation. To increase flexibility the binary cache contains package builds with different variants and built with different compilers. The purpose of the binary cache is to drastically speed up package installation, especially when long dependency chains exist.</p> <p>The binary cache is periodically updated with the latest versions of packages, and is released in conjunction with Spack releases. Thus you can use the\u00a0v0.18\u00a0binary cache to have packages specifically from that Spack release. Alternatively, you can make use of the\u00a0develop\u00a0binary cache, which is kept up to date with the Spack\u00a0develop\u00a0branch.</p> <ul> <li>To add the develop binary cache, and trusting the associated gpg keys:</li> </ul> <pre><code>spack mirror add binary_mirror https://binaries.spack.io/develop\nspack buildcache keys -it\n</code></pre>"},{"location":"aws/setup/#installing-packages","title":"Installing packages","text":"<p>The notation for installing packages, when the binary cache has been enabled is unchanged. Spack will first check to see if the package is installable from the binary cache, and only upon failure will it install from source. We see confirmation of this in the output:</p> <pre><code>$ spack install bzip2\n==&gt; Installing bzip2-1.0.8-paghlsmxrq7p26qna6ml6au4fj2bdw6k\n==&gt; Fetching https://binaries.spack.io/develop/build_cache/linux-amzn2-x86_64_v4-gcc-7.3.1-bzip2-1.0.8-paghlsmxrq7p26qna6ml6au4fj2bdw6k.spec.json.sig\ngpg: Signature made Fri 01 Jul 2022 04:21:22 AM UTC using RSA key ID 3DB0C723\ngpg: Good signature from \"Spack Project Official Binaries &lt;maintainers@spack.io&gt;\"\n==&gt; Fetching https://binaries.spack.io/develop/build_cache/linux-amzn2-x86_64_v4/gcc-7.3.1/bzip2-1.0.8/linux-amzn2-x86_64_v4-gcc-7.3.1-bzip2-1.0.8-paghlsmxrq7p26qna6ml6au4fj2bdw6k.spack\n==&gt; Extracting bzip2-1.0.8-paghlsmxrq7p26qna6ml6au4fj2bdw6k from binary cache\n[+] /shared/spack/opt/spack/linux-amzn2-x86_64_v4/gcc-7.3.1/bzip2-1.0.8-paghlsmxrq7p26qna6ml6au4fj2bdw6k\n</code></pre>"},{"location":"aws/setup/#bypassing-the-binary-cache","title":"Bypassing the binary cache","text":"<ul> <li> <p>Sometimes we might want to install a specific package from source, and bypass the binary cache. To achieve this we can pass the\u00a0<code>--no-cache</code>\u00a0flag to the install command. We can use this notation to install cowsay. <pre><code>spack install --no-cache cowsay\n</code></pre></p> </li> <li> <p>To compile any software we are going to need a compiler. Out of the box Spack does not know about any compilers on the system. To list your registered compilers, please use the following command: <pre><code>spack compiler list\n</code></pre></p> </li> </ul> <p>It will return an empty list the first time you used after installing Spack <pre><code> ==&gt; No compilers available. Run `spack compiler find` to autodetect compilers\n</code></pre></p> <ul> <li>AWS ParallelCluster installs GCC by default, so you can ask Spack to discover compilers on the system: <pre><code>spack compiler find\n</code></pre></li> </ul> <p>This should identify your GCC install. In your case a conmpiler should be found. <pre><code>==&gt; Added 1 new compiler to /home/ec2-user/.spack/linux/compilers.yaml\n     gcc@7.3.1\n ==&gt; Compilers are defined in the following files:\n     /home/ec2-user/.spack/linux/compilers.yaml\n</code></pre></p>"},{"location":"aws/setup/#install-other-compilers","title":"Install other compilers","text":"<p>This default GCC compiler may be sufficient for many applications, we may want to install a newer version of GCC or other compilers in general. Spack is able to install compilers like any other package.</p>"},{"location":"aws/setup/#newer-gcc-version","title":"Newer GCC version","text":"<p>For example we can install a version of GCC 11.2.0, complete with\u00a0binutils, and then add it to the Spack compiler list. ```\u00b7bash spack install -j [num cores] gcc@11.2.0+binutils spack load gcc@11.2.0 spack compiler find spack unload <pre><code>As Spack is building GCC and all of the dependency packages this install can take a long time (&gt;30 mins).\n\n## Arm Compiler for Linux\n\nThe Arm Compiler for Linux (ACfL) can be installed by Spack on Arm systems, like the Graviton2 (C6g) or Graviton3 (C7g).o\n```bash\nspack install arm@22.0.1\nspack load arm@22.0.1\nspack compiler find\nspack unload\n</code></pre></p>"},{"location":"aws/setup/#where-to-build-softwares","title":"Where to build softwares","text":"<p>The cluster has quite a small headnode, this means that the compilation of complex software is prohibited. One simple solution is to use the compute nodes to perform the Spack installations, by submitting the command through Slurm. <pre><code>srun -N1 -c 36 spack install -j36 gcc@11.2.0+binutils\n</code></pre></p>"},{"location":"aws/setup/#aws-environment","title":"AWS Environment","text":"<ul> <li> <p>The versions of these external packages may change and are included for reference.</p> </li> <li> <p>The Cluster comes pre-installed with\u00a0Slurm\u00a0,\u00a0libfabric\u00a0,\u00a0PMIx\u00a0,\u00a0Intel MPI\u00a0, and\u00a0Open MPI\u00a0. To use these packages, you need to tell\u00a0spack where to find them. <pre><code>cat &lt;&lt; EOF &gt; $SPACK_ROOT/etc/spack/packages.yaml\npackages:\n    libfabric:\n        variants: fabrics=efa,tcp,udp,sockets,verbs,shm,mrail,rxd,rxm\n        externals:\n        - spec: libfabric@1.13.2 fabrics=efa,tcp,udp,sockets,verbs,shm,mrail,rxd,rxm\n          prefix: /opt/amazon/efa\n        buildable: False\n    openmpi:\n        variants: fabrics=ofi +legacylaunchers schedulers=slurm ^libfabric\n        externals:\n        - spec: openmpi@4.1.1 %gcc@7.3.1\n          prefix: /opt/amazon/openmpi\n        buildable: False\n    pmix:\n        externals:\n          - spec: pmix@3.2.3 ~pmi_backwards_compatibility\n            prefix: /opt/pmix\n        buildable: False\n    slurm:\n        variants: +pmix sysconfdir=/opt/slurm/etc\n        externals:\n        - spec: slurm@21.08.8-2 +pmix sysconfdir=/opt/slurm/etc\n          prefix: /opt/slurm\n        buildable: False\n    armpl:\n        externals:\n        - spec: armpl@21.0.0%gcc@9.3.0\n          prefix: /opt/arm/armpl/21.0.0/armpl_21.0_gcc-9.3/\n        buildable: False\nEOF\n</code></pre></p> </li> </ul>"},{"location":"aws/setup/#add-the-gcc-93-compiler","title":"Add the GCC 9.3 Compiler","text":"<p>The Graviton image ships with an additional compiler within the ArmPL project. We can add this compiler to the Spack environment with the following command: <code>spack compiler add /opt/arm/armpl/gcc/9.3.0/bin/</code></p>"},{"location":"aws/setup/#open-mpi","title":"Open MPI","text":"<p>For Open MPI we have already made the definition to set\u00a0libfabric\u00a0as a dependency of Open MPI. So by default it will configure it correctly. <pre><code>spack install openmpi%gcc@11.2.0\n</code></pre></p>"},{"location":"aws/setup/#additional-resources","title":"Additional resources","text":"<ul> <li>Job submission relies on the Slurm scheduler. Please refer to the following page for more details.</li> <li>Spack tutorial on AWS ParallelCluster</li> </ul>"},{"location":"connect/access/","title":"Login Nodes","text":"<p>Opening an SSH connection to ULHPC systems results in a connection to an access node.</p> IrisAionIris (X11)Aion (X11) <pre><code>ssh iris-cluster\n</code></pre> <pre><code>ssh aion-cluster\n</code></pre> <p>To be able to further run GUI applications within your [interactive] jobs: <pre><code>ssh -X iris-cluster   # OR on Mac OS: ssh -Y iris-cluster\n</code></pre></p> <p>To be able to further run GUI applications within your [interactive] jobs: <pre><code>ssh -X aion-cluster   # OR on Mac OS: ssh -Y aion-cluster\n</code></pre></p> <p>Important</p> <p>Recall that you SHOULD NOT run any HPC application on the login nodes.</p> <p>That's why the <code>module</code> command is NOT available on them.</p>"},{"location":"connect/access/#usage","title":"Usage","text":"<p>On access nodes, typical user tasks include</p> <ul> <li>Transferring and managing files</li> <li>Editing files</li> <li>Submitting jobs</li> </ul> <p>Appropriate Use</p> <p>Do not run compute- or memory-intensive applications on access nodes. These nodes are a shared resource. ULHPC admins may terminate processes which are having negative impacts on other users or the systems.</p> <p>Avoid <code>watch</code></p> <p>If you must use the <code>watch</code> command, please use a much longer interval such as 5 minutes (=300 sec), e.g., <code>watch -n 300 &lt;your_command&gt;</code>.</p> <p>Avoid <code>Visual Studio Code</code></p> <p>Avoid using <code>Visual Studio Code</code> to connect to the HPC, as it consumes a lot of resources in the login nodes. Heavy development shouldn't be done directly on the HPC. For most tasks using a terminal based editor should be enough like: <code>Vim</code> or <code>Emacs</code>. If you want to have some more advanced features try <code>Neovim</code> where you can add plugins to meet your specific needs.</p>"},{"location":"connect/access/#tips","title":"Tips","text":"<p>ULHPC provides a wide variety of qos's</p> <ul> <li>An <code>interactive</code> qos is available on Iris and Aion for compute- and memory-intensive interactive work. Please, use an interactive job for resource-intensive processes instead of running them on access nodes.</li> </ul> <p>Tip</p> <p>To help identify processes that make heavy use of resources, you can use:</p> <ul> <li><code>top -u $USER</code></li> <li><code>/usr/bin/time -v ./my_command</code></li> </ul> <p>Running GUI Application over X11</p> <p>If you intend to run GUI applications (MATLAB, Stata, ParaView etc.), you MUST connect by SSH to the login nodes with the <code>-X</code> (or <code>-Y</code> on Mac OS) option:</p> IrisAion <pre><code>ssh -X iris-cluster   # OR on Mac OS: ssh -Y iris-cluster\n</code></pre> <pre><code>ssh -X aion-cluster   # OR on Mac OS: ssh -Y aion-cluster\n</code></pre> <p>Install Neovim using Micormamba</p> <p>Neovim is not installed by default on the HPC but you can install it using Micromamba.</p> <p><pre><code>micromamba create --name editor-env\nmicromamba install --name editor-env conda-forge::nvim\n</code></pre> After installation you can create a alias in your <code>.bashrc</code> for easy access: <pre><code>alias nvim='micromamba run --name editor-env nvim'\n</code></pre></p>"},{"location":"connect/ipa/","title":"ULHPC Identity Management Portal (IdM)","text":"<p> ULHPC Identity Management Portal</p>"},{"location":"connect/ipa/#first-connection","title":"First connection","text":"<p>You need to setup a password to access the Identity Management Portal for the first time. Click Forgot Password? and enter either your HPC username or the email address linked to your account.</p> <p></p> <p>You will then be prompted to setup a Two Factor Authentication (2FA) method. Scan the QR code with an application of your choice such as FreeOPT, Authy, Google Authenticator, etc.</p> <p>Enter the code generated by your application in the one-time code field and name your 2FA device to help you identify it as you can register multiple 2FA devices.</p>"},{"location":"connect/ipa/#upload-your-ssh-key-on-the-ulhpc-identity-management-portal","title":"Upload your SSH key on the ULHPC Identity Management Portal","text":"<p>SSH Key Management</p> <p>You are responsible for uploading and managing your authorized public SSH keys for your account, under the terms of the Acceptable Use Policy. Be aware that the ULHPC team review on a periodical basis the compliance to the policy, as well as the security of your keys. See also the note on deprecated/weak DSA/RSA keys</p> <p>You need to upload your public SSH key(s) <code>*.pub</code> on the  ULHPC Identity Management Portal to be able to connect to the clusters. For that, connect to the ULHPC IdM portal (use the URL communicated to you by the UL HPC team in your \"welcome\" mail) and connect using your HPC username or the email address linked to your account.</p> <p>Copy the content of the key you want to add (see SSH key generation or SSH key generation on windows)</p> <pre><code># Example with ED25519 **public** key\n(laptop)$&gt; cat ~/.ssh/id_ed25519.pub\nssh-ed25519 AAAA[...]\n# OR the RSA **public** key\n(laptop)$&gt; cat ~/.ssh/id_rsa.pub\nssh-rsa AAAA[...]\n</code></pre> <p>Then on the portal:</p> <ol> <li>Go to the Personal info page</li> <li>Paste the Base64-encoded public key string</li> <li>Click on Add SSH Public Key to add more keys</li> <li>Click Save</li> <li>You should see a popup that confirms your change</li> </ol> <p>Listing SSH keys attached to your account through SSSD</p> <p>SSSD is a system daemon used on ULHPC computational resources. Its primary function is to provide access to local or remote identity and authentication resources through a common framework that can provide caching and offline support to the system. To easily access the authorized keys configured for your account from the command-line (i.e. without login on the ULHPC IPA portal), you can use: <pre><code>sss_ssh_authorizedkeys $(whoami)\n</code></pre></p>"},{"location":"connect/linux/","title":"Installation notes","text":"<p>Normally, SSH is installed natively on your machine and the <code>ssh</code> command should be accessible from the command line (or a Terminal) through the ssh command:</p> <pre><code>(your_workstation)$&gt; ssh -V\nOpenSSH_8.4p1, OpenSSL 1.1.1h  22 Sep 2020\n</code></pre> <p>If that's not the case, consider installing the package <code>openssh-client</code> (Debian-like systems) or <code>ssh</code> (Redhat-like systems).</p> <p>Your local SSH configuration is located in the  <code>~/.ssh/</code> directory and consists of:</p> <ul> <li> <p><code>~/.ssh/id_rsa.pub</code>: your SSH public key. This one is the only one SAFE to distribute.</p> </li> <li> <p><code>~/.ssh/id_rsa</code>: the associated private key. NEVER EVER TRANSMIT THIS FILE</p> </li> <li> <p>(eventually) the configuration of the SSH client <code>~/.ssh/config</code></p> </li> <li> <p><code>~/.ssh/known_hosts</code>: Contains a list of host keys for all hosts you have logged into that are not already in the system-wide list of known host keys. This permits to detect man-in-the-middle attacks.</p> </li> </ul>"},{"location":"connect/linux/#ssh-key-management","title":"SSH Key Management","text":"<p>To generate an SSH keys, just use the <code>ssh-keygen</code> command, typically as follows:</p> <pre><code>(your_workstation)$&gt; ssh-keygen -t rsa -b 4096\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/user/.ssh/id_rsa):\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nYour identification has been saved in /home/user/.ssh/id_rsa.\nYour public key has been saved in /home/user/.ssh/id_rsa.pub.\nThe key fingerprint is:\nfe:e8:26:df:38:49:3a:99:d7:85:4e:c3:85:c8:24:5b username@yourworkstation\nThe key's randomart image is:\n+---[RSA 4096]----+\n|                 |\n|      . E        |\n|       * . .     |\n|      . o . .    |\n|        S. o     |\n|       .. = .    |\n|       =.= o     |\n|      * ==o      |\n|       B=.o      |\n+-----------------+\n</code></pre> <p>Warning</p> <p>To ensure the security of the platform and your data stored on it, you must protect your SSH keys with a passphrase! Additionally, your private key and passphrase should never be transmitted to anybody.</p> <p>After the execution of <code>ssh-keygen</code> command, the keys are generated and stored in the following files:</p> <ul> <li>SSH RSA Private key: <code>~/.ssh/id_rsa</code>. Again, NEVER EVER TRANSMIT THIS FILE</li> <li>SSH RSA Public key:  <code>~/.ssh/id_rsa.pub</code>.  This file is the ONLY one SAFE to distribute</li> </ul> <p>Ensure the access rights are correct on the generated keys using the ' <code>ls -l</code> ' command. The private key should be readable only by you:</p> <pre><code>(your_workstation)$&gt; ls -l ~/.ssh/id_*\n-rw------- 1 git git 751 Mar  1 20:16 /home/username/.ssh/id_rsa\n-rw-r--r-- 1 git git 603 Mar  1 20:16 /home/username/.ssh/id_rsa.pub\n</code></pre>"},{"location":"connect/linux/#configuration","title":"Configuration","text":"<p>In order to be able to login to the clusters, you will have to add this public key (i.e. <code>id_rsa.pub</code>) into your account, using the IPA user portal (use the URL communicated to you by the UL HPC team in your \"welcome\" mail).</p> <p></p> <p>The port on which the SSH servers are listening is not the default one (i.e. 22) but 8022. Consequently, if you want to connect to the Iris cluster, open a terminal and run (substituting yourlogin with the login name you received from us):</p> <pre><code>(your_workstation)$&gt; ssh -p 8022 yourlogin@access-iris.uni.lu\n</code></pre> <p>For the Aion cluster, the access server host name is <code>access-aion.uni.lu</code>:</p> <pre><code>(your_workstation)$&gt; ssh -p 8022 yourlogin@access-aion.uni.lu\n</code></pre> <p>Alternatively, you may want to save the configuration of this connection (and create an alias for it) by editing the file <code>~/.ssh/config</code> (create it if it does not already exist) and adding the following entries:</p> <pre><code>Host iris-cluster\n    Hostname access-iris.uni.lu\n\nHost aion-cluster\n    Hostname access-aion.uni.lu\n\nHost *-cluster\n    User yourlogin\n    Port 8022\n    ForwardAgent no\n</code></pre> <p>Now you'll be able to issue the following (simpler) command to connect to the cluster and obtain the welcome banner:</p> <pre><code>(your_workstation)$&gt; ssh iris-cluster\n================================================================================\n  Welcome to access1.iris-cluster.uni.lux\n  WARNING: For use by authorized users only!\n               _____      _        ____ _           _          __\n              / /_ _|_ __(_)___   / ___| |_   _ ___| |_ ___ _ _\\ \\\n             | | | || '__| / __| | |   | | | | / __| __/ _ \\ '__| |\n             | | | || |  | \\__ \\ | |___| | |_| \\__ \\ ||  __/ |  | |\n             | ||___|_|  |_|___/  \\____|_|\\__,_|___/\\__\\___|_|  | |\n              \\_\\                                              /_/\n\n=== Support and community ======================================================\n\n  - Technical Docs ........... https://hpc-docs.uni.lu\n  - Discourse / Forum ........ https://hpc-discourse.uni.lu/\n  - Helpdesk / Service Now ... https://hpc.uni.lu/support\n  - User Mailing-list ........ hpc-users@uni.lu\n\n=== HPC utilization as of 2025-02-18 15h00 =====================================\n\n  AION  batch    74.42%\n  IRIS  batch    47.94%\n  IRIS  gpu      85.42%\n  IRIS  bigmem   51.79%\n\n=== Status =====================================================================\n\n New environment modules (env/development/2023b env/release/default env/release/2020b)\n The default module currently points to 'env/release/2020b' \n      but will be updated to 2023b upon release.\n Add 'module use env/release/2020b' to your scripts to ensure that\n      they will continue working after the switch.\n\n Cluster maintenance complete (https://gitlab.com/uniluxembourg/hpc/support/infra/-/issues/29)\n - General update to latest RedHat 8.10\n - Slurm update to 23.11.10\n - General security update\n - New software set release (foss 2023a toolchain)\n\n================================================================================\nLast login: Tue Feb 18 10:40:24 2025 from 10.187.28.101\nLinux access1.iris-cluster.uni.lux 4.18.0-553.34.1.el8_10.x86_64 x86_64\n 15:09:51 up 19 days, 5 min, 41 users,  load average: 2.24, 2.55, 3.79\n[yourlogin@access2 ~]$\n</code></pre>"},{"location":"connect/linux/#activate-the-ssh-agent","title":"Activate the SSH agent","text":"<p>To be able to use your SSH key in a public-key authentication scheme, it must be loaded by an SSH agent.</p> <ul> <li> <p> Mac OS X (&gt;= 10.5), this will be handled automatically; you will be asked to fill in the passphrase on the first connection.</p> </li> <li> <p> Linux, this will be handled automatically; you will be asked to fill the passphrase on the first connection. However if you get a message similar to the following:</p> <p>(your_workstation)$&gt; ssh -vv iris-cluster [...] Agent admitted failure to sign using the key. Permission denied (publickey).</p> </li> </ul> <p>This means that you have to manually load your key in the SSH agent by running:</p> <pre><code>$&gt; ssh-add ~/.ssh/id_rsa\n</code></pre>"},{"location":"connect/linux/#ssh-resources","title":"SSH Resources","text":"<ul> <li> <p> Mac OS X: Cyberduck is a free Cocoa FTP and SFTP client.</p> </li> <li> <p> Linux: OpenSSH is available in every good linux distro, and every *BSD, and Mac OS X.</p> </li> </ul>"},{"location":"connect/linux/#ssh-advanced-tips","title":"SSH Advanced Tips","text":"<ul> <li> <p>Bash completion: The <code>bash-completion</code> package eases the ssh command usage by providing completion for hostnames and more (assuming you set the directive <code>HashKnownHost</code> to <code>no</code> in your <code>~/etc/ssh_config</code>)</p> </li> <li> <p>Forwarding a local port: You can forward a local port to a host behind a firewall.</p> </li> </ul> <p></p> <p>This is useful if you run a server on one of the cluster nodes (let's say listening on port 2222) and you want to access it via the local port 1111 on your machine. Then you'll run:</p> <pre><code>(your_workstation)$&gt; ssh iris-cluster -L 1111:iris-014:2222\n</code></pre> <ul> <li>Forwarding a remote port: You can forward a remote port back to a host protected by your firewall.</li> </ul> <p></p> <ul> <li> <p>Tunnelling for others: By using the <code>-g</code> parameter, you allow connections from other hosts than localhost to use your SSH tunnels. Be warned that anybody within your network may access the tunnelized host this way, which may be a security issue.</p> </li> <li> <p>Using OpenSSH SOCKS proxy feature (with Firefox for instance): the OpenSSH ssh client also embeds a SOCKS proxy. You may activate it by using the <code>-D</code> parameter and a value for a port (e.g. 3128), then configuring your application (Firefox for instance) to use localhost:port (i.e. \"localhost:3128\") as a SOCKS proxy. The FoxyProxy module is typically useful for that.</p> </li> </ul> <p></p> <p>One very nice feature of FoxyProxy is that you can use the host resolution on the remote server. This permits you to access your local machine within the university for instance with the same name you would use within the UL network. To summarize, that's better than the VPN proxy ;)</p> <p>Once you setup a SSH SOCKS proxy, you can also use <code>tsocks</code>, a Shell wrapper to simplify the use of the tsocks(8) library to transparently allow an application (not aware of SOCKS) to transparently use a SOCKS proxy. For instance, assuming you create a VNC server on a given remote server as follows:</p> <pre><code>(remote_server)$&gt; vncserver -geometry 1366x768\nNew 'X' desktop is remote_server:1\n\nStarting applications specified in /home/username/.vnc/xstartup\nLog file is /home/username/.vnc/remote_server:1.log\n</code></pre> <p>Then you can make the VNC client on your workstation use this tunnel to access the VNS server as follows:</p> <pre><code>(your_workstation)$&gt; tsocks vncviewer &lt;IP_of_remote_server&gt;:1\n</code></pre> <ul> <li>Escape character: use <code>~.</code> to disconnect, even if your remote command hangs.</li> </ul>"},{"location":"connect/ood/","title":"ULHPC Open On Demand (OOD) Portal","text":"<p>Open OnDemand (OOD) is a Web portal compatible with Windows, Linux and MacOS. You should login with your ULHPC credential using the URL communicated to you by the UL HPC team.</p> <p>OOD provides a convenient web access to the HPC resources and integrates</p> <ul> <li>a file management system</li> <li>a job management system (job composer, monitoring your submitted jobs, ...)</li> <li>an interactive command-line shell access</li> <li>interactive apps with graphical desktop environments</li> </ul> <p>ULHPC OOD Portal limitations</p> <p>The ULHPC OOD portal is NOT accessible outside the UniLu network. If you want to use it, you will need to setup a VPN to access the UniLu network Note: The portal is in _still under active development state: missing features and bugs can be reported to the ULHPC team via the support portal</p> <p>Live tests and demo are proposed during the ULHPC Tutorial: Preliminaries / OOD.</p> <p>Below are illustrations of OOD capabilities on the ULHPC facility.</p>"},{"location":"connect/ood/#file-management","title":"File management","text":""},{"location":"connect/ood/#job-composer-and-job-list","title":"Job composer and Job List","text":""},{"location":"connect/ood/#shell-access","title":"Shell access","text":""},{"location":"connect/ood/#interactive-sessions","title":"Interactive sessions","text":""},{"location":"connect/ood/#graphical-desktop-environment","title":"Graphical Desktop Environment","text":""},{"location":"connect/ssh/","title":"SSH","text":"<p>All ULHPC servers are reached using either the Secure Shell (SSH) communication and encryption protocol (version 2).</p> <p>Developed by SSH Communications Security Ltd., Secure Shell is a an encrypted network protocol used to log into another computer over an unsecured network, to execute commands in a remote machine, and to move files from one machine to another in a secure way. On UNIX/LINUX/BSD type systems, SSH is also the name of a suite of software applications for connecting via the SSH protocol. The SSH applications can execute commands on a remote machine and transfer files from one machine to another.  All communications are automatically and transparently encrypted, including passwords. Most versions of SSH provide login (<code>ssh</code>, <code>slogin</code>), a remote copy operation (<code>scp</code>), and many also provide a secure ftp client (<code>sftp</code>). Additionally, SSH allows secure X Window connections.</p> <p>To use SSH, you have to generate a pair of keys, one public and the other private. The public key authentication is the most secure and flexible approach to ensure a multi-purpose transparent connection to a remote server. This approach is enforced on the ULHPC platforms and assumes that the public key is known by the system in order to perform an authentication based on a challenge/response protocol instead of the classical password-based protocol.</p> <p>The way SSH handles the keys and the configuration files is illustrated in the following figure:</p> <p></p>"},{"location":"connect/ssh/#installation","title":"Installation","text":"<ul> <li>OpenSSH is natively supported on Linux / Mac OS / Unix / WSL (see below)</li> <li>On Windows, you are thus encouraged to install Windows Subsystem for Linux (WSL) and setup an Ubuntu subsystem from  Microsoft Store.<ul> <li>You probably want to also install Windows Terminal and  MobaXterm</li> <li>Better performance of your Linux subsystem can be obtained by migrating to WSL 2</li> <li>Follow the ULHPC Tutorial: Setup Pre-Requisites / Windows for detailed instructions.</li> </ul> </li> </ul>"},{"location":"connect/ssh/#ssh-key-generation","title":"SSH Key Generation","text":"<p>To generate an RSA SSH keys of 4096-bit length, just use the <code>ssh-keygen</code> command as follows:</p> <pre><code>ssh-keygen -t rsa -b 4096 -a 100\n</code></pre> <p>After the execution of this command, the generated keys are stored in the following files:</p> <ul> <li>SSH RSA Private key: <code>~/.ssh/id_rsa</code>.      NEVER EVER TRANSMIT THIS FILE</li> <li>SSH RSA Public key:  <code>~/.ssh/id_rsa.pub</code>.  This file is the ONLY one SAFE to distribute</li> </ul> <p>To passphrase or not to passphrase</p> <p>To ensure the security of your SSH key-pair on your laptop, you MUST protect your SSH keys with a passphrase! Note however that while possible, this passphrase is purely private and has a priori nothing to do with your University or your ULHPC credentials. Nevertheless, a strong passphrase follows the same recommendations as for strong passwords (for instance: see password requirements and guidelines.</p> <p>Finally, just like encryption keys, passphrases need to be kept safe and protected from unauthorised access. A Password Manager  can help you to store all your passwords safely. The University is currently not offering a university wide password manger but there are many free and paid ones you can use, for example: KeePassX, PWSafe, Dashlane, 1Password or LastPass.</p> <p>You may want to generate also ED25519 Key Pairs (which is the most recommended public-key algorithm available today) -- see explaination</p> <pre><code>ssh-keygen -t ed25519 -a 100\n</code></pre> <p>Your key pairs will be located under <code>~/.ssh/</code> and follow the following format -- the <code>.pub</code> extension indicated the public key part and is the ONLY one SAFE to distribute:</p> <pre><code>$ ls -l ~/.ssh/id_*\n-rw------- username groupname ~/.ssh/id_rsa\n-rw-r--r-- username groupname ~/.ssh/id_rsa.pub     # Public  RSA key\n-rw------- username groupname ~/.ssh/id_ed25519\n-rw-r--r-- username groupname ~/.ssh/id_ed25519.pub # Public ED25519 key\n</code></pre> <p>Ensure the access rights are correct on the generated keys using the '<code>ls -l</code>' command. In particular, the private key should be readable only by you:</p> <p>For more details, follow the ULHPC Tutorials: Preliminaries / SSH.</p> (deprecated - Windows only): SSH key management with MobaKeyGen tool <p>On Windows with MobaXterm, a tool exists and can be used to generate an SSH key pair. While not recommended (we encourage you to run WSL), here are the instructions to follow to generate these keys:</p> <ul> <li>Open the application Start &gt; Program Files &gt; MobaXterm.</li> <li>Change the default home directory for a persistent home directory instead of the default Temp directory. Go onto Settings &gt; Configuration &gt; General &gt; Persistent home directory.<ul> <li>choose a location for your home directory.<ul> <li>your local SSH configuration will be located under <code>HOME/.ssh/</code></li> </ul> </li> </ul> </li> <li>Go onto Tools &gt; Network &gt; MobaKeyGen (SSH key generator).<ul> <li>Choose RSA as the type of key to generate and change \"Number of bits in a generated key\" to 4096.</li> <li>Click on the Generate button. Move your mouse to generate some randomness.</li> <li>Select a strong passphrase in the Key passphrase field for your key.</li> </ul> </li> <li>Save the public and private keys as respectively <code>id_rsa.pub</code> and <code>id_rsa.ppk</code>.<ul> <li>Please keep a copy of the public key, you will have to add this public key into your account, using the IPA user portal (use the URL communicated to you by the UL HPC team in your \"welcome\" mail).</li> </ul> </li> </ul> <p></p> (deprecated - Windows only): SSH key management with PuTTY <p>While no longer recommended, you may still want to use Putty and the associated tools, more precisely:</p> <ul> <li>PuTTY, the free SSH client</li> <li>Pageant, an SSH authentication agent for PuTTY tools</li> <li>PuTTYgen, an RSA key generation utility</li> <li>PSCP, an SCP (file transfer) client, i.e. command-line secure file copy</li> <li>WinSCP, SCP/SFTP (file transfer) client with easy-to-use graphical interface</li> </ul> <p>The different steps involved in the installation process are illustrated below (REMEMBER to tick the option \"Associate .PPK files (PuTTY Private Key) with Pageant and PuTTYGen\"):</p> <p></p> <p>Now you can use the PuTTYgen utility to generate an RSA key pair. The main steps for the generation of the keys are illustrated below (yet with 4096 bits instead of 2048):</p> <p></p> <p></p> <p></p> <ul> <li>Save the public and private keys as respectively <code>id_rsa.pub</code> and <code>id_rsa.ppk</code>.<ul> <li>Please keep a copy of the public key, you will have to add this public key into your account, using the IPA user portal (use the URL communicated to you by the UL HPC team in your \"welcome\" mail).</li> </ul> </li> </ul>"},{"location":"connect/ssh/#password-less-logins-and-transfers","title":"Password-less logins and transfers","text":"<p>Password based authentication is disabled on all ULHPC servers. You can only use public-key authentication. This assumes that you upload your public SSH keys <code>*.pub</code> to your user entry on the ULHPC Identity Management Portal.</p> <p>Consult the associated documentation to discover how to do it.</p> <p>Once done, you can connect by SSH to the ULHPC clusters. Note that the port on which the SSH servers are listening is not the default SSH one (i.e. 22) but 8022. Consequently, if you want to connect to the Iris cluster, open a terminal and run (substituting yourlogin with the login name you received from us):</p> IrisAion <pre><code># ADAPT 'yourlogin' accordingly\nssh -p 8022 yourlogin@access-iris.uni.lu\n</code></pre> <pre><code># ADAPT 'yourlogin' accordingly\nssh -p 8022 yourlogin@access-aion.uni.lu\n</code></pre> <p>Of course, we advise you to setup your SSH configuration to avoid typing this detailed command. This is explained in the next section.</p>"},{"location":"connect/ssh/#ssh-configuration","title":"SSH Configuration","text":"<p>On Linux / Mac OS / Unix / WSL, your SSH configuration is defined in <code>~/.ssh/config</code>. As recommended in the ULHPC Tutorials: Preliminaries / SSH, you probably want to create the following configuration to easiest further access and data transfers:</p> <pre><code># ~/.ssh/config -- SSH Configuration\n# Common options\nHost *\n    Compression yes\n    ConnectTimeout 15\n\n# ULHPC Clusters\nHost iris-cluster\n    Hostname access-iris.uni.lu\n\nHost aion-cluster\n    Hostname access-aion.uni.lu\n\n# /!\\ ADAPT 'yourlogin' accordingly\nHost *-cluster\n    User yourlogin\n    Port 8022\n    ForwardAgent no\n</code></pre> <p>You should now be able to connect as follows</p> IrisAion <pre><code>ssh iris-cluster\n</code></pre> <pre><code>ssh aion-cluster\n</code></pre> (Windows only) Remote session configuration with MobaXterm <p>This part of the documentation comes from MobaXterm documentation page MobaXterm allows you to launch remote sessions. You just have to click on the \"Sessions\" button to start a new session. Select SSH session on the second screen.</p> <p></p> <p></p> <p>Enter the following parameters:</p> <ul> <li>Remote host: <code>access-iris.uni.lu</code> (repeat with <code>access-aion.uni.lu</code>)</li> <li>Check the Specify username box</li> <li>Username: <code>yourlogin</code><ul> <li>Adapt to match the one that was sent to you in the Welcome e-mail once your HPC account was created</li> </ul> </li> <li>Port: <code>8022</code></li> <li>Go in Advanced SSH settings and check the Use private key box.<ul> <li>Select your previously generated key <code>id_rsa.ppk</code>.</li> </ul> </li> </ul> <p></p> <p>You can now click on Connect and enjoy.</p> (deprecated - Windows only) - Remote session configuration with PuTTY <p>If you want to connect to one of the ULHPC cluster, open Putty and enter the following settings:</p> <ul> <li>In Category:Session :</li> <li>Host Name: <code>access-iris.uni.lu</code> (or <code>access-aion.uni.lu</code> if you want to access Aion)</li> <li>Port: <code>8022</code></li> <li>Connection Type: <code>SSH</code> (leave as default)</li> <li>In Category:Connection:Data :</li> <li>Auto-login username: <code>yourlogin</code><ul> <li>Adapt to match the one that was sent to you in the Welcome e-mail once your HPC account was created</li> </ul> </li> <li>In Category:SSH:Auth :</li> <li>Upload your private key: <code>Options controlling SSH authentication</code></li> </ul> <p>Click on Open button. If this is the first time connecting to the server from this computer a Putty Security Alert will appear. Accept the connection by clicking Yes.</p> <p>You should now be logged into the selected ULHPC login node.</p> <p>Now you probably want want to save the configuration of this connection:</p> <ul> <li>Go onto the Session category.</li> <li>Enter the settings you want to save.</li> <li>Enter a name in the Saved session field (for example <code>Iris</code> for access to Iris cluster).</li> <li>Click on the Save button.</li> </ul> <p>Next time you want to connect to the cluster, click on Load button and Open to open a new connection.</p>"},{"location":"connect/ssh/#ssh-agent","title":"SSH Agent","text":"<p>Using passphrase protected private keys can be tedious as you have to unlock the key every time it is used. SSH agent mitigate this issue by reducing the number of times you have to enter your passphrase.</p> <p>SSH agents are programs that safely hold unencrypted private keys used for public key authentication schemes. The agent unlocks the encrypted private key once per session<sup>1</sup>, and stores the unencrypted key is in program memory. A socket is provided where programs that have access to the socket can request responses to challenges on the public key.</p> <p>Due to the challenge-response protocol architecture of SSH agents, there are differences between how SSH works with and without SSH agents.</p> <ul> <li>The clients of the SSH agent, such as the SSH program, only get responses to challenges on the public key, not the private key itself.</li> <li>The socket of the SSH agent can be forwarded to remove hosts, where every process that has access to socket can request a response to a public key challenge as if it was located in the local host.</li> </ul>"},{"location":"connect/ssh/#security-implications","title":"Security implications","text":"<p>The most critical security risk when using SSH agent is the man in the middle attack. Any sufficiently privileged user, like a system administrator, can hijack the process by accessing the exposed socket and use it to impersonate you in a remote connection. While this does not reveal the user's private keys, it still violates the integrity of your communications.</p> <ul> <li>When agent forwarding is enabled, a sufficiently privileged user in the remote machine can impersonate the identities linked to keys in your local machine.</li> <li>When you enable the agent after connecting to a shared machine, a sufficiently privileged user can impersonate the identities linked to keys in the shared machine.</li> </ul> <p>Because of the risk of man in the middle attacks, SSH agent forwarding is prohibited in UL HPC systems and it is recommended that you also explicitly disable it in your SSH configuration file (see [<code>ForwardAgent no</code> option in recommended SSH configuration file).</p>"},{"location":"connect/ssh/#adding-keys","title":"Adding keys","text":"<p>In most UNIX like operating systems the SSH agent is installed and setup by default.</p> <ul> <li>Mac OS X (&gt;= 10.5): the SSH agent is installed and setup by default.</li> <li>Linux: in most distribution the SSH agent is installed and setup by default.</li> </ul> <p>However if you get a message similar to the following,</p> <pre><code>(laptop) $ ssh -vv iris-cluster\n[...]\nAgent admitted failure to sign using the key.\nPermission denied (publickey).\n</code></pre> <p>this means that you have to manually load your keys to the SSH agent by running the <code>ssh-add</code> command of the SSH agent. When running the command without any arguments</p> <pre><code>(laptop) $ ssh-add\n</code></pre> <p>the agent scans the home directory for some standard keys and adds imports them. By default, the agent looks for:</p> <ul> <li><code>~/.ssh/id_rsa</code></li> <li><code>~/.ssh/id_ed25519</code></li> <li><code>~/.ssh/id_ecdsa</code></li> </ul> <p>If you would like to add specific keys, then provide the private key of the key pair as argument to the agent. For instance to add only the RSA and ED25519 keys call the following command.</p> <pre><code>(laptop) $ ssh-add ~/.ssh/id_rsa\nEnter passphrase for ~/.ssh/id_rsa: # &lt;-- enter your passphrase here\nIdentity added: ~/.ssh/id_rsa (&lt;login&gt;@&lt;hostname&gt;)\n(laptop) $ ssh-add ~/.ssh/id_ed25519\nEnter passphrase for ~/.ssh/id_ed25519: # &lt;-- enter your passphrase here\nIdentity added: ~/.ssh/id_ed25519 (&lt;login&gt;@&lt;hostname&gt;)\n</code></pre> <p>Useful commands</p> <p>You can add as many identities as you need in your <code>ssh-agent</code>.</p> <ul> <li>list the identities added with: <code>ssh-add -L</code></li> <li>delete an identity added with: <code>ssh-add -d &lt;private key file&gt;</code></li> <li>delete all identities added with: <code>ssh-add -D</code></li> </ul> Issues in Ubuntu on WSL <p>If you experience issues when using <code>ssh-add</code>, the install the <code>keychain</code> package.</p> SSH Agent within MobaXterm (Windows only) <ul> <li>Go in Settings &gt; SSH Tab.</li> <li>In SSH agents section, check Use internal SSH agent \"MobAgent\".</li> </ul> <p></p> <ul> <li>Click on the <code>+</code> button on the right.</li> <li>Select your private key file. If you have several keys, you can add them by doing steps above again.</li> <li>Click on \"Show keys currently loaded in MobAgent\". An advertisement window may appear asking if you want to run MobAgent. Click on \"Yes\".</li> <li>Check that your key(s) appears in the window.</li> </ul> <p></p> <ul> <li>Close the window.</li> <li>Click on <code>OK</code> and then restart MobaXterm.</li> </ul> SSH Agent with PuTTY Pageant (Windows only - deprecated) <p>To be able to use your PuTTY key in a public-key authentication scheme, it must be loaded by an SSH agent. You should run Pageant for that. To load your SSH key in Pageant,</p> <ul> <li>right-click on the pageant icon in the system tray,</li> <li>click on the <code>Add key</code> menu item,</li> <li>select the private key file you saved while running <code>puttygen.exe</code>,</li> <li>save the private key by clicking on the <code>Open</code> button which opens a new dialog pop up and asks for your passphrase, and</li> <li>add your passphrase and click <code>OK</code>.</li> </ul> <p>Once your passphrase is entered, your key will be loaded in pageant, enabling you to connect with Putty.</p>"},{"location":"connect/ssh/#the-keychain-package","title":"The keychain package","text":"<p>The keychain utility (program) checks for a running <code>ssh-agent</code> and starts one if it\u2019s not already running. It saves the <code>ssh-agent</code> environment variables to the <code>~/.keychain/${HOSTNAME}-sh</code> file, allowing subsequent logins and non-interactive shells such as <code>cron</code> jobs to source the file and establish passwordless <code>ssh</code> connections.</p> <p>Install the <code>keychain</code> package with the following command.</p> <pre><code>(laptop) $ sudo apt install keychain\n</code></pre> <p>Then, save the passphrase for all the keys you would like to use. For instance to save the passphrases for the RSA and ED25519 keys call the following command.</p> <pre><code>(laptop) $ /usr/bin/keychain --nogui ~/.ssh/id_rsa\n(laptop) $ /usr/bin/keychain --nogui ~/.ssh/id_ed25519\n</code></pre> <p>Finally to load the agent in you shell source the file generated for your local host in your current session.</p> <pre><code>(laptop) $ source ~/.keychain/$(hostname)-sh\n</code></pre> <p>You can also add the source command above in your <code>~/.profile</code> script to load the <code>ssh-agent</code> automatically in every session.</p>"},{"location":"connect/ssh/#on-ulhpc-clusters","title":"On ULHPC clusters","text":"<p>SSH agent forwarding is prohibited in UL HPC systems for security reasons mentioned above and it is recommended that you also explicitly disable it in your SSH configuration.For more details, see the <code>ForwardAgent no</code> configuration option in the SSH configuration proposed for UL HPC clusters. There are however legitimate uses of SSH agent. Consider for instance keys generated in the UL HPC cluster to access a remote service. You can use SSH agent to avoid entering the passphrase of SSH keys every time you access the remote service. </p> <p>In such cases an SSH agent must be manually loaded after connecting to UL HPC facilities. To load the agent, execute the command,</p> <pre><code>eval \"$(ssh-agent)\"\n</code></pre> <p>that exports the <code>SSH_AUTH_SOCK</code> and <code>SSH_AGENT_PID</code> environment variables required by the clients of the SSH agent. Then, you can setup the agent by adding your keys. For instance to add the RSA and ED25519 keys, execute the following command.</p> <pre><code>$ ssh-add ~/.ssh/id_rsa\nEnter passphrase for ~/.ssh/id_rsa: # &lt;-- enter your passphrase here\nIdentity added: ~/.ssh/id_rsa (&lt;login&gt;@&lt;hostname&gt;)\n$ ssh-add ~/.ssh/id_ed25519\nEnter passphrase for ~/.ssh/id_ed25519: # &lt;-- enter your passphrase here\nIdentity added: ~/.ssh/id_ed25519 (&lt;login&gt;@&lt;hostname&gt;)\n</code></pre> <p>Adding your keys to the agent exposes your session to a man in the middle attack, as discussed above <sup>2</sup>. Thus, you must kill your agent whenever it is no longer needed. Kill the agent executing the following command.</p> <pre><code>$ eval \"$(ssh-agent -k)\"\nAgent pid &lt;PID&gt; killed\n</code></pre> Useful resources <ol> <li>An Illustrated Guide to SSH Agent Forwarding</li> <li>SSH Agent Explained</li> <li>How to Use SSH Agent Safely</li> </ol>"},{"location":"connect/ssh/#key-fingerprints","title":"Key fingerprints","text":"<p>ULHPC may occasionally update the host keys on the major systems.  Check here to confirm the current fingerprints.</p> IrisAion <p>With regards <code>access-iris.uni.lu</code>:</p> <pre><code>256 SHA256:tkhRD9IVo04NPw4OV/s2LSKEwe54LAEphm7yx8nq1pE /etc/ssh/ssh_host_ed25519_key.pub (ED25519)\n2048 SHA256:WDWb2hh5uPU6RgaSotxzUe567F3scioJWy+9iftVmhI /etc/ssh/ssh_host_rsa_key.pub (RSA)\n</code></pre> <p>With regards <code>access-aion.uni.lu</code>:</p> <pre><code>256 SHA256:jwbW8pkfCzXrh1Xhf9n0UI+7hd/YGi4FlyOE92yxxe0 [access-aion.uni.lu]:8022 (ED25519)\n3072 SHA256:L9n2gT6aV9KGy0Xdh1ks2DciE9wFz7MDRBPGWPFwFK4 [access-aion.uni.lu]:8022 (RSA)\n</code></pre> <p>Get SSH key fingerprint</p> <p>The ssh fingerprints can be obtained via: <pre><code>ssh-keygen -lf &lt;(ssh-keyscan -t rsa,ed25519 $(hostname) 2&gt;/dev/null)\n</code></pre></p> Putty key fingerprint format <p>Depending on the ssh client you use to connect to ULHPC systems, you may see different key fingerprints. For example, Putty uses different format of fingerprints as follows:</p> <ul> <li><code>access-iris.uni.lu</code> <pre><code>ssh-ed25519 255 4096 07:6a:5f:11:df:d4:3f:d4:97:98:12:69:3a:63:70:2f\n</code></pre></li> </ul> <p>You may see the following warning when connecting to Cori with Putty, but it is safe to ingore.</p> <pre><code>PuTTY Security Alert\nThe server's host key is not cached in the registry. You have no guarantee that the server is the computer you think it is.\nThe server's ssh-ed25519 key fingerprint is:\nssh-ed25519 255 4096 07:6a:5f:11:df:d4:3f:d4:97:98:12:69:3a:63:70:2f\nIf you trust this host, hit Yes to add the key to PuTTY's cache and carry on connecting.\nIf you want to carry on connecting just once, without adding the key to the cache, hit No.\nIf you do not trust this host, hit Cancel to abandon the connection.\n</code></pre>"},{"location":"connect/ssh/#host-keys","title":"Host Keys","text":"<p>These are the entries in <code>~/.ssh/known_hosts</code>.</p> IrisAion <p>The known host SSH entry for the Iris cluster  should be as follows:</p> <pre><code>[access-iris.uni.lu]:8022 ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIOP1eF8uJ37h5jFQQShn/NHRGD/d8KsMMUTHkoPRANLn\n</code></pre> <p>The known host SSH entry for the Aion cluster should be as follows:</p> <pre><code>[access-aion.uni.lu]:8022 ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIFmcYJ7T6A1wOvIQaohgwVCrKLqIrzpQZAZrlEKx8Vsy\n</code></pre>"},{"location":"connect/ssh/#troubleshooting","title":"Troubleshooting","text":"<p>See the corresponding section.</p>"},{"location":"connect/ssh/#advanced-ssh-tips-and-tricks","title":"Advanced SSH Tips and Tricks","text":""},{"location":"connect/ssh/#cli-completion","title":"CLI Completion","text":"<p>The <code>bash-completion</code> package eases the ssh command usage by providing completion for hostnames and more (assuming you set the directive <code>HashKnownHost</code> to <code>no</code> in your <code>~/etc/ssh_config</code>).</p>"},{"location":"connect/ssh/#socks-5-proxy-plugin","title":"SOCKS 5 Proxy plugin","text":"<p>Many Data Analytics framework involves a web interface (at the level of the master and/or the workers) you probably want to access in a relative transparent way.</p> <p>For that, a convenient way is to rely on a SOCKS proxy, which is basically an SSH tunnel in which specific applications forward their traffic down the tunnel to the server, and then on the server end, the proxy forwards the traffic out to the general Internet. Unlike a VPN, a SOCKS proxy has to be configured on an app by app basis on the client machine, but can be set up without any specialty client agents. The general principle is depicted below.</p> <p></p>"},{"location":"connect/ssh/#setting-up-the-tunnel","title":"Setting Up the Tunnel","text":"<p>To initiate such a SOCKS proxy using SSH (listening on <code>localhost:1080</code> for instance), you simply need to use the <code>-D 1080</code> command line option when connecting to a remote server:</p> IrisAion <pre><code>ssh -D 1080 -C iris-cluster\n</code></pre> <pre><code>ssh -D 1080 -C aion-cluster\n</code></pre> <ul> <li><code>-D</code>: Tells SSH that we want a SOCKS tunnel on the specified port number (you can choose a number between 1025-65536)</li> <li><code>-C</code>: Compresses the data before sending it</li> </ul>"},{"location":"connect/ssh/#foxyproxy-firefox-extension","title":"FoxyProxy [Firefox] Extension","text":"<p>Now that you have an SSH tunnel, it's time to configure your web browser (recommended: Firefox) to use that tunnel. In particular, install the Foxy Proxy extension for Firefox and configure it to use your SOCKS proxy:</p> <ul> <li>Right click on the fox icon, Select Options</li> <li>Add a new proxy button</li> <li>Name: <code>ULHPC proxy</code></li> <li>Informations &gt; Manual configuration<ul> <li>Host IP: <code>127.0.0.1</code></li> <li>Port: <code>1080</code></li> <li>Check the Proxy SOCKS Option</li> </ul> </li> <li>Click on OK</li> <li>Close</li> <li>Open a new tab</li> <li>Click on the Fox</li> <li>Choose the ULHPC proxy<ul> <li>disable it when you no longer need it.</li> </ul> </li> </ul> <p>You can now access any web interface deployed on any service reachable from the SSH jump host i.e. the ULHPC login node.</p>"},{"location":"connect/ssh/#using-tsock","title":"Using <code>tsock</code>","text":"<p>Once you setup a SSH SOCKS proxy, you can also use <code>tsocks</code>, a Shell wrapper to simplify the use of the tsocks(8) library to transparently allow an application (not aware of SOCKS) to transparently use a SOCKS proxy. For instance, assuming you create a VNC server on a given remote server as follows:</p> <pre><code>(remote_server)$&gt; vncserver -geometry 1366x768\nNew 'X' desktop is remote_server:1\n\nStarting applications specified in /home/username/.vnc/xstartup\nLog file is /home/username/.vnc/remote_server:1.log\n</code></pre> <p>Then you can make the VNC client on your workstation use this tunnel to access the VNS server as follows:</p> <pre><code>(laptop)$&gt; tsocks vncviewer &lt;IP_of_remote_server&gt;:1\n</code></pre> <p>tsock Escape character</p> <p>Use <code>~.</code> to disconnect, even if your remote command hangs.</p>"},{"location":"connect/ssh/#ssh-port-forwarding","title":"SSH Port Forwarding","text":""},{"location":"connect/ssh/#forwarding-a-local-port","title":"Forwarding a local port","text":"<p>You can forward a local port to a host behind a firewall.</p> <p></p> <p>This is useful if you run a server on one of the cluster nodes (let's say listening on port 2222) and you want to access it via the local port 1111 on your machine. Then you'll run:</p> <pre><code># Here targeting iris cluster\n(laptop) $ ssh iris-cluster -L 1111:iris-014:2222\n</code></pre>"},{"location":"connect/ssh/#forwarding-a-remote-port","title":"Forwarding a remote port","text":"<p>You can forward a remote port back to a host protected by your firewall.</p> <p></p> <p>This is useful when you want the HPC node to access some local service. For instance is your local machine runs a service that is listening at some local port, say 2222, and you have some service in the HPC node that listens to some local port, say 1111, then the you'll run:</p> <pre><code># Here targeting the iris cluster\n(local machine) $ ssh iris-cluster -R 1111:$(hostname -i):2222\n</code></pre>"},{"location":"connect/ssh/#tunnelling-for-others","title":"Tunnelling for others","text":"<p>By using the <code>-g</code> parameter, you allow connections from other hosts than localhost to use your SSH tunnels. Be warned that anybody within your network may access the tunnelized host this way, which may be a security issue.</p>"},{"location":"connect/ssh/#ssh-jumps","title":"SSH jumps","text":"<p>Compute nodes are not directly accessible from the outside network. To login into a cluster node you will need to jump through a login node. Remember, you need a job running in a node before you can ssh into it. Assume that you have some job running on <code>aion-0014</code> for instance. Then, connect to <code>aion-0014</code> with,</p> <pre><code>ssh -J ${USER}@access-aion.uni.lu:8022 ${USER}@aion-0014\n</code></pre> <p>where <code>USER</code> is a variable containing your username in UL HPC clusters. The domain resolution in the login node will determine the IP of the <code>aion-0014</code>. You can always use the IP address of the node directly.</p> Obtaining the IP address of cluster nodes <p>To get the IP address of any node just run the following command.</p> <pre><code>hostname --ip-address\n</code></pre> Useful environment variables for SSH jumps <ul> <li><code>USER</code>: In most Linux systems the <code>USER</code> environment variable is defined in the <code>/etc/profile</code> and <code>/etc/prifile.d/*</code> scripts, and contains the username of the user.</li> <li><code>HOSTNAME</code>: A shell variable defined in BASH that contains the name of the host machine.</li> <li><code>ULHPC_CLUSTER</code>: An environment variable defined in environment initialization scripts of UL HPC, and contains and identifier of the system, <code>aion</code> or <code>iris</code>.</li> </ul>"},{"location":"connect/ssh/#authorizing-access-to-compute-nodes","title":"Authorizing access to compute nodes","text":"<p>In UL HPC clusters, the authorization for logging into the login nodes is not provided by the <code>authorized_keys</code> file, but by identity management system. However, connections to the compute nodes are authorized by the <code>authorized_keys</code> file. Use the command</p> <pre><code>ssh-copy-id -i &lt;private key&gt; aion-cluster\n</code></pre> <p>if you have configured SSH, or</p> <pre><code>ssh-copy-id -i &lt;private key&gt; -p 8022 &lt;user name&gt;@access-aion.uni.lu\n</code></pre> <p>if you haven't; the <code>&lt;private key&gt;</code> is the private key for the public key you would like to copy to the UL HPC cluster. For instance,</p> <ul> <li><code>&lt;private key&gt;</code> is <code>~/.ssh/id_ed25519</code> for the default ED25519 key pair, and</li> <li><code>&lt;private key&gt;</code> is <code>~/.ssh/id_rsa</code> for the default RSA key pair.</li> </ul> <p>Passwordless SSH jumps</p> <p>With a proxy jump command SSH logs into the jump host, initiates I/O forwarding, and then logs into the target remote host using the credentials of your local machine. Thus, the <code>ProxyJump</code> SSH option obviates the need for SSH agent forwarding. You do not need SSH agent forwarding to the jump host or the target remote host, just ensure that your private key is authorized both in the jump hosts and in the target remote host.</p> <p>You still need to run the SSH agent in your local machine to ensure passwordless to keys protected by passphrase.</p> the <code>authorized_keys</code> file <p>The <code>authorized_keys</code> file is located in  <pre><code>${HOME}/.ssh/authorized_keys\n</code></pre> and <code>ssh-copy-id</code> simply automatically appends the key at the end of the file in the remote host.</p>"},{"location":"connect/ssh/#port-forwarding-over-ssh-jumps","title":"Port forwarding over SSH jumps","text":"<p>You can combine the jump command with other options, such as port forwarding, for instance to access from you local machine a web server running in a compute node. Assume for instance that you have a server running in <code>iris-014</code> and listens at the IP <code>127.0.0.1</code> and port <code>2222</code>, and that you would like to forward the remote port <code>2222</code> to the <code>1111</code> port of you local machine. The, call the port forwarding command with a jump though the login node:</p> <pre><code>ssh -J iris-cluster -L 1111:127.0.0.1:2222 &lt;cluster username&gt;@iris-014\n</code></pre> <p>This command can be combined with passwordless access to the cluster node.</p>"},{"location":"connect/ssh/#extras-tools-around-ssh","title":"Extras Tools around SSH","text":"<ul> <li> <p>Assh - Advanced SSH config is a transparent wrapper that make <code>~/.ssh/config</code> easier to manage</p> <ul> <li>support for templates, aliases, defaults, inheritance etc.</li> <li>gateways: transparent ssh connection chaining</li> <li> <p>more flexible command-line. Ex: Connect to <code>hosta</code> using <code>hostb</code> as a gateway   <pre><code>$ ssh hosta/hostb\n</code></pre></p> </li> <li> <p>drastically simplify your SSH config</p> </li> <li>Linux / Mac OS only</li> </ul> </li> <li> <p>ClusterShell: <code>clush</code>, <code>nodeset</code> (or cluset),</p> <ul> <li>light, unified, robust command execution framework</li> <li>well-suited to ease daily administrative tasks of Linux clusters.<ul> <li>using tools like <code>clush</code> and <code>nodeset</code></li> </ul> </li> <li>efficient, parallel, scalable command execution engine \\hfill{\\tiny in Python}</li> <li>provides an unified node groups syntax and external group access<ul> <li>see <code>nodeset</code> and the NodeSet class</li> </ul> </li> </ul> </li> <li> <p>DSH - Distributed / Dancer's Shell</p> </li> <li>sshutle, \"where transparent proxy meets VPN meets ssh\"</li> </ul> <ol> <li> <p>SSH agents usually support session timeout to limit the time duration a key is exposed without a passphrase; this can guard against accidental key exposure, for instance if a user leaves their computer unlocked.\u00a0\u21a9</p> </li> <li> <p>If you store private keys in the UL HPC cluster file systems these keys are already visible to users with sufficiently high privileges. You must kill the SSH agent to avoid exposing these keys to man in the middle attack through SSH agent forwarding.\u00a0\u21a9</p> </li> </ol>"},{"location":"connect/troubleshooting/","title":"Troubleshooting","text":"<p>There are several possibilities and usually the error message can give you some hints.</p>"},{"location":"connect/troubleshooting/#your-account-has-expired","title":"Your account has expired","text":"<p>Please open a ticket on ServiceNow (HPC \u2192 User access &amp; accounts \u2192 Report issue with cluster access) or send us an email to hpc-team@uni.lu with the current end date of your contract and we will extend your account accordingly.</p>"},{"location":"connect/troubleshooting/#access-denied-or-permission-denied-publickey","title":"\"Access Denied\" or \"Permission denied (publickey)\"","text":"<p>Basically, you are NOT able to connect to the access servers until your SSH public key is configured. There can be several reason that explain the denied connection message:</p> <ul> <li>Make sure you are using the proper ULHPC user name (and not your local   username or University/Eduroam login).<ul> <li>Check your mail entitled \"<code>[HPC@Uni.lu] Welcome - Account information</code>\"    to get your ULHPC login</li> </ul> </li> <li>Log into IPA and double check your SSH public key settings.</li> <li>Ensure you have run your SSH agent</li> <li>If you have a new computer or for some other reason you have generated   new ssh key, please update your ssh keys on the IPA user portal.<ul> <li>See IPA for more details</li> </ul> </li> <li>You are using (deprecated) DSA/RSA keys. As per the   OpenSSH website: <p>\"OpenSSH 7.0 and greater similarly disable the ssh-dss (DSA) public key algorithm. It too is weak and we recommend against its use\". Solution: generate a new RSA keypair (3092 bit or more) and re-upload it on the IPA web portal (use the URL communicated to you by the UL HPC team in your \u201cwelcome\u201d mail). For more information on keys, see this website.</p> </li> </ul> <p></p> <ul> <li> <p>Your public key is corrupted, please verify and re-upload it on the IPA web portal.</p> </li> <li> <p>We have taken the cluster down for maintenance and we forgot to activate the   banner message mentioning this. Please check the calendar, the latest Twitter   messages (box on the right of this page) and the messages sent on the <code>hpc-users</code> mailing list.</p> </li> </ul> <p>If the above steps did not permit to solve your issue, please open a ticket on ServiceNow (HPC \u2192 User access &amp; accounts \u2192 Report issue with cluster access) or send us an email to hpc-team@uni.lu.</p>"},{"location":"connect/troubleshooting/#host-identification-changed","title":"Host identification changed","text":"<pre><code>@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\nIT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!\nSomeone could be eavesdropping on you right now (man-in-the-middle attack)!\nIt is also possible that a host key has just been changed.\n...\n</code></pre> <p>Ensure that your <code>~/.ssh/known_hosts</code> file contains the correct entries for the ULHPC clusters and confirm the fingerprints using the posted fingerprints</p> <ol> <li>Open <code>~/.ssh/known_hosts</code></li> <li>Remove any lines referring Iris and Aion and save the file</li> <li>Paste the specified host key entries (for all clusters) OR retry connecting to the host and    accept the new host key after verify that you have the correct \"fingerprint\"    from the reference list.</li> </ol>"},{"location":"connect/troubleshooting/#be-careful-with-permission-changes-to-your-home","title":"Be careful with permission changes to your $HOME","text":"<p>If you change your home directory to be writeable by the group, <code>ssh</code> will not let you connect anymore. It requires drwxr-xr-x or 755 (or less) on your <code>$HOME</code> and <code>~/.ssh</code>, and -rw-r--r-- or 644 (or less) on <code>~/.ssh/authorized_keys</code>.</p> <p>File and folder permissions can be verified at any time using <code>stat $path</code>, e.g.:</p> <pre><code>$&gt; stat $HOME\n$&gt; stat $HOME/.ssh\n$&gt; stat $HOME/.ssh/authorized_keys\n</code></pre> <p>Check out the description of the notation of file permissions in both symbolic and numeric mode.</p> <p>On your local machine, you also need to to have read/write permissions to <code>~/.ssh/config</code> for your user only. This can be ensured with the following command:</p> <pre><code>chmod 600 ~/.ssh/config\n</code></pre>"},{"location":"connect/troubleshooting/#open-a-ticket","title":"Open a ticket","text":"<p>If you cannot solve your problem, do not hesitate to open a ticket on the Service Now portal.</p>"},{"location":"connect/windows/","title":"SSH (Windows)","text":"<p>In this page, we cover two different SSH client software: MobaXterm and Putty. Choose your preferred tool.</p>"},{"location":"connect/windows/#mobaxterm","title":"MobaXterm","text":""},{"location":"connect/windows/#installation-notes","title":"Installation notes","text":"<p>The following steps will help you to configure MobaXterm to access the UL HPC clusters. You can also check out the MobaXterm demo which shows an overview of its features.</p> <p>First, download and install MobaXterm. Open the application Start &gt; Program Files &gt; MobaXterm.</p> <p>Change the default home directory for a persistent home directory instead of the default Temp directory. Go onto Settings &gt; Configuration &gt; General &gt; Persistent home directory. Choose a location for your home directory.</p> <p>Your local SSH configuration is located in the  <code>HOME/.ssh/</code> directory and consists of:</p> <ul> <li> <p><code>HOME/.ssh/id_rsa.pub</code>: your SSH public key. This one is the only one SAFE to distribute.</p> </li> <li> <p><code>HOME/.ssh/id_rsa</code>: the associated private key. NEVER EVER TRANSMIT THIS FILE</p> </li> <li> <p>(eventually) the configuration of the SSH client <code>HOME/.ssh/config</code></p> </li> <li> <p><code>HOME/.ssh/known_hosts</code>: Contains a list of host keys for all hosts you have logged into that are not already in the system-wide list of known host keys. This permits to detect man-in-the-middle attacks.</p> </li> </ul>"},{"location":"connect/windows/#ssh-key-management","title":"SSH Key Management","text":"<p>Choose the method you prefer: either the graphical interface MobaKeyGen or command line generation of the ssh key.</p>"},{"location":"connect/windows/#with-mobakeygen-tool","title":"With MobaKeyGen tool","text":"<p>Go onto Tools &gt; Network &gt; MobaKeyGen (SSH key generator). Choose RSA as the type of key to generate and change \"Number of bits in a generated key\" to 4096. Click on the Generate button. Move your mouse to generate some randomness.</p> <p>Warning</p> <p>To ensure the security of the platform and your data stored on it, you must protect your SSH keys with a passphrase! Additionally, your private key and passphrase should never be transmitted to anybody.</p> <p>Select a strong passphrase in the Key passphrase field for your key. Save the public and private keys as respectively <code>id_rsa.pub</code> and <code>id_rsa.ppk</code>. Please keep a copy of the public key, you will have to add this public key into your account, using the ULHPC Identity Management Portal (use the URL communicated to you by the UL HPC team in your \"welcome\" mail).</p> <p></p> <p></p>"},{"location":"connect/windows/#with-local-terminal","title":"With local terminal","text":"<p>Click on Start local terminal. To generate an SSH keys, just use the <code>ssh-keygen</code> command, typically as follows:</p> <pre><code>$&gt; ssh-keygen -t rsa -b 4096\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/user/.ssh/id_rsa):\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nYour identification has been saved in /home/user/.ssh/id_rsa.\nYour public key has been saved in /home/user/.ssh/id_rsa.pub.\nThe key fingerprint is:\nfe:e8:26:df:38:49:3a:99:d7:85:4e:c3:85:c8:24:5b username@yourworkstation\nThe key's randomart image is:\n+---[RSA 4096]----+\n|                 |\n|      . E        |\n|       * . .     |\n|      . o . .    |\n|        S. o     |\n|       .. = .    |\n|       =.= o     |\n|      * ==o      |\n|       B=.o      |\n+-----------------+\n</code></pre> <p>Warning</p> <p>To ensure the security of the platform and your data stored on it, you must protect your SSH keys with a passphrase!</p> <p>After the execution of <code>ssh-keygen</code> command, the keys are generated and stored in the following files:</p> <ul> <li>SSH RSA Private key: <code>HOME/.ssh/id_rsa</code>. Again, NEVER EVER TRANSMIT THIS FILE</li> <li>SSH RSA Public key:  <code>HOME/.ssh/id_rsa.pub</code>.  This file is the ONLY one SAFE to distribute</li> </ul>"},{"location":"connect/windows/#configuration","title":"Configuration","text":"<p>This part of the documentation comes from MobaXterm documentation page</p> <p>MobaXterm allows you to launch remote sessions. You just have to click on the \"Sessions\" button to start a new session. Select SSH session on the second screen.</p> <p></p> <p></p> <p>Enter the following parameters:</p> <ul> <li>Remote host: <code>access-iris.uni.lu</code> or <code>access-aion.uni.lu</code></li> <li>Check the Specify username box</li> <li>Username: <code>yourlogin</code></li> <li>as was sent to you in the Welcome e-mail once your HPC account was created</li> <li>Port: <code>8022</code></li> </ul> <p>Go in Advanced SSH settings and check the Use private key box. Select your previously generated key <code>id_rsa.ppk</code>.</p> <p></p> <p>Click on Connect. The following text appears.</p> <pre><code>================================================================================\n  Welcome to access1.iris-cluster.uni.lux\n  WARNING: For use by authorized users only!\n               _____      _        ____ _           _          __\n              / /_ _|_ __(_)___   / ___| |_   _ ___| |_ ___ _ _\\ \\\n             | | | || '__| / __| | |   | | | | / __| __/ _ \\ '__| |\n             | | | || |  | \\__ \\ | |___| | |_| \\__ \\ ||  __/ |  | |\n             | ||___|_|  |_|___/  \\____|_|\\__,_|___/\\__\\___|_|  | |\n              \\_\\                                              /_/\n\n=== Support and community ======================================================\n\n  - Technical Docs ........... https://hpc-docs.uni.lu\n  - Discourse / Forum ........ https://hpc-discourse.uni.lu/\n  - Helpdesk / Service Now ... https://hpc.uni.lu/support\n  - User Mailing-list ........ hpc-users@uni.lu\n\n=== HPC utilization as of 2025-02-18 15h00 =====================================\n\n  AION  batch    74.42%\n  IRIS  batch    47.94%\n  IRIS  gpu      85.42%\n  IRIS  bigmem   51.79%\n\n=== Status =====================================================================\n\n New environment modules (env/development/2023b env/release/default env/release/2020b)\n The default module currently points to 'env/release/2020b'\n      but will be updated to 2023b upon release.\n Add 'module use env/release/2020b' to your scripts to ensure that\n      they will continue working after the switch.\n\n Cluster maintenance complete (https://gitlab.com/uniluxembourg/hpc/support/infra/-/issues/29)\n - General update to latest RedHat 8.10\n - Slurm update to 23.11.10\n - General security update\n - New software set release (foss 2023a toolchain)\n\n================================================================================\nLast login: Tue Feb 18 10:40:24 2025 from 10.187.28.101\nLinux access1.iris-cluster.uni.lux 4.18.0-553.34.1.el8_10.x86_64 x86_64\n 15:09:51 up 19 days, 5 min, 41 users,  load average: 2.24, 2.55, 3.79\n[yourlogin@access1 ~]$\n</code></pre>"},{"location":"connect/windows/#putty","title":"Putty","text":""},{"location":"connect/windows/#installation-notes_1","title":"Installation notes","text":"<p>You need to install Putty and the associated tools, more precisely:</p> <ul> <li> <p>PuTTY, the free SSH client</p> </li> <li> <p>Pageant, an SSH authentication agent for PuTTY tools</p> </li> <li> <p>PuTTYgen, an RSA key generation utility</p> </li> <li> <p>PSCP, an SCP (file transfer) client, i.e. command-line secure file copy</p> </li> <li> <p>WinSCP, SCP/SFTP (file transfer) client with easy-to-use graphical interface</p> </li> </ul> <p>The simplest method is probably to download and run the latest Putty installer (does not include WinSCP).</p> <p>The different steps involved in the installation process are illustrated below (REMEMBER to tick the option \"Associate .PPK files (PuTTY Private Key) with Pageant and PuTTYGen\"):</p> <p></p> <p></p> <p></p> <p>Now you should have all the Putty programs available in <code>Start / All Programs / Putty</code>.</p>"},{"location":"connect/windows/#ssh-key-management_1","title":"SSH Key Management","text":"<p>Here you can use the PuTTYgen utility, an RSA key generation utility.</p> <p>The main steps for the generation of the keys are illustrated below:</p> <p></p> <p></p> <p></p>"},{"location":"connect/windows/#configuration_1","title":"Configuration","text":"<p>In order to be able to login to the clusters, you will have to add this public key into your account, using the ULHPC Identity Management Portal (use the URL communicated to you by the UL HPC team in your \"welcome\" mail).</p> <p></p> <p>The port on which the SSH servers are listening is not the default one (i.e. 22) but 8022. Consequently, if you want to connect to the Iris cluster, open Putty and enter the following settings:</p> <ul> <li>In Category:Session :</li> <li>Host Name: <code>access-iris.uni.lu</code> or <code>access-aion.uni.lu</code></li> <li>Port: <code>8022</code></li> <li>Connection Type: <code>SSH</code> (leave as default)</li> <li>In Category:Connection:Data :</li> <li>Auto-login username: <code>yourlogin</code></li> <li>In Category:SSH:Auth :</li> <li>Upload your private key: <code>Options controlling SSH authentication</code></li> </ul> <p>Click on Open button. If this is the first time connecting to the server from this computer a Putty Security Alert will appear. Accept the connection by clicking Yes.</p> <p>Enter your login (username of your HPC account). You are now logged into Iris access server with SSH.</p> <p>Alternatively, you may want to save the configuration of this connection. Go onto the Session category. Enter the settings you want to save. Enter a name in the Saved session field (for example <code>Iris</code> for access to Iris cluster). Click on the Save button. Next time you want to connect to the cluster, click on Load button and Open to open a new connexion.</p> <p>Now you'll be able to obtain the welcome banner:</p> <pre><code>================================================================================\n  Welcome to access1.iris-cluster.uni.lux\n  WARNING: For use by authorized users only!\n               _____      _        ____ _           _          __\n              / /_ _|_ __(_)___   / ___| |_   _ ___| |_ ___ _ _\\ \\\n             | | | || '__| / __| | |   | | | | / __| __/ _ \\ '__| |\n             | | | || |  | \\__ \\ | |___| | |_| \\__ \\ ||  __/ |  | |\n             | ||___|_|  |_|___/  \\____|_|\\__,_|___/\\__\\___|_|  | |\n              \\_\\                                              /_/\n\n=== Support and community ======================================================\n\n  - Technical Docs ........... https://hpc-docs.uni.lu\n  - Discourse / Forum ........ https://hpc-discourse.uni.lu/\n  - Helpdesk / Service Now ... https://hpc.uni.lu/support\n  - User Mailing-list ........ hpc-users@uni.lu\n\n=== HPC utilization as of 2025-02-18 15h00 =====================================\n\n  AION  batch    74.42%\n  IRIS  batch    47.94%\n  IRIS  gpu      85.42%\n  IRIS  bigmem   51.79%\n\n=== Status =====================================================================\n\n New environment modules (env/development/2023b env/release/default env/release/2020b)\n The default module currently points to 'env/release/2020b'\n      but will be updated to 2023b upon release.\n Add 'module use env/release/2020b' to your scripts to ensure that\n      they will continue working after the switch.\n\n Cluster maintenance complete (https://gitlab.com/uniluxembourg/hpc/support/infra/-/issues/29)\n - General update to latest RedHat 8.10\n - Slurm update to 23.11.10\n - General security update\n - New software set release (foss 2023a toolchain)\n\n================================================================================\n</code></pre>"},{"location":"connect/windows/#activate-the-ssh-agent","title":"Activate the SSH agent","text":"<p>To be able to use your SSH key in a public-key authentication scheme, it must be loaded by an SSH agent.</p> <p>You should run Pageant. To load your SSH key in Pageant, just right-click on the pageant icon in the system tray, click on the <code>Add key</code> menu item and select the private key file you saved while running puttygen.exe and click on the Open button: a new dialog will pop up and ask for your passphrase. Once your passphrase is entered, your key will be loaded in pageant, enabling you to connect with Putty.</p> <p>Open <code>Putty.exe</code> (connection type: <code>SSH</code>)</p> <ul> <li>In _Category:Session_:</li> <li>Host Name: <code>access-iris.uni.lu</code> or <code>access-aion.uni.lu</code></li> <li>Port: 8022</li> <li>Saved session: <code>Iris</code></li> <li>In Category:Connection:Data:</li> <li>Auto-login username: <code>yourlogin</code></li> <li>Go back to Category:Session and click on Save</li> <li>Click on Open</li> </ul>"},{"location":"connect/windows/#ssh-resources","title":"SSH Resources","text":"<ul> <li>OpenSSH/Cygwin: OpenSSH is available with Cygwin. You may then find the same features in your SSH client even if you run Windows. Furthermore, Cygwin also embeds many other GNU Un*x like tools, and even a FREE X server for windows.</li> <li>Putty: Free windowish SSH client</li> <li>ssh.com Free for non commercial use windows client</li> </ul>"},{"location":"contributing/","title":"Overview","text":"<p>You are more than welcome to contribute to the development of this project. You are however expected to follow the model of Github Flow for your contributions.</p> <p>What is a [good] Git Workflow?</p> <p>A Git Workflow is a recipe or recommendation for how to use Git to accomplish work in a consistent and productive manner. Indeed, Git offers a lot of flexibility in how changes can be managed, yet there is no standardized process on how to interact with Git. The following questions are expected to be addressed by a successful workflow:</p> <ol> <li>Q1: Does this workflow scale with team size?</li> <li>Q2: Is it possible to prevent/limit mistakes and errors ?</li> <li>Q3: Is it easy to undo mistakes and errors with this workflow?</li> <li>Q4: Does this workflow permits to easily test new feature/functionnalities before production release ?</li> <li>Q5: Does this workflow allow for Continuous Integration (even if not yet planned at the beginning)</li> <li>Q6: Does this workflow permit to master the production release</li> <li>Q7: Does this workflow impose any new unnecessary cognitive overhead to the team?</li> <li>Q8: The workflow is easy to use/setup and maintain</li> </ol> <p>In particular, the default \"workflow\" centralizedgitl (where everybody just commit to the single <code>master</code> branch), while being the only one satisfying Q7, proved to be easily error-prone and can break production system relying on the underlying repository. For this reason, other more or less complex workflows have emerged -- all feature-branch-based, that supports teams and projects where production deployments are made regularly:</p> <ul> <li> <p>Git-flow, the historical successful workflow featuring two main branches with an infinite lifetime (<code>production</code> and <code>{master | devel}</code>)</p> <ul> <li>all operations are facilitated by the <code>git-flow</code> CLI extension</li> <li>maintaining both branches can be bothersome - <code>make up</code></li> <li>the only one permitting to really control production release</li> </ul> </li> <li> <p>Github Flow, a lightweight version with a single branch (<code>master</code>)</p> <ul> <li>pull-request based - requires interaction with Gitlab/Github web interface (<code>git request-pull</code> might help)</li> </ul> </li> </ul> <p>The ULHPC team enforces an hydrid workflow detailed below, HOWEVER you can safely contribute to this documentation by following the Github Flow explained now.</p>"},{"location":"contributing/#default-git-workflow-for-contributions","title":"Default Git workflow for contributions","text":"<p>We expect contributors to follow the Github Flow concept.</p> <p></p> <p>This flow is ideal for organizations that need simplicity, and roll out frequently. If you are already using Git, you are probably using a version of the Github flow. Every unit of work, whether it be a bugfix or feature, is done through a branch that is created from master. After the work has been completed in the branch, it is reviewed and tested before being merged into master and pushed out to production.</p> <p>In details:</p> <ul> <li> <p>As preliminaries (to be done only once),  Fork the <code>ULHPC/ulhpc-docs</code> repository under <code>&lt;YOUR-USERNAME&gt;/ulhpc-docs</code></p> <ul> <li>A fork is a copy of a repository placed under your Github namespace. Forking a repository allows you to freely experiment with changes without affecting the original project.</li> <li>In the top-right corner of the  <code>ULHPC/ulhpc-docs</code> repository, click \"Fork\" button.</li> <li>Under Settings, change the repository name from <code>docs</code> to <code>ulhpc-docs</code></li> <li>Once done, you can clone your copy (forked) repository: select the SSH url under the \"Code\" button: <pre><code># (Recommended) Place your repo in a clean (and self-explicit) directory layout\n# /!\\ ADAPT 'YOUR-USERNAME' with your Github username\n$&gt; mkdir -p ~/git/github.com/YOUR-USERNAME\n$&gt; cd ~/git/github.com/YOUR-USERNAME\n# /!\\ ADAPT 'YOUR-USERNAME' with your Github username\ngit clone git@github.com:YOUR-USERNAME/ulhpc-docs.git\n$&gt; cd ulhpc-docs\n$&gt; make setup\n</code></pre></li> <li> <p>Configure your working forked copy to sync with the original <code>ULHPC/ulhpc-docs</code> repository through a dedicated <code>upstream</code> remote <pre><code># Check current remote: only 'origin' should be listed\n$&gt; git remote -v\norigin  git@github.com:YOUR-USERNAME/ulhpc-docs.git (fetch)\norigin  git@github.com:YOUR-USERNAME/ulhpc-docs.git (push)\n# Add upstream\n$&gt; make setup-upstream\n# OR, manually:\n$&gt; git remote add upstream https://github.com/ULHPC/ulhpc-docs.git\n# Check the new remote\n$&gt; git remote -v\norigin  git@github.com:YOUR-USERNAME/ulhpc-docs.git (fetch)\norigin  git@github.com:YOUR-USERNAME/ulhpc-docs.git (push)\nupstream https://github.com/ULHPC/ulhpc-docs.git (fetch)\nupstream https://github.com/ULHPC/ulhpc-docs.git (push)\n</code></pre></p> </li> <li> <p>At this level, you probably want to follow the setup instructions to configure your <code>ulhpc-docs</code> python virtualenv and deploy locally the documentation with <code>make doc</code></p> <ul> <li>access the local documentation with your favorite browser by visiting the URL http://localhost:8000</li> </ul> </li> </ul> </li> </ul> <p>Then, to bring your contributions:</p> <ol> <li>Pull the latest changes from the <code>upstream</code> remote using:    <pre><code>make sync-upstream\n</code></pre></li> <li>Create your own feature branch with appropriate name <code>&lt;name&gt;</code>:    <pre><code># IF you have installed git-flow: {brew | apt | yum |...} install gitflow git-flow\n# /!\\ ADAPT &lt;name&gt; with appropriate name: this will create and checkout to branch feature/&lt;name&gt;\ngit-flow feature start &lt;name&gt;\n# OR\ngit checkout -b feature/&lt;name&gt;\n</code></pre></li> <li>Commit your changes once satisfied with them    <pre><code>git add [...]\ngit commit -s -m 'Added some feature'\n</code></pre></li> <li>Push to the feature branch and publish it <pre><code># IF you have installed git-flow\n# /!\\ ADAPT &lt;name&gt; accordingly\ngit-flow feature publish &lt;name&gt;\n# OR\ngit push -u origin feature/&lt;name&gt;\n</code></pre></li> <li>Create a new Pull Request to submit your changes to the ULHPC team.</li> <li> <p>Commit first!    <pre><code># check what would be put in the pull request\ngit request-pull master ./\n# Open Pull Request from web interface\n# Github: Open 'new pull request'\n#      Base = feature/&lt;name&gt;,   compare = master\n</code></pre></p> </li> <li> <p>Pull request will be reviewed, eventually with comments/suggestion for modifications -- see official doc</p> </li> <li>you may need to apply new commits to resolve the comments -- remember to mention the pull request in the commit message with the prefix  '<code>[PR#&lt;ID&gt;]</code>' (Ex: <code>[PR#5]</code>) in your commit message    <pre><code>cd /path/to/ulhpc-docs\ngit checkout feature/&lt;name&gt;\ngit pull\n# [...]\ngit add [...]\n# /!\\ ADAPT Pull Request ID accordingly\ngit commit -s -m '[PR#&lt;ID&gt;] ...'\n</code></pre></li> </ol> <p>After your pull request has been reviewed and merged, you can safely delete the feature branch.</p> <pre><code># Adapt &lt;name&gt; accordingly\ngit checkout feature/&lt;name&gt; # Eventually, if needed\nmake sync-upstream\ngit-flow feature finish &lt;name&gt; # feature branch 'feature/&lt;name&gt;' will be merged into 'devel'\n#                              # feature branch 'feature/&lt;name&gt;' will be locally deleted\n#                              # you will checkout back to the 'master' branch\ngit push origin --delete feature/&lt;name&gt;   # /!\\ WARNING: Ensure you delete the CORRECT remote branch\ngit push  # sync master branch\n</code></pre>"},{"location":"contributing/#ulhpc-git-workflow","title":"ULHPC Git Workflow","text":"<p>Throughout all its projects, the ULHPC team has enforced a stricter workflow for Git repository summarized in the below figure:</p> <p></p> <p>The main concepts inherited from both advanced workflows (Git-flow and Github Flow) are listed below:</p> <ul> <li>The central repository holds two main branches with an infinite lifetime:<ul> <li><code>production</code>: the production-ready branch, used for the deployed version of the documentation.</li> <li><code>devel | master | main</code> (<code>master</code> in this case): the main (master) branch where the latest developments intervene (name depends on repository purpose). This is the default branch you get when you clone the repository.</li> </ul> </li> <li>You should always setup your local copy of the repository with <code>make setup</code><ul> <li>ensure also you have installed the <code>gitflow</code> extension</li> <li>ensure you are properly made the initial configuration of git -- see also sample <code>.gitconfig</code></li> </ul> </li> </ul> <p>In compliment to the Github Flow described above, several additional operations are facilitated by the root <code>Makefile</code>:</p> <ul> <li>Initial setup of the repository with <code>make setup</code></li> <li>Release of a new version of this repository with <code>make start_bump_{patch,minor,major}</code> and <code>make release</code><ul> <li>this action is managed by the ULHPC team according to the semantic versioning scheme implemented within this this project.</li> </ul> </li> </ul>"},{"location":"contributing/versioning/","title":"Semantic Versioning","text":"<p>The operation consisting of releasing a new version of this repository is automated by a set of tasks within the root <code>Makefile</code>. In this context, a version number have the following format:</p> <pre><code>  &lt;major&gt;.&lt;minor&gt;.&lt;patch&gt;[-b&lt;build&gt;]\n</code></pre> <p>where:</p> <ul> <li><code>&lt; major &gt;</code> corresponds to the major version number</li> <li><code>&lt; minor &gt;</code> corresponds to the minor version number</li> <li><code>&lt; patch &gt;</code> corresponds to the patching version number</li> <li>(eventually) <code>&lt; build &gt;</code> states the build number i.e. the total number of commits within the <code>devel</code> branch.</li> </ul> <p>Example: `1.0.0-b28`.</p> <p><code>VERSION</code> file</p> <p>The current version number is stored in the root file <code>VERSION</code>. /!\\ IMPORTANT: NEVER MAKE ANY MANUAL CHANGES TO THIS FILE</p> <p><code>ULHPC/docs</code> repository release</p> <p>Only the ULHPC team is allowed to perform the releasing operations (and push to the <code>production</code> branch). By default, the main documentation website is built against the <code>production</code> branch.</p> <p>For more information on the version, run:</p> <pre><code> $&gt; make versioninfo\n</code></pre> ULHPC Team procedure for repository release <p>If a new version number such be bumped, the following command is issued: <pre><code>make start_bump_{major,minor,patch}\n</code></pre> This will start the release process for you using <code>git-flow</code> within the <code>release/&lt;new-version&gt;</code> branch - see also Git(hub) flow. Once the last changes are committed, the release becomes effective by running: <pre><code>make release\n</code></pre> It will finish the release using <code>git-flow</code>, create the appropriate tag in the <code>production</code> branch and merge all things the way they should be in the <code>master</code> branch.</p>"},{"location":"data/backups/","title":"Backups","text":"<p>Backups are vital to safeguard important data. Always maintain a well defined backup policy. You can build your backup policy on top of the backup services offered in the UL HPC systems. A high level overview of the backup policies in place for the HPC systems is presented here. If you require specific details, please contact the HPC team directly.</p> <p>Limitations of backup policies in UL</p> <p>The UL HPC and SIU do not offer cold backups (offline backups). All our backups are maintained in live systems.</p> <p>All UL HPC users should back up important files on a regular basis. Ultimately, it is your responsibility to protect yourself from data loss.</p>"},{"location":"data/backups/#directories-on-the-ul-hpc-clusters-infrastructure","title":"Directories on the UL HPC clusters infrastructure","text":"<p>There are multiple cluster file systems used in UL HPC clusters to support computation jobs. These file systems are accessible through home, project, and scratch directories.</p> <p>The cluster file systems are not meant to be used for data storage, so there are minimal back ups created for files in the cluster file systems. The backups are only accessible by UL HPC staff for disaster recovery purposes only. The following table summarizes the backups kept for each file system mount point.</p> Directory Path Backup location Frequency Retention home directories <code>${HOME}</code> not backed up - scratch <code>${SCRATCH}</code> not backed up - projects <code>${PROJECTHOME}</code> CDC, Belval Weekly One backup per week of the backup directory ONLY (<code>${PROJECTHOME}/&lt;project name&gt;/backup/</code>). <p>Project backups</p> <p>Use the <code>backup</code> subdirectory in your project directories to store important configuration files for you projects that are specific to the UL HPC clusters.</p> <p>UL HPC backup policy</p> <p>Data are copied live from the GPFS file system to a backup server (due to limitation regarding snapshots in GPFS). The backup data are copied to a Disaster Recovery Site (DRS) in a location outside the server room where the primary backup server is located.</p>"},{"location":"data/backups/#directories-on-the-siu-isilon-infrastructure","title":"Directories on the SIU Isilon infrastructure","text":"<p>Projects stored on the Isilon system are snapshotted regularly. This includes the NFS export of Isilon in HL HPC systems, personal and project directories in Atlas, the SMB export of Isilon, but not the personal directories of the students exported through the Poseidon SMB export of Isilon. The following snapshot schedule and retention strategy are used:</p> Backed up snapshot Retention Daily 14 days Weekly 5 months Monthly 12 months <p>SIU back up policy</p> <p>Snapshots do not protect on themselves against a system failure, they only permit recovering files in case of accidental deletion. To ensure the safe storage, snapshots data is copied to a Disaster Recovery Site (DRS) in a location outside the server room where the primary data storage (Isilon) is located.</p> <p>Users can access some backed up data through the snapshots in Isilon file systems. This can help restoration after incidents such as accidental data deletion. Each project directory, in <code>/mnt/isilon/projects/</code> contains a hidden sub-directory <code>.snapshot</code>.</p> <ul> <li>The <code>.snapshot</code> directory is invisible to <code>ls</code>, but also to <code>ls -a</code>, <code>find</code> and similar commands.</li> <li>Snapshots can be browsed normally after changing into the snapshot directory (<code>cd .snapshot</code>).</li> <li>Files cannot be created, deleted, or edited in snapshots; files can only be copied out of a snapshot.</li> <li>Only a few, strategically selected snapshots are exposed to the users. See the section on backup restoration on how to access other snapshots if you need them.</li> </ul>"},{"location":"data/backups/#restore","title":"Restore","text":"<p>If you require the restoration of lost data in Isilon that cannot be accomplished via the snapshots capability, please create a new request on Service Now portal, with pathnames and timestamps of the missing data. Create a new request for all data stored in the <code>backup</code> directory of projects in the clustered file systems.</p> <p>Such restore requests may take a few days to complete.</p>"},{"location":"data/backups/#backup-tools","title":"Backup Tools","text":"<p>In practice, the ULHPC backup infrastructure is fully puppetized and make use of several tools facilitating the operations:</p> <ul> <li>backupninja, which allows you to coordinate system backup by dropping a few simple configuration files into <code>/etc/backup.d/</code>;</li> <li>a forked version of bontmia, which stands for \"Backup Over Network To Multiple Incremental Archives\";</li> <li>BorgBackup, a deduplicating backup program supporting compression and authenticated encryption;</li> <li>several internal scripts to pilot LVM snapshots/backup/restore operations.</li> </ul>"},{"location":"data/datasets/","title":"Datasets management","text":"<p>The ULHPC team provides central storage for various public and freely usable datasets and databases in the shared directory <code>/work/project/bigdata_sets/</code>. This directory is read-only for all users of the HPC platform. The advantage of this central location is twofold:</p> <ul> <li>Prevents data duplication: It avoids many users downloading the same dataset to their own directories.</li> <li>Performance: <code>/work/project/bigdata_sets/</code> is hosted on our fast storage tier.</li> <li>Scientific reproducibility: It ensures that different users can use the same version of the dataset.</li> </ul> <p>Users who use public data for their calculations should always check first if it's already available in <code>/work/project/bigdata_sets/</code>.</p>"},{"location":"data/datasets/#requesting-a-new-dataset","title":"Requesting a New Dataset","text":"<p>You can request to host a new dataset in this dedicated space on service.uni.lu (Section Research &gt; HPC &gt; Storage &amp; projects &gt; Request a dataset upload). </p> <p>Note that some datasets may not be shareable publicly in this directory due to licensing or distribution constraints.</p>"},{"location":"data/datasets/#data-manipulation","title":"Data Manipulation","text":"<p>Some datasets contain millions of small files. Manipulating this amount of small files is a worst-case scenario for performance on any parallel filesystem. Additionally, it may not be feasible to manipulate such a volume of files in user directories due to inode quotas.</p> <p>Instead, we advise keeping the dataset archived in <code>tar</code> format on the shared filesystem and uncompressing the archives in the <code>/tmp/</code> directory at the beginning of your job. Iris and Aion feature a local SSD or NVME that is user-writeable and mounted in <code>/tmp</code> for temporary files.</p> <p>To uncompress such a file efficiently, you can use <code>pigz</code>, which is a parallelized implementation of <code>gzip</code>:</p> <pre><code>cd /tmp\ntar -I pigz -xf /scratch/users/hcartiaux/dataset.tar.gz\n</code></pre> <p>For example, uncompressing the Malnet image dataset (an 80GB tarball) into the /tmp/ directory of an Aion node takes less than 5 minutes. This is an order of magnitude faster than manipulating over one billion files on any of our shared filesystems.</p>"},{"location":"data/encryption/","title":"Sensitive Data Protection","text":"<p>The advent of the EU General Data Protection Regulation (GDPR) permitted to highlight the need to protect sensitive information from leakage.</p>"},{"location":"data/encryption/#gpg","title":"GPG","text":"<p>A basic approach relies on GPG to encrypt single files -- see this tutorial for more details</p> <pre><code># File encryption\n$ gpg --encrypt [-r &lt;recipient&gt;] &lt;file&gt;     # =&gt; produces &lt;file&gt;.gpg\n$ rm -f &lt;file&gt;    # /!\\ WARNING: encryption DOES NOT delete the input (clear-text) file\n$ gpg --armor --detach-sign &lt;file&gt;          # Generate signature file &lt;file&gt;.asc\n\n# Decryption\n$ gpg --verify &lt;file&gt;.asc           # (eventually but STRONGLY encouraged) verify signature file\n$ gpg --decrypt &lt;file&gt;.gpg          # Decrypt PGP encrypted file\n</code></pre> <p>One drawback is that files need to be completely decrypted for processing</p> <p> Tutorial: Using GnuPG aka Gnu Privacy Guard aka GPG</p>"},{"location":"data/encryption/#file-encryption-frameworks-encfs-gocryptfs","title":"File Encryption Frameworks (EncFS, GoCryptFS...)","text":"<p>In contrast to disk-encryption software that operate on whole disks (TrueCrypt, dm-crypt etc), file encryption operates on individual files that can be backed up or synchronised easily, especially within a Git repository.</p> <ul> <li>Comparison matrix</li> <li>gocryptfs, aspiring successor of EncFS written in Go</li> <li>EncFS, mature with known security issues</li> <li>eCryptFS, integrated into the Linux kernel</li> <li>Cryptomator, strong cross-platform support through Java and WebDAV</li> <li>securefs, a cross-platform project implemented in C++.</li> <li>CryFS, result of a master thesis at the KIT University that uses chunked storage to obfuscate file sizes.</li> </ul> <p>Assuming you are working from <code>/path/to/my/project</code>, your workflow (mentionned below for EncFS, but it can be adpated to all the other tools) operated on encrypted vaults and would be as follows:</p> <ul> <li>(eventually) if operating within a working copy of a git repository, you should ignore the mounting directory (ex: <code>vault/*</code>) in the root <code>.gitignore</code> of the repository<ul> <li>this ensures neither you nor a collaborator will commit any unencrypted version of a file by mistake</li> <li>you commit only the EncFS / GocryptFS / eCryptFS / Cryptomator / securefs / CryFS raw directory (ex: <code>.crypt/</code>) in your repository. Thus only encrypted form or your files are commited</li> </ul> </li> <li>You create the EncFS / GocryptFS / eCryptFS / Cryptomator / securefs / CryFS encrypted vault</li> <li>You prepare macros/scripts/Makefile/Rakefile tasks to lock/unlock the vault on demand</li> </ul> <p>Here are for instance a few example of these operations in live to create a encrypted vault:</p> EncFSGoCryptFS <pre><code>$ cd /path/to/my/project\n$ rawdir=.crypt      # /!\\ ADAPT accordingly\n$ mountdir=vault     # /!\\ ADAPT accordingly\n#\n# (eventually) Ignore the mount dir\n$ echo $mountdir &gt;&gt; .gitignore\n### EncFS: Creation of an EncFS vault (only once)\n$ encfs --standard $rawdir $mountdir\n</code></pre> <p>you SHOULD be on a computing node to use GoCryptFS.</p> <pre><code>$ cd /path/to/my/project\n$ rawdir=.crypt      # /!\\ ADAPT accordingly\n$ mountdir=vault     # /!\\ ADAPT accordingly\n#\n# (eventually) Ignore the mount dir\n$ echo $mountdir &gt;&gt; .gitignore\n### GoCryptFS: load the module - you SHOULD be on a computing node\n$ module load tools/gocryptfs\n# Creation of a GoCryptFS vault (only once)\n$&gt; gocryptfs -init $rawdir\n</code></pre> <p>Then you can mount/unmount the vault as follows:</p> Tool OS Opening/Unlocking the vault Closing/locking the vault EncFS Linux <code>encfs -o nonempty  --idle=60  $rawdir $mountdir</code> <code>fusermount -u $mountdir</code> EncFS Mac OS <code>encfs  --idle=60  $rawdir $mountdir</code> <code>umount $mountdir</code> GocryptFS <code>gocryptfs $rawdir $mountdir</code> as above <p>The fact that GoCryptFS is available as a module brings the advantage that it can be mounted in a view folder (<code>vault/</code>) where you can read and write the unencrypted files, which is Automatically unmounted upon job termination.</p>"},{"location":"data/encryption/#file-encryption-using-ssh-rsa-key-pairs","title":"File Encryption using SSH [RSA] Key Pairs","text":"<ul> <li>Man pages: <code>openssl rsa</code>, <code>openssl rsautl</code> and <code>openssl enc</code></li> <li>Tutorial: Encryption with RSA Key Pairs</li> <li>Tutorial: How to encrypt a big file using OpenSSL and someone's public key</li> <li>OpenSSL Command-Line HOWTO, in particular the section 'How do I simply encrypt a file?'</li> </ul> <p>If you encrypt/decrypt files or messages on more than a one-off occasion, you should really use GnuPGP as that is a much better suited tool for this kind of operations. But if you already have someone's public SSH key, it can be convenient to use it, and it is safe.</p> <p>Warning</p> <p>The below instructions are NOT compliant with the new OpenSSH format which is used for storing encrypted (or unencrypted) RSA, EcDSA and Ed25519 keys (among others) when you use the <code>-o</code> option of <code>ssh-keygen</code>. You can recognize these keys by the fact that the private SSH key <code>~/.ssh/id_rsa</code> starts with <code>- ----BEGIN OPENSSH PRIVATE KEY-----</code></p>"},{"location":"data/encryption/#encrypt-a-file-using-a-public-ssh-key","title":"Encrypt a file using a public SSH key","text":"<p>(eventually) SSH RSA public key conversion to PEM PKCS8</p> <p>OpenSSL encryption/decryption operations performed  using the RSA algorithm relies on keys following the PEM format <sup>1</sup> (ideally in the  PKCS#8 format). It is possible to convert OpenSSH public keys (private ones are already compliant) to the PEM PKCS8 format (a more secure format). For that one can either use the <code>ssh-keygen</code> or the <code>openssl</code> commands, the first one being recomm ended.</p> <p><pre><code># Convert the public key of your collaborator to the PEM PKCS8 format (a more secure format)\n$ ssh-keygen -f id_dst_rsa.pub -e -m pkcs8 &gt; id_dst_rsa.pkcs8.pub\n# OR use OpenSSL for that...\n$ openssl rsa -in id_dst_rsa -pubout -outform PKCS8 &gt; id_dst_rsa.pkcs8.pub\n</code></pre> Note that you don't actually need to save the PKCS#8 version of his public key file -- the below command will make this conversion on demand.</p> <p>Generate a 256 bit (32 byte) random symmetric key</p> <p>There is a limit to the maximum length of a message i.e. size of a file  that can be encrypted using asymmetric RSA public key encryption keys (which is what SSH ke ys are). For this reason, you should better rely on a 256 bit key to use for symmetric AES encryption and then encrypt/decrypt that symmetric AES key with the asymmetric RSA k eys This is how encrypted connections usually work, by the way.</p> <p>Generate the unique symmetric key <code>key.bin</code> of 32 bytes (i.e. 256 bit) as follows:</p> <pre><code>openssl rand -base64 32 -out key.bin\n</code></pre> <p>You should only use this key once. If you send something else to the recipient at another time, you should regenerate another key.</p> <p>Encrypt the (potentially big) file with the symmetric key</p> <pre><code>openssl enc -aes-256-cbc -salt -in bigdata.dat -out bigdata.dat.enc  -pass file:./key.bin\n</code></pre> Indicative performance of OpenSSL Encryption time <p>You can quickly generate random files of 1 or 10 GiB size as follows: <pre><code># Random generation of a 1GiB file\n$ dd if=/dev/urandom of=bigfile_1GiB.dat  bs=64M count=16  iflag=fullblock\n# Random generation of a 1GiB file\n$ dd if=/dev/urandom of=bigfile_10GiB.dat bs=64M count=160 iflag=fullblock\n</code></pre> An indicated encryption time taken for above generated random file on a local laptop (Mac OS X, local filesystem over SSD) is proposed in the below table, using <pre><code>openssl enc -aes-256-cbc -salt -in bigfile_&lt;N&gt;GiB.dat -out bigfile_&lt;N&gt;GiB.dat.enc  -pass file:./key.bin\n</code></pre></p> File size Encryption time <code>bigfile_1GiB.dat</code> 1 GiB 0m5.395s <code>bigfile_10GiB.dat</code> 10 GiB 2m50.214s <p>Encrypt the symmetric key, using your collaborator public SSH key in PKCS8 format:</p> <pre><code>$ openssl rsautl -encrypt -pubin -inkey &lt;(ssh-keygen -e -m PKCS8 -f id_dst_rsa.pub) -in key.bin -out key.bin.enc\n# OR, if you have a copy of the PKCS#8 version of his public key\n$ openssl rsautl -encrypt -pubin -inkey  id_dst_rsa.pkcs8.pub -in key.bin -out key.bin.enc\n</code></pre> <p>Delete the unencrypted symmetric key as you don't need it any more (and you should not use it anymore)</p> <pre><code>  $&gt; rm key.bin\n</code></pre> <p>Now you can transfer the <code>*.enc</code> files i.e. send the (potentially big) encrypted file <code>&lt;file&gt;.enc</code> and the encrypted symmetric key (i.e. <code>key.bin.enc</code> ) to the recipient _i.e. your collaborator. Note that you are encouraged to send the encrypted file and the encrypted key separately. Although it's not absolutely necessary, it's good practice to separate the two. If you're allowed to, transfer them by SSH to an agreed remote server. It is even safe to upload the files to a public file sharing service and tell the recipient to download them from there.</p>"},{"location":"data/encryption/#decrypt-a-file-encrypted-with-a-public-ssh-key","title":"Decrypt a file encrypted with a public SSH key","text":"<p>First decrypt the symmetric key using the SSH private counterpart:</p> <pre><code># Decrypt the key -- /!\\ ADAPT the path to the private SSH key\n$ openssl rsautl -decrypt -inkey ~/.ssh/id_rsa -in key.bin.enc -out key.bin\nEnter pass phrase for ~/.ssh/id_rsa:\n</code></pre> <p>Now the (potentially big) file can be decrypted, using the symmetric key:</p> <pre><code>openssl enc -d -aes-256-cbc -in bigdata.dat.enc -out bigdata.dat -pass file:./key.bin\n</code></pre>"},{"location":"data/encryption/#misc-qd-for-small-files","title":"Misc Q&amp;D for small files","text":"<p>For a 'quick and dirty' encryption/decryption of small files:</p> <pre><code># Encrypt\n$  openssl rsautl -encrypt -inkey &lt;(ssh-keygen -e -m PKCS8 -f ~/.ssh/id_rsa.pub) -pubin -in &lt;cleartext_file&gt;.dat -out &lt;encrypted_file&gt;.dat.enc\n# Decrypt\n$ openssl rsautl -decrypt -inkey ~/.ssh/id_rsa -in &lt;encrypted_file&gt;.dat.enc -out &lt;cleartext_file&gt;.dat\n</code></pre>"},{"location":"data/encryption/#data-encryption-in-git-repository-with-git-crypt","title":"Data Encryption in Git Repository with <code>git-crypt</code>","text":"<p>It is of course even more important in the context of git repositories, whether public or private, since the disposal of a working copy of the repository enable the access to the full history of commits, in particular the ones eventually done by mistake (<code>git commit -a</code>) that used to include sensitive files. That's where git-crypt comes for help. It is an open source, command line utility that empowers developers to protect specific files within a git repository.</p> <p>git-crypt enables transparent encryption and decryption of files in a git repository. Files which you choose to protect are encrypted when committed, and decrypted when checked out. git-crypt lets you freely share a repository containing a mix of public and private content. git-crypt gracefully degrades, so developers without the secret key can still clone and commit to a repository with encrypted files. This lets you store your secret material (such as keys or passwords) in the same repository as your code, without requiring you to lock down your entire repository.</p> <p>The biggest advantage of git-crypt is that private data and public data can live in the same location.</p>"},{"location":"data/encryption/#petasuite-protect","title":"PetaSuite Protect","text":"<p>PetaSuite is a compression suite for Next-Generation-Sequencing (NGS) data. It consists of a command-line tool and a user-mode library. The command line tool performs compression and decompression operations on files. The user-mode library allows other tools and pipelines to transparently access the NGS data in their original file formats.</p> <p>PetaSuite is used within LCSB and provides the following features:</p> <ul> <li>Encrypt and compress genomic data</li> <li>Encryption keys and access managed centrally</li> <li>Decryption and decompression on-the-fly using a library that intercepts all FS access</li> </ul> <p>This is a commercial software -- contact <code>lcsb.software@uni.lu</code> if you would like to use it</p> <ol> <li> <p>Defined in RFCs 1421 through 1424, is a container format for public/private keys or certificates used preferentially by open-source software such as OpenSSL. The name is from Privacy Enhanced Mail (PEM) (a failed method for secure email, but the container format it used lives on, and is a base64 translation of the x509 ASN.1 keys.\u00a0\u21a9</p> </li> </ol>"},{"location":"data/gdpr/","title":"GDPR Compliance","text":"<p> UL HPC Acceptable Use Policy (AUP) (pdf)</p> <p>Warning</p> <p>Personal data is/may be visible, accessible or handled:</p> <ul> <li>directly on the HPC clusters</li> <li>through Resource and Job Management System (RJMS) tools (Slurm) and associated monitoring interfaces</li> <li>through service portals (like OpenOnDemand)</li> <li>on code management portals such GitLab, GitHub</li> <li>on secondary storage systems used within the University such as Atlas, DropIT, etc.</li> </ul>"},{"location":"data/gdpr/#data-use","title":"Data Use","text":"<p>Use of UL HPC data storage resources (file systems, data storage tiers, backup, etc.) should be used only for work directly related to the projects for which the resources were requested and granted, and primarily to advance University\u2019s missions of education and research. Use of UL HPC data resources for personal activities is prohibited.</p> <p>The UL HPC Team maintains up-to-date documentation on its data storage resources and their proper use, and provides regular training and support to users. Users assume the responsibility for following the documentation, training sessions and best practice guides in order to understand the proper and considerate use of the UL HPC data storage resources.</p> <p>Authors/generators/owners of information or data are responsible for its correct categorization as sensitive or non-sensitive. Owners of sensitive information are responsible for its secure handling, transmission, processing, storage, and disposal on the UL HPC systems. The UL HPC Team recommends use of encryption to protect the data from unauthorized access. Data Protection inquiries, especially as regards sensitive information processing can be directed to the Data Protection Officer.</p> <p>Users are prohibited from intentionally accessing, modifying or deleting data they do not own or have not been granted explicit permission to access.</p> <p>Users are responsible to ensure the appropriate level of protection, backup and integrity checks on their critical data and applications. It is their responsibility to set appropriate access controls for the data they bring, process and generate on UL HPC facilities.</p> <p>In the event of system failure or malicious actions, UL HPC makes no guarantee against loss of data or that user or project data can be recovered nor that it cannot be accessed, changed, or deleted by another individual.</p>"},{"location":"data/gdpr/#personal-information-agreement","title":"Personal information agreement","text":"<p>UL HPC retains the right to monitor all activities on its facilities.</p> <p>Users acknowledge that data regarding their activity on UL HPC facilities will be collected. The data is collected (e.g. by the Slurm workload manager) for utilization accounting and reporting purposes, and for the purpose of understanding typical patterns of user\u2019s behavior on the system in order to further improve the services provided by UL HPC. Another goal is to identify intrusions, misuse, security incidents or illegal actions in order to protect UL HPC users and facilities..</p> <p>Users agree that this data may be processed to extract information contributing to the above stated purposes.</p> <p>Users agree that their name, surname, email address, affiliation, work place and phone numbers are processed by the UL HPC Team in order to provide HPC and associated services.</p> <p>Data Protection inquiries can be directed to the Data Protection Officer. Further information about Data Protection can be found at: https://wwwen.uni.lu/university/data_protection</p>"},{"location":"data/project/","title":"Project Data Management","text":"<p>Project directories share files within a group of researchers, and are accessible under the path <code>/work/projects/&lt;project name&gt;</code> in every cluster node. Use project directories to share files and to store large data files that are actively used in computations.</p> <ul> <li>Project directories are not mirrored in Tier 0 storage, so small file I/O performance is lower that the home directory.</li> <li>Since project directories are not mirrored in Tier 0 storage, the available storage space is much larger.</li> <li>The GPFS file system storing the project directories is redundant, so it is safe to data and access loss due to hardware failure.</li> </ul> <p>The environment variable <code>${PROJECTHOME}</code> points to the parent directory of all projects (<code>/work/projects</code>). The absolute path to the project home directory may change, but <code>${PROJECTHOME}</code> is guaranteed to point to the parent directory of all projects directories.</p> <p>Research Project Allocations, Accounting and Reporting</p> <p>The Research Support and Accounting Departments of the University keep track of the list of research projects funded within the University. Starting 2021, a new procedure has been put in place to provide a detailed reporting of the HPC usage for such projects. As part of this process, the following actions are taken by the ULHPC team:</p> <ol> <li>a dedicated project account <code>&lt;name&gt;</code> (normally the acronym of the project) is created for accounting purpose at the Slurm level (L3 account - see Account Hierarchy);</li> <li>a dedicated project directory with the same name (<code>&lt;name&gt;</code>) is created, allowing to share data within a group of project researchers, under <code>$PROJECTHOME/&lt;name&gt;</code>, i.e., <code>/work/projects/&lt;name&gt;</code></li> </ol> <p>You are then entitled to submit jobs associated to the project using <code>-A &lt;name&gt;</code> such that the HPC usage is reported accurately. The ULHPC team will provide to the project PI (Principal Investigator) and the Research Support department a regular report detailing the corresponding HPC usage. In all cases, job billing under the conditions defined in the Job Accounting and Billing section may apply.</p>"},{"location":"data/project/#new-project-directory","title":"New project directory","text":"<p>You can request a new project directory under ServiceNow (HPC \u2192 Storage &amp; projects \u2192 Request for a new project).</p>"},{"location":"data/project/#quotas-and-backup-policies","title":"Quotas and Backup Policies","text":"<p>See quotas for detailed information about inode, space quotas, and file system purge policies. Your projects backup directories are backed up weekly, according to the policy detailed in the ULHPC backup policies.</p> <p>Access rights to project directory: Quota for <code>clusterusers</code> group in project directories is 0 !!!</p> <p>When a project <code>&lt;name&gt;</code> is created, a group of the same name (<code>&lt;name&gt;</code>) is also created and researchers allowed to collaborate on the project are made members of this group,which grant them access to the project directory.</p> <p>Be aware that your default group as a user is <code>clusterusers</code> which has (on purpose) a quota in project directories set to 0. You thus need to ensure you always write data in your project directory using the <code>&lt;name&gt;</code> group (instead of yoru default one.). This can be achieved by ensuring the setgid bit is set on all folders in the project directories: <code>chmod g+s [...]</code></p> <p>When using <code>rsync</code> to transfer file toward the project directory <code>/work/projects/&lt;name&gt;</code> as destination, be aware that rsync will not use the correct permissions when copying files into your project directory. As indicated in the Data transfer section, you also need to:</p> <ul> <li>give new files the destination-default permissions with <code>--no-p</code> (<code>--no-perms</code>), and</li> <li>use the default group <code>&lt;name&gt;</code> of the destination dir with <code>--no-g</code> (<code>--no-group</code>)</li> <li>(eventually) instruct rsync to preserve whatever executable permissions existed on the source file and aren't masked at the destination using <code>--chmod=ug=rwX</code></li> </ul> <p>Your full <code>rsync</code> command becomes (adapt accordingly):</p> <pre><code>  rsync -avz {--update | --delete} --no-p --no-g [--chmod=ug=rwX] &lt;source&gt; /work/projects/&lt;name&gt;/[...]\n</code></pre> <p>For the same reason detailed above, in case you are using a build command or more generally any command meant to write data in your project directory <code>/work/projects/&lt;name&gt;</code>, you want to use the <code>sg</code> as follows:</p> <pre><code># /!\\ ADAPT &lt;name&gt; accordingly\nsg &lt;name&gt; -c \"&lt;command&gt; [...]\"\n</code></pre> <p>This is particularly important if you are building dedicated software with Easybuild for members of the project - you typically want to do it as follows:</p> <pre><code># /!\\ ADAPT &lt;name&gt; accordingly\nsg &lt;name&gt; -c \"eb [...] -r --rebuild -D\"   # Dry-run - enforce using the '&lt;name&gt;' group\nsg &lt;name&gt; -c \"eb [...] -r --rebuild\"      # Dry-run - enforce using the '&lt;name&gt;' group\n</code></pre>"},{"location":"data/project/#project-directory-modification","title":"Project directory modification","text":"<p>You can request changes for your project directory (quotas extension, add/remove a group member) under ServiceNow:</p> <ul> <li>HPC \u2192 Storage &amp; projects \u2192 Extend quota/Request information</li> <li>HPC \u2192 User access &amp; accounts \u2192 Add/Remove user within project</li> </ul>"},{"location":"data/project_acl/","title":"Project acl","text":"<p>Global Project quotas and backup policies</p> <p>See quotas for detailed information about inode, space quotas, and file system purge policies. Your projects backup directories are backuped weekly, according to the policy detailed in the ULHPC backup policies.</p> <p>Access rights to project directory: Quota for <code>clusterusers</code> group in project directories is 0 !!!</p> <p>When a project <code>&lt;name&gt;</code> is created, a group of the same name (<code>&lt;name&gt;</code>) is also created and researchers allowed to collaborate on the project are made members of this group,which grant them access to the project directory.</p> <p>Be aware that your default group as a user is <code>clusterusers</code> which has (on purpose) a quota in project directories set to 0. You thus need to ensure you always write data in your project directory using the <code>&lt;name&gt;</code> group (instead of yoru default one.). This can be achieved by ensuring the setgid bit is set on all folders in the project directories: <code>chmod g+s [...]</code></p> <p>When using <code>rsync</code> to transfer file toward the project directory <code>/work/projects/&lt;name&gt;</code> as destination, be aware that rsync will not use the correct permissions when copying files into your project directory. As indicated in the Data transfer section, you also need to:</p> <ul> <li>give new files the destination-default permissions with <code>--no-p</code> (<code>--no-perms</code>), and</li> <li>use the default group <code>&lt;name&gt;</code> of the destination dir with <code>--no-g</code> (<code>--no-group</code>)</li> <li>(eventually) instruct rsync to preserve whatever executable permissions existed on the source file and aren't masked at the destination using <code>--chmod=ug=rwX</code></li> </ul> <p>Your full <code>rsync</code> command becomes (adapt accordingly):</p> <pre><code>  rsync -avz {--update | --delete} --no-p --no-g [--chmod=ug=rwX] &lt;source&gt; /work/projects/&lt;name&gt;/[...]\n</code></pre> <p>For the same reason detailed above, in case you are using a build command or more generally any command meant to write data in your project directory <code>/work/projects/&lt;name&gt;</code>, you want to use the <code>sg</code> as follows:</p> <pre><code># /!\\ ADAPT &lt;name&gt; accordingly\nsg &lt;name&gt; -c \"&lt;command&gt; [...]\"\n</code></pre> <p>This is particularly important if you are building dedicated software with Easybuild for members of the project - you typically want to do it as follows:</p> <pre><code># /!\\ ADAPT &lt;name&gt; accordingly\nsg &lt;name&gt; -c \"eb [...] -r --rebuild -D\"   # Dry-run - enforce using the '&lt;name&gt;' group\nsg &lt;name&gt; -c \"eb [...] -r --rebuild\"      # Dry-run - enforce using the '&lt;name&gt;' group\n</code></pre>"},{"location":"data/sharing/","title":"Data Sharing","text":""},{"location":"data/sharing/#security-and-data-integrity","title":"Security and Data Integrity","text":"<p>Sharing data with other users must be done carefully. Permissions should be set to the minimum necessary to achieve the desired access. For instance, consider carefully whether it's really necessary before sharing write permssions on data. Be sure to have archived backups of any critical shared data. It is also important to ensure that private login secrets (such as SSH private keys or apache htaccess files) are NOT shared with other users (either intentionally or accidentally). Good practice is to keep things like this in a separare directory that is as locked down as possible.</p> <p>The very first protection is to maintain your Home with access rights <code>700</code></p> <pre><code>chmod 700 $HOME\n</code></pre>"},{"location":"data/sharing/#sharing-data-within-ulhpc-facility","title":"Sharing Data within ULHPC Facility","text":""},{"location":"data/sharing/#sharing-with-other-members-of-your-project","title":"Sharing with Other Members of Your Project","text":"<p>We can setup a project directory with specific group read and write permissions, allowing to share data with other members of your project.</p>"},{"location":"data/sharing/#sharing-with-ulhpc-users-outside-of-your-project","title":"Sharing with ULHPC Users Outside of Your Project","text":""},{"location":"data/sharing/#unix-file-permissions","title":"Unix File Permissions","text":"<p>You can share files and directories with ULHPC users outside of your project by adjusting the unix file permissions. We have an extensive write up of unix file permissions and how they work here.</p>"},{"location":"data/sharing/#sharing-data-outside-of-ulhpc","title":"Sharing Data outside of ULHPC","text":"<p>The IT service of the University can be contacted to easily and quickly share data over the web using a dedicated Data Transfer service. Open the appropriate ticket on the Service Now portal.</p>"},{"location":"data/transfer/","title":"Data Transfer to/from/within UL HPC Clusters","text":""},{"location":"data/transfer/#introduction","title":"Introduction","text":"<p>Directories such as <code>$HOME</code>, <code>$WORK</code> or <code>$SCRATCH</code> are shared among the nodes of the cluster that you are using (including the login node) via shared filesystems (SpectrumScale, Lustre) meaning that:</p> <ul> <li>every file/directory pushed or created on the login node is available on the computing nodes</li> <li>every file/directory pushed or created on the computing nodes is available on the login node</li> </ul> <p>The two most common commands you can use for data transfers over SSH:</p> <ul> <li><code>scp</code>: for the full transfer of files and directories (only works fine for single files or directories of small/trivial size)</li> <li><code>rsync</code>: a software application which synchronizes files and directories from one location to another while minimizing data transfer as only the outdated or inexistent elements are transferred (practically required for lengthy complex transfers, which are more likely to be interrupted in the middle).</li> </ul> <p>scp or rsync?</p> <p>While both ensure a secure transfer of the data within an encrypted tunnel, <code>rsync</code> should be preferred: as mentionned in the from openSSH 8.0 release notes: \"The <code>scp</code> protocol is outdated, inflexible and not readily fixed. We recommend the use of more modern protocols like sftp and rsync for file transfer instead\".</p> <p><code>scp</code> is also relatively slow when compared to <code>rsync</code> as exhibited for instance in the below sample Distem experience:</p> <p></p> <p>You will find below notes on <code>scp</code> usage, but kindly prefer to use rsync.</p> Consider <code>scp</code> as deprecated! Click nevertheless to get usage details <p><code>scp</code> (see scp(1) ) or secure copy is probably the easiest of all the methods. The basic syntax is as follows:</p> <pre><code>scp [-P 8022] [-Cr] source_path destination_path\n</code></pre> <ul> <li>the <code>-P</code> option specifies the SSH port to use (in this case 8022)</li> <li>the <code>-C</code> option activates the compression (actually, it passes the -C flag to ssh(1) to enable compression).</li> <li>the <code>-r</code> option states to recursively copy entire directories (in this case, <code>scp</code> follows symbolic links encountered in the tree traversal).  Please note that in this case, you must specify the source file as a directory for this to work.</li> </ul> <p>The syntax for declaring a remote path is as follows on the cluster:  <code>yourlogin@iris-cluster:path/from/homedir</code></p> <p>Transfer from your local machine to the remote cluster login node</p> <p>For instance, let's assume you have a local directory <code>~/devel/myproject</code> you want to transfer to the cluster, in your remote homedir.</p> <pre><code># /!\\ ADAPT yourlogin to... your ULHPC login\n$&gt; scp -P 8022 -r ~/devel/myproject yourlogin@iris-cluster:\n</code></pre> <p>This will transfer recursively your local directory <code>~/devel/myproject</code>  on the cluster login node (in your homedir).</p> <p>Note that if you configured (as advised elsewhere) the SSH connection in your <code>~/.ssh/config</code> file, you can use a much simpler syntax:</p> <pre><code>$&gt; scp -r ~/devel/myproject iris-cluster:\n</code></pre> <p>Transfer from the remote cluster front-end to your local machine</p> <p>Conversely, let's assume you want to retrieve the files <code>~/experiments/parallel_run/*</code> <pre><code>$&gt; scp -P 8022 yourlogin@iris-cluster:experiments/parallel_run/* /path/to/local/directory\n</code></pre></p> <p>Again, if you configured the SSH connection in your <code>~/.ssh/config</code> file, you can use a simpler syntax:</p> <pre><code>$&gt; scp iris-cluster:experiments/parallel_run/* /path/to/local/directory\n</code></pre> <p>See the scp(1) man page or <code>man scp</code> for more details.</p> <p>Danger</p> <p><code>scp</code> SHOULD NOT be used in the following cases:</p> <ul> <li>When you are copying more than a few files, as scp spawns a new process for each file and can be quite slow and resource intensive when copying a large number of files.</li> <li>When using the <code>-r</code> switch, scp does not know about symbolic links and will blindly follow them, even if it has already made a copy of the file. That can lead to scp copying an infinite amount of data and can easily fill up your hard disk (or worse, a system shared disk), so be careful.</li> </ul> <p>N.B. There are many alternative ways to transfer files in HPC platforms and you should check your options according to the problem at hand.</p> <p>Windows and OS X users may wish to transfer files from their systems to the clusters' login nodes with easy-to-use GUI applications such as:</p> <ul> <li>WinSCP (Windows only)</li> <li>FileZilla Client (Windows, OS X)</li> <li>Cyberduck (Windows, OS X)</li> </ul> <p>These applications will need to be configured to connect to the frontends with the same parameters as discussed on the SSH access page.</p>"},{"location":"data/transfer/#using-rsync","title":"Using <code>rsync</code>","text":"<p>The clever alternative to <code>scp</code> is <code>rsync</code>, which has the advantage of transferring only the files which differ between the source and the destination. This feature is often referred to as fast incremental file transfer. Additionally, symbolic links can be  preserved. The typical syntax of <code>rsync</code> (see rsync(1) ) for the cluster is similar to the one of <code>scp</code>:</p> <pre><code># /!\\ ADAPT &lt;/path/to/source&gt; and &lt;/path/to/destination&gt;\n# From LOCAL directory (/path/to/local/source) toward REMOTE server &lt;hostname&gt;\nrsync --rsh='ssh -p 8022' -avzu /path/to/local/source  [user@]hostname:/path/to/destination\n# Ex: from REMOTE server &lt;hostname&gt; to LOCAL directory\nrsync --rsh='ssh -p 8022' -avzu [user@]hostname:/path/to/source  /path/to/local/destination\n</code></pre> <ul> <li>the <code>--rsh</code> option specifies the connector to use (here SSH on port 8022)</li> <li>the <code>-a</code> option corresponds to the \"Archive\" mode. Most likely you should always keep this on as it preserves file permissions and does not follow symlinks.</li> <li>the <code>-v</code> option enables the verbose mode</li> <li>the <code>-z</code> option enable compression, this will compress each file as it gets sent over the pipe. This can greatly decrease time, depending on what sort of files you are copying.</li> <li>the <code>-u</code> option (or <code>--update</code>) corresponds to an updating process which skips files that are newer on the receiver. At this level, you may prefer the more dangerous option <code>--delete</code> that deletes extraneous files from dest dirs. Just like <code>scp</code>, the syntax for qualifying a remote path is as follows on the cluster: <code>yourlogin@iris-cluster:path/from/homedir</code></li> </ul>"},{"location":"data/transfer/#transfer-from-your-local-machine-to-the-remote-cluster","title":"Transfer from your local machine to the remote cluster","text":"<p>Coming back to the previous examples, let's assume you have a local directory <code>~/devel/myproject</code> you want to transfer to the cluster, in your remote homedir. In that case:</p> <p><pre><code># /!\\ ADAPT yourlogin to... your ULHPC login\n$&gt; rsync --rsh='ssh -p 8022' -avzu ~/devel/myproject yourlogin@access-iris.uni.lu:\n</code></pre> This will synchronize your local directory <code>~/devel/myproject</code>  on the cluster front-end (in your homedir).</p> <p>Transfer to Iris, Aion or both?</p> <p>The above example target the access server of Iris. Actually, you could have targetted the access server of Aion: it doesn't matter since the storage is SHARED between both clusters.</p> <p>Note that if you configured (as advised above) your SSH connection in your <code>~/.ssh/config</code> file with a dedicated SSH entry <code>{iris,aion}-cluster</code>, you can use a simpler syntax:</p> <pre><code>$&gt; rsync -avzu ~/devel/myproject iris-cluster:\n# OR (it doesn't matter)\n$&gt; rsync -avzu ~/devel/myproject aion-cluster:\n</code></pre>"},{"location":"data/transfer/#transfer-from-your-local-machine-to-a-project-directory-on-the-remote-cluster","title":"Transfer from your local machine to a project directory on the remote cluster","text":"<p>When transferring data to a project directory you should keep the group and group permissions imposed by the project directory and quota. Therefore you need to add the options <code>--no-p --no-g</code> to your rsync command:</p> <pre><code>$&gt; rsync -avP --no-p --no-g ~/devel/myproject iris-cluster:/work/projects/myproject/\n</code></pre>"},{"location":"data/transfer/#transfer-from-the-remote-cluster-to-your-local-machine","title":"Transfer from the remote cluster to your local machine","text":"<p>Conversely, let's assume you want to synchronize (retrieve) the remote files <code>~/experiments/parallel_run/*</code> on your local machine:</p> <pre><code># /!\\ ADAPT yourlogin to... your ULHPC login\n$&gt; rsync --rsh='ssh -p 8022' -avzu yourlogin@access-iris.uni.lu:experiments/parallel_run /path/to/local/directory\n</code></pre> <p>Again, if you configured the SSH connection in your <code>~/.ssh/config</code> file, you can use a simpler syntax:</p> <pre><code>$&gt; rsync -avzu iris-cluster:experiments/parallel_run /path/to/local/directory\n# OR (it doesn't matter)\n$&gt; rsync -avzu aion-cluster:experiments/parallel_run /path/to/local/directory\n</code></pre> <p>As always, see the man page or <code>man rsync</code> for more details.</p> Windows Subsystem for Linux (WSL) <p>In WSL, the home directory in Linux virtual machines is not your home directory in Windows. If you want to access the files that you downloaded with <code>rsync</code> inside a Linux virtual machine, please consult the WSL documentation and the file system section in particular.</p>"},{"location":"data/transfer/#data-transfer-within-project-directories","title":"Data Transfer within Project directories","text":"<p>The ULHPC facility features a Global Project directory <code>$PROJECTHOME</code> hosted within the GPFS/SpecrumScale file-system. You have to pay a particular attention when using <code>rsync</code> to transfer data within your project directory as depicted below.</p> <p>Access rights to project directory: Quota for <code>clusterusers</code> group in project directories is 0 !!!</p> <p>When a project <code>&lt;name&gt;</code> is created, a group of the same name (<code>&lt;name&gt;</code>) is also created and researchers allowed to collaborate on the project are made members of this group,which grant them access to the project directory.</p> <p>Be aware that your default group as a user is <code>clusterusers</code> which has (on purpose) a quota in project directories set to 0. You thus need to ensure you always write data in your project directory using the <code>&lt;name&gt;</code> group (instead of yoru default one.). This can be achieved by ensuring the setgid bit is set on all folders in the project directories: <code>chmod g+s [...]</code></p> <p>When using <code>rsync</code> to transfer file toward the project directory <code>/work/projects/&lt;name&gt;</code> as destination, be aware that rsync will not use the correct permissions when copying files into your project directory. As indicated in the Data transfer section, you also need to:</p> <ul> <li>give new files the destination-default permissions with <code>--no-p</code> (<code>--no-perms</code>), and</li> <li>use the default group <code>&lt;name&gt;</code> of the destination dir with <code>--no-g</code> (<code>--no-group</code>)</li> <li>(eventually) instruct rsync to preserve whatever executable permissions existed on the source file and aren't masked at the destination using <code>--chmod=ug=rwX</code></li> </ul> <p>Your full <code>rsync</code> command becomes (adapt accordingly):</p> <pre><code>  rsync -avz {--update | --delete} --no-p --no-g [--chmod=ug=rwX] &lt;source&gt; /work/projects/&lt;name&gt;/[...]\n</code></pre> <p>For the same reason detailed above, in case you are using a build command or more generally any command meant to write data in your project directory <code>/work/projects/&lt;name&gt;</code>, you want to use the <code>sg</code> as follows:</p> <pre><code># /!\\ ADAPT &lt;name&gt; accordingly\nsg &lt;name&gt; -c \"&lt;command&gt; [...]\"\n</code></pre> <p>This is particularly important if you are building dedicated software with Easybuild for members of the project - you typically want to do it as follows:</p> <pre><code># /!\\ ADAPT &lt;name&gt; accordingly\nsg &lt;name&gt; -c \"eb [...] -r --rebuild -D\"   # Dry-run - enforce using the '&lt;name&gt;' group\nsg &lt;name&gt; -c \"eb [...] -r --rebuild\"      # Dry-run - enforce using the '&lt;name&gt;' group\n</code></pre> Debugging quota issues <p>Sometimes, when copying files with <code>rsync</code> or <code>scp</code> commands and you are not careful with the options of these commands, you copy files with incorrect permissions and ownership. If a directory is copied with the wrong permissions and ownership, all files created within the directory may maintain the incorrect permissions and ownership. Typical issues that you may encounter include:</p> <ul> <li>If a directory is copied incorrectly from a project directory to your home directory, the contents of the directory may continue counting towards the group data instead of your personal data and data usage may be misquoted by the <code>df-ulphc</code> utility. Actual data usage takes into account the file group not only its location.</li> <li>If a directory is copied incorrectly from a personal directory or another machine to a project directory, you may be unable to create files, since the <code>clusterusers</code> group has no quota inside project directories. Note the group special permission (g\u00b1s) in directories ensures that all files created in the directory will have the group of the directory instead of the process that creates the file.</li> </ul> <p>Typical resolutions techniques involve resetting the correct file ownership and permissions:</p> Files in project directoriesFiles in user home directories <pre><code>chown -R &lt;username&gt;:&lt;project name&gt; &lt;path to directory or file&gt;\nfind &lt;path to directory or file&gt; -type d | xargs -I % chmod g+s '%'\n</code></pre> <pre><code>chown -R &lt;username&gt;:clusterusers &lt;path to directory or file&gt;\nfind &lt;path to directory or file&gt; -type d | xargs -I % chmod g-s '%'\n</code></pre>"},{"location":"data/transfer/#using-mobaxterm-windows","title":"Using MobaXterm (Windows)","text":"<p>If you are under Windows and you have MobaXterm installed and configured, you probably want to use it to transfer your files to the clusters. Here are the steps to use <code>rsync</code> inside MobaXterm in Windows.</p> <p>Warning</p> <p>Be aware that you SHOULD enable MobaXterm SSH Agent -- see SSH Agent instructions for more instructions.</p>"},{"location":"data/transfer/#using-a-local-bash-transfer-your-files","title":"Using a local bash, transfer your files","text":"<ul> <li> <p>Open a local \"bash\" shell. Click on Start local terminal on the welcome page of MobaXterm.</p> </li> <li> <p>Find the location of the files you want to transfer. They should be located under <code>/drives/&lt;name of your disk&gt;</code>. You will have to use the Linux command line to move from one directory to the other. The <code>cd</code> command is used to change the current directory and <code>ls</code> to list files. For example, if your files are under <code>C:\\\\Users\\janedoe\\Downloads\\</code> you should then go to <code>/drives/c/Users/janedoe/Downloads/</code> with this command:</p> </li> </ul> <pre><code>cd /drives/c/Users/janedoe/Downloads/\n</code></pre> <p>Then list the files with <code>ls</code> command. You should see the list of your data files.</p> <ul> <li> <p>When you have retrieved the location of your files, we can begin the transfer with <code>rsync</code>. For example <code>/drives/c/Users/janedoe/Downloads/</code> (watch out, there is no <code>/</code> character at the end of the path, it is important).</p> </li> <li> <p>Launch the command <code>rsync</code> with this parameters to transfer all the content of the <code>Downloads</code> directory to the <code>/isilon/projects/market_data/</code> directory on the cluster (the syntax is very important, be careful)</p> </li> </ul> <pre><code>rsync -avzpP -e \"ssh -p 8022\" /drives/c/Users/janedoe/Downloads/ yourlogin@access-iris.uni.lu:/isilon/projects/market_data/\n</code></pre> <ul> <li>You should see the output of transfer in progress. Wait for it to finish (it can be very long).</li> </ul> <p></p>"},{"location":"data/transfer/#interrupt-and-resume-a-transfer-in-progress","title":"Interrupt and resume a transfer in progress","text":"<ul> <li> <p>If you want to interrupt the transfer to resume it later, press <code>Ctrl-C</code> and exit MobaXterm.</p> </li> <li> <p>To resume a transfer, go in the right location and execute the <code>rsync</code> command again. Only the files that have not been transferred will be transferred again.</p> </li> </ul>"},{"location":"data/transfer/#alternative-approaches","title":"Alternative approaches","text":"<p>You can also consider alternative approaches to synchronize data with the cluster login node:</p> <ul> <li>rely on a versioning system such as Git; this approach works well for source code trees;</li> <li>mount your remote homedir by SSHFS:<ul> <li>on Mac OS X, you should consider installing MacFusion for this purpose, where as</li> <li>on Linux, just use the command-line <code>sshfs</code> or, <code>mc</code>;</li> </ul> </li> <li>use GUI tools like FileZilla, Cyberduck, or WindSCP (or proprietary options like ExpanDrive or ForkLift 3).</li> </ul>"},{"location":"data/transfer/#sshfs","title":"SSHFS","text":"<p>SSHFS (SSH Filesystem) is a file system client that mounts directories located on a remote server onto a local directory over a normal ssh connection. Install the requires packages if they are not already available in your system.</p> LinuxMac OS X <p><pre><code># Debian-like\nsudo apt-get install sshfs\n# RHEL-like\nsudo yum install sshfs\n</code></pre> You may need to add yourself to the <code>fuse</code> group.</p> <p><pre><code># Assuming HomeBrew -- see https://brew.sh\nbrew install osxfuse sshfs\n</code></pre> You can also directly install macFUSE from: https://osxfuse.github.io/. You must reboot for the installation of osxfuse to take effect. You can then update to the latest version.</p> <p>With SSHFS any user can mount their ULHPC home directory onto a local workstation through an ssh connection. The CLI format is as follows: <pre><code>sshfs [user@]host:[dir] mountpoint [options]\n</code></pre></p> <p>Proceed as follows (assuming you have a working SSH connection): <pre><code># Create a local directory for the mounting point, e.g. ~/ulhpc\nmkdir -p ~/ulhpc\n# Mount the remote file system\nsshfs iris-cluster: ~/ulhpc -o follow_symlinks,reconnect,dir_cache=no\n</code></pre> Note the leaving the <code>[dir]</code> argument blanck, mounts the user's home directory by default. The options (<code>-o</code>) used are:</p> <ul> <li><code>follow_symlinks</code> presents symbolic links in the remote files system as regular files in the local file system, useful when the symbolic link points outside the mounted directory;</li> <li><code>reconnect</code> allows the SSHFS client to automatically reconnect to server if connection is interrupted;</li> <li><code>dir_cache</code> enables or disables the directory cache which holds the names of directory entries (can be slow for mounted remote directories with many files).</li> </ul> <p>When you no longer need the mounted remote directory, you must unmount your remote file system:</p> LinuxMac OS X <pre><code>fusermount -u ~/ulhpc\n</code></pre> <pre><code>diskutil umount ~/ulhpc\n</code></pre>"},{"location":"data/transfer/#transfers-between-long-term-storage-and-the-hpc-facilities","title":"Transfers between long term storage and the HPC facilities","text":"<p>The university provides central data storage services for all employees and students. The data are stored securely on the university campus and are managed by the IT department. The storage servers most commonly used at the university are</p> <ul> <li>Atlas (atlas.uni.lux) for staff members, and</li> <li>Poseidon (poseidon.uni.lux) for students.</li> </ul> <p>For more details on the university central storage, you can have a look at</p> <ul> <li>Usage of Atlas and Poseidon, and</li> <li>Backup of your files on Atlas.</li> </ul> <p>Connecting to central data storage services from a personal machine</p> <p>The examples presented here are targeted to the university HPC machines. To connect to the university central data storage with a (Linux) personal machine from outside of the university network, you need to start first a VPN connection.</p> <p>The SMB shares exported for directories in the central data storage are meant to be accessed interactively. Transfer your data manually before and after your jobs are run. You can mount directories from the central data storage in the login nodes, and access the central data storage through the interface of <code>smbclient</code> from both the compute nodes during interactive jobs and the login nodes.</p> <p>Never store your password in plain text</p> <p>Unlike mounting with <code>sshfs</code>, you will always need to enter your password to access a directory in an SMB share. Avoid, storing your password in any manner that it makes it recoverable from plain text. For instance, do not create job scripts that contain your password in plain text just to move data to Atlas within a job.</p> <p>The following commands target Atlas, but commands for Poseidon are similar.</p>"},{"location":"data/transfer/#mounting-an-smb-share-to-a-login-node","title":"Mounting an SMB share to a login node","text":"<p>The UL HPC team provides the <code>smb-storage</code> script to mount SMB shares in login nodes.</p> <ul> <li>There exists an SMB share <code>users</code> where all staff member have a directory named after their user name (<code>name.surname</code>). To mount your directory in an shell session at a login node execute the command <pre><code>smb-storage mount name.surname\n</code></pre> and your directory will be mounted to the default mount location: <pre><code>~/atlas.uni.lux-users-name.surname\n</code></pre></li> <li>To mount a project share <code>project_name</code> in a shell session at a login node execute the command <pre><code>smb-storage mount name.surname --project project_name\n</code></pre> and the share will be mounted in the default mount location: <pre><code>~/atlas.uni.lux-project_name\n</code></pre></li> <li>To unmount any share, simply call the <code>unmount</code> subcommand with the mount point path, for instance <pre><code>smb-storage unmount ~/atlas.uni.lux-users-name.surname\n</code></pre> or: <pre><code>smb-storage unmount ~/atlas.uni.lux-project_name\n</code></pre></li> </ul> <p>The <code>smb-storage</code> script provides optional flags to modify the default options:</p> <ul> <li><code>--help</code> prints information about the usage and options of he script;</li> <li><code>--server &lt;server url&gt;</code> specifies the server from which the SMB share is mounted (defaults to <code>--server atlas.uni.lux</code> if not specified, use <code>--server poseidon.uni.lux</code> to mount a share from Poseidon);</li> <li><code>--project &lt;project name&gt; [&lt;directory in project&gt;]</code> mounts the share <code>&lt;project name&gt;</code> and creates a symbolic link to the optionally provided location <code>&lt;directory in project&gt;</code>, or to the project root directory if a location is not provided (defaults to <code>--project users name.surname</code> if not specified);</li> <li><code>--mountpoint &lt;path&gt;</code> selects the path where the share directory will be available (defaults to <code>~/&lt;server url&gt;-&lt;project name&gt;-&lt;directory in project&gt;</code> if nbot specified);</li> <li><code>--debug</code> prints details of the operations performed by the mount script.</li> </ul> <p>Best practices</p> <p>Mounted SMB shares will be available in the login node, and he mount point will appear as a dead symbolic link in compute nodes. This is be design, you can only mount SMB shares in login nodes because SMB shares are meant to be used in interactive sections.</p> <p>Mounted shares will remain available as long as the login session where the share was mounted remains active. You can mount shares in a <code>tmux</code> session in a login node, and access the share from any other session in the login node.</p> Details of the mounting process <p>There exists an SMB share <code>users</code> where all staff member have a directory named after their user name (<code>name.surname</code>). All other projects have an SMB share named after the project name (in lowercase characters).</p> <p>The <code>smb-storage</code> scripts uses <code>gio mount</code> to mount SMB shares. Shares are mounted in a specially named mount point in <code>/run/user/${UID}/gvfs</code>. Then, <code>smb-storage</code> creates a symbolic link to the requested <code>directory in project</code> in the path specified in the <code>--mountpoint</code> option.</p> <p>During unmounting, the symbolic links are deleted by the <code>smb-storage</code> script and then the shares mounted in <code>/run/user/${UID}/gvfs</code> are unmounted and their mount points are removed using <code>gio mount --unmount</code>. If a session with mounted SMB shares terminates without unmounting the shares, the shares in <code>/run/user/${UID}/gvfs</code> will be unmounted and their mount points deleted, but the symbolic links created by <code>smb-storage</code> must be removed manually.</p>"},{"location":"data/transfer/#accessing-smb-shares-with-smbclient","title":"Accessing SMB shares with <code>smbclient</code>","text":"<p>The <code>smbclient</code> program is available in both login and compute nodes. In compute nodes the only way to access SMB shares is through the client program. With the SMB client one can connect to the <code>users</code> share and browse their personal directory with the command: <pre><code>smbclient //atlas.uni.lux/users --directory='name.surname' --user=name.surname@uni.lu\n</code></pre> Project directories are accessed with the command: <pre><code>smbclient //atlas.uni.lux/project_name --user=name.surname@uni.lu\n</code></pre></p> <p>Type <code>help</code> to get a list of all available commands or <code>help (command_name)</code> to get more information for a specific command. Some useful commands are</p> <ul> <li><code>ls</code> to list all the files in a directory,</li> <li><code>mkdir (directory_name)</code> to create a directory,</li> <li><code>rm (file_name)</code> to remove a file,</li> <li><code>rmdir (directory_name)</code> to remove a directory,</li> <li><code>scopy (source_full_path) (destination_full_path)</code> to move a file within the SMN shared directory,</li> <li><code>get (file_name) [destination]</code> to move a file from Atlas to the local machine (placed in the working directory, if the destination is not specified), and</li> <li><code>put (file_name) [destination]</code> to move a file to Atlas from the local machine (placed in the working directory, if a full path is not specified),</li> <li><code>mget (file name pattern) [destination]</code> to download multiple files, and</li> <li><code>mput (file name pattern) [destination]</code> to upload multiple files.</li> </ul> <p>The patterns used in <code>mget</code>/<code>mput</code> are either normal file names, or globular expressions (e.g. <code>*.txt</code>). </p> <p>Connecting into an interactive SMB session means that you will have to maintain a shell session dedicated to SMB. However, it saves you from entering your password for every operation. If you would like to perform a single operation and exit, you can avoid maintaining an interactive session with the <code>--command</code> flag. For instance, <pre><code>smbclient //atlas.uni.lux/users --directory='name.surname' --user=name.surname@uni.lu --command='get \"full path/to/remote file.txt\" \"full path/to/local file.txt\"'\n</code></pre> copies a file from the SMB directory to the local machine. Notice the use of double quotes to handle file names with spaces. Similarly, <pre><code>smbclient //atlas.uni.lux/users --directory='name.surname' --user=name.surname@uni.lu --command='put \"full path/to/local file.txt\" \"full path/to/remote file.txt\"'\n</code></pre> copies a file from the local machine to the SMB directory.</p> <p>Moving whole directories is a bit more involved, as it requires setting some state variables for the session, both for interactive and non-interactive sessions. To download a directory for instance, use <pre><code>smbclient //atlas.uni.lux/users --directory='name.surname' --user=name.surname@uni.lu --command='recurse ON; prompt OFF; mget \"full path/to/remote directory\" \"full path/to/local directory\"'\n</code></pre> and to upload a directory use <pre><code>smbclient //atlas.uni.lux/users --directory='name.surname' --user=name.surname@uni.lu --command='recurse ON; prompt OFF; mput \"full path/to/remote local\" \"full path/to/remote directory\"'\n</code></pre> respectively. The session option</p> <ul> <li><code>recurse ON</code> enables recursion into directories, and the option</li> <li><code>prompt OFF</code> disables prompting for confirmation before moving each file.</li> </ul> <p>Sources</p> <ul> <li>Cheat-sheet for SMB access from linux</li> </ul>"},{"location":"data/transfer/#special-transfers","title":"Special transfers","text":"<p>Sometimes you may have the case that a lot of files need to go from point A to B over a Wide Area Network (eg. across the Atlantic). Since packet latency and other factors on the network will naturally slow down the transfers, you need to find workarounds, typically with either rsync or tar.</p>"},{"location":"data-center/","title":"ULHPC Data Center - Centre de Calcul (CDC)","text":"<p>The ULHPC facilities are hosted within the University's \"Centre de Calcul\" (CDC) data center located in the Belval Campus.</p>"},{"location":"data-center/#power-and-cooling-capacities","title":"Power and Cooling Capacities","text":"<p>Established over two floors underground (CDC-S01 and CDC-S02) of ~1000~100m<sup>2</sup> each, the CDC features five server rooms per level (each of them offering ~100m<sup>2</sup> as IT rooms surface). When the first level CDC-S01 is hosting administrative IT and research equipment, the second floor (CDC-S02) is primarily targeting the hosting of HPC equipment (compute, storage and interconnect).</p> <p></p> <p>A power generation station supplies the HPC floor with up to 3 MW of electrical power, and 3 MW of cold water at a 12-18\u00b0C regime used for traditional Airflow with In-Row cooling. A separate hot water circuit (between 30 and 40\u00b0C) allow to implement Direct Liquid Cooling (DLC) solutions as for the Aion supercomputer in two dedicated server rooms.</p> Location Cooling Usage Max Capa. CDC S-02-001 Airflow Future extension 280 kW CDC S-02-002 Airflow Future extension 280 kW CDC S-02-003 DLC Future extension - High Density/Energy efficient HPC 1050 kW CDC S-02-004 DLC High Density/Energy efficient HPC: <code>aion</code> 1050 kW CDC S-02-005 Airflow Storage / Traditional HPC: <code>iris</code> and common equipment 300 kW"},{"location":"data-center/#data-center-cooling-technologies","title":"Data-Center Cooling technologies","text":""},{"location":"data-center/#airflow-with-in-row-cooling","title":"Airflow with In-Row cooling","text":"<p> Most server rooms are designed for traditional airflow-based cooling and implement hot or cold aisle containment, as well as In-row cooling systems work within a row of standard server rack engineered to take up the smallest footprint and offer high-density cooling. Ducting and baffles ensure that the cooling air gets where it needs to go.</p> <p>Iris compute, storage and interconnect equipment are hosted in such a configuration</p> <p></p>"},{"location":"data-center/#direct-liquid-cooling","title":"[Direct] Liquid Cooling","text":"<p>Traditional solutions implemented in most data centers use air as a medium to remove the heat from the servers and computing equipment and are not well suited to cutting-edge high-density HPC environments due to the limited thermal capacity of air. Liquids\u2019 thermal conductivity is higher than the air, thus concludes the liquid can absorb (through conductivity) more heat than the air. The replacement of air with a liquid cooling medium allows to drastically improve the energy-efficiency as well as the density of the implemented solution, especially with Direct Liquid Cooling (DLC) where the heat from the IT components is directly transferred to a liquid cooling medium through liquid-cooled plates.</p> <p>The Aion supercomputer based on the fan-less Atos BullSequana XH2000 DLC cell design relies on this water-cooled configuration.</p>"},{"location":"development/build-tools/easybuild/","title":"Building [custom] software with EasyBuild on the UL HPC platform","text":"<p>EasyBuild can be used to ease, automate and script the build of software on the UL HPC platforms.</p> <p>Indeed, as researchers involved in many cutting-edge and hot topics, you probably have access to many theoretical resources to understand the surrounding concepts. Yet it should normally give you a wish to test the corresponding software. Traditionally, this part is rather time-consuming and frustrating, especially when the developers did not rely on a \"regular\" building framework such as CMake or the autotools (i.e. with build instructions as <code>configure --prefix &lt;path&gt; &amp;&amp; make &amp;&amp; make install</code>).</p> <p>And when it comes to have a build adapted to an HPC system, you are somehow forced to make a custom build performed on the target machine to ensure you will get the best possible performance. EasyBuild is one approach to facilitate this step.</p> <p></p> <p>EasyBuild is a tool that allows to perform automated and reproducible compilation and installation of software. A large number of scientific software are supported (1504 supported software packages in the last release 3.6.1) -- see also What is EasyBuild?</p> <p>All builds and installations are performed at user level, so you don't need the admin (i.e. <code>root</code>) rights. The software are installed in your home directory (by default in <code>$HOME/.local/easybuild/software/</code>) and a module file is generated (by default in <code>$HOME/.local/easybuild/modules/</code>) to use the software.</p> <p>EasyBuild relies on two main concepts: Toolchains and EasyConfig files.</p> <p>A toolchain corresponds to a compiler and a set of libraries which are commonly used to build a software. The two main toolchains frequently used on the UL HPC platform are the <code>foss</code> (\"Free and Open Source Software\") and the <code>intel</code> one.</p> <ol> <li><code>foss</code>  is based on the GCC compiler and on open-source libraries (OpenMPI, OpenBLAS, etc.).</li> <li><code>intel</code> is based on the Intel compiler and on Intel libraries (Intel MPI, Intel Math Kernel Library, etc.).</li> </ol> <p>An EasyConfig file is a simple text file that describes the build process of a software. For most software that uses standard procedures (like <code>configure</code>, <code>make</code> and <code>make install</code>), this file is very simple. Many EasyConfig files are already provided with EasyBuild. By default, EasyConfig files and generated modules are named using the following convention: <code>&lt;Software-Name&gt;-&lt;Software-Version&gt;-&lt;Toolchain-Name&gt;-&lt;Toolchain-Version&gt;</code>. However, we use a hierarchical approach where the software are classified under a category (or class) -- see  the <code>CategorizedModuleNamingScheme</code> option for the <code>EASYBUILD_MODULE_NAMING_SCHEME</code> environmental variable), meaning that the layout will respect the following hierarchy: <code>&lt;Software-Class&gt;/&lt;Software-Name&gt;/&lt;Software-Version&gt;-&lt;Toolchain-Name&gt;-&lt;Toolchain-Version&gt;</code></p> <p>Additional details are available on EasyBuild website:</p> <ul> <li>EasyBuild homepage</li> <li>EasyBuild tutorial</li> <li>EasyBuild documentation</li> <li>What is EasyBuild?</li> <li>Toolchains</li> <li>EasyConfig files</li> <li>List of supported software packages</li> </ul>"},{"location":"development/build-tools/easybuild/#a-installation","title":"a. Installation","text":"<ul> <li>the official instructions.</li> </ul> <p>What is important for the installation of EasyBuild are the following variables:</p> <ul> <li><code>EASYBUILD_PREFIX</code>: where to install local modules and software, i.e. <code>$HOME/.local/easybuild</code></li> <li><code>EASYBUILD_MODULES_TOOL</code>: the type of modules tool you are using, i.e. <code>LMod</code> in this case</li> <li><code>EASYBUILD_MODULE_NAMING_SCHEME</code>: the way the software and modules should be organized (flat view or hierarchical) -- we're advising on <code>CategorizedModuleNamingScheme</code></li> </ul> <p>Add the following entries to your <code>~/.bashrc</code> (use your favorite CLI editor like <code>nano</code> or <code>vim</code>):</p> <pre><code># Easybuild\nexport EASYBUILD_PREFIX=$HOME/.local/easybuild\nexport EASYBUILD_MODULES_TOOL=Lmod\nexport EASYBUILD_MODULE_NAMING_SCHEME=CategorizedModuleNamingScheme\n# Use the below variable to run:\n#    module use $LOCAL_MODULES\n#    module load tools/EasyBuild\nexport LOCAL_MODULES=${EASYBUILD_PREFIX}/modules/all\n\nalias ma=\"module avail\"\nalias ml=\"module list\"\nfunction mu(){\n   module use $LOCAL_MODULES\n   module load tools/EasyBuild\n}\n</code></pre> <p>Then source this file to expose the environment variables:</p> <pre><code>$&gt; source ~/.bashrc\n$&gt; echo $EASYBUILD_PREFIX\n/home/users/&lt;login&gt;/.local/easybuild\n</code></pre> <p>Now let's install EasyBuild following the official procedure. Install EasyBuild in a temporary directory and use this temporary installation to build an EasyBuild module in your <code>$EASYBUILD_PREFIX</code>:</p> <pre><code># pick installation prefix, and install EasyBuild into it\nexport EB_TMPDIR=/tmp/$USER/eb_tmp\npython3 -m pip install --ignore-installed --prefix $EB_TMPDIR easybuild\n\n# update environment to use this temporary EasyBuild installation\nexport PATH=$EB_TMPDIR/bin:$PATH\nexport PYTHONPATH=$(/bin/ls -rtd -1 $EB_TMPDIR/lib*/python*/site-packages | tail -1):$PYTHONPATH\nexport EB_PYTHON=python3\n\n# install Easybuild in your $EASYBUILD_PREFIX\neb --install-latest-eb-release --prefix $EASYBUILD_PREFIX\n</code></pre> <p>Now you can use your freshly built software. The main EasyBuild command is <code>eb</code>:</p> <pre><code>$&gt; eb --version             # expected ;)\n-bash: eb: command not found\n\n# Load the newly installed Easybuild\n$&gt; echo $MODULEPATH\n/opt/apps/resif/data/stable/default/modules/all/\n\n$&gt; module use $LOCAL_MODULES\n$&gt; echo $MODULEPATH\n/home/users/&lt;login&gt;/.local/easybuild/modules/all:/opt/apps/resif/data/stable/default/modules/all\n\n$&gt; module spider Easybuild\n$&gt; module load tools/EasyBuild       # TAB is your friend...\n$&gt; eb --version\nThis is EasyBuild 3.6.1 (framework: 3.6.1, easyblocks: 3.6.1) on host iris-001.\n</code></pre> <p>Since you are going to use quite often the above command to use locally built modules and load easybuild, an alias <code>mu</code> is provided and can be used from now on. Use it now.</p> <p><pre><code>$&gt; mu\n$&gt; module avail     # OR 'ma'\n</code></pre> To get help on the EasyBuild options, use the <code>-h</code> or <code>-H</code> option flags:</p> <pre><code>$&gt; eb -h\n$&gt; eb -H\n</code></pre>"},{"location":"development/build-tools/easybuild/#b-local-vs-global-usage","title":"b. Local vs. global usage","text":"<p>As you probably guessed, we are going to use two places for the installed software:</p> <ul> <li>local builds <code>~/.local/easybuild</code>          (see <code>$LOCAL_MODULES</code>)</li> <li>global builds (provided to you by the UL HPC team) in <code>/opt/apps/resif/data/stable/default/modules/all</code> (see default <code>$MODULEPATH</code>).</li> </ul> <p>Default usage (with the <code>eb</code> command) would install your software and modules in <code>~/.local/easybuild</code>.</p> <p>Before that, let's explore the basic usage of EasyBuild and the <code>eb</code> command.</p> <pre><code># Search for an Easybuild recipy with 'eb -S &lt;pattern&gt;'\n$&gt; eb -S Spark\nCFGS1=/opt/apps/resif/data/easyconfigs/ulhpc/default/easybuild/easyconfigs/s/Spark\nCFGS2=/home/users/&lt;login&gt;/.local/easybuild/software/tools/EasyBuild/3.6.1/lib/python2.7/site-packages/easybuild_easyconfigs-3.6.1-py2.7.egg/easybuild/easyconfigs/s/Spark\n * $CFGS1/Spark-2.1.1.eb\n * $CFGS1/Spark-2.3.0-intel-2018a-Hadoop-2.7-Java-1.8.0_162-Python-3.6.4.eb\n * $CFGS2/Spark-1.3.0.eb\n * $CFGS2/Spark-1.4.1.eb\n * $CFGS2/Spark-1.5.0.eb\n * $CFGS2/Spark-1.6.0.eb\n * $CFGS2/Spark-1.6.1.eb\n * $CFGS2/Spark-2.0.0.eb\n * $CFGS2/Spark-2.0.2.eb\n * $CFGS2/Spark-2.2.0-Hadoop-2.6-Java-1.8.0_144.eb\n * $CFGS2/Spark-2.2.0-Hadoop-2.6-Java-1.8.0_152.eb\n * $CFGS2/Spark-2.2.0-intel-2017b-Hadoop-2.6-Java-1.8.0_152-Python-3.6.3.eb\n</code></pre>"},{"location":"development/build-tools/easybuild/#c-build-software-using-provided-easyconfig-file","title":"c. Build software using provided EasyConfig file","text":"<p>In this part, we propose to build High Performance Linpack (HPL) using EasyBuild. HPL is supported by EasyBuild, this means that an EasyConfig file allowing to build HPL is already provided with EasyBuild.</p> <p>First of all, let's check if that software is not available by default:</p> <pre><code>$&gt; module spider HPL\n\nLmod has detected the following error: Unable to find: \"HPL\"\n</code></pre> <p>Then, search for available EasyConfig files with HPL in their name. The EasyConfig files are named with the <code>.eb</code> extension.</p> <pre><code># Search for an Easybuild recipy with 'eb -S &lt;pattern&gt;'\n$&gt; eb -S HPL-2.2\nCFGS1=/home/users/svarrette/.local/easybuild/software/tools/EasyBuild/3.6.1/lib/python2.7/site-packages/easybuild_easyconfigs-3.6.1-py2.7.egg/easybuild/easyconfigs/h/HPL\n * $CFGS1/HPL-2.2-foss-2016.07.eb\n * $CFGS1/HPL-2.2-foss-2016.09.eb\n * $CFGS1/HPL-2.2-foss-2017a.eb\n * $CFGS1/HPL-2.2-foss-2017b.eb\n * $CFGS1/HPL-2.2-foss-2018a.eb\n * $CFGS1/HPL-2.2-fosscuda-2018a.eb\n * $CFGS1/HPL-2.2-giolf-2017b.eb\n * $CFGS1/HPL-2.2-giolf-2018a.eb\n * $CFGS1/HPL-2.2-giolfc-2017b.eb\n * $CFGS1/HPL-2.2-gmpolf-2017.10.eb\n * $CFGS1/HPL-2.2-goolfc-2016.08.eb\n * $CFGS1/HPL-2.2-goolfc-2016.10.eb\n * $CFGS1/HPL-2.2-intel-2017.00.eb\n * $CFGS1/HPL-2.2-intel-2017.01.eb\n * $CFGS1/HPL-2.2-intel-2017.02.eb\n * $CFGS1/HPL-2.2-intel-2017.09.eb\n * $CFGS1/HPL-2.2-intel-2017a.eb\n * $CFGS1/HPL-2.2-intel-2017b.eb\n * $CFGS1/HPL-2.2-intel-2018.00.eb\n * $CFGS1/HPL-2.2-intel-2018.01.eb\n * $CFGS1/HPL-2.2-intel-2018.02.eb\n * $CFGS1/HPL-2.2-intel-2018a.eb\n * $CFGS1/HPL-2.2-intelcuda-2016.10.eb\n * $CFGS1/HPL-2.2-iomkl-2016.09-GCC-4.9.3-2.25.eb\n * $CFGS1/HPL-2.2-iomkl-2016.09-GCC-5.4.0-2.26.eb\n * $CFGS1/HPL-2.2-iomkl-2017.01.eb\n * $CFGS1/HPL-2.2-intel-2017.02.eb\n * $CFGS1/HPL-2.2-intel-2017.09.eb\n * $CFGS1/HPL-2.2-intel-2017a.eb\n * $CFGS1/HPL-2.2-intel-2017b.eb\n * $CFGS1/HPL-2.2-intel-2018.00.eb\n * $CFGS1/HPL-2.2-intel-2018.01.eb\n * $CFGS1/HPL-2.2-intel-2018.02.eb\n * $CFGS1/HPL-2.2-intel-2018a.eb\n * $CFGS1/HPL-2.2-intelcuda-2016.10.eb\n * $CFGS1/HPL-2.2-iomkl-2016.09-GCC-4.9.3-2.25.eb\n * $CFGS1/HPL-2.2-iomkl-2016.09-GCC-5.4.0-2.26.eb\n * $CFGS1/HPL-2.2-iomkl-2017.01.eb\n * $CFGS1/HPL-2.2-iomkl-2017a.eb\n * $CFGS1/HPL-2.2-iomkl-2017b.eb\n * $CFGS1/HPL-2.2-iomkl-2018.02.eb\n * $CFGS1/HPL-2.2-iomkl-2018a.eb\n * $CFGS1/HPL-2.2-pomkl-2016.09.eb\n</code></pre> <p>We are going to build HPL 2.2 against the <code>intel</code> toolchain, typically the 2017a version which is available by default on the platform.</p> <p>Pick the corresponding recipy (for instance <code>HPL-2.2-intel-2017a.eb</code>), install it with</p> <pre><code>   eb &lt;name&gt;.eb [-D] -r\n</code></pre> <ul> <li><code>-D</code> enables the dry-run mode to check what's going to be install -- ALWAYS try it first</li> <li><code>-r</code> enables the robot mode to automatically install all dependencies while searching for easyconfigs in a set of pre-defined directories -- you can also prepend new directories to search for eb files (like the current directory <code>$PWD</code>) using the option and syntax <code>--robot-paths=$PWD:</code> (do not forget the ':'). See Controlling the robot search path documentation</li> <li>The <code>$CFGS&lt;n&gt;/</code> prefix should be dropped unless you know what you're doing (and thus have previously defined the variable -- see the first output of the <code>eb -S [...]</code> command).</li> </ul> <p>So let's install <code>HPL</code> version 2.2 and FIRST check which dependencies are satisfied with <code>-Dr</code>:</p> <pre><code>$&gt; eb HPL-2.2-intel-2017a.eb -Dr\n== temporary log file in case of crash /tmp/eb-CTC2hq/easybuild-gfLf1W.log\nDry run: printing build status of easyconfigs and dependencies\nCFGS=/home/users/svarrette/.local/easybuild/software/tools/EasyBuild/3.6.1/lib/python2.7/site-packages/easybuild_easyconfigs-3.6.1-py2.7.egg/easybuild/easyconfigs\n * [x] $CFGS/m/M4/M4-1.4.17.eb (module: devel/M4/1.4.17)\n * [x] $CFGS/b/Bison/Bison-3.0.4.eb (module: lang/Bison/3.0.4)\n * [x] $CFGS/f/flex/flex-2.6.0.eb (module: lang/flex/2.6.0)\n * [x] $CFGS/z/zlib/zlib-1.2.8.eb (module: lib/zlib/1.2.8)\n * [x] $CFGS/b/binutils/binutils-2.27.eb (module: tools/binutils/2.27)\n * [x] $CFGS/g/GCCcore/GCCcore-6.3.0.eb (module: compiler/GCCcore/6.3.0)\n * [x] $CFGS/m/M4/M4-1.4.18-GCCcore-6.3.0.eb (module: devel/M4/1.4.18-GCCcore-6.3.0)\n * [x] $CFGS/z/zlib/zlib-1.2.11-GCCcore-6.3.0.eb (module: lib/zlib/1.2.11-GCCcore-6.3.0)\n * [x] $CFGS/h/help2man/help2man-1.47.4-GCCcore-6.3.0.eb (module: tools/help2man/1.47.4-GCCcore-6.3.0)\n * [x] $CFGS/b/Bison/Bison-3.0.4-GCCcore-6.3.0.eb (module: lang/Bison/3.0.4-GCCcore-6.3.0)\n * [x] $CFGS/f/flex/flex-2.6.3-GCCcore-6.3.0.eb (module: lang/flex/2.6.3-GCCcore-6.3.0)\n * [x] $CFGS/b/binutils/binutils-2.27-GCCcore-6.3.0.eb (module: tools/binutils/2.27-GCCcore-6.3.0)\n * [x] $CFGS/i/icc/icc-2017.1.132-GCC-6.3.0-2.27.eb (module: compiler/icc/2017.1.132-GCC-6.3.0-2.27)\n * [x] $CFGS/i/ifort/ifort-2017.1.132-GCC-6.3.0-2.27.eb (module: compiler/ifort/2017.1.132-GCC-6.3.0-2.27)\n * [x] $CFGS/i/iccifort/iccifort-2017.1.132-GCC-6.3.0-2.27.eb (module: toolchain/iccifort/2017.1.132-GCC-6.3.0-2.27)\n * [x] $CFGS/i/impi/impi-2017.1.132-iccifort-2017.1.132-GCC-6.3.0-2.27.eb (module: mpi/impi/2017.1.132-iccifort-2017.1.132-GCC-6.3.0-2.27)\n * [x] $CFGS/i/iimpi/iimpi-2017a.eb (module: toolchain/iimpi/2017a)\n * [x] $CFGS/i/imkl/imkl-2017.1.132-iimpi-2017a.eb (module: numlib/imkl/2017.1.132-iimpi-2017a)\n * [x] $CFGS/i/intel/intel-2017a.eb (module: toolchain/intel/2017a)\n * [ ] $CFGS/h/HPL/HPL-2.2-intel-2017a.eb (module: tools/HPL/2.2-intel-2017a)\n== Temporary log file(s) /tmp/eb-CTC2hq/easybuild-gfLf1W.log* have been removed.\n== Temporary directory /tmp/eb-CTC2hq has been removed.\n</code></pre> <p>As can be seen, there is a single element to install and this has not been done so far (box not checked). All the dependencies are already present (box checked). Let's really install the selected software -- you may want to prefix the <code>eb</code> command with the <code>time</code> to collect the installation time:</p> <pre><code>$&gt; time eb HPL-2.2-intel-2017a.eb -r       # Remove the '-D' (dry-run) flags\n== temporary log file in case of crash /tmp/eb-nub_oL/easybuild-J8sNzx.log\n== resolving dependencies ...\n== processing EasyBuild easyconfig /home/users/svarrette/.local/easybuild/software/tools/EasyBuild/3.6.1/lib/python2.7/site-packages/easybuild_easyconfigs-3.6.1-py2.7.egg/easybuild/easyconfigs/h/HPL/HPL-2.2-intel-2017a.eb\n== building and installing tools/HPL/2.2-intel-2017a...\n== fetching files...\n== creating build dir, resetting environment...\n== unpacking...\n== patching...\n== preparing...\n== configuring...\n== building...\n== testing...\n== installing...\n== taking care of extensions...\n== postprocessing...\n== sanity checking...\n== cleaning up...\n== creating module...\n== permissions...\n== packaging...\n== COMPLETED: Installation ended successfully\n== Results of the build can be found in the log file(s) /home/users/svarrette/.local/easybuild/software/tools/HPL/2.2-intel-2017a/easybuild/easybuild-HPL-2.2-20180608.094831.log\n== Build succeeded for 1 out of 1\n== Temporary log file(s) /tmp/eb-nub_oL/easybuild-J8sNzx.log* have been removed.\n== Temporary directory /tmp/eb-nub_oL has been removed.\n\nreal    0m56.472s\nuser    0m15.268s\nsys     0m19.998s\n</code></pre> <p>Check the installed software:</p> <pre><code>$&gt; module av HPL\n\n------------------------- /home/users/&lt;login&gt;/.local/easybuild/modules/all -------------------------\n   tools/HPL/2.2-intel-2017a\n\nUse \"module spider\" to find all possible modules.\nUse \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\".\n\n$&gt; module spider HPL\n\n----------------------------------------------------------------------------------------------------\n  tools/HPL: tools/HPL/2.2-intel-2017a\n----------------------------------------------------------------------------------------------------\n    Description:\n      HPL is a software package that solves a (random) dense linear system in double precision\n      (64 bits) arithmetic on distributed-memory computers. It can thus be regarded as a portable\n      as well as freely available implementation of the High Performance Computing Linpack Benchmark.\n\n    This module can be loaded directly: module load tools/HPL/2.2-intel-2017a\n\n    Help:\n\n      Description\n      ===========\n      HPL is a software package that solves a (random) dense linear system in double precision\n      (64 bits) arithmetic on distributed-memory computers. It can thus be regarded as a portable\n      as well as freely available implementation of the High Performance Computing Linpack Benchmark.\n\n\n      More information\n      ================\n       - Homepage: http://www.netlib.org/benchmark/hpl/\n\n$&gt; module show tools/HPL\n---------------------------------------------------------------------------------------------------\n   /home/users/svarrette/.local/easybuild/modules/all/tools/HPL/2.2-intel-2017a.lua:\n---------------------------------------------------------------------------------------------------\nhelp([[\nDescription\n===========\nHPL is a software package that solves a (random) dense linear system in double precision\n(64 bits) arithmetic on distributed-memory computers. It can thus be regarded as a portable\nas well as freely available implementation of the High Performance Computing Linpack Benchmark.\n\n\nMore information\n================\n - Homepage: http://www.netlib.org/benchmark/hpl/\n]])\nwhatis(\"Description: HPL is a software package that solves a (random) dense linear system in\n double precision (64 bits) arithmetic on distributed-memory computers. It can thus be regarded\n as a portable as well as freely available implementation of the High Performance Computing\n Linpack Benchmark.\")\nwhatis(\"Homepage: http://www.netlib.org/benchmark/hpl/\")\nconflict(\"tools/HPL\")\nload(\"toolchain/intel/2017a\")\nprepend_path(\"PATH\",\"/home/users/svarrette/.local/easybuild/software/tools/HPL/2.2-intel-2017a/bin\")\nsetenv(\"EBROOTHPL\",\"/home/users/svarrette/.local/easybuild/software/tools/HPL/2.2-intel-2017a\")\nsetenv(\"EBVERSIONHPL\",\"2.2\")\nsetenv(\"EBDEVELHPL\",\"/home/users/svarrette/.local/easybuild/software/tools/HPL/2.2-intel-2017a/easybuild/tools-HPL-2.2-intel-2017a-easybuild-devel\")\n</code></pre> <p>Note: to see the (locally) installed software, the <code>MODULEPATH</code> variable should include the <code>$HOME/.local/easybuild/modules/all/</code> (of <code>$LOCAL_MODULES</code>) path (which is what happens when using <code>module use &lt;path&gt;</code> -- see the <code>mu</code> command)</p> <p>You can now load the freshly installed module like any other:</p> <pre><code>$&gt; module load tools/HPL\n$&gt; module list\n\nCurrently Loaded Modules:\n  1) tools/EasyBuild/3.6.1                          7) mpi/impi/2017.1.132-iccifort-2017.1.132-GCC-6.3.0-2.27\n  2) compiler/GCCcore/6.3.0                         8) toolchain/iimpi/2017a\n  3) tools/binutils/2.27-GCCcore-6.3.0              9) numlib/imkl/2017.1.132-iimpi-2017a\n  4) compiler/icc/2017.1.132-GCC-6.3.0-2.27        10) toolchain/intel/2017a\n  5) compiler/ifort/2017.1.132-GCC-6.3.0-2.27      11) tools/HPL/2.2-intel-2017a\n  6) toolchain/iccifort/2017.1.132-GCC-6.3.0-2.27\n</code></pre> <p>Tips: When you load a module <code>&lt;NAME&gt;</code> generated by Easybuild, it is installed within the directory reported by the <code>$EBROOT&lt;NAME&gt;</code> variable. In the above case, you will find the generated binary for HPL in <code>${EBROOTHPL}/bin/xhpl</code>.</p> <p>You may want to test the newly built HPL benchmark (you need to reserve at least 4 cores for that to succeed):</p> <pre><code># In another terminal, connect to the cluster frontend\n# Have an interactive job\n############### iris cluster (slurm) ###############\n(access-iris)$&gt; si -n 4        # this time reserve for 4 (mpi) tasks\n$&gt; mu\n$&gt; module load tools/HPL\n$&gt; cd $EBROOTHPL\n$&gt; ls\n$&gt; cd bin\n$&gt; ls\n$&gt; srun -n $SLURM_NTASKS ./xhpl\n</code></pre> <p>Running HPL benchmarks requires more attention -- a full tutorial is dedicated to it. Yet you can see that we obtained HPL 2.2 without writing any EasyConfig file.</p>"},{"location":"development/build-tools/easybuild/#d-build-software-using-a-customized-easyconfig-file","title":"d. Build software using a customized EasyConfig file","text":"<p>There are multiple ways to amend an EasyConfig file. Check the <code>--try-*</code> option flags for all the possibilities.</p> <p>Generally you want to do that when the up-to-date version of the software you want is not available as a recipy within Easybuild. For instance, a very popular building environment CMake has recently released a new version (3.11.3), which you want to give a try.</p> <p>It is not available as module, so let's build it.</p> <p>First let's check for available easyconfigs recipy if one exist for the expected version:</p> <pre><code>$&gt; eb -S Cmake-3\n[...]\n * $CFGS2/CMake-3.9.1.eb\n * $CFGS2/CMake-3.9.4-GCCcore-6.4.0.eb\n * $CFGS2/CMake-3.9.5-GCCcore-6.4.0.eb\n</code></pre> <p>We are going to reuse one of the latest EasyConfig available, for instance lets copy <code>$CFGS2/CMake-3.9.1.eb</code></p> <pre><code># Work in a dedicated directory\n$&gt; mkdir -p ~/software/CMake\n$&gt; cd ~/software/CMake\n\n$&gt; eb -S Cmake-3|less   # collect the definition of the CFGS2 variable\n$&gt; CFGS2=/home/users/svarrette/.local/easybuild/software/tools/EasyBuild/3.6.1/lib/python2.7/site-packages/easybuild_easyconfigs-3.6.1-py2.7.egg/easybuild/easyconfigs/c/CMake\n$&gt; cp $CFGS2/CMake-3.9.1.eb .\n$&gt; mv CMake-3.9.1.eb CMake-3.11.3.eb        # Adapt version suffix to the lastest realse\n</code></pre> <p>You need to perform the following changes (here: version upgrade, and adapted checksum)</p> <pre><code>--- CMake-3.9.1.eb      2018-06-08 10:56:24.447699000 +0200\n+++ CMake-3.11.3.eb     2018-06-08 11:07:39.716672000 +0200\n@@ -1,7 +1,7 @@\n easyblock = 'ConfigureMake'\n\n name = 'CMake'\n-version = '3.9.1'\n+version = '3.11.3'\n\n homepage = 'http://www.cmake.org'\n description = \"\"\"CMake, the cross-platform, open-source build system.\n@@ -11,7 +11,7 @@\n\n source_urls = ['http://www.cmake.org/files/v%(version_major_minor)s']\n sources = [SOURCELOWER_TAR_GZ]\n-checksums = ['d768ee83d217f91bb597b3ca2ac663da7a8603c97e1f1a5184bc01e0ad2b12bb']\n+checksums = ['287135b6beb7ffc1ccd02707271080bbf14c21d80c067ae2c0040e5f3508c39a']\n\n configopts = '-- -DCMAKE_USE_OPENSSL=1'\n</code></pre> <p>If the checksum is not provided on the official software page, you will need to compute it yourself by downloading the sources and collect the checksum:</p> <pre><code>$&gt; gsha256sum ~/Download/cmake-3.11.3.tar.gz\n287135b6beb7ffc1ccd02707271080bbf14c21d80c067ae2c0040e5f3508c39a  cmake-3.11.3.tar.gz\n</code></pre> <p>Let's build it:</p> <pre><code>$&gt;  eb ./CMake-3.11.3.eb -Dr\n== temporary log file in case of crash /tmp/eb-UX7APP/easybuild-gxnyIv.log\nDry run: printing build status of easyconfigs and dependencies\nCFGS=/mnt/irisgpfs/users/&lt;login&gt;/software/CMake\n * [ ] $CFGS/CMake-3.11.3.eb (module: devel/CMake/3.11.3)\n== Temporary log file(s) /tmp/eb-UX7APP/easybuild-gxnyIv.log* have been removed.\n== Temporary directory /tmp/eb-UX7APP has been removed.\n</code></pre> <p>Dependencies are fine, so let's build it:</p> <pre><code>$&gt; time eb ./CMake-3.11.3.eb -r\n== temporary log file in case of crash /tmp/eb-JjF92B/easybuild-RjzRjb.log\n== resolving dependencies ...\n== processing EasyBuild easyconfig /mnt/irisgpfs/users/&lt;login&gt;/software/CMake/CMake-3.11.3.eb\n== building and installing devel/CMake/3.11.3...\n== fetching files...\n== creating build dir, resetting environment...\n== unpacking...\n== patching...\n== preparing...\n== configuring...\n== building...\n== testing...\n== installing...\n== taking care of extensions...\n== postprocessing...\n== sanity checking...\n== cleaning up...\n== creating module...\n== permissions...\n== packaging...\n== COMPLETED: Installation ended successfully\n== Results of the build can be found in the log file(s) /home/users/&lt;login&gt;/.local/easybuild/software/devel/CMake/3.11.3/easybuild/easybuild-CMake-3.11.3-20180608.111611.log\n== Build succeeded for 1 out of 1\n== Temporary log file(s) /tmp/eb-JjF92B/easybuild-RjzRjb.log* have been removed.\n== Temporary directory /tmp/eb-JjF92B has been removed.\n\nreal    7m40.358s\nuser    5m56.442s\nsys 1m15.185s\n</code></pre> <p>Note you can follow the progress of the installation in a separate shell on the node:</p> <p>Check the result:</p> <pre><code>$&gt; module av CMake\n</code></pre> <p>That's all ;-)</p> <p>Final remaks</p> <p>This workflow (copying an existing recipy, adapting the filename, the version and the source checksum) covers most of the test cases. Yet sometimes you need to work on a more complex dependency check, in which case you'll need to adapt many eb files. In this case, for each build, you need to instruct Easybuild to search for easyconfigs also in the current directory, in which case you will use:</p> <pre><code>$&gt; eb &lt;filename&gt;.eb --robot=$PWD:$EASYBUILD_ROBOT -D\n$&gt; eb &lt;filename&gt;.eb --robot=$PWD:$EASYBUILD_ROBOT\n</code></pre>"},{"location":"development/build-tools/easybuild/#old-build-software-using-your-own-easyconfig-file","title":"(OLD) Build software using your own EasyConfig file","text":"<p>Below are obsolete instructions to write a full Easyconfig file, left for archiving and informal purposes.</p> <p>For this example, we create an EasyConfig file to build GZip 1.4 with the GOOLF toolchain. Open your favorite editor and create a file named <code>gzip-1.4-goolf-1.4.10.eb</code> with the following content:</p> <pre><code>easyblock = 'ConfigureMake'\n\nname = 'gzip'\nversion = '1.4'\n\nhomepage = 'http://www.gnu.org/software/gzip/'\ndescription = \"gzip (GNU zip) is a popular data compression program as a replacement for compress\"\n\n# use the GOOLF toolchain\ntoolchain = {'name': 'goolf', 'version': '1.4.10'}\n\n# specify that GCC compiler should be used to build gzip\npreconfigopts = \"CC='gcc'\"\n\n# source tarball filename\nsources = ['%s-%s.tar.gz'%(name,version)]\n\n# download location for source files\nsource_urls = ['http://ftpmirror.gnu.org/gzip']\n\n# make sure the gzip and gunzip binaries are available after installation\nsanity_check_paths = {\n                      'files': [\"bin/gunzip\", \"bin/gzip\"],\n                      'dirs': []\n                     }\n\n# run 'gzip -h' and 'gzip --version' after installation\nsanity_check_commands = [True, ('gzip', '--version')]\n</code></pre> <p>This is a simple EasyConfig. Most of the fields are self-descriptive. No build method is explicitely defined, so it uses by default the standard configure/make/make install approach.</p> <p>Let's build GZip with this EasyConfig file:</p> <pre><code>$&gt; time eb gzip-1.4-goolf-1.4.10.eb\n\n== temporary log file in case of crash /tmp/eb-hiyyN1/easybuild-ynLsHC.log\n== processing EasyBuild easyconfig /mnt/nfs/users/homedirs/mschmitt/gzip-1.4-goolf-1.4.10.eb\n== building and installing base/gzip/1.4-goolf-1.4.10...\n== fetching files...\n== creating build dir, resetting environment...\n== unpacking...\n== patching...\n== preparing...\n== configuring...\n== building...\n== testing...\n== installing...\n== taking care of extensions...\n== packaging...\n== postprocessing...\n== sanity checking...\n== cleaning up...\n== creating module...\n== COMPLETED: Installation ended successfully\n== Results of the build can be found in the log file /home/users/mschmitt/.local/easybuild/software/base/gzip/1.4-goolf-1.4.10/easybuild/easybuild-gzip-1.4-20150624.114745.log\n== Build succeeded for 1 out of 1\n== temporary log file(s) /tmp/eb-hiyyN1/easybuild-ynLsHC.log* have been removed.\n== temporary directory /tmp/eb-hiyyN1 has been removed.\n\nreal    1m39.982s\nuser    0m52.743s\nsys     0m11.297s\n</code></pre> <p>We can now check that our version of GZip is available via the modules:</p> <pre><code>$&gt; module avail gzip\n\n--------- /mnt/nfs/users/homedirs/mschmitt/.local/easybuild/modules/all ---------\n    base/gzip/1.4-goolf-1.4.10\n</code></pre>"},{"location":"development/build-tools/easybuild/#to-go-further-into-details","title":"To go further into details","text":"<p>Please refer to the following pointers to get additionnal features:</p> <ul> <li>EasyBuild homepage</li> <li>EasyBuild tutorial</li> <li>EasyBuild documentation</li> <li>Getting started</li> <li>Using EasyBuild</li> <li>Step-by-step guide</li> </ul>"},{"location":"development/performance-debugging-tools/advisor/","title":"Intel Advisor","text":"<p> Intel Advisor provides two workflows to help ensure that Fortran, C, and C++ applications can make the most of modern Intel processors. Advisor contains three key capabilities:</p> <ul> <li>Vectorization   Advisor   identifies loops that will benefit most from vectorization, specifies what is   blocking effective vectorization, finds the benefit of alternative data   reorganizations, and increases the confidence that vectorization is safe.</li> <li>Threading   Advisor is used   for threading design and prototyping and to analyze, design, tune, and check   threading design options without disrupting normal code development.</li> <li>Advisor   Roofline   enables visualization of actual performance against hardware-imposed   performance ceilings (rooflines) such as memory bandwidth and compute   capacity - which provide an ideal roadmap of potential optimization steps.</li> </ul> <p>The links to each capability above provide detailed information regarding how to use each feature in Advisor. For more information on Intel Advisor, visit this page.</p>"},{"location":"development/performance-debugging-tools/advisor/#environmental-models-for-advisor-on-ul-hpc","title":"Environmental models for Advisor on UL-HPC\u00b6","text":"<pre><code>module purge \nmodule load swenv/default-env/v1.2-20191021-production\nmodule load toolchain/intel/2019a\nmodule load perf/Advisor/2019_update4\nmodule load vis/GTK+/3.24.8-GCCcore-8.2.0\n</code></pre>"},{"location":"development/performance-debugging-tools/advisor/#interactive-mode","title":"Interactive mode","text":"<p><pre><code># Compilation\n$ icc -qopenmp example.c\n\n# Code execution\n$ export OMP_NUM_THREADS=16\n$ advixe-cl -collect survey -project-dir my_result -- ./a.out\n\n# Report collection\n$ advixe-cl -report survey -project-dir my_result\n\n# To see the result in GUI\n$ advixe-gui my_result\n</code></pre> </p> <p><code>$ advixe-cl</code> will list out the analysis types and <code>$ advixe-cl -hlep report</code> will list out available reports in Advisor.</p>"},{"location":"development/performance-debugging-tools/advisor/#batch-mode","title":"Batch mode","text":""},{"location":"development/performance-debugging-tools/advisor/#shared-memory-programming-model-openmp","title":"Shared memory programming model (OpenMP)","text":"<p>Example for the batch script: <pre><code>#!/bin/bash -l\n#SBATCH -J Advisor\n#SBATCH -N 1\n###SBATCH -A &lt;project_name&gt;\n#SBATCH -c 28\n#SBATCH --time=00:10:00\n#SBATCH -p batch\n\nmodule purge \nmodule load swenv/default-env/v1.2-20191021-production\nmodule load toolchain/intel/2019a\nmodule load perf/Advisor/2019_update4\nmodule load vis/GTK+/3.24.8-GCCcore-8.2.0\n\nexport OMP_NUM_THREADS=16\nadvixe-cl -collect survey -project-dir my_result -- ./a.out\n</code></pre></p>"},{"location":"development/performance-debugging-tools/advisor/#distributed-memory-programming-model-mpi","title":"Distributed memory programming model (MPI)","text":"<p>To compile just <code>MPI</code> application run <code>$ mpiicc example.c</code> and for <code>MPI+OpenMP</code> run <code>$ mpiicc -qopenmp example.c</code></p> <p>Example for the batch script: <pre><code>#!/bin/bash -l\n#SBATCH -J Advisor\n#SBATCH -N 2\n###SBATCH -A &lt;project_name&gt;\n#SBATCH --ntasks-per-node=28\n#SBATCH --time=00:10:00\n#SBATCH -p batch\n\nmodule purge \nmodule load swenv/default-env/v1.2-20191021-production\nmodule load toolchain/intel/2019a\nmodule load perf/Advisor/2019_update4\nmodule load vis/GTK+/3.24.8-GCCcore-8.2.0\n\nsrun -n ${SLURM_NTASKS} advixe-cl --collect survey --project-dir result -- ./a.out\n</code></pre> To collect the result and see the result in GUI use the below commands <pre><code># Report collection\n$ advixe-cl --report survey --project-dir result\n\n# Result visualization \n$ advixe-gui result\n</code></pre> The below figure shows the hybrid(MPI+OpenMP) programming analysis results:</p> <p></p> <p>Tip</p> <p>If you find some issues with the instructions above, please report it to us using support ticket.</p>"},{"location":"development/performance-debugging-tools/aps/","title":"Application Performance Snapshot (APS)","text":"<p> Application Performance Snapshot (APS) is a lightweight open source profiling tool developed by the Intel VTune developers. Use Application Performance Snapshot for a quick view into a shared memory or MPI application's use of available hardware (CPU, FPU, and memory). Application Performance Snapshot analyzes your application's time spent in MPI, MPI and OpenMP imbalance, memory access efficiency, FPU usage, and I/O and memory footprint. After analysis, it displays basic performance enhancement opportunities for systems using Intel platforms. Use this tool as a first step in application performance analysis to get a simple snapshot of key optimization areas and learn about profiling tools that specialize in particular aspects of application performance.</p>"},{"location":"development/performance-debugging-tools/aps/#prerequisites","title":"Prerequisites","text":"Optional Configuration <p>Optional: Use the following software to get an advanced metric set when running Application Performance Snapshot:</p> <ul> <li>Recommended compilers: Intel C/C++ or Fortran Compiler (other compilers can   be used, but information about OpenMP imbalance is only available from the   Intel OpenMP library)</li> <li>Use Intel MPI library version 2017 or later. Other MPICH-based MPI   implementations can be used, but information about MPI imbalance is only   available from the Intel MPI library. There is no support for OpenMPI.</li> </ul> <p>Optional: Enable system-wide monitoring to reduce collection overhead and collect memory bandwidth measurements. Use one of these options to enable system-wide monitoring:</p> <ul> <li>Set the <code>/proc/sys/kernel/perf_event_paranoid</code> value to 0 (or less), or</li> <li>Install the Intel VTune Amplifier drivers. Driver sources are available in   <code>&lt;APS_install_dir&gt;/internal/sepdk/src</code>. Installation instructions are   available online at   https://software.intel.com/en-us/vtune-amplifier-help-building-and-installing-the-sampling-drivers-for-linux-targets.</li> </ul> <p>Before running the tool, set up your environment appropriately:</p> <pre><code>module purge\nmodule load swenv/default-env/v1.2-20191021-production\nmodule load tools/VTune/2019_update4\nmodule load toolchain/intel/2019a\n</code></pre>"},{"location":"development/performance-debugging-tools/aps/#analyzing-shared-memory-applications","title":"Analyzing Shared Memory Applications","text":"<p>Run the following commands (interactive mode):</p> <pre><code># Compilation\n$ icc -qopenmp example.c\n\n# Code execution\naps --collection-mode=all -r report_output ./a.out\n</code></pre> <p><code>aps -help</code> will list out <code>--collection-mode=&lt;mode&gt;</code> available in APS.</p> <p><pre><code># To create a .html file\naps-report -g report_output\n\n# To open an APS results in the browser\nfirefox report_output_&lt;postfix&gt;.html\n</code></pre> The below figure shows the example of result can be seen in the browser:</p> <p></p> <pre><code># To see the command line output\n$ aps-report &lt;result_dir&gt;\n</code></pre> <p>Example for the batch script: <pre><code>#!/bin/bash -l\n#SBATCH -J APS\n#SBATCH -N 1\n###SBATCH -A &lt;project_name&gt;\n#SBATCH -c 28\n#SBATCH --time=00:10:00\n#SBATCH -p batch\n#SBATCH --nodelist=node0xx\n\nmodule purge\nmodule load swenv/default-env/v1.2-20191021-production\nmodule load tools/VTune/2019_update4\nmodule load toolchain/intel/2019a\n\nexport OMP_NUM_THREADS=16\naps --collection-mode=all -r report_output ./a.out\n</code></pre></p>"},{"location":"development/performance-debugging-tools/aps/#analyzing-mpi-applications","title":"Analyzing MPI Applications","text":"<p>To compile just <code>MPI</code> application run <code>$ mpiicc example.c</code> and for <code>MPI+OpenMP</code> run <code>$ mpiicc -qopenmp example.c</code></p> <p>Example for the batch script: <pre><code>#!/bin/bash -l\n#SBATCH -J APS\n#SBATCH -N 2\n###SBATCH -A &lt;project_name&gt;\n#SBATCH --ntasks-per-node=14\n#SBATCH -c 2\n#SBATCH --time=00:10:00\n#SBATCH -p batch\n#SBATCH --reservation=&lt;name&gt;\n\nmodule purge\nmodule load swenv/default-env/v1.2-20191021-production\nmodule load tools/VTune/2019_update4\nmodule load toolchain/intel/2019a\n\n# To collect all the results\nexport MPS_STAT_LEVEL=${SLURM_CPUS_PER_TASK:-1}\n# An option for the OpenMP+MPI application\nexport OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK:-1}\nsrun -n ${SLURM_NTASKS} aps --collection-mode=mpi -r result_output ./a.out\n</code></pre></p> <p>The below figure shows the hybrid(MPI+OpenMP) programming analysis results:</p> <p></p>"},{"location":"development/performance-debugging-tools/aps/#next-steps","title":"Next Steps","text":"<ul> <li>Intel Trace Analyzer and Collector   is a graphical tool for understanding MPI application behavior, quickly   identifying bottlenecks, improving correctness, and achieving high   performance for parallel cluster applications running on Intel architecture.   Improve weak and strong scaling for applications.   Get started.</li> <li>Intel VTune Amplifier   provides a deep insight into node-level performance including algorithmic   hotspot analysis, OpenMP threading, general exploration microarchitecture   analysis, memory access efficiency, and more. It supports C/C++, Fortran,   Java, Python, and profiling in containers.   Get started.</li> <li>Intel Advisor provides   two tools to help ensure your Fortran, C, and C++ applications realize full   performance potential on modern processors.   Get started.<ul> <li>Vectorization Advisor is an optimization tool to identify loops that will   benefit most from vectorization, analyze what is blocking effective   vectorization, and forecast the benefit of alternative data   reorganizations</li> <li>Threading Advisor is a threading design and prototyping tool to analyze,   design, tune, and check threading design options without disrupting a   regular environment</li> </ul> </li> </ul> Quick Metrics Reference <p>The following metrics are collected with Application Performance Snapshot. Additional detail about each of these metrics is available in the Intel VTune Amplifier online help.</p> <p>Elapsed Time: Execution time of specified application in seconds.</p> <p>SP GFLOPS: Number of single precision giga-floating point operations calculated per second. All double operations are converted to two single operations. SP GFLOPS metrics are only available for 3<sup>rd</sup> Generation Intel Core processors, 5<sup>th</sup> Generation Intel processors, and 6<sup>th</sup> Generation Intel processors.</p> <p>Cycles per Instruction Retired (CPI): The amount of time each executed instruction took measured by cycles. A CPI of 1 is considered acceptable for high performance computing (HPC) applications, but different application domains will have varied expected values. The CPI value tends to be greater when there is long-latency memory, floating-point, or SIMD operations, non-retired instructions due to branch mispredictions, or instruction starvation at the front end.</p> <p>MPI Time: Average time per process spent in MPI calls. This metric does not include the time spent in <code>MPI_Finalize</code>. High values could be caused by high wait times inside the library, active communications, or sub-optimal settings of the MPI library. The metric is available for MPICH-based MPIs.</p> <p>MPI Imbalance: CPU time spent by ranks spinning in waits on communication operations. A high value can be caused by application workload imbalance between ranks, or non-optimal communication schema or MPI library settings. This metric is available only for Intel MPI Library version 2017 and later.</p> <p>OpenMP Imbalance: Percentage of elapsed time that your application wastes at OpenMP synchronization barriers because of load imbalance. This metric is only available for the Intel OpenMP Runtime Library.</p> <p>CPU Utilization: Estimate of the utilization of all logical CPU cores on the system by your application. Use this metric to help evaluate the parallel efficiency of your application. A utilization of 100% means that your application keeps all of the logical CPU cores busy for the entire time that it runs. Note that the metric does not distinguish between useful application work and the time that is spent in parallel runtimes.</p> <p>Memory Stalls: Indicates how memory subsystem issues affect application performance. This metric measures a fraction of slots where pipeline could be stalled due to demand load or store instructions. If the metric value is high, review the Cache and DRAM Stalls and the percent of remote accesses metrics to understand the nature of memory-related performance bottlenecks. If the average memory bandwidth numbers are close to the system bandwidth limit, optimization techniques for memory bound applications may be required to avoid memory stalls.</p> <p>FPU Utilization: The effective FPU usage while the application was running. Use the FPU Utilization value to evaluate the vector efficiency of your application. The value is calculated by estimating the percentage of operations that are performed by the FPU. A value of 100% means that the FPU is fully loaded. Any value over 50% requires additional analysis. FPU metrics are only available for 3<sup>rd</sup> Generation Intel Core processors, 5<sup>th</sup> Generation Intel processors, and 6<sup>th</sup> Generation Intel processors.</p> <p>I/O Operations: The time spent by the application while reading data from the disk or writing data to the disk. Read and Write values denote mean and maximum amounts of data read and written during the elapsed time. This metric is only available for MPI applications.</p> <p>Memory Footprint: Average per-rank and per-node consumption of both virtual and resident memory.</p>"},{"location":"development/performance-debugging-tools/aps/#documentation-and-resources","title":"Documentation and Resources","text":"<ul> <li>Intel Performance Snapshot User Forum:   User forum dedicated to all Intel Performance Snapshot tools, including   Application Performance Snapshot</li> <li>Application Performance Snapshot:   Application Performance Snapshot product page, see this page for support and   online documentation</li> <li>Application Performance Snapshot User's Guide:   Learn more about Application Performance Snapshot, including details on specific metrics and best practices for application optimization</li> </ul> <p>Tip</p> <p>If you find some issues with the instructions above, please report it to us using support ticket.</p>"},{"location":"development/performance-debugging-tools/arm-forge/","title":"Arm Forge","text":"<p> Arm Forge is the leading server and HPC development tool suite in research, industry, and academia for C, C++, Fortran, and Python high performance code on Linux. Arm Forge includes Arm DDT, the best debugger for time-saving high performance application debugging, Arm MAP, the trusted performance profiler for invaluable optimization advice, and Arm Performance Reports to help you analyze your HPC application runs.</p>"},{"location":"development/performance-debugging-tools/arm-forge/#environmental-models-for-arm-forge-in-ulhpc","title":"Environmental models for Arm Forge in ULHPC","text":"<pre><code>module purge\nmodule load swenv/default-env/v1.2-20191021-production\nmodule load toolchain/intel/2019a\nmodule load tools/ArmForge/19.1\nmodule load tools/ArmReports/19.1\n</code></pre>"},{"location":"development/performance-debugging-tools/arm-forge/#interactive-mode","title":"Interactive Mode","text":"<p>To compile <pre><code>$ icc -qopenmp example.c\n</code></pre> For debugging, profiling and analysing <pre><code># for debugging\n$ ddt ./a .out\n\n# for profiling\n$ map ./a .out\n\n# for analysis\n$ perf-report ./a .out\n</code></pre></p>"},{"location":"development/performance-debugging-tools/arm-forge/#batch-mode","title":"Batch Mode","text":""},{"location":"development/performance-debugging-tools/arm-forge/#shared-memory-programming-model-openmp","title":"Shared memory programming model (OpenMP)","text":"<p>Example for the batch script:</p> <pre><code>#!/bin/bash -l\n#SBATCH -J ArmForge\n#SBATCH -N 1\n###SBATCH -A &lt;project_name&gt;\n#SBATCH -c 16\n#SBATCH --time=00:10:00\n#SBATCH -p batch\n\nmodule purge\nmodule load swenv/default-env/v1.2-20191021-production\nmodule load toolchain/intel/2019a\nmodule load tools/ArmForge/19.1\nmodule load tools/ArmReports/19.1\n\nexport OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK:-1}\n\n# for debugging\n$ ddt ./a .out\n\n# for profiling\n$ map ./a .out\n\n# for analysis\n$ perf-report ./a .out\n</code></pre>"},{"location":"development/performance-debugging-tools/arm-forge/#distributed-memory-programming-model-mpi","title":"Distributed memory programming model (MPI)","text":"<p>Example for the batch script:</p> <p><pre><code>#!/bin/bash -l\n#SBATCH -J ArmForge\n###SBATCH -A &lt;project_name&gt;\n#SBATCH -N 2\n#SBATCH --ntasks-per-node 28\n#SBATCH --time=00:10:00\n#SBATCH -p batch\n\nmodule purge\nmodule load swenv/default-env/v1.2-20191021-production\nmodule load toolchain/intel/2019a\nmodule load tools/ArmForge/19.1\nmodule load tools/ArmReports/19.1\n\n# for debugging\n$ ddt srun -n ${SLURM_NTASKS} ./a .out\n\n# for profiling\n$ map srun -n ${SLURM_NTASKS} ./a .out\n\n# for analysis\n$ perf-report srun -n ${SLURM_NTASKS} ./a .out\n</code></pre> To see the result </p> <p>Tip</p> <p>If you find some issues with the instructions above, please report it to us using support ticket.</p>"},{"location":"development/performance-debugging-tools/inspector/","title":"Intel Inspector","text":"<p> Intel Inspector is a memory and threading error checking tool for users developing serial and multithreaded applications on Windows and Linux operating systems. The essential features of Intel Inspector for Linux are:</p> <ul> <li>Standalone GUI and command-line environments</li> <li>Preset analysis configurations (with some configurable settings) and the   ability to create custom analysis configurations to help the user control   analysis scope and cost</li> <li>Interactive debugging capability so one can investigate problems more deeply   during the analysis</li> <li>A large number of reported memory errors, including on-demand memory leak   detection</li> <li>Memory growth measurement to help ensure that the application uses no more   memory than expected</li> <li>Data race, deadlock, lock hierarchy violation, and cross-thread stack access   error detection</li> </ul>"},{"location":"development/performance-debugging-tools/inspector/#options-for-the-collect-action","title":"Options for the Collect Action","text":"Option Description mi1 Detect memory leaks mi2 Detect memory problems mi3 Locate memory problems ti1 Detect deadlocks ti2 Detect deadlocks and data races ti3 Locate deadlocks and data races"},{"location":"development/performance-debugging-tools/inspector/#options-for-the-report-action","title":"Options for the Report Action","text":"Option Description summary A brief statement of the total number of new problems found grouped by problem type problems A detailed report of detected problem sets in the result, along with their location in the source code observations A detailed report of all code locations used to form new problem sets status A brief statement of the total number of detected problems and the number that are not investigated, grouped by category <p>For more information on Intel Inspector, please visit https://software.intel.com/en-us/intel-inspector-xe.</p>"},{"location":"development/performance-debugging-tools/inspector/#environmental-models-for-inspector-on-ul-hpc","title":"Environmental models for Inspector on UL-HPC","text":"<pre><code>module purge \nmodule load swenv/default-env/v1.2-20191021-production\nmodule load toolchain/intel/2019a\nmodule load tools/Inspector/2019_update4\nmodule load vis/GTK+/3.24.8-GCCcore-8.2.0\n</code></pre>"},{"location":"development/performance-debugging-tools/inspector/#interactive-mode","title":"Interactive Mode","text":"<p>To launch Inspector on Iris, we recommend that you use the command line tool  <code>inspxe-cl</code>  to collect data via batch jobs and then display results using the GUI, <code>inspxe-gui</code>, on a login node.</p> <pre><code># Compilation\n$ icc -qopenmp example.cc\n\n# Result collection\n$ inspxe-cl -collect mi1 -result-dir mi1 -- ./a.out\n\n# Result view\n$ cat inspxe-cl.txt\n=== Start: [2020/04/08 02:11:50] ===\n2 new problem(s) found\n1 Memory leak problem(s) detected\n1 Memory not deallocated problem(s) detected\n=== End: [2020/04/08 02:11:55] ===\n</code></pre>"},{"location":"development/performance-debugging-tools/inspector/#batch-mode","title":"Batch Mode","text":""},{"location":"development/performance-debugging-tools/inspector/#shared-memory-programming-model-openmp","title":"Shared memory programming model (OpenMP)","text":"<p>Example for the batch script:</p> <p><pre><code>#!/bin/bash -l\n#SBATCH -J Inspector\n#SBATCH -N 1\n###SBATCH -A &lt;project_name&gt;\n#SBATCH -c 28\n#SBATCH --time=00:10:00\n#SBATCH -p batch\n\nmodule purge \nmodule load swenv/default-env/v1.2-20191021-production\nmodule load toolchain/intel/2019a\nmodule load tools/Inspector/2019_update4\nmodule load vis/GTK+/3.24.8-GCCcore-8.2.0\n\ninspxe-cl -collect mi1 -result-dir mi1 -- ./a.out`\n</code></pre> To see the result:</p> <pre><code># Result view\n$ cat inspxe-cl.txt\n=== Start: [2020/04/08 02:11:50] ===\n2 new problem(s) found\n1 Memory leak problem(s) detected\n1 Memory not deallocated problem(s) detected\n=== End: [2020/04/08 02:11:55] ===\n</code></pre>"},{"location":"development/performance-debugging-tools/inspector/#distributed-memory-programming-model-mpi","title":"Distributed memory programming model (MPI)","text":"<p>To compile: <pre><code># Compilation\n$ mpiicc -qopenmp example.cc\n</code></pre> Example for batch script: <pre><code>#!/bin/bash -l\n#SBATCH -J Inspector\n#SBATCH -N 2\n###SBATCH -A &lt;project_name&gt;\n#SBATCH --ntasks-per-node 28\n#SBATCH --time=00:10:00\n#SBATCH -p batch\n\nmodule purge \nmodule load swenv/default-env/v1.2-20191021-production\nmodule load toolchain/intel/2019a\nmodule load tools/Inspector/2019_update4\nmodule load vis/GTK+/3.24.8-GCCcore-8.2.0\n\nsrun -n {SLURM_NTASKS} inspxe-cl -collect=ti2 -r result ./a.out\n</code></pre></p> <p>To see result output: <pre><code>$ cat inspxe-cl.txt\n0 new problem(s) found\n=== End: [2020/04/08 16:41:56] ===\n=== End: [2020/04/08 16:41:56] ===\n0 new problem(s) found\n=== End: [2020/04/08 16:41:56] ===\n</code></pre></p> <p>Tip</p> <p>If you find some issues with the instructions above, please report it to us using support ticket.</p>"},{"location":"development/performance-debugging-tools/itac/","title":"Intel Trace Analyzer and Collector","text":"<p> Intel Trace Analyzer and Collector (ITAC) are two tools used for analyzing MPI behavior in parallel applications. ITAC identifies MPI load imbalance and communication hotspots in order to help developers optimize MPI parallelization and minimize communication and synchronization in their applications. Using Trace Collector on Cori must be done with a command line interface, while Trace Analyzer supports both a command line and graphical user interface which analyzes the data from Trace Collector.</p>"},{"location":"development/performance-debugging-tools/itac/#environmental-models-for-itac-in-ulhpc","title":"Environmental models for ITAC in ULHPC","text":"<pre><code>module load purge\nmodule load swenv/default-env/v1.2-20191021-production\nmodule load toolchain/intel/2019a\nmodule load tools/itac/2019.4.036\nmodule load vis/GTK+/3.24.8-GCCcore-8.2.0\n</code></pre>"},{"location":"development/performance-debugging-tools/itac/#interactive-mode","title":"Interactive mode","text":"<pre><code># Compilation\n$ icc -qopenmp -trance example.c\n\n# Code execution\n$ export OMP_NUM_THREADS=16\n$ -trace-collective ./a.out\n\n# Report collection\n$ export VT_STATISTICS=ON\n$ stftool tracefile.stf --print-statistics\n</code></pre>"},{"location":"development/performance-debugging-tools/itac/#batch-mode","title":"Batch mode","text":""},{"location":"development/performance-debugging-tools/itac/#shared-memory-programming-model-openmp","title":"Shared memory programming model (OpenMP)","text":"<p>Example for the batch script: <pre><code>#!/bin/bash -l\n#SBATCH -J ITAC\n###SBATCH -A &lt;project_name&gt;\n#SBATCH -N 1\n#SBATCH -c 16\n#SBATCH --time=00:10:00\n#SBATCH -p batch\n\nmodule purge\nmodule load swenv/default-env/v1.2-20191021-production\nmodule load toolchain/intel/2019a\nmodule load tools/itac/2019.4.036\nmodule load vis/GTK+/3.24.8-GCCcore-8.2.0\n\n$ export OMP_NUM_THREADS=16\n$ -trace-collective ./a.out\n</code></pre></p> <p>To see the result <pre><code>$ export VT_STATISTICS=ON\n$ stftool tracefile.stf --print-statistics\n</code></pre></p>"},{"location":"development/performance-debugging-tools/itac/#distributed-memory-programming-model-mpi","title":"Distributed memory programming model (MPI)","text":"<p>To compile <pre><code>$ mpiicc -trace example.c\n</code></pre> Example for the batch script: <pre><code>#!/bin/bash -l\n#SBATCH -J ITAC\n###SBATCH -A &lt;project_name&gt;\n#SBATCH -N 2\n#SBATCH --ntasks-per-node=28\n#SBATCH --time=00:10:00\n#SBATCH -p batch\n\nmodule purge\nmodule load swenv/default-env/v1.2-20191021-production\nmodule load toolchain/intel/2019a\nmodule load tools/itac/2019.4.036\nmodule load vis/GTK+/3.24.8-GCCcore-8.2.0\n\nsrun -n ${SLURM_NTASKS} -trace-collective ./a.out\n</code></pre> To collect the result and see the result in GUI use the below commands <pre><code>$ export VT_STATISTICS=ON\n$ stftool tracefile.stf --print-statistics\n</code></pre></p> <p></p> <p></p> <p></p> <p>Tip</p> <p>If you find some issues with the instructions above, please report it to us using support ticket.</p>"},{"location":"development/performance-debugging-tools/scalasca/","title":"Scalasca","text":"<p> Scalasca is a performance analysis tool that supports large-scale systems, including IBM Blue Gene and CrayXT and small systems. The Scalasca provides information about the communication and synchronization among the processors. This information will help to do the performance analysis, optimization, and tunning of scientificcodes. Scalasca supports OpenMP, MPI, and hybrid programming model, and a analysis can be done by using the GUI which can be seen in below figure.</p> <p></p>"},{"location":"development/performance-debugging-tools/scalasca/#environmental-models-for-scalasca-on-ulhpc","title":"Environmental models for Scalasca on ULHPC","text":"<pre><code>module load purge\nmodule load swenv/default-env/v1.1-20180716-production\nmodule load toolchain/foss/2018a\nmodule load perf/Scalasca/2.3.1-foss-2018a\nmodule load perf/Score-P/3.1-foss-2018a\n</code></pre>"},{"location":"development/performance-debugging-tools/scalasca/#interactive-mode","title":"Interactive Mode","text":"<p>Work flow: <pre><code># instrument\n$ scorep mpicxx example.cc\n\n# analyze\nscalasca -analyze mpirun -n 28 ./a.out\n\n# examine\n$ scalasca -examine -s scorep_a_28_sum\nINFO: Post-processing runtime summarization report...\nINFO: Score report written to ./scorep_a_28_sum/scorep.score\n\n# graphical visualization\n$ scalasca -examine result_folder\n</code></pre></p>"},{"location":"development/performance-debugging-tools/scalasca/#batch-mode","title":"Batch mode","text":""},{"location":"development/performance-debugging-tools/scalasca/#shared-memory-programming-openmp","title":"Shared memory programming (OpenMP)","text":"<p><pre><code>#!/bin/bash -l\n#SBATCH -J Scalasca\n###SBATCH -A &lt;project_name&gt;\n#SBATCH -N 1\n#SBATCH -c 16\n#SBATCH --time=00:10:00\n#SBATCH -p batch\n\nmodule load purge\nmodule load swenv/default-env/v1.1-20180716-production\nmodule load toolchain/foss/2018a\nmodule load perf/Scalasca/2.3.1-foss-2018a\nmodule load perf/Score-P/3.1-foss-2018a\n\nexport OMP_NUM_THREADS=16\n\n# analyze\nscalasca -analyze ./a.out\n</code></pre> Report collection and visualization <pre><code># examine\n$ scalasca -examine -s scorep_a_28_sum\nINFO: Post-processing runtime summarization report...\nINFO: Score report written to ./scorep_a_28_sum/scorep.score\n\n# graphical visualization\n$ scalasca -examine result_folder\n</code></pre></p>"},{"location":"development/performance-debugging-tools/scalasca/#distributed-memory-programming-mpi","title":"Distributed memory programming (MPI)","text":"<pre><code>#!/bin/bash -l\n#SBATCH -J Scalasca\n###SBATCH -A &lt;project_name&gt;\n#SBATCH -N 2\n#SBATCH --ntasks-per-node=28\n#SBATCH --time=00:10:00\n#SBATCH -p batch\n\nmodule load purge\nmodule load swenv/default-env/v1.1-20180716-production\nmodule load toolchain/foss/2018a\nmodule load perf/Scalasca/2.3.1-foss-2018a\nmodule load perf/Score-P/3.1-foss-2018a\n\nscalasca -analyze srun -n ${SLURM_NTASKS} ./a.out\n</code></pre> <p>Report collection and visualization <pre><code># examine\n$ scalasca -examine -s scorep_a_56_sum\nINFO: Post-processing runtime summarization report...\nINFO: Score report written to ./scorep_a_56_sum/scorep.score\n\n# graphical visualization\n$ scalasca -examine result_folder\n</code></pre></p> <p>Tip</p> <p>If you find some issues with the instructions above, please report it to us using support ticket.</p>"},{"location":"development/performance-debugging-tools/valgrind/","title":"Valgrind","text":"<p> The Valgrind tool suite provides a number of debugging and profiling tools that help you make your programs faster and more correct. The most popular of these tools is called Memcheck which can detect many memory-related errors and memory leaks.</p>"},{"location":"development/performance-debugging-tools/valgrind/#prepare-your-program","title":"Prepare Your Program","text":"<p>Compile your program with -g to include debugging information so that Memcheck's error messages include exact line numbers. Using -O0 is also a good idea, if you can tolerate the slowdown. With -O1 line numbers in error messages can be inaccurate, although generally speaking running Memcheck on code compiled at -O1 works fairly well, and the speed improvement compared to running -O0 is quite significant. Use of -O2 and above is not recommended as Memcheck occasionally reports uninitialised-value errors which don't really exist.</p>"},{"location":"development/performance-debugging-tools/valgrind/#environmental-models-for-valgrind-in-ulhpc","title":"Environmental models for Valgrind in ULHPC","text":"<pre><code>$ module purge\n$ module load debugger/Valgrind/3.15.0-intel-2019a\n</code></pre>"},{"location":"development/performance-debugging-tools/valgrind/#interactive-mode","title":"Interactive mode","text":"<p>Example code: <pre><code>#include &lt;iostream&gt;                                                                                           \nusing namespace std;                                                                                          \nint main()                                                                                                    \n{                                                                                                             \n  const int SIZE = 1000;                                                                                      \n  int *array = new int(SIZE);                                                                                 \n\n  for(int i=0; i&lt;SIZE; i++)                                                                                   \n    array[i] = i+1;                                                                                           \n\n  // delete[] array                                                                                           \n\n  return 0;                                                                                                   \n}\n</code></pre></p> <p><pre><code># Compilation\n$ icc -g example.cc\n\n# Code execution\n$ valgrind --leak-check=full --show-leak-kinds=all ./a.out\n</code></pre> Result output (with leak)</p> <p>If we do not delete <code>delete[] array</code> the memory, then there will be a memory leak. <pre><code>==26756== Memcheck, a memory error detector\n==26756== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.\n==26756== Using Valgrind-3.15.0 and LibVEX; rerun with -h for copyright info\n==26756== Command: ./a.out\n==26756== \n==26756== Invalid write of size 4\n==26756==    at 0x401275: main (mem-leak.cc:10)\n==26756==  Address 0x5309c84 is 0 bytes after a block of size 4 alloc'd\n==26756==    at 0x402DBE9: operator new(unsigned long) (vg_replace_malloc.c:344)\n==26756==    by 0x401265: main (mem-leak.cc:8)\n==26756== \n==26756== \n==26756== HEAP SUMMARY:\n==26756==     in use at exit: 4 bytes in 1 blocks\n==26756==   total heap usage: 2 allocs, 1 frees, 72,708 bytes allocated\n==26756== \n==26756== 4 bytes in 1 blocks are definitely lost in loss record 1 of 1\n==26756==    at 0x402DBE9: operator new(unsigned long) (vg_replace_malloc.c:344)\n==26756==    by 0x401265: main (mem-leak.cc:8)\n==26756== \n==26756== LEAK SUMMARY:\n==26756==    definitely lost: 4 bytes in 1 blocks\n==26756==    indirectly lost: 0 bytes in 0 blocks\n==26756==      possibly lost: 0 bytes in 0 blocks\n==26756==    still reachable: 0 bytes in 0 blocks\n==26756==         suppressed: 0 bytes in 0 blocks\n==26756== \n==26756== For lists of detected and suppressed errors, rerun with: -s\n==26756== ERROR SUMMARY: 1000 errors from 2 contexts (suppressed: 0 from 0)\n</code></pre></p> <p>Result output (without leak)</p> <p>When we delete <code>delete[] array</code> the allocated memory, there will not be leaked memory. <pre><code>==26172== Memcheck, a memory error detector\n==26172== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.\n==26172== Using Valgrind-3.15.0 and LibVEX; rerun with -h for copyright info\n==26172== Command: ./a.out\n==26172== \n==26172== \n==26172== HEAP SUMMARY:\n==26172==     in use at exit: 4 bytes in 1 blocks\n==26172==   total heap usage: 2 allocs, 1 frees, 72,708 bytes allocated\n==26172== \n==26172== 4 bytes in 1 blocks are definitely lost in loss record 1 of 1\n==26172==    at 0x402DBE9: operator new(unsigned long) (vg_replace_malloc.c:344)\n==26172==    by 0x401283: main (in /mnt/irisgpfs/users/ekrishnasamy/BPG/Valgrind/a.out)\n==26172== \n==26172== LEAK SUMMARY:\n==26172==    definitely lost: 4 bytes in 1 blocks\n==26172==    indirectly lost: 0 bytes in 0 blocks\n==26172==      possibly lost: 0 bytes in 0 blocks\n==26172==    still reachable: 0 bytes in 0 blocks\n==26172==         suppressed: 0 bytes in 0 blocks\n==26172== \n==26172== For lists of detected and suppressed errors, rerun with: -s\n==26172== ERROR SUMMARY: 1 errors from 1 contexts (suppressed: 0 from 0)\n</code></pre></p>"},{"location":"development/performance-debugging-tools/valgrind/#additional-information","title":"Additional information","text":"<p>This page is based on the \"Valgrind Quick Start Page\". For more information about valgrind, please refer to http://valgrind.org/.</p> <p>Tip</p> <p>If you find some issues with the instructions above, please report it to us using support ticket.</p>"},{"location":"development/performance-debugging-tools/vtune/","title":"VTune","text":"<p> Use Intel VTune Profiler to profile serial and multithreaded applications that are executed on a variety of hardware platforms (CPU, GPU, FPGA). The tool is delivered as a Performance Profiler with Intel Performance Snapshots and supports local and remote target analysis on the Windows, Linux, and Android* platforms. Without the right data, you\u2019re guessing about how to improve software performance and are unlikely to make the most effective improvements. Intel\u00ae VTune\u2122 Profiler collects key profiling data and presents it with a powerful interface that simplifies its analysis and interpretation. </p>"},{"location":"development/performance-debugging-tools/vtune/#environmental-models-for-vtune-on-ulhpc","title":"Environmental models for VTune on ULHPC:","text":"<pre><code>module purge \nmodule load swenv/default-env/v1.2-20191021-production\nmodule load toolchain/intel/2019a\nmodule load tools/VTune/2019_update4\nmodule load vis/GTK+/3.24.8-GCCcore-8.2.0\n</code></pre>"},{"location":"development/performance-debugging-tools/vtune/#interactive-mode","title":"Interactive Mode","text":"<p><pre><code># Compilation\n$ icc -qopenmp example.c\n\n# Code execution\n$ export OMP_NUM_THREADS=16\n$ amplxe-cl -collect hotspots -r my_result ./a.out\n</code></pre> To see the result in GUI <code>$ amplxe-gui my_result</code></p> <p></p> <p><code>$ amplxe-cl</code> will list out the analysis types and <code>$ amplxe-cl -hlep report</code> will list out available reports in VTune.</p>"},{"location":"development/performance-debugging-tools/vtune/#batch-mode","title":"Batch Mode","text":""},{"location":"development/performance-debugging-tools/vtune/#shared-memory-programming-model-openmp","title":"Shared Memory Programming Model (OpenMP)","text":"<pre><code>#!/bin/bash -l\n#SBATCH -J VTune\n###SBATCH -A &lt;project_name&gt;\n#SBATCH -N 1\n#SBATCH -c 28\n#SBATCH --time=00:10:00\n#SBATCH -p batch\n\nmodule purge \nmodule load swenv/default-env/v1.2-20191021-production\nmodule load toolchain/intel/2019a\nmodule load tools/VTune/2019_update4\nmodule load vis/GTK+/3.24.8-GCCcore-8.2.0\n\nexport OMP_NUM_THREADS=16\namplxe-cl -collect hotspots-r my_result ./a.out\n</code></pre>"},{"location":"development/performance-debugging-tools/vtune/#distributed-memory-programming-model","title":"Distributed Memory Programming Model","text":"<p>To compile just <code>MPI</code> application run <code>$ mpiicc example.c</code> and for <code>MPI+OpenMP</code> run <code>$ mpiicc -qopenmp example.c</code></p> <pre><code>#!/bin/bash -l\n#SBATCH -J VTune\n###SBATCH -A &lt;project_name&gt;\n#SBATCH -N 2\n#SBATCH --ntasks-per-node=28\n#SBATCH --time=00:10:00\n#SBATCH -p batch\n\nmodule purge \nmodule load swenv/default-env/v1.2-20191021-production\nmodule load toolchain/intel/2019a\nmodule load tools/VTune/2019_update4\nmodule load vis/GTK+/3.24.8-GCCcore-8.2.0\n\nsrun -n ${SLURM_NTASKS} amplxe-cl -collect uarch-exploration -r vtune_mpi -- ./a.out\n</code></pre> <p><pre><code># Report collection\n$ amplxe-cl -report uarch-exploration -report-output output -r vtune_mpi\n\n# Result visualization \n$ amplxe-gui vtune_mpi\n</code></pre> The below figure shows the hybrid(MPI+OpenMP) programming analysis results:</p> <p></p> <p>Tip</p> <p>If you find some issues with the instructions above, please report it to us using support ticket.</p>"},{"location":"environment/","title":"ULHPC User Environment","text":"<p>Your typical journey on the ULHPC facility is illustrated in the below figure.</p> <p></p> Typical workflow on UL HPC resources <p>You daily interaction with the ULHPC facility includes the following actions:</p> <p>Preliminary setup</p> <ol> <li>Connect to the access/login servers<ul> <li>This can be done either by <code>ssh</code> (recommended) or via the ULHPC OOD portal</li> <li>(advanced users) at this point, you probably want to create (or reattach) to a <code>screen</code> or <code>tmux</code> session</li> </ul> </li> <li>Synchronize you code and/or transfer your input data using <code>rsync/svn/git</code> typically<ul> <li>recall that the different storage filesystems are shared (via a high-speed interconnect network) among the computational resources of the ULHPC facilities. In particular, it is sufficient to exchange data with the access servers to make them available on the clusters</li> </ul> </li> <li>Reserve a few interactive resources with <code>salloc -p interactive [...]</code><ul> <li>recall that the <code>module</code> command (used to load the ULHPC User    software) is only available on the compute    nodes</li> <li>(eventually) build your program, typically using <code>gcc/icc/mpicc/nvcc..</code></li> <li>Test your workflow / HPC analysis on a small size problem (<code>srun/python/sh...</code>)</li> <li>Prepare a launcher script <code>&lt;launcher&gt;.{sh|py}</code></li> </ul> </li> </ol> <p>Then you can proceed with your Real Experiments:</p> <ol> <li>Reserve passive resources: <code>sbatch [...] &lt;launcher&gt;</code></li> <li>Grab the results and (eventually) transfer back your output    results using <code>rsync/svn/git</code></li> </ol> <p>For more information:</p> <ul> <li>Getting Started</li> <li>Connecting to ULHPC supercomputers</li> <li>ULHPC Storage Systems Overview</li> </ul> <p>Getting error '<code>-bash: module: command not found</code>' on access/login servers</p> <p>Recall that by default, the <code>module</code> command is NOT available on the access/login servers (on purpose). The module command is only available on computing nodes, within a slurm job.</p>"},{"location":"environment/#home-and-directories-layout","title":"Home and Directories Layout","text":"<p>All UL HPC systems use global home directories. You also have access with environment variables to several other pre-defined directories setup over several different File Systems which co-exist on the UL HPC facility and are configured for different purposes. They are listed below:</p> <p>Cluster file systems</p> Directory Environment variable File system Backup Interconnect <code>/home/users/&lt;username&gt;</code> <code>${HOME}</code> GPFS/Spectrumscale<sup>[1]</sup> no Infiniband <code>/work/projects/&lt;project name&gt;</code> <code>${PROJECTHOME}/&lt;project name&gt;</code> GPFS/Spectrumscale<sup>[1]</sup> yes (partial, only <code>backup</code> subdirectory) Infiniband <code>/scratch/users/&lt;username&gt;</code> <code>${SCRATCH}</code> Lustre no Infiniband <code>/mnt/isilon/projects/&lt;project name&gt;</code> - OneFS yes (and live sync<sup>[2]</sup>) Ehternet <ol> <li> <p>The  file system mounted on the home directories (<code>/home/users</code>) and project directories (<code>/work/projects</code>) are both exported by the GPFS/Spectrumscale file system.</p> <ul> <li>Storage for both directories is redundant, so they are safe against hardware failure.</li> <li>Only <code>/home/users</code> is mirrored in a SSD cache, so <code>/home/users</code> is a significantly faster for random and small file I/O.</li> </ul> </li> <li> <p>Live sync replicates data across multiple OneFS instances for high availability.</p> </li> </ol>"},{"location":"environment/#shell-and-dotfiles","title":"Shell and Dotfiles","text":"<p>The default login shell is <code>bash</code> -- see <code>/etc/shells</code> for supported shells.</p> <p>ULHPC dotfiles vs. default dotfiles</p> <p>The ULHPC team DOES NOT populate shell initialization files (also known as dotfiles) on users' home directories - the default system ones are used in your home -- you can check them in <code>/etc/skel/.*</code> on the access/login servers. However, you may want to install the <code>ULHPC/dotfiles</code> available as a Github repository. See installation notes. A working copy of that repository exists in <code>/etc/dotfiles.d</code> on the access/login servers. You can thus use it: <pre><code>$ /etc/dotfiles.d/install.sh -h\n# Example to install ULHPC GNU screen configuration file\n$ /etc/dotfiles.d/install.sh -d /etc/dotfiles.d/ --screen -n   # Dry-run\n$ /etc/dotfiles.d/install.sh -d /etc/dotfiles.d/ --screen      # real install\n</code></pre></p> Changing Default Login Shell (or NOT) <p>If you want to change your your default login shell, you should set that up using the ULHPC IPA portal (change the Login Shell attribute). Note however that we STRONGLY discourage you to do so. You may hit unexpected issues with system profile scripts expecting <code>bash</code> as running shell.</p>"},{"location":"environment/#system-profile","title":"System Profile","text":"<p><code>/etc/profile</code> contains Linux system wide environment and startup programs. Specific scripts are set to improve your ULHPC experience, in particular those set in the <code>ULHPC/tools</code> repository, for instance:</p> <ul> <li>/etc/profile.d/slurm-prompt.sh: provide info of your running Slurm job on your prompt</li> <li>/etc/profile.d/slurm.sh: several helper function to</li> </ul>"},{"location":"environment/#customizing-shell-environment","title":"Customizing Shell Environment","text":"<p>You can create dotfiles (e.g., <code>.bashrc</code>, <code>.bash_profile</code>, or <code>.profile</code>, etc) in your <code>$HOME</code> directory to put your personal shell modifications.</p> Custom Bash Initialisation Files <p>On ULHPC system <code>~/.bash_profile</code> and <code>~/.profile</code> are sourced by login shells, while <code>~/.bashrc</code> is sourced by most of the shell invocations including the login shells. In general you can put the environment variables, such as <code>PATH</code>, which are inheritable to subshells in <code>~/.bash_profile</code> or <code>~/.profile</code> and functions and aliases in the <code>~/.bashrc</code> file in order to make them available in subshells. <code>ULHPC/dotfiles</code> bash configuration even source the following files for that specific purpose:</p> <ul> <li><code>~/.bash_private</code>: custom private functions</li> <li><code>~/.bash_aliases</code>: custom private aliases.</li> </ul> Understanding Bash Startup Files order <p>See reference documentation. That's somehow hard to understand. Some tried to explicit it under the form of a \"simple\" graph -- credits for the one below to Ian Miell (another one)</p> <p></p> <p>This explains why normally all ULHPC launcher scripts start with the following sha-bang (<code>#!</code>) header</p> <p><pre><code>#!/bin/bash -l\n#\n#SBATCH [...]\n[...]\n</code></pre> That's indeed the only way (i.e. using <code>/bin/bash -l</code> instead of the classical <code>/bin/bash</code>) to ensure that <code>/etc/profile</code> is sourced natively, and thus that all ULHPC environments variables and modules are loaded. If you don't proceed that way (i.e. following the classical approach), you MUST then use the following template you may see from other HPC centers: <pre><code>#!/bin/bash\n#\n#SBATCH [...]\n[...]\n# Load ULHPC Profile\nif [ -f  /etc/profile ]; then\n   .  /etc/profile\nfi\n</code></pre></p> <p>Since all ULHPC systems share the Global HOME filesystem, the same <code>$HOME</code> is available regardless of the platform. To make system specific customizations use the pre-defined environment <code>ULHPC_CLUSTER</code> variable:</p> <p>Example of cluster specific settings</p> <pre><code>```shell\ncase $ULHPC_CLUSTER in\n    \"iris\")\n        : # Settings for iris\n        export MYVARIABLE=\"value-for-iris\"\n        ;;\n    \"aion\")\n        : # settings for aion\n        export MYVARIABLE=\"value-for-aion\"\n        ;;\n    *)\n        : # default value for\n        export MYVARIABLE=\"default-value\"\n        ;;\nesac\n```\n</code></pre> <p></p>"},{"location":"environment/#operating-systems","title":"Operating Systems","text":"<p>The ULHPC facility runs RedHat-based Linux Distributions, in particular:</p> <ul> <li>the Iris cluster and the Aion cluster run RedHat (RHEL) Linux operating system, version 8 on the access and compute nodes. Servers (not accessible to users) run Rocky Linux 8, which is RHEL compatible, when appropriate. </li> <li>Experimental Grid5000 clusters run Debian Linux, version 11</li> </ul> <p>Thus, you are more than encouraged to become familiar - if not yet - with Linux commands. We can recommend the following sites and resources:</p> <ul> <li>Software Carpentry: The Unix Shell</li> <li>Unix/Linux Command Reference</li> </ul>"},{"location":"environment/#discovering-visualizing-and-reserving-ul-hpc-resources","title":"Discovering, visualizing and reserving UL HPC resources","text":"<p>See ULHPC Tutorial / Getting Started</p>"},{"location":"environment/#ulhpc-user-software-environment","title":"ULHPC User Software Environment","text":"<p>The UL HPC facilities provides a large variety of scientific applications to its user community, including domain-specific codes and general purpose development tools for a wide range of applications.<sup>1</sup> An environment module system, LMod, is used to manage the shell environment and provide access to installed software.</p> <p>The main advantages of using an environment module system are the following:</p> <ol> <li>Many different versions and/or installations of a single software package can be provided on a given machine, including a default version as well as several older and newer version.</li> <li>Users can easily switch to different versions or installations of a software package without having to explicitly modify their shell environment.</li> </ol> <p>Most UL HPC modules are automatically generated by Easybuild.</p> <p></p> <p>EasyBuild is a software build and installation framework that allows you to manage scientific and other software on High Performance Computing systems in an efficient way. A large number of scientific software are supported (at least 3670 supported software packages since the 4.9.4 release).<sup>2</sup></p> <p>For several years now, Easybuild is used to manage the ULHPC User Software Set and generate automatically the module files available to you on our computational resources in either <code>release</code> (default) or <code>testing</code> (pre-release/testing) environment. This enables users to easily extend the global software set with their own local software builds, either stored within their global home directory, or preferably in a shared project directory. Easybuild generates automatically module files compliant with the ULHPC module setup.</p> <p> ULHPC Environment modules  Using Easybuild on ULHPC Clusters</p>"},{"location":"environment/#self-management-of-work-environments-in-ul-hpc-with-conda","title":"Self management of work environments in UL HPC with Conda","text":"<p>Packages provided through the standard channels of modules and containers are optimized for the ULHPC clusters to ensure their performance and stability. However, many packages where performance is not critical and are used by few users are not provided through the standard channels. These packages can still be installed locally by the users through an environment management system such as Conda.</p> <p>Contact the ULHPC before installing any software with Conda</p> <p>Prefer binaries provided through modules or containers. Conda installs generic binaries that may be suboptimal for the configuration of the ULHPC clusters. Furthermore, installing packages locally with Conda consumes quotas in your or your project's account in terms of storage space and number of files.</p> <p>Contact the ULHPC High Level Support Team in the service portal [Home &gt; Research &gt; HPC &gt; Software environment &gt; Request expertise] to discuss possible options before installing any software.</p> <p>Conda is an open source environment and package management system. With Conda you can create independent environments, where you can install applications such as python and R, together with any packages which will be used by these applications. The environments are independent, with the Conda package manager managing the binaries, resolving dependencies, and ensuring that package used in multiple environments are stored only once. In a typical setting, each user has their own installation of a Conda and a set of personal environments.</p> <p> Management of work environments with Conda</p> <ol> <li> <p>See our software list for a detailed list of available applications.\u00a0\u21a9</p> </li> <li> <p>See also \"What is EasyBuild?\".\u00a0\u21a9</p> </li> </ol>"},{"location":"environment/conda/","title":"Self management of work environments in UL HPC with Conda","text":"<p>Packages provided through the standard channels of modules and containers are optimized for the ULHPC clusters to ensure their performance and stability. However, many packages where performance is not critical and are used by few users are not provided through the standard channels. These packages can still be installed locally by the users through an environment management system such as Conda.</p> <p>Contact the ULHPC before installing any software with Conda</p> <p>Prefer binaries provided through modules or containers. Conda installs generic binaries that may be suboptimal for the configuration of the ULHPC clusters. Furthermore, installing packages locally with Conda consumes quotas in your or your project's account in terms of storage space and number of files.</p> <p>Contact the ULHPC High Level Support Team in the service portal [Home &gt; Research &gt; HPC &gt; Software environment &gt; Request expertise] to discuss possible options before installing any software.</p> <p>Conda is an open source environment and package management system. With Conda you can create independent environments, where you can install applications such as python and R, together with any packages which will be used by these applications. The environments are independent, with the Conda package manager managing the binaries, resolving dependencies, and ensuring that package used in multiple environments are stored only once. In a typical setting, each user has their own installation of a Conda and a set of personal environments.</p> <p>TL;DR: install and use the Micromamba package manager.</p>"},{"location":"environment/conda/#a-brief-introduction-to-conda","title":"A brief introduction to Conda","text":"<p>A few concepts are necessary to start working with Conda. In brief, these are package managers which are the programs used to create and manage environments, channels which are the repositories that contain the packages from which environments are composed, and distributions which are methods for shipping package managers.</p>"},{"location":"environment/conda/#package-managers","title":"Package managers","text":"<p>Package managers are the programs that install and manage the Conda environments. There are multiple package managers, such as <code>conda</code>, <code>mamba</code>, and <code>micromamba</code>.</p> <p>The UL HPC centre supports the use of <code>micromamba</code> for the creation and management of personal Conda environments.</p>"},{"location":"environment/conda/#channels","title":"Channels","text":"<p>Conda channels are the locations where packages are stored. There are also multiple channels, with some important channels being:</p> <ul> <li><code>defaults</code>, the default channel,</li> <li><code>anaconda</code>, a mirror of the default channel,</li> <li><code>bioconda</code>, a distribution of bioinformatics software, and</li> <li><code>conda-forge</code>, a community-led collection of recipes, build infrastructure, and distributions for the conda package manager.</li> </ul> <p>The most useful channel that comes pre-installed in all distributions, is Conda-Forge. Channels are usually hosted in the official Anaconda page, but in some rare occasions custom channels may be used. For instance the default channel is hosted independently from the official Anaconda page. Many channels also maintain web pages with documentation both for their usage and for packages they distribute:</p> <ul> <li>Default Conda channel</li> <li>Bioconda</li> <li>Conda-Forge</li> </ul>"},{"location":"environment/conda/#distributions","title":"Distributions","text":"<p>Quite often, the package manager is not distributed on its own, but with a set of packages that are required for the package manager to work, or even with some additional packages that required for most applications. For instance, the <code>conda</code> package manager is distributed with the Miniconda and Anaconda distributions. Miniconda contains the bare minimum packages for the <code>conda</code> package manager to work, and Anaconda contains multiple commonly used packages and a graphical user interface. The relation between these distributions and the package manager is depicted in the following diagram.</p> <p></p> <p>The situation is similar for Mamba distributions. These distributions are supported by Conda-Forge, and their default installation options set-up <code>conda-forge</code> as the default and only channel during installation. The <code>defaults</code> or its mirror <code>anaconda</code> must be explicitly added if required. The distribution using the Mamba package manager was originally distributed as Mambaforge and was recently renamed to Miniforge. Miniforge comes with a minimal set of python packages required by the Mamba package manager. The distribution using the Micromamba package manager ships no accompanying packages, as Micromamba is a standalone executable with no dependencies. Micromamba is using <code>libmamba</code>, a C++ library implementing the Conda API.</p>"},{"location":"environment/conda/#the-micromamba-package-manager","title":"The Micromamba package manager","text":"<p>The Micromaba package manager is a minimal yet fairly complete implementation of the Conda interface in C++, that is shipped as a standalone executable. The package manager operates strictly on the user-space and thus it requires no special permissions are required to install packages. It maintains all its files in a couple of places, so uninstalling the package manager itself is also easy. Finally, the package manager is also lightweight and fast.</p> <p>UL HPC provides support only for the Micromamba package manager.</p>"},{"location":"environment/conda/#installation","title":"Installation","text":"<p>A complete guide regarding Micromamba installation can be found in the official documentation. To install micromamaba in the HPC clusters, log in to Aion or Iris. Working on a login node, run the installation script, <pre><code>\"${SHELL}\" &lt;(curl -L micro.mamba.pm/install.sh)\n</code></pre> which will install the executable and setup the environment. There are 4 options to select during the installation of Micromamba:</p> <ul> <li>The directory for the installation of the binary file:   <pre><code>Micromamba binary folder? [~/.local/bin]\n</code></pre>   Leave empty and press enter to select the default displayed within brackets. Your <code>.bashrc</code> script should include <code>~/.local/bin</code> in the <code>$PATH</code> by default.</li> <li>The option to add to the environment autocomplete options for <code>micromamba</code>:   <pre><code>Init shell (bash)? [Y/n]\n</code></pre>   Press enter to select the default option <code>Y</code>. This will append a clearly marked section in the <code>.bashrc</code> shell. Do not forget to remove this section when uninstalling Micromamba.</li> <li>The option to configure the channels by adding conda-forge:   <pre><code>Configure conda-forge? [Y/n]\n</code></pre>   Press enter to select the default option <code>Y</code>. This will setup the <code>~/.condarc</code> file with <code>conda-forge</code> as the default channel. Note that Mamba and Micromamba will not use the <code>defaults</code> channel if it is not present in <code>~/.condarc</code> like <code>conda</code>.</li> <li>The option to select the directory where environment information and packages will be stored:   <pre><code>Prefix location? [~/micromamba]\n</code></pre>   Press enter to select the default option displayed within brackets.</li> </ul> <p>To setup the environment log-out and log-in again. Now you can use <code>micromamba</code>, including the auto-completion feature.</p>"},{"location":"environment/conda/#managing-environments","title":"Managing environments","text":"<p>As an example, the creation and use of an environment for R jobs is presented. The command, <pre><code>micromamba create --name R-project\n</code></pre> creates an environment named <code>R-project</code>. The environment is activated with the command <pre><code>micromamba activate R-project\n</code></pre> anywhere in the file system.</p> <p>Next, install the base R environment package that contains the R program, and any R packages required by the project. To install packages, first ensure that the <code>R-project</code> environment is active, and then install any package with the command <pre><code>micromamba install &lt;package_name&gt;\n</code></pre> all the required packages. Quite often, the channel name must also be specified: <pre><code>micromamba install --channel &lt;channel_name&gt; &lt;package_name&gt;\n</code></pre> Packages can be found by searching the conda-forge channel.</p> <p>For instance, the basic functionality of the R software environment is contained in the <code>r-base</code> package. Calling <pre><code>micromamba install --channel conda-forge r-base\n</code></pre> will install all the components required to run standalone R scripts. More involved scripts use functionality defined in various packages. The R packages are prepended with a prefix 'r-'. Thus, <code>plm</code> becomes <code>r-plm</code> and so on. After all the required packages have been installed, the environment is ready for use.</p> <p>Packages in the conda-forge channel come with instructions for their installation. Quite often the channel is specified in the installation instructions, <code>-c conda-forge</code> or <code>--channel conda-forge</code>. While the Micromamba installer sets-up <code>conda-forge</code> as the default channel, latter modification in <code>~/.condarc</code> may change the channel priority. Thus it is a good practice to explicitly specify the source channel when installing a package.</p> <p>After work in an environment is complete, deactivate the environment, <pre><code>micromamba deactivate\n</code></pre> to ensure that it does not interfere with any other operations. In contrast to modules, Conda is designed to operate with a single environment active at a time. Create one environment for each project, and Conda will ensure that any package that is shared between multiple environments is installed once.</p> <p>Micromamba supports almost all the subcommands of Conda. For more details see the official documentation.</p>"},{"location":"environment/conda/#using-environments-in-submission-scripts","title":"Using environments in submission scripts","text":"<p>Since all computationally heavy operations must be performed in compute nodes, Conda environments are also used in jobs submitted to the queuing system. Returning to the R example, a submission script running a single core R job can use the <code>R-project_name</code> environment as follows: <pre><code>#SBATCH --job-name R-test-job\n#SBATCH --nodes 1\n#SBATCH --ntasks-per-node 1\n#SBATCH --cpus-per-task 1\n#SBATCH --time=0-02:00:00\n#SBATCH --partition batch\n#SBATCH --qos normal\n\necho \"Launched at $(date)\"\necho \"Job ID: ${SLURM_JOBID}\"\necho \"Node list: ${SLURM_NODELIST}\"\necho \"Submit dir.: ${SLURM_SUBMIT_DIR}\"\necho \"Numb. of cores: ${SLURM_CPUS_PER_TASK}\"\n\nmicromamba activate R-project\n\nexport OMP_NUM_THREADS=1\nsrun Rscript --no-save --no-restore script.R\n\nmicromamba deactivate\n</code></pre></p> <p>Useful scripting resources</p> <ul> <li>Formatting submission scripts for R (and other systems)</li> </ul>"},{"location":"environment/conda/#cleaning-up-package-data","title":"Cleaning up package data","text":"<p>The Conda environment managers download and store a sizable amount of data to provided packages to the various environments. Even though the package data are shared between the various environments, they still consume space in your or your project's account. There are limits in the storage space and number of files that are available to projects and users in the cluster. Since Conda packages are self managed, you need to clean unused data yourself.</p> <p>There are two main sources of unused data, the compressed archives of the packages that Conda stores in its cache when downloading a package, and the data of removed packages. All unused data in Micromoamba can be removed with the command <pre><code>micromamba clean --all\n</code></pre> that opens up an interactive dialogue with details about the operations performed. You can follow the default option, unless you have manually edited any files in you package data directory (default location <code>${HOME}/micromamba</code>).</p> Updating environments to remove old package versions <p>As we create new environments, we often install the latest version of each package. However, if the environments are not updated regularly, we may end up with different versions of the same package across multiple environments. If we have the same version of a package installed in all environments, we can save space by removing unused older versions.</p> <p>To update a package across all environments, use the command <pre><code>for e in $(micromamba env list | awk 'FNR&gt;2 {print $1}'); do micromamba update --name $e &lt;package name&gt;; done\n</code></pre> and to update all packages across all environments <pre><code>for e in $(micromamba env list | awk 'FNR&gt;2 {print $1}'); do micromamba update --name $e --all; done\n</code></pre> where <code>FNR&gt;2</code> removes the headers in the output of <code>micromamba env list</code>, and is thus sensitive to changes in the user interface of Micromamba.</p> <p>After updating packages, the <code>clean</code> command can be called to removed the data of unused older package versions.</p> <p>Sources</p> <ul> <li>Oficial Conda <code>clean</code> documentation</li> <li>Understanding Conda <code>clean</code></li> </ul>"},{"location":"environment/conda/#combining-conda-with-other-package-and-environment-management-tools","title":"Combining Conda with other package and environment management tools","text":"<p>It may be desirable to use Conda to manage environments but a different tool to manage packages, such as <code>pip</code>. Or subenvironments may need to be used inside a Conda environment, as for instance with tools for creating and managing isolated Python installation, such as <code>virtualenv</code>, or with tools for integrating managed Python installations and packages in project directories, such as Pipenv and Poetry.</p> <p>Conda integrates well with any such tool. Some of the most frequent cases are described bellow.</p>"},{"location":"environment/conda/#managing-packages-with-external-tools","title":"Managing packages with external tools","text":"<p>Quite often a package that is required in an environment is not available through a Conda channel, but it is available through some other distribution channel, such as the Python Package Index (PyPI). In these cases the only solution is to create a Conda environment and install the required packages with <code>pip</code> from the Python Package Index.</p> <p>Using an external packaging tool is possible because of the method that Conda uses to install packages. Conda installs package versions in a central directory (e.g. <code>~/micromamba/pkgs</code>). Any environment that requires a package links to the central directory with hard links. Links are added to the home directory (e.g. <code>~/micromamba/envs/R-project</code> for the <code>R-project</code> environment) of any environment that requires them. When using an external package tool, package components are installed in the same directory where Conda would install the corresponding link. Thus, external package management tools integrate seamlessly with Conda, with a couple of caveats:</p> <ul> <li>each package must be managed by one tool, otherwise package components will get overwritten, and</li> <li>packages installed by the package tool are specific to an environment and cannot be shared as with Conda, since components are installed directly and not with links.</li> </ul> <p>Prefer Conda over external package managers</p> <p>Installing the same package in multiple environments with an external package tool consumes quotas in terms of storage space and number of files, so prefer Conda when possible. This is particularly important for the <code>inode</code> limit, since some packages install a large number of files, and the hard links used by Conda do not consume inodes or disk space.</p>"},{"location":"environment/conda/#pip","title":"Pip","text":"<p>In this example <code>pip</code> is used to manage packages in a Conda environment with MkDocs related packages. To install the packages, create an environment <pre><code>micromamba env create --name mkdocs\n</code></pre> activate the environment, <pre><code>micromamba activate mkdocs\n</code></pre> and install <code>pip</code> <pre><code>micromamba install --channel conda-forge pip\n</code></pre> which will be used to install the remaining packages.</p> <p>The <code>pip</code> will be the only package that will be managed with Conda. For instance, to update Pip activate the environment, <pre><code>micromamba activate mkdocs\n</code></pre> and run <pre><code>micromaba update --all\n</code></pre> to update all installed packaged (only <code>pip</code> in our case). All other packages are managed by <code>pip</code>.</p> <p>For instance, assume that a <code>mkdocs</code> project requires the following packages:</p> <ul> <li><code>mkdocs</code></li> <li><code>mkdocs-minify-plugin</code></li> </ul> <p>The package <code>mkdocs-minify-plugin</code> is less popular and thus is is not available though a Conda channel, but it is available in PyPI. To install it, activate the <code>mkdocs</code> environment <pre><code>micromamba activate mkdocs\n</code></pre> and install the required packages with <code>pip</code> <pre><code>pip install --upgrade mkdocs mkdocs-minify-plugin\n</code></pre> inside the environment. The packages will be installed inside a directory that <code>micromamba</code> created for the Conda environment, for instance <pre><code>${HOME}/micromamba/envs/mkdocs\n</code></pre> along side packages installed by <code>micromamba</code>. As a results, 'system-wide' installations with <code>pip</code> inside a Conda environment do not interfere with system packages.</p> <p>Do not install packages in Conda environments with pip as a user</p> <p>User installed packages (e.g.<code>pip install --user --upgrade mkdocs-minify-plugin</code>) are installed in the same directory for all environments, typically in <code>~/.local/</code>, and can interfere with other versions of the same package installed from other Conda environments.</p>"},{"location":"environment/conda/#pkg","title":"Pkg","text":"<p>The Julia programming language provides its own package and environment manager, Pkg. The package manager of Julia provides many useful capabilities and it is recommended that it is used with Julia projects. Details about the use of Pkg can be found in the official documentation.</p> <p>The Pkg package manager comes packages with Julia. Start by creating an environment, <pre><code>mocromamba env create --name julia\n</code></pre> activate the environment, <pre><code>micromamba activate julia\n</code></pre> and install Julia, <pre><code>micromamba install --channel conda-forge julia\n</code></pre> to start using Pkg.</p> <p>In order to install a Julia package, activate the Julia environment, and start an interactive REPL session, <pre><code>$ julia\njulia&gt;\n</code></pre> by just calling <code>julia</code> without any input files.</p> <ul> <li>Enter the Pkg package manager by pressing <code>]</code>.</li> <li>Exit the package manager by clearing all the input from the line with backspace, and then pressing backspace one more time.</li> </ul> <p>In the package manager you can see the status of the current environment, <pre><code>(@julia) pkg&gt; status\nStatus `~/micromamba/envs/julia/share/julia/environments/julia/Project.toml` (empty project)\n</code></pre> add or remove packages, <pre><code>(@julia) pkg&gt; add Example\n(@julia) pkg&gt; remove Example\n</code></pre> update the packages in the environment, <pre><code>(@julia) pkg&gt; update\n</code></pre> and perform many other operations, such as exporting and importing environments from plain text files which describe the environment setup, and pinning packages to specific versions. The Pkg package manager maintains a global environment, but also supports the creation and use of local environments that are used within a project directory. The use of local environments is highly recommended, please read the documentation for more information.</p> <p>After installing the Julia language in a Conda environment, the language distribution itself should be managed with <code>micromamba</code> and all packages in global or local environments with the Pkg package manager. To update Julia activate the Conda environment where Julia is stored and call <pre><code>micromamba update julia\n</code></pre> where as to update packages installed with Pgk use the <code>update</code> command of Pkg. The packages for local and global environments are stored in the Julia installation directory, typically <pre><code>${HOME}/micromamba/envs/julia/share\n</code></pre> if the default location for the Micromamba environment directory is used.</p> Advanced management of package data <p>Julia packages will consume storage and number of files quota. Pkg uses automatic garbage collection to cleanup packages that are no longer is use. In general you don't need to manage then package data, simply remove the package and its data will be deleted automatically after some time. However, when you exceed your quota you need to delete files immediately.</p> <p>The immediate removal of the data of uninstalled packages can be forced with the command: <pre><code>using Pkg\nusing Dates\nPkg.gc(;collect_delay=Dates.Day(0))\n</code></pre> Make sure that the packages have been removed from all the environments that use them</p> <p>Sources: Immediate package data clean up</p> <p>Useful resources</p> <ul> <li>Pkg documentation</li> </ul>"},{"location":"environment/conda/#combining-conda-with-external-environment-management-tools","title":"Combining Conda with external environment management tools","text":"<p>Quite often it is required to create isolated environments using external tools. For instance, tools such as <code>virtualenv</code> can install and manage a Python distribution in a given directory and export and import environment descriptions from text files. This functionalities allows for instance the shipping of a description of the Python environment as part of a project. Higher level tools such as <code>pipenv</code> automate the process by managing the Python environment as part of a project directory. The description of the environment is stored in version controlled files, and the Python packages are stored in a non-tracked directory within the project directory. Some wholistic project management tools, such as <code>poetry</code>, further integrate the management of the Python environment withing the project management workflow.</p> <p>Installing and using in Conda environments tools that create isolated environments is relatively straight forward. Create an environment where only the required that tool is installed, and manage any subenvironments using the installed tool.</p> <p>Create a different environment for each tool</p> <p>While this is not a requirement it is a good practice. For instance, <code>pipenv</code> and <code>poetry</code> used to and may still have conflicting dependencies; Conda detects the dependency and aborts the conflicting installation.</p>"},{"location":"environment/conda/#pipenv","title":"Pipenv","text":"<p>To demonstrate the usage of <code>pipenv</code>, create a Conda environment, <pre><code>micromamba enc create --name pipenv\n</code></pre> activate it <pre><code>micromamba activate pipenv\n</code></pre> and install the <code>pipenv</code> package <pre><code>micromamba install --channel conda-forge pipenv\n</code></pre>  as the only package in this environment. Now the <code>pipenv</code> is managed with Conda, for instance to update <code>pipenv</code> activate the environment <pre><code>micromamba activate pipenv\n</code></pre> and call <pre><code>micromamba update --all\n</code></pre> to update the single installed package. Inside the environment use <code>pipenv</code> as usual to create and manage project environments.</p>"},{"location":"environment/containers/","title":"Containers","text":"<p>Many applications and libraries can also be used through container systems. The UL HPC clusters provide the Apptainer container platform (formerly Singularity). The Apptainer platform provides multiple features targeted towards HPC systems, such as support for Open Containers Initiative (OCI) containers, including Docker OCI, and support for secure containers, that is building and running encrypted containers with RSA keys and passphrases<sup>1</sup>.</p>"},{"location":"environment/containers/#apptainer","title":"Apptainer","text":"<p>The UL HPC supports Apptainer containers. Apptainer is an open source container platform designed to be simple, fast, and secure. Apptainer is optimized for Enterprise Performance Computing (EPC)<sup>2</sup> and High Performance Computing (HPC) workloads, allowing users to run containers in a trusted way.</p>"},{"location":"environment/containers/#loading-singularity","title":"Loading Singularity","text":"<p>To use Apptainer load the corresponding module.</p> <pre><code>module load tools/Apptainer\n</code></pre> <p>Warning</p> <p>Modules are not allowed on the access nodes. To test interactively Singularity, rememberer to ask for an interactive job first. <pre><code>salloc --partition=interactive --qos=normal\n</code></pre></p>"},{"location":"environment/containers/#pulling-container-images","title":"Pulling container images","text":"<p>Like Docker, Apptainer provide a way to pull images from a registry such as dockerhub and Sylabs cloud library. You pull an image using the <code>pull</code> command:</p> <p><pre><code>apptainer pull docker://ubuntu:latest\n</code></pre> You should see the following output:</p> <p>Output</p> <p><pre>INFO:    Converting OCI blobs to SIF format\nINFO:    Starting build...\nINFO:    Fetching OCI image...\n28.3MiB / 28.3MiB [===================================] 100 % 8.7 MiB/s 0s\nINFO:    Extracting OCI image...\nINFO:    Inserting Apptainer configuration...\nINFO:    Creating SIF file...\nINFO:    To see mksquashfs output with progress bar enable verbose logging\n\n<p>You may now test the container by executing some command inside the container with the <code>exec</code> command of Apptainer:</p>\n<pre><code>apptainer exec ubuntu_latest.sif cat /etc/os-release\n</code></pre>\n\n<p>Output</p>\n<pre><code>PRETTY_NAME=\"Ubuntu 24.04.2 LTS\"\nNAME=\"Ubuntu\"\nVERSION_ID=\"24.04\"\nVERSION=\"24.04.2 LTS (Noble Numbat)\"\nVERSION_CODENAME=noble\nID=ubuntu\nID_LIKE=debian\nHOME_URL=\"https://www.ubuntu.com/\"\nSUPPORT_URL=\"https://help.ubuntu.com/\"\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\nUBUNTU_CODENAME=noble\nLOGO=ubuntu-logo\n</code></pre>"},{"location":"environment/containers/#building-container-images","title":"Building container images","text":"<p>Building container images requires root privileges. Therefore, users have to build images on their local machine before transferring them to the UL HPC platform. Please refer to the Data transfer section for this purpose.</p>\n\n<p>Building containers in the cloud</p>\n<p>Syslabs, an Apptainer container service, provides a service for building containers in the cloud. You can create containers for your applications without special privileges or setting up a container platform on your local system. The Remote Builder can securely build a container from a definition file provided through the online interface.</p>"},{"location":"environment/containers/#gpu-enabled-apptainer-containers","title":"GPU-enabled Apptainer containers","text":"<p>This section relies on the very excellent documentation from CSCS. In the following example, a container with CUDA features is build, transferred and tested on the ULHPC platform. This example will pull a CUDA container from DockrHub and setup CUDA examples. For this purpose, a singularity definition file, i.e., <code>cuda_samples.def</code>  needs to be created with the following content:</p>\n<pre><code>Bootstrap: docker\nFrom: nvidia/cuda:10.1-devel\n\n%post\n    apt-get update\n    apt-get install -y git\n    git clone https://github.com/NVIDIA/cuda-samples.git /usr/local/cuda_samples\n    cd /usr/local/cuda_samples\n    git fetch origin --tags\n    git checkout 10.1.1\n    make\n\n%runscript\n    /usr/local/cuda_samples/Samples/deviceQuery/deviceQuery\n</code></pre>\n<p>On a local machine having Apptainer installed, we can build the container image, i.e., <code>cuda_samples.sif</code> using the definition file using the following command:</p>\n<pre><code>sudo apptainer build cuda_samples.sif cuda_samples.def\n</code></pre>\n\n<p>Warning</p>\n<p>You should have root privileges on this machine, without them, you will not be able to built the definition file.</p>\n\n<p>Once the container is built and transferred to your dedicated storage on the UL HPC platform, the container can be executed with the following command:</p>\n<pre><code># Inside an interactive job on a gpu-enabled node\napptainer run --nv cuda_samples.sif\n</code></pre>\n\n<p>Warning</p>\n<p>In order to run a CUDA-enabled container, the <code>--nv</code> option has to be passed to the Apptainer command <code>run</code>. According to this option, Apptainer is going to setup the container environment to use the NVIDIA GPU and the basic CUDA libraries.</p>\n\n<p>The latest command should print:</p>\n\n<p>Output</p>\n<pre><code>CUDA Device Query (Runtime API) version (CUDART static linking)\n\nDetected 1 CUDA Capable device(s)\n\nDevice 0: &amp;quot;Tesla V100-SXM2-16GB&amp;quot;\n  CUDA Driver Version / Runtime Version          10.2 / 10.1\n  CUDA Capability Major/Minor version number:    7.0\n  Total amount of global memory:                 16160 MBytes (16945512448 bytes)\n  (80) Multiprocessors, ( 64) CUDA Cores/MP:     5120 CUDA Cores\n  GPU Max Clock rate:                            1530 MHz (1.53 GHz)\n  Memory Clock rate:                             877 Mhz\n  Memory Bus Width:                              4096-bit\n  L2 Cache Size:                                 6291456 bytes\n  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)\n  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers\n  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers\n  Total amount of constant memory:               65536 bytes\n  Total amount of shared memory per block:       49152 bytes\n  Total number of registers available per block: 65536\n  Warp size:                                     32\n  Maximum number of threads per multiprocessor:  2048\n  Maximum number of threads per block:           1024\n  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n  Maximum memory pitch:                          2147483647 bytes\n  Texture alignment:                             512 bytes\n  Concurrent copy and kernel execution:          Yes with 5 copy engine(s)\n  Run time limit on kernels:                     No\n  Integrated GPU sharing Host Memory:            No\n  Support host page-locked memory mapping:       Yes\n  Alignment requirement for Surfaces:            Yes\n  Device has ECC support:                        Enabled\n  Device supports Unified Addressing (UVA):      Yes\n  Device supports Compute Preemption:            Yes\n  Supports Cooperative Kernel Launch:            Yes\n  Supports MultiDevice Co-op Kernel Launch:      Yes\n  Device PCI Domain ID / Bus ID / location ID:   0 / 30 / 0\n  Compute Mode:\n     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;\n\ndeviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 10.2, CUDA Runtime Version = 10.1, NumDevs = 1\nResult = PASS\n</code></pre>"},{"location":"environment/containers/#mpi-and-apptainer-containers","title":"MPI and Apptainer containers","text":"<p>This section relies on the very excellent documentation from CSCS. The following singularity definition file mpi_osu.def can be used to build a container with the osu benchmarks using mpi:</p>\n<pre><code>bootstrap: docker\nfrom: debian:jessie\n\n%post\n    # Install software\n    apt-get update\n    apt-get install -y file g++ gcc gfortran make gdb strace realpath wget curl --no-install-recommends\n\n    # Install mpich\n    curl -kO https://www.mpich.org/static/downloads/3.1.4/mpich-3.1.4.tar.gz\n    tar -zxvf mpich-3.1.4.tar.gz\n    cd mpich-3.1.4\n    ./configure --disable-fortran --enable-fast=all,O3 --prefix=/usr\n    make -j$(nproc)\n    make install\n    ldconfig\n\n    # Build osu benchmarks\n    wget -q http://mvapich.cse.ohio-state.edu/download/mvapich/osu-micro-benchmarks-5.3.2.tar.gz\n    tar xf osu-micro-benchmarks-5.3.2.tar.gz\n    cd osu-micro-benchmarks-5.3.2\n    ./configure --prefix=/usr/local CC=$(which mpicc) CFLAGS=-O3\n    make\n    make install\n    cd ..\n    rm -rf osu-micro-benchmarks-5.3.2\n    rm osu-micro-benchmarks-5.3.2.tar.gz\n\n%runscript\n    /usr/local/libexec/osu-micro-benchmarks/mpi/pt2pt/osu_bw\n</code></pre>\n<pre><code>sudo apptainer build mpi_osu.sif mpi_osu.def\n</code></pre>\n<p>Once the container image is ready, you can use it for example inside the following Slurm launcher to start a best-effort job:</p>\n<p><pre><code>#!/bin/bash --login\n#SBATCH --job-name=Containerized_MPI\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=1\n#SBATCH --time=05:00\n#SBATCH --partition=batch\n#SBATCH --qos=besteffort\n\nmodule load tools/Apptainer\nsrun apptainer run mpi_osu.sif\n</code></pre>\nThe content of the output:</p>\n\n<p>Job output</p>\n<pre><code># OSU MPI Bandwidth Test v5.3.2\n# Size      Bandwidth (MB/s)\n1                       0.35\n2                       0.78\n4                       1.70\n8                       3.66\n16                      7.68\n32                     16.38\n64                     32.86\n128                    66.61\n256                    80.12\n512                    97.68\n1024                  151.57\n2048                  274.60\n4096                  408.71\n8192                  456.51\n16384                 565.84\n32768                 582.62\n65536                 587.17\n131072                630.64\n262144                656.45\n524288                682.37\n1048576               712.19\n2097152               714.55\n</code></pre>\n\n\n\n<ol>\n<li>\n<p>Encrypted containers are not currently supported in UL HPC systems.\u00a0\u21a9</p>\n</li>\n<li>\n<p>Typical examples of Enterprise Performance Computing workloads are deep learning inference and other machine learning workloads.\u00a0\u21a9</p>\n</li>\n</ol>"},{"location":"environment/easybuild/","title":"Easybuild","text":"<p>EasyBuild is a software build and installation framework that allows you to manage scientific and other software on High Performance Computing systems in an efficient way. A large number of scientific software are supported (at least 3670 supported software packages since the 4.9.4 release).<sup>1</sup></p> <p>For several years now, Easybuild is used to manage the ULHPC User Software Set and generate automatically the module files available to you on our computational resources in either <code>release</code> (default) or <code>testing</code> (pre-release/testing) environment. This enables users to easily extend the global software set with their own local software builds, either stored within their global home directory, or preferably in a shared project directory. Easybuild generates automatically module files compliant with the ULHPC module setup.</p> Why use automatic building tools like Easybuild or Spack on HPC environments? <p>While it may seem obvious to some of you, scientific software is often surprisingly difficult to build. Not all software packages rely on standard building tools like Autotools/Automake (the famous <code>configure; make; make install</code>) or CMake. Even with standard building tools, parsing the available option to ensure that the build matches the underlying hardware is time consuming and error-prone. Furthermore, scientific software often contains hardcoded build parameters or the documentation on how to optimize the build is poorly maintained.</p> <p>Software build and installation frameworks like Easybuild or Spack automate and document software building, while generating also the LMod modulefiles. We select Easybuild as our primary building tool to ensure optimized builds. Some HPC sites use both [1]. Often though, simply documenting the build instructions in an organized manner is sufficient [2].</p> <p>Resources</p> <ol> <li>See this talk by William Lucas at EPCC for instance.</li> <li>HPC-UK provides a large collection of system specific build instructions notes which you can copy and adjust.</li> </ol>"},{"location":"environment/easybuild/#easybuild-concepts-and-terminology","title":"Easybuild Concepts and terminology","text":"<p> Official Easybuild Tutorial</p> <p>EasyBuild relies on two main concepts, toolchains and EasyConfig files. A toolchain corresponds to a compiler and a set of libraries which are commonly used to build a software. The two main toolchains frequently used on the UL HPC platform are <code>foss</code> (Free and Open Source Software) and <code>intel</code> toolchains.</p> <ol> <li><code>foss</code> is based on the GCC compiler and on open-source libraries (OpenMPI, OpenBLAS, etc.).</li> <li><code>intel</code> is based on the oneAPI Intel compiler suit and on Intel libraries, such as Intel MPI and Intel Math Kernel Library (MKL).</li> </ol> <p>An EasyConfig file is a simple text file that describes the build process of a software. For most software that uses standard procedures (like <code>configure</code>, <code>make</code> and <code>make install</code>), this file is very simple. Many EasyConfig files are already provided with EasyBuild.</p>"},{"location":"environment/easybuild/#ulhpc-easybuild-configuration","title":"ULHPC Easybuild Configuration","text":"<p>To build software with Easybuild compliant with the configuration of the ULHPC facility, you need to be aware of the following setup:</p> <ul> <li>Modules tool (<code>${EASYBUILD_MODULES_TOOL}</code>): Lmod</li> <li>Module Naming Scheme (<code>${EASYBUILD_MODULE_NAMING_SCHEME}</code>): we use the hierarchical organization where the software are classified/categorized under a pre-defined class.</li> </ul> <p>These variables are defined at the global profile level, under <code>/etc/profile.d/ulhpc_resif.sh</code> on the compute nodes with environment variables:</p> <pre><code>export EASYBUILD_MODULES_TOOL=Lmod\nexport EASYBUILD_MODULE_NAMING_SCHEME=CategorizedModuleNamingScheme\n</code></pre>"},{"location":"environment/easybuild/#configuring-the-build-process","title":"Configuring the build process","text":"<p>All builds and installations are performed at user level, so you don't need the admin (i.e. <code>root</code>) rights. The Easybuild prefix path for instance determines the location where the compiled software is configured and installed. You can configure EasyBuild variables such as the prefix path either through</p> <ul> <li>environment variables, like <code>${EASYBUILD_PREFIX}</code>, or</li> <li>with flags like <code>--prefix</code>.</li> </ul> <p>You can change the prefix path either by exporting the environment variable</p> <pre><code>export EASYBUILD_PREFIX=/path/to/desired/location/easybuild\n</code></pre> <p>or by passing the flag</p> <pre><code>eb --prefix=/path/to/desired/location/easybuild\n</code></pre> <p>when calling EasyBuild.</p> <p>Flags and environment variable in EasyBuild</p> <p>Each setting in EasyBuild can be controlled by an environment variable <code>${EASYBUILD_&lt;NAME&gt;}</code> or the corresponding option flag, <code>--&lt;name&gt;</code>, of the EasyBuild (<code>eb</code>) script. The flags take precedence over the corresponding environment variable.</p> <p>When installing software with EasyBuild, the program automatically detects the modules loaded in the system and only compiles the missing modules in the location pointed by <code>${EASYBUILD_PREFIX}</code>. Both system modules and modules previously build by the user are detected by EasyBuild. The installed software effectively extends the ULHPC software set with your own local builds.</p>"},{"location":"environment/easybuild/#selecting-the-installation-location","title":"Selecting the installation location","text":"<p>The installation path of locally compiled EasyBuild modules is by default a subdirectory of the EasyBuild prefix path, <code>${EASYBUILD_PREFIX}</code>,</p> <pre><code>${EASYBUILD_PREFIX}/${EASYBUILD_SUBDIR_SOFTWARE}\n</code></pre> <p>where</p> <pre><code>${EASYBUILD_SUBDIR_SOFTWARE} = software\n</code></pre> <p>by default. The default value of prefix path is</p> <pre><code>${HOME}/.local/easybuild\n</code></pre> <p>which implies that software is installed in you home directory by default. As a rule of thump,</p> <ul> <li>install any software on shared project directories, so that is can be shared by all project members and save space, by setting   <pre><code>export EASYBUILD_PREFIX=${PROJECTHOME}/&lt;name&gt;/easybuild\n</code></pre>   for a project <code>&lt;name&gt;</code>;</li> <li>install any software that is used by yourself only on your home directory; set explicitly   <pre><code>export EASYBUILD_PREFIX=${HOME}/.local/easybuild\n</code></pre>   or use the <code>--prefix</code> flag in case the default location is modified in the future.</li> </ul>"},{"location":"environment/easybuild/#configuring-the-structure-of-the-installation-directory","title":"Configuring the structure of the installation directory","text":"<p>In order to integrate the modules that you create in your local directories seamlessly in the modules of the software set you need to set some environment variables. The default location where modules are stored is</p> <pre><code>${EASYBUILD_PREFIX}/${EASYBUILD_SUBDIR_MODULES}/${EASYBUILD_SUFFIX_MODULES_PATH}\n</code></pre> <p>where by default</p> <ul> <li><code>${EASYBUILD_SUBDIR_MODULES} = modules</code> and</li> <li><code>${EASYBUILD_SUFFIX_MODULES_PATH} = all</code>.</li> </ul> <p>To access the compiled modules, you need to add the module path to the <code>${MODULEPATH}</code> environment variable. Add the variable to the module using the <code>use</code> sub-command of the <code>module</code> environment management program:</p> <pre><code>module use ${EASYBUILD_PREFIX}/${EASYBUILD_SUBDIR_MODULES}/${EASYBUILD_SUFFIX_MODULES_PATH}\n</code></pre> <p>Structuring the module directory</p> <p>The value</p> <pre><code>${EASYBUILD_MODULE_NAMING_SCHEME} = CategorizedModuleNamingScheme\n</code></pre> <p>ensures that the modules will appear in the correct category along side the software set modules when loading the module directory with <code>module use</code>.</p>"},{"location":"environment/easybuild/#building-optimized-binaries","title":"Building optimized binaries","text":"<p>The advantage of EasyBuild over manual configuration and compilation of software is that it builds optimized binaries targeting the hardware used. If you build a single executable for all the architectures available in the UL HPC clusters you are achieving sub-par performance in all but the architecture for which your build is optimized. To help you configure your compilation, UL HPC systems export the following variables in all compute nodes,</p> <ul> <li><code>${ULHPC_CLUSTER}</code> with values<ul> <li><code>aion</code>: in Aion compute nodes,</li> <li><code>iris</code>: in Iris compute nodes;</li> </ul> </li> <li><code>${RESIF_ARCH}</code> with values<ul> <li><code>epyc</code>: in Aion compute nodes;</li> <li><code>broadwell</code>: in Iris Broadwell and Skylake compute nodes in the batch partition;<sup>2</sup></li> <li><code>skylake</code>: in Iris large memory nodes;</li> <li><code>gpu</code>: in Iris GPU nodes.</li> </ul> </li> </ul> <p>Compiling against the optimized software set</p> <p>When compiling your software you have to ensure that</p> <ol> <li>you are using the software set modules that are optimized for your target hardware, and</li> <li>you are installing in a location where only modules for the target hardware are installed.</li> </ol> <p>To load the correct modules for the compilation, simply login to a session on a compute node and load the desired environment setting module with the command, <pre><code>module load env/&lt;environment type&gt;/&lt;environment name&gt;\n</code></pre> where by default the <code>env/release/default</code> is loaded implicitly at login. Then, set your prefix path in your <code>bashrc</code> to:</p> <pre><code>export EASYBUILD_PREFIX=&lt;path to desired root directory&gt;/easybuild/${ULHPC_CLUSTER}/&lt;environment type&gt;/&lt;environment name&gt;/${RESIF_ARCH}\n</code></pre> <p>Automatically load local modules when logging in to compute nodes</p> <p>You can optionally add the following to your <code>bashrc</code> to automatically load your modules in compute nodes,</p> <pre><code>if command -v module &gt;/dev/null 2&gt;&amp;1; then\n    module use \"&lt;path to desired root directory&gt;/easybuild/${ULHPC_CLUSTER}/&lt;environment type&gt;/&lt;environment name&gt;/${RESIF_ARCH}/modules/all\"\nfi\n</code></pre> <p>assuming that you used the default values for <code>${EASYBUILD_SUBDIR_MODULES}</code> and <code>${EASYBUILD_SUFFIX_MODULES_PATH}</code>.</p>"},{"location":"environment/easybuild/#installation-and-update-of-local-easybuild","title":"Installation and update of local Easybuild","text":"<p>The ULHPC software provides an EasyBuild module that can be loaded with the command:</p> <pre><code>module load tools/EasyBuild\n</code></pre> <p>You can use EasyBuild to bootstrap the installation of an up-to-date version of EasyBuild in your local module set. More detailed instructions are available in the official documentation.</p> <ol> <li> <p>See also \"What is EasyBuild?\".\u00a0\u21a9</p> </li> <li> <p>We don't optimize the binaries for the Skylake architecture in the batch partition of Iris; jobs in Iris may use a mix of Broadwell and Skylake architectures, so we try to use the same binary in all machines.\u00a0\u21a9</p> </li> </ol>"},{"location":"environment/modules/","title":"ULHPC Software/Modules Environment","text":"<p>The UL HPC facilities provides a large variety of scientific applications to its user community, including domain-specific codes and general purpose development tools for a wide range of applications.<sup>1</sup> An environment module system, LMod, is used to manage the shell environment and provide access to installed software.</p> <p>The main advantages of using an environment module system are the following:</p> <ol> <li>Many different versions and/or installations of a single software package can be provided on a given machine, including a default version as well as several older and newer version.</li> <li>Users can easily switch to different versions or installations of a software package without having to explicitly modify their shell environment.</li> </ol> <p>Most UL HPC modules are automatically generated by Easybuild.</p>"},{"location":"environment/modules/#environment-modules","title":"Environment modules","text":"<p>Environment module systems are a standard set of tools deployed in most HPC sites to allow dynamic modification of user environments. The environment module framework was fist implement in Environment Modules in the Tcl language, and later in other tools such as Lmod which is written in Lua. All implementations provide the <code>module</code> command to</p> <ul> <li>manage the <code>PATH</code>, <code>LD_LIBRARY_PATH</code>, <code>MANPATH</code>, and other shell environment variables,</li> <li>define shell functions, and</li> <li>call other environment modifying tools, like Conda and Python virtual environments.</li> </ul> <p>By automatically modifying the shell environment, the modules can load and unload an application, and any profile files and libraries on which it depends. This enables the</p> <ul> <li>automatic management of complex liking dependencies in libraries used in scientific software, and</li> <li>the provision of multiple version of software packages that can co-exists independently in different module environments.</li> </ul> <p>The <code>module</code> command is only available on the compute nodes</p> <p>There is no environment module system installed in login nodes. This is a deliberate choice to prevent users from running large jobs on login nodes. You need to be within a job (interactive or not) to load modules provided by UL HPC or private modules.</p> Inner working of environment modules systems <p>When users login to a Linux system, they get a login shell and the shell uses Environment variables to run commands and applications. Most common are:</p> <ul> <li><code>PATH</code>:  colon-separated list of directories in which your system looks for executable files;</li> <li><code>MANPATH</code>: colon-separated list of directories in which <code>man</code> searches for the man pages;</li> <li><code>LD_LIBRARY_PATH</code>: colon-separated list of directories in which your system looks for for ELF / <code>*.so</code> libraries at execution time needed by applications.</li> </ul> <p>There are also application specific environment variables such as <code>CPATH</code>, <code>LIBRARY_PATH</code>, <code>JAVA_HOME</code>, <code>LM_LICENSE_FILE</code>, <code>MKLROOT</code> etc.</p> <p>A traditional way to setup these Environment variables is by customizing the shell initialization files: i.e. <code>/etc/profile</code>, <code>.bash_profile</code>, and <code>.bashrc</code>. This proves to be very impractical on multi-user systems with various applications and multiple application versions installed as on an HPC facility.</p> <p>To overcome the difficulty of setting and changing the Environment variables, the Tcl/C Environment Modules were introduced over 2 decades ago. The Environment Modules package is a tool that simplify shell initialization and lets users easily modify their environment during the session with modulefiles.</p> <ul> <li>Each modulefile contains the information needed to configure the shell for an application. Once the Modules package is initialized, the environment can be modified on a per-module basis using the <code>module</code> command which interprets modulefiles. Typically modulefiles instruct the <code>module</code> command to alter or set shell environment variables such as <code>PATH</code>, <code>MANPATH</code>, etc.</li> <li>Modulefiles may be shared by many users on a system (as done on the ULHPC clusters) and users may have their own collection to supplement or replace the shared modulefiles.</li> </ul> <p>Modules can be loaded and unloaded dynamically and atomically, in an clean fashion. All popular shells are supported, including <code>bash</code>, <code>ksh</code>, <code>zsh</code>, <code>sh</code>, <code>csh</code>, <code>tcsh</code>, <code>fish</code>, as well as some scripting languages such as <code>perl</code>, <code>ruby</code>, <code>tcl</code>, <code>python</code>, <code>cmake</code> and <code>R</code>. Modules are useful in managing different versions of applications. Modules can also be bundled into metamodules that will load an entire suite of different applications: this is precisely the way the UL HPC Software Set is managed.</p> Tcl/C Environment Modules vs. Tcl Environment Modules vs. Lmod <p>There exists several implementation of the <code>module</code> tool:</p> <ul> <li>Tcl/C Environment Modules (3.2.10 \\leq version &lt; 4), also known as <code>Tmod</code>: the seminal (old) implementation</li> <li>Environment modules (version \\geq 4), previously called <code>Modules-Tcl</code>: Tcl-only variant of Environment modules</li> <li>(recommended) Lmod: a Lua based Environment Module system<ul> <li>Lmod (\"L\" stands for Lua) provides all of the functionality of Tcl/C Environment Modules plus more features:<ul> <li>support for hierarchical module file structure</li> <li><code>MODULEPATH</code> is dynamically updated when modules are loaded.</li> <li>makes loaded modules inactive and active to provide sane environment.</li> <li>supports for hidden modules</li> <li>support for optional usage tracking (implemented on ULHPC facilities)</li> </ul> </li> </ul> </li> <li>In particular, Lmod enforces the following safety features which were only reently provided by Environment Modules:<ol> <li>The One Name Rule: Users can only have one version active</li> <li>Users can only load one compiler or MPI stack at a time (through the <code>family(...)</code> directive)</li> </ol> </li> </ul> <p>The ULHPC Facility relies on Lmod, a Lua-based Environment module system that easily handles the <code>MODULEPATH</code> Hierarchical problem.</p>"},{"location":"environment/modules/#working-with-environment-modules","title":"Working with environment modules","text":"<p>In the UL HPC systems Lmod is used to provide an environment management system. The associated Modulefiles are almost exclusively generated automatically by Easybuild. The <code>module</code> command supports the following subcommands:</p> Command Description <code>module avail</code> Lists all the modules which are available to be loaded <code>module spider &lt;pattern&gt;</code> Search for <code>&lt;pattern&gt;</code> among available modules (Lmod only) <code>module load &lt;mod1&gt; [mod2...]</code> Load a module <code>module unload &lt;module&gt;</code> Unload a module <code>module list</code> List loaded modules <code>module purge</code> Unload all modules (purge) <code>module display &lt;module&gt;</code> Display what a module does <code>module use &lt;path&gt;</code> Prepend the directory to the <code>MODULEPATH</code> environment variable <code>module unuse &lt;path&gt;</code> Remove the directory from the <code>MODULEPATH</code> environment variable <p>At the heart of environment modules interaction resides the following components:</p> <ul> <li>the <code>MODULEPATH</code> environment variable, which defines a colon-separated list of directories to search for module files, and</li> <li>a <code>modulefile</code> (see an example) associated with each available software package.</li> </ul> Example of ULHPC <code>toolchain/foss</code> (auto-generated) module file <pre><code>----------------------------------------------------------------------------------------------------------------\n   /opt/apps/easybuild/systems/aion/rhel810-20250405/2023b/epyc/modules/all/toolchain/foss/2023b.lua:\n----------------------------------------------------------------------------------------------------------------\nhelp([[\nDescription\n===========\nGNU Compiler Collection (GCC) based compiler toolchain, including\n OpenMPI for MPI support, OpenBLAS (BLAS and LAPACK support), FFTW and ScaLAPACK.\n\n\nMore information\n================\n - Homepage: https://easybuild.readthedocs.io/en/master/Common-toolchains.html#foss-toolchain\n]])\nwhatis(\"Description: GNU Compiler Collection (GCC) based compiler toolchain, including\n OpenMPI for MPI support, OpenBLAS (BLAS and LAPACK support), FFTW and ScaLAPACK.\")\nwhatis(\"Homepage: https://easybuild.readthedocs.io/en/master/Common-toolchains.html#foss-toolchain\")\nwhatis(\"URL: https://easybuild.readthedocs.io/en/master/Common-toolchains.html#foss-toolchain\")\nconflict(\"toolchain/foss\")\ndepends_on(\"compiler/GCC/13.2.0\")\ndepends_on(\"mpi/OpenMPI/4.1.6-GCC-13.2.0\")\ndepends_on(\"lib/FlexiBLAS/3.3.1-GCC-13.2.0\")\ndepends_on(\"numlib/FFTW/3.3.10-GCC-13.2.0\")\ndepends_on(\"numlib/FFTW.MPI/3.3.10-gompi-2023b\")\ndepends_on(\"numlib/ScaLAPACK/2.2.0-gompi-2023b-fb\")\nsetenv(\"EBROOTFOSS\",\"/opt/apps/easybuild/systems/aion/rhel810-20250405/2023b/epyc/software/foss/2023b\")\nsetenv(\"EBVERSIONFOSS\",\"2023b\")\nsetenv(\"EBDEVELFOSS\",\"/opt/apps/easybuild/systems/aion/rhel810-20250405/2023b/epyc/software/foss/2023b/easybuild/toolchain-foss-2023b-easybuild-devel\")\n</code></pre>"},{"location":"environment/modules/#meta-modules-for-software-set-management-in-ul-hpc","title":"Meta-modules for software set management in UL HPC","text":"<p>In UL HPC we are using environment meta-modules to modify the available module set. This is done to prevent accidental mixing of modules from different software sets. Meta-modules are modules that modify the <code>MODULEPATH</code> environment variable to change the set of available modules. To load a set of software modules, load the appropriate meta-module. Loading a meta-module will</p> <ul> <li>remove any other meta-module that modifies the software set, and</li> <li>will add in the <code>MODULEPATH</code> variable the path to the modules of the required software set.</li> </ul> <p>There are two types of software sets.</p> <ul> <li>Modules under <code>env</code>: this is a set of modules optimized for the UL HPC systems. The modules are designed to load a different natively optimized set of modules for each system in UL HPC.</li> <li>Modules under <code>EESSI</code>: this modules load EESSI software sets. The software sets distributed under EESSI provide binaries generically optimized for a number of architectures. The EESSI modules are design to provide a uniform collection of software sets across multiple HPC center to support reproducibility.</li> </ul> <p>When to use EESSI modules</p> <p>In general the performance of EESSI modules is slightly lower than the natively optimized software set modules (under <code>env</code>). The main advantage of EESSI modules in that they make it easier to move your computations to new systems that support EESSI. If you plan to also run your computations in a centre where EESSI is available, you should consider using EESSI, otherwise use the local modules.</p> <p>The difference in performance between EESSI modules and the natively optimized software set is particularly large in applications that use a lot of MPI communication. At the moment we are not linking the EESSI application with an MPI version optimized for our site, as a result they rely on the generically optimized MPI distributed by EESSI.</p> <p>When you login to a compute node, the <code>default</code> software set is loaded automatically. You can change that by loading another software set.</p>"},{"location":"environment/modules/#loading-a-natively-optimized-software-set-under-env","title":"Loading a natively optimized software set under <code>env</code>","text":"<p>Each software set under <code>env</code> corresponds to a different software set release on UL HPC. The local software sets under <code>env</code> are mutually exclusive, you cannot have 2 of them loaded at the same time. This is done to prevent accidental mixing of modules from different software sets. To load for instance <code>env/development/2023b</code>, use the command: <pre><code>$ module load env/development/2023b\n\nLmod is automatically replacing \"env/release/default\" with \"env/development/2023b\".\n</code></pre> The command informs us that the <code>default</code> software set was replaces with the <code>2023b</code> software set from the <code>development</code> category. The meta-modules configuring the environment are sticky modules, meaning that you can only unload/purge them with the <code>--force</code> flag: <pre><code>$ module unload env/development/2023b\nThe following modules were not unloaded:\n  (Use \"module --force purge\" to unload all):\n\n  1) env/development/2023b\n$ module --force unload env/development/2023b\n</code></pre> This saves you from loading environment setting module every time you purge your environment modules during normal operations.</p> Inner workings of meta-module mutual exclusiveness <p>The meta-modules in the UL HPC system are all members of the <code>env</code> family. This is defined in the module file with the <pre><code>family(\"env\")\n</code></pre> command. Members of the same family cannot be loaded at the same time, so whenever you load a module of the <code>env</code> family, all other <code>env</code> modules that are loaded are removed.</p>"},{"location":"environment/modules/#loading-an-eessi-software-set","title":"Loading an EESSI software set","text":"<p>The EESSI software set is distributed in a flat manner, meaning that all modules from all releases are available in the same software set. While there is no danger when using a software set like that, the sheer number of available alternatives can lead to mistakes if someone is not careful. Furthermore, the EESSI modules that load the EESSI software set are not sticky, so when you purge your modules you have to reload your EESSI software environment module. For instance, consider loading <code>EESSI/2023.06</code>:</p> <pre><code>$ module load EESSI/2023.06\n</code></pre> <p>Then, listing the available modules</p> <pre><code>$ module avail\n---- /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen2/modules/all ----\n...\n   Abseil/20240116.1-GCCcore-13.2.0         (D)\n   Archive-Zip/1.68-GCCcore-12.2.0\n   Armadillo/11.4.3-foss-2022b\n   Armadillo/12.6.2-foss-2023a\n   Armadillo/12.8.0-foss-2023b              (D)\n   Arrow/11.0.0-gfbf-2022b\n...\n</code></pre> <p>there are multiple versions of the same software available from multiple toolchains (foss-2022b, foss-2023a, foss-2023b). Loading a module and purging the loaded modules also removes the <code>EESSI/2023.06</code> module.</p> <pre><code>$ module load EESSI/2023.06\n$ module load foss/2023b\n$ module list EESSI\n\nCurrently Loaded Modules Matching: EESSI\n  1) EESSI/2023.06\n\n$ module purge\nThe following modules were not unloaded:\n  (Use \"module --force purge\" to unload all):\n\n  1) env/development/2023b\n\n$ module list EESSI\n\nCurrently Loaded Modules Matching: EESSI\n  None found.\n</code></pre>"},{"location":"environment/modules/#ul-hpc-toolchains-and-software-set-versioning","title":"UL HPC toolchains and software set versioning","text":"<p>Our centre offers a yearly release of the UL HPC software set based on corresponding release of EasyBuid toolchains.<sup>2</sup> Count at least 6 months of validation and testing after an EasyBuild release before a UL HPC release.</p> <p>Tool chains and software releases</p> <p>The idea behind toolchains is that a core set of modules is fixed per release and the rest of the software in the release is built around the core set. Only one version of the toolchain modules is present in the software set, where as multiple versions of other software can be present.</p> <p>For an exhaustive list of components version fixed per release have a look as the foss and intel toolchains.</p> <p>An overview of the currently available core toolchain component versions in the UL HPC releases is depicted below:</p> Name Type 2019b (<code>legacy</code>) 2020b (<code>release</code>) 2023b (<code>development</code>) (<code>testing</code>) GCCCore compiler 8.3.0 10.2.0 13.2.0 foss toolchain 2019b 2020b 2023b intel toolchain 2019b 2020b 2023b binutils 2.32 2.35 2.40 Python 3.7.4 (and 2.7.16) 3.8.6 3.11.5 Clang compiler 9.0.1 11.0.0 17.0.6 OpenMPI MPI 3.1.4 4.0.5 4.1.6 <p>In the natively optimized software sets loaded with the modules under <code>env</code>, you should always have a single core component of each type available. The EESSI software sets in contrast follows a more flat layout, and multiple core components will be available at once per type when the software set is loaded.</p>"},{"location":"environment/modules/#architecture-of-the-software-set","title":"Architecture of the software set","text":"<p>By default, the environment module system uses the contents of the <code>MODULEPATH</code> environment variable as the path where it looks for modules. In UL HPC the environment variable contains by default the following paths.</p> <ul> <li><code>/opt/apps/easybuild/environment/modules</code>: Location of sticky meta-modules under <code>env</code> that provide the native optimized software modules.</li> <li><code>/cvmfs/software.eessi.io/init/modules</code>: Location of modules under <code>EESSI</code>, a set of meta-modules that provide the EESSI software sets.</li> </ul> <p>The natively optimized modules under <code>env</code> prepend one of the following paths to <code>${MODULEPATH}</code> in the order described below.</p> <ul> <li>On all nodes except for the <code>gpu</code> partition of Iris:<ul> <li><code>/opt/apps/easybuild/systems/&lt;cluster name&gt;/&lt;build version&gt;/&lt;software set version&gt;/&lt;target architecture&gt;/modules/all</code>: Location of natively optimized modules.</li> <li><code>/opt/apps/easybuild/systems/binary/&lt;build version&gt;/&lt;software set version&gt;/generic/modules/all</code>: Location of software distributed as binaries that cannot be optimized for any target architecture.</li> </ul> </li> <li> <p>On nodes of the <code>gpu</code> partition of Iris:</p> <ul> <li><code>/opt/apps/easybuild/systems/iris/&lt;build version&gt;/&lt;software set version&gt;/gpu/modules/all</code>: Location of natively optimized modules that use the GPU.</li> <li><code>/opt/apps/easybuild/systems/iris/&lt;build version&gt;/&lt;software set version&gt;/skylake/modules/all</code>: Location of natively optimized modules.</li> <li><code>/opt/apps/easybuild/systems/binary/&lt;build version&gt;/&lt;software set version&gt;/generic/modules/all</code>: Location of software distributed as binaries that cannot be optimized for any target architecture.</li> </ul> <p>The GPU optimized modules still need the CPU modules to function, like for instance the MPI module. The GPU nodes use Skylake CPUs, so the modules optimized for Skylake are loaded.</p> </li> </ul> Parameters in the software set directory paths <p>The follow parameters are used in the paths to the software set directories:</p> <ul> <li><code>&lt;cluster name&gt;</code>: the name of the cluster (<code>iris</code> or <code>aion</code>), as set in the environment variable <code>${ULHPC_CLUSTER}</code>.</li> <li><code>&lt;build version&gt;</code>: the version of the software build, determined by the operating system (OS) and the date of the build as <code>&lt;OS version&gt;-&lt;ISO date squashed&gt;</code>, where for instance<ul> <li>RHEL 8.10 become <code>&lt;OS version&gt;</code>=<code>rhel810</code>, and</li> <li>2025-02-16 becomes <code>&lt;ISO date squashed&gt;</code>=<code>20252010</code>.</li> </ul> </li> <li><code>&lt;software set version&gt;</code>: the ULHPC Software set release, aligned with Easybuid toolchains release.</li> <li><code>&lt;target architecture&gt;</code>: the architecture for which the software set has been optimized, as set in the <code>${RESIF_ARCH}</code> environment variable.</li> </ul> <p>There are nodes with <code>broadwell</code> and nodes with <code>skylake</code> CPUs in the CPU partitions (<code>batch</code> and <code>interactive</code>) of Iris. To ensure that a compatible binary is used in all CPUs of the partition, modules loading software sets are configured to load binaries that are compatible for <code>broadwell</code>, the older architecture of the two (note that the binary is selected in the primary node of an allocation).</p> <p>The <code>RESIF_ARCH</code> environment variable is used to load the software set for the appropriate architecture in the natively optimized software sets under <code>env</code>. The <code>${RESIF_ARCH}</code> value used for all nodes in the CPU partitions of Iris is <code>broadwell</code>.</p> <p>Similarly to <code>RESIF_ARCH</code>, EESSI provides the <code>EESSI_ARCHDETECT_OPTIONS_OVERRIDE</code> environment variable to enforce an architecture; by default <code>EESSI_ARCHDETECT_OPTIONS_OVERRIDE</code> is unset, and the EESSI module selects an appropriate architecture for the software set (as the name suggests). The <code>EESSI_ARCHDETECT_OPTIONS_OVERRIDE</code> variable is set to <code>x86_64/intel/haswell</code> in the CPU partitions of iris by default and unset in every other partition. Note that architectural support in EESSI is relatively limited. The available CPU architectures in EESSI for Iris nodes are</p> <ul> <li><code>x86_64/intel/haswell</code> for <code>broadwell</code> CPUs, and</li> <li><code>x86_64/intel/skylake</code> for <code>skylake</code> CPUs.</li> </ul> <p>EESSI does not provide builds optimized for all architectures, so the older <code>haswell</code> was chosen as the best alternative for <code>broadwell</code> which is missing.</p> <p>Values for the <code>RESIF_ARCH</code> and <code>EESSI_ARCHDETECT_OPTIONS_OVERRIDE</code> environment variables in UL HPC systems</p> Cluster Partition (<code>--parition=</code>) Native architecture desciptor (<code>${RESIF_ARCH}</code>) EESSI Architecture descriptor (<code>${EESSI_ARCHDETECT_OPTIONS_OVERRIDE}</code>) Iris <code>batch</code> <code>broadwell</code> <code>x86_64/intel/haswell</code> Iris <code>interactive</code> <code>broadwell</code> <code>x86_64/intel/haswell</code> Iris <code>bigmem</code> <code>skylake</code> Iris <code>gpu</code> <code>gpu</code> Aion <code>batch</code> <code>epyc</code> Aion <code>interactive</code> <code>epyc</code> <p>Note that all <code>bigmen</code> and <code>skylake</code> nodes use Skylake CPUs.</p>"},{"location":"environment/modules/#manual-selection-of-the-software-set","title":"Manual selection of the software set","text":"<p>There are occasion where a user may want to set the software set manually. For instance, a job can be constrained to run on a single kind of CPU, using for instance the <code>--constraint=skylake</code> flag on <code>sbatch</code> or <code>salloc</code> to force the job to run only on Skylake nodes of the <code>batch</code> partition in Iris. In this case it makes sense to use a software set optimized for Skylake.</p> Selecting a natively optimized software set for Skylake CPUs in the Iris CPU partitions <p>The <code>${RESIF_ARCH}</code> value used for all nodes in the CPU partitions of Iris is <code>broadwell</code>. To use the more optimized <code>skylake</code> software set, first purge any loaded natively optimized software sets:</p> <pre><code>$ module --force purge\n</code></pre> <p>There there are 2 options to select the natively optimized software set:</p> <ul> <li>Set the <code>RESIF_ARCH</code> variable manually and load the software set you require with a <code>module</code>:   <pre><code>$ export RESIF_ARCH=skylake\n$ module load env/development/2023b\n</code></pre></li> <li>Edit the <code>MODULEPATH</code> variable with the <code>use</code> option of the <code>module</code> command:   <pre><code>$ module use /opt/apps/easybuild/systems/iris/rhel810-20250216/2023b/skylake/modules/all\n</code></pre></li> </ul> Use an optimal EESSI software sets for Skylake CPUs in the Iris CPU partitions <p>The EESSI module loading the software set is configured to load modules for a CPU architecture that is compatible with all CPUs in the CPU partitions of Iris. If you are sure that your program will run on a single type of CPU architecture simply unset the variable and load the EESSI software modules:</p> <pre><code>$ unset EESSI_ARCHDETECT_OPTIONS_OVERRIDE\n$ module load EESSI\n</code></pre> <p>Then the EESSI module automatically detects the architecture and load the appropriate modules.</p> <p>You can always add a software set manually to <code>MODULEPATH</code> using the <code>use</code> option of the <code>module</code> command. To facilitate the organization of the natively optimized software sets the values of the <code>RESIF_ARCH</code> are used to determine the storage path of each software set. These location are summarized in the following table.</p> <p>Location of natively optimized software set</p> Cluster Arch. <code>${RESIF_ARCH}</code> <code>${MODULEPATH}</code> Environment variable Iris <code>broadwell</code> <code>/opt/apps/easybuild/systems/iris/&lt;build version&gt;/&lt;software set version&gt;/broadwell/modules/all</code> Iris <code>skylake</code> <code>/opt/apps/easybuild/systems/iris/&lt;build version&gt;/&lt;software set version&gt;/skylake/modules/all</code> Iris <code>gpu</code> <code>/opt/apps/easybuild/systems/iris/&lt;build version&gt;/&lt;software set version&gt;/gpu/modules/all</code> Aion <code>epyc</code> <code>/opt/apps/easybuild/systems/aion/&lt;build version&gt;/&lt;software set version&gt;/epyc/modules/all</code>"},{"location":"environment/modules/#default-native-module-set","title":"Default native module set","text":"<p>By default a native module set is loaded when you login into a node. This software set is <pre><code>env/release/default\n</code></pre> and its simply a symbolic link pointing to a some of the software sets in <code>end/release</code>. You can change the default software set by setting the environment variable <code>LMOD_SYSTEM_DEFAULT_MODULES</code> to a colon separated list of the modules you want to be loaded by default when you link into a node. For instance, <pre><code>export LMOD_SYSTEM_DEFAULT_MODULES=/env/testing/2023b\n</code></pre> loads the <code>/env/testing/2023b</code> software set, and <pre><code>export LMOD_SYSTEM_DEFAULT_MODULES=/env/testing/2023b:EESSI/2023.06\n</code></pre> loads both <code>/env/testing/2023b</code> and <code>EESSI/2023.06</code> software sets. You can force a clean default environment by setting <code>LMOD_SYSTEM_DEFAULT_MODULES</code> to the empty string.</p> Modify the default software set <p>You can define the <code>LMOD_SYSTEM_DEFAULT_MODULES</code> environment variable in your <code>${HOME}/.bashrc</code> file to permanently modify the default software set loaded when logging into a compute node.</p> Inner workings of default modules <p>During login to a compute node, the command <pre><code>module --initial_load restore\n</code></pre> is executed (in a <code>profile.d</code> script). The <code>LMOD_SYSTEM_DEFAULT_MODULES</code> environment variable is used by the <code>restore</code> argument of the module command to purge the environment and load a default set of modules. The flag <code>--initial_load</code> is used to avoid the output of a report of the operations performed.</p>"},{"location":"environment/modules/#module-naming-schemes","title":"Module Naming Schemes","text":"<p>Module Naming Schemes on UL HPC system</p> <p>ULHPC modules are organised through the Categorized Naming Scheme.</p> <p>Format: <code>&lt;category&gt;/&lt;name&gt;/&lt;version&gt;-&lt;toolchain&gt;&lt;versionsuffix&gt;</code></p> <p>This means that the typical module hierarchy has as prefix a category level, taken from one of the supported software category or module class: <pre><code>$ eb --show-default-moduleclasses\nDefault available module classes:\n\n        base:      Default module class\n        ai:        Artificial Intelligence (incl. Machine Learning)\n        astro:     Astronomy, Astrophysics and Cosmology\n        bio:       Bioinformatics, biology and biomedical\n        cae:       Computer Aided Engineering (incl. CFD)\n        chem:      Chemistry, Computational Chemistry and Quantum Chemistry\n        compiler:  Compilers\n        data:      Data management &amp; processing tools\n        debugger:  Debuggers\n        devel:     Development tools\n        geo:       Earth Sciences\n        ide:       Integrated Development Environments (e.g. editors)\n        lang:      Languages and programming aids\n        lib:       General purpose libraries\n        math:      High-level mathematical software\n        mpi:       MPI stacks\n        numlib:    Numerical Libraries\n        perf:      Performance tools\n        quantum:   Quantum Computing\n        phys:      Physics and physical systems simulations\n        system:    System utilities (e.g. highly depending on system OS and hardware)\n        toolchain: EasyBuild toolchains\n        tools:     General purpose tools\n        vis:       Visualization, plotting, documentation and typesetting\n</code></pre></p> <p>It follows that the UL HPC software modules are structured accordingly.</p>"},{"location":"environment/modules/#using-easybuild-to-create-custom-modules","title":"Using EasyBuild to Create Custom Modules","text":"<p>You may want to use EasyBuild to complete the existing software set with your own modules and software builds. See Building Custom (or missing) software documentation for more details.</p>"},{"location":"environment/modules/#creating-a-custom-module-environment","title":"Creating a Custom Module Environment","text":"<p>You can modify your environment so that certain modules are loaded whenever you log in. Use <code>module save [&lt;name&gt;]</code> and <code>module restore [&lt;name&gt;]</code> for that purpose, see Lmod documentation on User collections for more details.</p> <p>You can also create and install your own modules for your convenience or for sharing software among collaborators. See the modulefile documentation for details of the required format and available commands. These custom modulefiles can be made visible to the <code>module</code> command by:</p> <pre><code>module use /path/to/the/custom/modulefiles\n</code></pre> <p>Warning</p> <ol> <li>Make sure the UNIX file permissions grant access to all users who want to use the software.</li> <li>Do not give write permissions to your home directory to anyone else.</li> </ol> <p>Note</p> <p>The <code>module use</code> command adds new directories before other module search paths (defined as <code>${MODULEPATH}</code>), so modules defined in a custom directory will have precedence if there are other modules with the same name in the module search paths. If you prefer to have the new directory added at the end of <code>${MODULEPATH}</code>, use <code>module use -a</code> instead of <code>module use</code>.</p>"},{"location":"environment/modules/#module-faq","title":"Module FAQ","text":"What is <code>module</code>? <p><code>module</code> is a shell function that modifies user shell upon load of a modulefile. It is defined as follows <pre><code>$ type module\nmodule is a function\nmodule ()\n{\n    eval $($LMOD_CMD bash \"$@\") &amp;&amp; eval $(${LMOD_SETTARG_CMD:-:} -s sh)\n}\n</code></pre> In particular, <code>module</code> is NOT a program.</p> Is there an environment variable that captures loaded modules? <p>Yes, active modules can be retrieved via <code>$LOADEDMODULES</code>, this environment variable is automatically changed to reflect active loaded modules that is reflected via <code>module list</code>. If you want to access the path to module files for loaded modules you can retrieve via <code>$_LM_FILES</code>.</p> What is a Module Naming Scheme? <p>The full software and module install paths for a particular software package are determined by the active module naming scheme along with the general software and modules install paths specified by the EasyBuild configuration.</p> <p>You can list the supported module naming schemes of Easybuild using: <pre><code>$ eb --avail-module-naming-schemes\nList of supported module naming schemes:\n    EasyBuildMNS\n    CategorizedHMNS\n    MigrateFromEBToHMNS\n    HierarchicalMNS\n    CategorizedModuleNamingScheme\n</code></pre> See Flat vs. Hierarchical module naming scheme for an illustrated explaination of the difference between two extreme cases: flat or 3-level hierarchical. On ULHPC systems, we selected an intermediate scheme called <code>CategorizedModuleNamingScheme</code>.</p> <ol> <li> <p>See our software list for a detailed list of available applications.\u00a0\u21a9</p> </li> <li> <p>See the basic info section for the terminology related to toolchains.\u00a0\u21a9</p> </li> </ol>"},{"location":"environment/workflow/","title":"Workflow","text":""},{"location":"environment/workflow/#ulhpc-workflow","title":"ULHPC Workflow","text":"<p>Your typical journey on the ULHPC facility is illustrated in the below figure.</p> <p></p> Typical workflow on UL HPC resources <p>You daily interaction with the ULHPC facility includes the following actions:</p> <p>Preliminary setup</p> <ol> <li>Connect to the access/login servers<ul> <li>This can be done either by <code>ssh</code> (recommended) or via the ULHPC OOD portal</li> <li>(advanced users) at this point, you probably want to create (or reattach) to a <code>screen</code> or <code>tmux</code> session</li> </ul> </li> <li>Synchronize you code and/or transfer your input data using <code>rsync/svn/git</code> typically<ul> <li>recall that the different storage filesystems are shared (via a high-speed interconnect network) among the computational resources of the ULHPC facilities. In particular, it is sufficient to exchange data with the access servers to make them available on the clusters</li> </ul> </li> <li>Reserve a few interactive resources with <code>salloc -p interactive [...]</code><ul> <li>recall that the <code>module</code> command (used to load the ULHPC User    software) is only available on the compute    nodes</li> <li>(eventually) build your program, typically using <code>gcc/icc/mpicc/nvcc..</code></li> <li>Test your workflow / HPC analysis on a small size problem (<code>srun/python/sh...</code>)</li> <li>Prepare a launcher script <code>&lt;launcher&gt;.{sh|py}</code></li> </ul> </li> </ol> <p>Then you can proceed with your Real Experiments:</p> <ol> <li>Reserve passive resources: <code>sbatch [...] &lt;launcher&gt;</code></li> <li>Grab the results and (eventually) transfer back your output    results using <code>rsync/svn/git</code></li> </ol>"},{"location":"filesystems/","title":"Storage overview","text":"<p>The UL HPC clusters provide access to several different File Systems (FS) which are configured for different purposes. There are the file systems local to the compute nodes, and cluster file systems accessible across the network. The available file systems can be seen in the following overview of the UL HPC cluster architecture.</p> <p></p> What is a File System (FS)? <p>A File System (FS) is just the logical manner to store, organize &amp; access data. There are different types of file systems available nowadays:</p> <ul> <li>Local FS you find on laptops and servers, such as <code>NTFS</code>, <code>HFS+</code>, <code>ext4</code>, <code>{x,z,btr}fs</code>, and other;</li> <li>Network FS, such as <code>NFS</code>, <code>CIFS</code>/<code>SMB</code>, <code>AFP</code>, allowing to access a remote storage system as a NAS (Network Attached Storage);</li> <li>Parallel and Distributed FS (clustered FS), such as <code>SpectrumScale/GPFS</code> or <code>Lustre</code>; those are file systems used in HPC and HTC (High Throughput Computing) facilities, such that<ul> <li>data is spread across multiple storage nodes for redundancy and performance, and</li> <li>global capacity and performance are scalable and increase as additional nodes are added to the storage infrastructure.</li> </ul> </li> </ul> <p>In the UL HPC cluster nodes, there are 3 types of file system in use.</p> <ul> <li>Clustered file systems attached to cluster nodes through the fast Infiniband network. These are,<ul> <li>a GPFS file system storing home and project directories, and</li> <li>a Lustre file system for storing scratch data.</li> </ul> </li> <li>A networked file system attached to cluster nodes through the Ethernet network. This is<ul> <li>an NFS export of an Isilon file system that stores directories.</li> </ul> </li> <li>File systems local to the compute nodes. These are<ul> <li><code>etx4</code> file systems mounted on <code>/tmp</code> of cluster nodes.</li> </ul> </li> </ul> File systems not directly visible to users <p>The ULHPC team relies on other file systems within its internal backup infrastructure, such as <code>xfs</code>, a high-performant disk file-system deployed on storage/backup servers.</p>"},{"location":"filesystems/#storage-systems-overview","title":"Storage Systems Overview","text":"<p>The following table summarize the mount location, backing up, and environment setup for each one of the network file systems in the cluster.</p> <p>Cluster file systems</p> Directory Environment variable File system Backup Interconnect <code>/home/users/&lt;username&gt;</code> <code>${HOME}</code> GPFS/Spectrumscale<sup>[1]</sup> no Infiniband <code>/work/projects/&lt;project name&gt;</code> <code>${PROJECTHOME}/&lt;project name&gt;</code> GPFS/Spectrumscale<sup>[1]</sup> yes (partial, only <code>backup</code> subdirectory) Infiniband <code>/scratch/users/&lt;username&gt;</code> <code>${SCRATCH}</code> Lustre no Infiniband <code>/mnt/isilon/projects/&lt;project name&gt;</code> - OneFS yes (and live sync<sup>[2]</sup>) Ehternet <ol> <li> <p>The  file system mounted on the home directories (<code>/home/users</code>) and project directories (<code>/work/projects</code>) are both exported by the GPFS/Spectrumscale file system.</p> <ul> <li>Storage for both directories is redundant, so they are safe against hardware failure.</li> <li>Only <code>/home/users</code> is mirrored in a SSD cache, so <code>/home/users</code> is a significantly faster for random and small file I/O.</li> </ul> </li> <li> <p>Live sync replicates data across multiple OneFS instances for high availability.</p> </li> </ol> <p>A local file system is also accessible through <code>/tmp</code>. The following table summarizes the type and capacity of the local storage drives.</p> <p>Local file systems</p> <p>In cluster nodes there is a local file system mounted in <code>/tmp</code>. The file systems in the compute nodes are the following:</p> Cluster Partition Storage drive interface Mount directory File system Size Aion <code>batch</code> SATA <code>/tmp</code> ext4 367 GB Iris <code>batch</code> SATA <code>/tmp</code> ext4 73 GB Iris <code>gpu</code> NVME <code>/tmp</code> ext4 1.5 TB Iris <code>bigmem</code> NVME <code>/tmp</code> ext4 1.4 TB <ul> <li>Use the local file system mounted in <code>/tmp</code> for small file I/O. Cluster file systems can be slow when handling many small file I/O operations due to botlenecks in the metadata server bandwidth and latency.</li> <li>In compute nodes, the contents of <code>/tmp</code> are whipped out after the completion of a job.</li> </ul>"},{"location":"filesystems/#intended-usage-of-file-systems","title":"Intended usage of file systems","text":"<p>Each file system in the cluster performs best in a specific set of functions.</p> <ul> <li>Use the GPFS file system directory mounted in your <code>${HOME}</code> for configuration files and files that need to be accessed with low latency and high throughput, for instance for storing environments and container sandboxes.</li> <li>Use the GPFS file system directories mounted in <code>${PROJECTHOME}</code> to store input and output files for your jobs.</li> <li>Use the Lustre file system directory mounted in <code>${SCRATCH}</code> to store working files for running jobs that need to be accessed with low latency and high throughput, like checkpoint files. Scratch is meant for temporary storage only; remove files from scratch as soon as they are not needed for any running jobs.</li> <li>Use project directories in the NFS and SMB exports of Isilon to archive data that need to be stored safely; the OneFS file system of Isilon is backed up regularly.</li> <li>Use the local file systems mounted in <code>/tmp</code> for small file I/O in running jobs, like compilations. Clustered file systems like GPFS and Lustre do not handle high throughput small file I/O well.</li> </ul> <p>Many file system technologies (e.g. ZFS) can hide a lot of the complexity of using a file system. HPC clusters tend to provide low level access to file system functionality so that users can select the technology that provides the best performance for their workload.</p>"},{"location":"filesystems/#clustered-file-systems","title":"Clustered file systems","text":"<p>Current statistics of the available file systems are depicted on the side figure. The ULHPC facility relies on 2 types of Distributed/Parallel File Systems to deliver high-performant Data storage at a BigData scale:</p> <ul> <li>IBM Spectrum Scale, formerly known as the General Parallel File System (GPFS), a global high-performance clustered file system hosting your <code>${HOME}</code> and projects data mounted in <code>${PROJECTHOME}</code>.</li> <li>Lustre, an open-source, parallel file system dedicated to large, parallel scratch storage mounted in <code>${SCRATCH}</code></li> </ul>"},{"location":"filesystems/#home-directory","title":"Home directory","text":"<p>Home directories provide access to configuration files (such as dotfiles) and work files (such as binaries and input and output files) across all cluster nodes. Use home to store working data that are accessed by multiple processes in compute nodes and configuration files.</p> <ul> <li>Home directories are mirrored and cached in Tier 0 storage, so small file and random I/O is relatively fast but not as fast as local storage.</li> <li>The GPFS file system storing the home directories is redundant, so it is safe to data and access loss due to hardware failure.</li> </ul> <p>The environment variable <code>${HOME}</code> points to a user's home directory. The absolute path may change, but the value of <code>${HOME}</code> will always be correct.</p>"},{"location":"filesystems/#project-directories","title":"Project directories","text":"<p>Project directories share files within a group of researchers, and are accessible under the path <code>/work/projects/&lt;project name&gt;</code> in every cluster node. Use project directories to share files and to store large data files that are actively used in computations.</p> <ul> <li>Project directories are not mirrored in Tier 0 storage, so small file I/O performance is lower that the home directory.</li> <li>Since project directories are not mirrored in Tier 0 storage, the available storage space is much larger.</li> <li>The GPFS file system storing the project directories is redundant, so it is safe to data and access loss due to hardware failure.</li> </ul> <p>The environment variable <code>${PROJECTHOME}</code> points to the parent directory of all projects (<code>/work/projects</code>). The absolute path to the project home directory may change, but <code>${PROJECTHOME}</code> is guaranteed to point to the parent directory of all projects directories.</p>"},{"location":"filesystems/#scratch-directory","title":"Scratch directory","text":"<p>The scratch area is a Lustre-based file system that provides high performance temporary storage of large files and is accessible across all cluster nodes. Use scratch to store working files and temporary large data files.</p> <ul> <li>The scratch file system is not fully redundant, so do not use scratch to store files that cannot be recreated. For instance store only simulation output that can be recalculated.</li> </ul> <p>Small file and random I/O</p> <p>The scratch is best used to write large files in a continuous manner. Even though the Lustre file system can handle small file and random I/O better that our GPFS system, it still slows down considerably as the number of I/O operations increases. Typical example of operations with a lot random and small file I/O operations in the parallel compilation of large projects.</p> <p>Prefer the locally mounted file system in <code>/tmp</code> for small file and random I/O.</p> Origin of the term scratch <p>The term scratch originates from scratch data tapes. People uses scratch tapes to write and read data that did not fit into the main memory, and since it was a tape, it could only perform continuous I/O. The term scratch is a bit abused in modern times as most storage systems nowadays support random access. In the case of the lustre system in UL HPC, the terms scratch serves as a reminder that the file system is best used for contiguous I/O, even though it supports random access quite well.</p> <p>The environment variable <code>${SCRATCH}</code> (which expands to <code>/scratch/users/$(whoami)</code>) points to a users scratch directory. The absolute path may change, but the value of <code>${SCRATCH}</code> will always be correct.</p>"},{"location":"filesystems/#networked-file-systems","title":"Networked file systems","text":"<p>The HPC systems also provide direct access through mount points on cluster nodes to the central data storage of the university. The central data storage uses a Dell/EMC Isilon system for the safe archiving of data. Clustered file systems are not meant for the long term storage of data. If you want your data backed up, move your data to the central data storage.</p>"},{"location":"filesystems/#cold-project-data-and-archives","title":"Cold project data and archives","text":"<p>In the UL HPC platform the NFS exported by Isilon is used to host project data when they are not actively used in computations, and for archival purposes. Projects are mounted under <code>/mnt/isilon/projects</code>.</p> <ul> <li> <p>The file system in Isilon is redundant, regularly snapshot, and backed up, including off site backups. Data is replicated across multiple OneFS instances for high availability with the live sync feature of OneFS. Isilon is thus resilient to hardware failure, protected against catastrophic data loss, and also highly available.</p> </li> <li> <p>The NFS share exported from Isilon to the UL HPC platform is not using the Infiniband high performance network and the OneFS file system has lower I/O performance that GPFS and lustre file systems. However, the central data storage has significantly higher capacity.</p> </li> </ul> <p>Long term data storage</p> <p>Please move all your data to OneFS directories of the central data storage as soon as your computations finish.</p> <p>The central data storage is the intended place for storing data. Clustered file systems using the inifiniband network are meant as working storage only. For this reason, backups in cluster file systems are very limited.</p> <p>Users have to ask for a project directories in the Isilon mount point (<code>/mnt/isilon/projects</code>) separately from the GPFS project directories. However, all users have a personal directory in the university central data storage which they can access through the ATLAS SMB system. Users may also ask for project directories that are accessible through ATLAS, however these project cannot be mounted on the Isilon NFS share (<code>/mnt/isilon/projects</code>).</p>"},{"location":"filesystems/#quota","title":"Quota","text":"<p>The UL HPC systems provide the <code>df-ulhpc</code> command on the cluster login nodes to display the current space and inode (with the option flag <code>-i</code>) quota usage. For more details see the documentation section about quotas.</p> <p>Users and project groups are assigned a fixed amount of storage. There are quota limits in terms of storage space and number of files (inodes). When a quota limit is reached writes to the relevant directories will fail. The storage limits are described below.</p> <p>Quota limits in cluster file systems</p> Directory Default space quota Default inode quota <code>${HOME}</code> 500 GB 1 M <code>${SCRATCH}</code> 10 TB 1 M <code>${PROJECTHOME}/&lt;project name&gt;</code> 1 TB<sup>[1]</sup> 1 M<sup>[1]</sup> <code>/mnt/isilon/projects/&lt;project name&gt;</code> 1.14 PB globally<sup>[2]</sup> - <ol> <li>This is the default and free of charge allocation for projects; requests for more space may incur charges.</li> <li>On Isilon all projects share one global quota limit and the HPC Platform team sets up individual project quotas. Unfortunately it is not currently possible for users to see the quota status on Isilon.</li> </ol>"},{"location":"filesystems/#backups","title":"Backups","text":"<p>Backups are vital to safeguard important data. Always maintain a well defined backup policy. You can build your backup policy on top of the backup services offered in the UL HPC systems. A high level overview of the backup policies in place for the HPC systems is presented here. If you require specific details, please contact the HPC team directly.</p> <p>Limitations of backup policies in UL</p> <p>The UL HPC and SIU do not offer cold backups (offline backups). All our backups are maintained in live systems.</p> <p>All UL HPC users should back up important files on a regular basis. Ultimately, it is your responsibility to protect yourself from data loss.</p> <p>More details and information on how to recover your backed up data can be found in the section of the documentation about backups.</p>"},{"location":"filesystems/#ul-hpc-clustered-file-systems","title":"UL HPC clustered file systems","text":"<p>The cluster file systems are not meant to be used for data storage, so there are minimal back ups created for files in the cluster file systems. The backups are only accessible by UL HPC staff for disaster recovery purposes only. The following table summarizes the backups kept for each file system mount point.</p> Directory Path Backup location Frequency Retention home directories <code>${HOME}</code> not backed up - scratch <code>${SCRATCH}</code> not backed up - projects <code>${PROJECTHOME}</code> CDC, Belval Weekly One backup per week of the backup directory ONLY (<code>${PROJECTHOME}/&lt;project name&gt;/backup/</code>). <p>Project backups</p> <p>Use the <code>backup</code> subdirectory in your project directories to store important configuration files for you projects that are specific to the UL HPC clusters.</p> <p>UL HPC backup policy</p> <p>Data are copied live from the GPFS file system to a backup server (due to limitation regarding snapshots in GPFS). The backup data are copied to a Disaster Recovery Site (DRS) in a location outside the server room where the primary backup server is located.</p>"},{"location":"filesystems/#isilon-networked-file-system","title":"Isilon networked file system","text":"<p>Projects stored on the Isilon system are snapshotted regularly. This includes the NFS export of Isilon in HL HPC systems, personal and project directories in Atlas, the SMB export of Isilon, but not the personal directories of the students exported through the Poseidon SMB export of Isilon. The following snapshot schedule and retention strategy are used:</p> Backed up snapshot Retention Daily 14 days Weekly 5 months Monthly 12 months <p>SIU back up policy</p> <p>Snapshots do not protect on themselves against a system failure, they only permit recovering files in case of accidental deletion. To ensure the safe storage, snapshots data is copied to a Disaster Recovery Site (DRS) in a location outside the server room where the primary data storage (Isilon) is located.</p>"},{"location":"filesystems/#useful-resources","title":"Useful resources","text":"<ul> <li>ULHPC backup policies</li> <li>Quotas</li> <li>ULHPC GPFS/SpectrumScale and Lustre filesystems</li> <li>UL Isilon/OneFS filesystems</li> </ul>"},{"location":"filesystems/gpfs/","title":"GPFS/SpectrumScale","text":""},{"location":"filesystems/gpfs/#introduction","title":"Introduction","text":"<p>IBM Spectrum Scale, formerly known as the General Parallel File System (GPFS), is high-performance cluster file system available on all ULHPC computational systems through a Dell-based storage infrastructure.</p> <p>It allows sharing home directories and project data between machines, users, systems, and (if needed) with the \"outside world\". In terms of raw storage capacities, it represents more than 4PB of raw space (more than 3PB of usable space).</p> <p>The file system is composed of two tiers, home directories are stored on the Tier 0 (Flash-based, 246TB) while project directories are stored on the Tier 1 (Disk-based, 2956TB). The placement policy can be adjusted, in example, for publicly shared datasets.</p>"},{"location":"filesystems/gpfs/#home-directory-home","title":"Home directory (<code>${HOME}</code>)","text":"<p>Home directories provide access to configuration files (such as dotfiles) and work files (such as binaries and input and output files) across all cluster nodes. Use home to store working data that are accessed by multiple processes in compute nodes and configuration files.</p> <ul> <li>Home directories are mirrored and cached in Tier 0 storage, so small file and random I/O is relatively fast but not as fast as local storage.</li> <li>The GPFS file system storing the home directories is redundant, so it is safe to data and access loss due to hardware failure.</li> </ul> <p>The environment variable <code>${HOME}</code> points to a user's home directory. The absolute path may change, but the value of <code>${HOME}</code> will always be correct.</p>"},{"location":"filesystems/gpfs/#project-directories-projecthome","title":"Project directories <code>(${PROJECTHOME})</code>","text":"<p>Project directories share files within a group of researchers, and are accessible under the path <code>/work/projects/&lt;project name&gt;</code> in every cluster node. Use project directories to share files and to store large data files that are actively used in computations.</p> <ul> <li>Project directories are not mirrored in Tier 0 storage, so small file I/O performance is lower that the home directory.</li> <li>Since project directories are not mirrored in Tier 0 storage, the available storage space is much larger.</li> <li>The GPFS file system storing the project directories is redundant, so it is safe to data and access loss due to hardware failure.</li> </ul> <p>The environment variable <code>${PROJECTHOME}</code> points to the parent directory of all projects (<code>/work/projects</code>). The absolute path to the project home directory may change, but <code>${PROJECTHOME}</code> is guaranteed to point to the parent directory of all projects directories.</p> <p>Global Project quotas and backup policies</p> <p>See quotas for detailed information about inode, space quotas, and file system purge policies. Your projects backup directories are backuped weekly, according to the policy detailed in the ULHPC backup policies.</p> <p>Access rights to project directory: Quota for <code>clusterusers</code> group in project directories is 0 !!!</p> <p>When a project <code>&lt;name&gt;</code> is created, a group of the same name (<code>&lt;name&gt;</code>) is also created and researchers allowed to collaborate on the project are made members of this group,which grant them access to the project directory.</p> <p>Be aware that your default group as a user is <code>clusterusers</code> which has (on purpose) a quota in project directories set to 0. You thus need to ensure you always write data in your project directory using the <code>&lt;name&gt;</code> group (instead of yoru default one.). This can be achieved by ensuring the setgid bit is set on all folders in the project directories: <code>chmod g+s [...]</code></p> <p>When using <code>rsync</code> to transfer file toward the project directory <code>/work/projects/&lt;name&gt;</code> as destination, be aware that rsync will not use the correct permissions when copying files into your project directory. As indicated in the Data transfer section, you also need to:</p> <ul> <li>give new files the destination-default permissions with <code>--no-p</code> (<code>--no-perms</code>), and</li> <li>use the default group <code>&lt;name&gt;</code> of the destination dir with <code>--no-g</code> (<code>--no-group</code>)</li> <li>(eventually) instruct rsync to preserve whatever executable permissions existed on the source file and aren't masked at the destination using <code>--chmod=ug=rwX</code></li> </ul> <p>Your full <code>rsync</code> command becomes (adapt accordingly):</p> <pre><code>  rsync -avz {--update | --delete} --no-p --no-g [--chmod=ug=rwX] &lt;source&gt; /work/projects/&lt;name&gt;/[...]\n</code></pre> <p>For the same reason detailed above, in case you are using a build command or more generally any command meant to write data in your project directory <code>/work/projects/&lt;name&gt;</code>, you want to use the <code>sg</code> as follows:</p> <pre><code># /!\\ ADAPT &lt;name&gt; accordingly\nsg &lt;name&gt; -c \"&lt;command&gt; [...]\"\n</code></pre> <p>This is particularly important if you are building dedicated software with Easybuild for members of the project - you typically want to do it as follows:</p> <pre><code># /!\\ ADAPT &lt;name&gt; accordingly\nsg &lt;name&gt; -c \"eb [...] -r --rebuild -D\"   # Dry-run - enforce using the '&lt;name&gt;' group\nsg &lt;name&gt; -c \"eb [...] -r --rebuild\"      # Dry-run - enforce using the '&lt;name&gt;' group\n</code></pre> <p>Danger</p> <p>Redundancy does not replace backups. Home and project directories are redundant but not backed up, so do not leave data stored in home and project directories.</p> <p><code>${HOME}</code> quotas and backup policies</p> <p>See quotas for detailed information about inode, space quotas, and file system purge policies. Your HOME is backuped weekly, according to the policy detailed in the ULHPC backup policies.</p>"},{"location":"filesystems/gpfs/#storage-system-implementation","title":"Storage System Implementation","text":"<p>The way the UL HPC GPFS file system is implemented is depicted on the below figure.</p> <p></p> <p>It is composed of:</p> <ul> <li>Two gateway NFS servers (see below)</li> <li>Two metadata servers (Dell R750 containing 4x 6.4TB NVMe each)</li> <li>Two Tier 0 servers (Dell R750 containing 16x 15.36TB NVMe each), configured as replicas</li> <li>Two Tier 1 servers (Dell R750, attached to the disk enclosures below)</li> <li>One ME484 disk enclosure, containing 84x SAS hard disks of 22TB, configured in 2x ADAPT volumes plus 4x hot spares</li> <li>One ME5084 disk enclosure, containing 84x SAS hard disks of 22TB, configured in 2x ADAPT volumes plus 4x hot spares</li> </ul> <p>There is no single point of failure within the storage solution and the setup is fully redundant (servers are set-up in pairs, and the system can tolerate the loss of one server in each pair). There are redundant power supplies, redundant fans, redundant storage controller and battery backup to secure the cache data when power is lost completely. The data paths to the disk enclosures are redundant so that links can fail, and the system will still be fully operational.</p> <p>Finally, each server is connected directly to the Aion Infiniband network via redundant HDR200 links, in such a way that all connections are perfectly balanced across the 4 Aion racks and 8 leaf switches, and connected to a redundant stack of ethernet switches via redundant 10GbE links.</p> (Obsolete) Initial DDN-Based GPFS infrastructure <p>Our DDN-based GPFS infrastructure has unfortunately reach End-of-Life and could not be supported anymore. It has been replaced in 2024 by the new Dell-based infrastructure described above. For the record, the following section describes the now decommissioned DDN system:</p> <p></p> <ul> <li>Two NAS protocol servers (see below</li> <li>One DDN GridScaler 7K system acquired as part of RFP 160019 deployed in 2017 and later extended, composed of<ul> <li>1x DDN GS7K enclosure (~11GB/s IO throughput)</li> <li>4x SS8460 disk expansion enclosures</li> <li>350x HGST disks (7.2K RPM HDD, 6TB, Self Encrypted Disks (SED) configured over 35 RAID6 (8+2) pools</li> <li>28x Sandisk SSD 400GB disks</li> </ul> </li> <li>Another DDN GridScaler 7K system acquired as part of RFP 190027 deployed in 2020 as part of Aion  and later extended.<ul> <li>1x DDN GS7990-EDR embedded storage</li> <li>4x SS9012 disk expansion enclosures</li> <li>360x NL-SAS HDDs (6TB, Self Encrypted Disks (SED)) configured over 36 RAID6 (8+2) pools</li> <li>10x 3.2TB SED SAS-SSD for metadata.</li> </ul> </li> </ul>"},{"location":"filesystems/gpfs/#filesystem-performance","title":"Filesystem Performance","text":"<p>The performance of the storage infrastructure via native GPFS and RDMA based data transport for the HPC filesystem is expected to be:</p> <ul> <li>on Tier 0, in the range of 23GB/s for sequential reads, and 55GB/s for sequential writes</li> <li>on Tier 1, in the range of 10GB/s for large sequential reads and writes Performance measurement by IOR, a synthetic benchmark for testing the performance of distributed filesystems, has been performed prior to the acceptance of the storage solution.</li> </ul> The IOR benchmark <p>IOR is a parallel IO benchmark that can be used to test the performance of parallel storage systems using various interfaces and access patterns. It supports a variety of different APIs to simulate IO load and is nowadays considered as a reference Parallel filesystem I/O benchmark. It recently embedded another well-known benchmark suite called MDTest, a synthetic MPI parallel benchmark for testing the metadata performance of filesystems (such as Lustre or Spectrum Scale GPFS) where each thread is operating its own working set (to create directory/files, read files, delete files or directory tree).</p>"},{"location":"filesystems/gpfs/#gatewaynfs-servers","title":"Gateway/NFS Servers","text":"<p>Two Gateway servers are available, each connected via 2 x IB HDR200 links to the IB fabric and exporting the filesystem via NFS and SMB over 2 x 10GE links into the Ethernet network.</p>"},{"location":"filesystems/isilon/","title":"Dell EMC Isilon (Cold project data and Archives)","text":"<p>The university central data storage services uses a NAS system to provide long term storage for user data. The NAS system used is a Dell/EMC Isilon (Isilon) server with OneFS, a proprietary and distributed file system and environment, used to store the data. The OneFS file systems is exported by Isilon through the NFS protocol to the HPC platform and other Linux-based services, and through SMB (CIFS) to Windows-based clients. In UL HPC login nodes the script <code>smb-storage</code> is provided to mount SMB shares.</p> <p>In the UL HPC platform the NFS exported by Isilon is used to host project data when they are not actively used in computations, and for archival purposes. Projects are mounted under <code>/mnt/isilon/projects</code>.</p> <ul> <li> <p>The file system in Isilon is redundant, regularly snapshot, and backed up, including off site backups. Data is replicated across multiple OneFS instances for high availability with the live sync feature of OneFS. Isilon is thus resilient to hardware failure, protected against catastrophic data loss, and also highly available.</p> </li> <li> <p>The NFS share exported from Isilon to the UL HPC platform is not using the Infiniband high performance network and the OneFS file system has lower I/O performance that GPFS and lustre file systems. However, the central data storage has significantly higher capacity.</p> </li> </ul> <p>Long term data storage</p> <p>Please move all your data to OneFS directories of the central data storage as soon as your computations finish.</p> <p>The central data storage is the intended place for storing data. Clustered file systems using the inifiniband network are meant as working storage only. For this reason, backups in cluster file systems are very limited.</p> <p>Users have to ask for a project directories in the Isilon mount point (<code>/mnt/isilon/projects</code>) separately from the GPFS project directories. However, all users have a personal directory in the university central data storage which they can access through the ATLAS SMB system. Users may also ask for project directories that are accessible through ATLAS, however these project cannot be mounted on the Isilon NFS share (<code>/mnt/isilon/projects</code>).</p> <p>In 2014, the IT Department of the University, the UL HPC and the LCSB join their forces (and their funding) to acquire a scalable and modular NAS solution able to sustain the need for an internal big data storage, i.e. provides space for centralized data and backups of all devices used by the UL staff and all research-related data, including the one proceed on the UL HPC platform.</p> <p>At the end of a public call for tender released in 2014, the EMC Isilon system was finally selected with an effective deployment in 2015. It is physically hosted in the new CDC (Centre de Calcul) server room in the Maison du Savoir. Composed by a large number of disk enclosures featuring the OneFS File System, it currently offers an effective capacity of 3.360 PB.</p> <p>A secondary Isilon cluster, acquired in 2020 and deployed in 2021 is duplicating this setup in a redundant way.</p> <p></p>"},{"location":"filesystems/lfs/","title":"Scratch Data Management","text":""},{"location":"filesystems/lfs/#understanding-lustre-io","title":"Understanding Lustre I/O","text":"<p>When a client (a compute node from your job) needs to create or access a file, the client queries the metadata server (MDS) and the metadata target (MDT) for the layout and location of the file's stripes. Once the file is opened and the client obtains the striping information, the MDS is no longer involved in the file I/O process. The client interacts directly with the object storage servers (OSSes) and OSTs to perform I/O operations such as locking, disk allocation, storage, and retrieval.</p> <p>If multiple clients try to read and write the same part of a file at the same time, the Lustre distributed lock manager enforces coherency, so that all clients see consistent results.</p>"},{"location":"filesystems/lfs/#discover-mdts-and-osts","title":"Discover MDTs and OSTs","text":"<p>ULHPC's Lustre file systems look and act like a single logical storage, but a large files on Lustre can be divided into multiple chunks (stripes) and stored across over OSTs. This technique is called file striping. The stripes are distributed among the OSTs in a round-robin fashion to ensure load balancing. It is thus important to know the number of OST on your running system.</p> <p>As mentioned in the Lustre implementation section, the ULHPC Lustre infrastructure is composed of 2 MDS servers (2 MDT), 2 OSS servers and 16 OSTs. You can list the MDTs and OSTs with the command <code>lfs df</code>:</p> <pre><code>$ cds      # OR: cd $SCRATCH\n$ lfs df -h\nUUID                       bytes        Used   Available Use% Mounted on\nlscratch-MDT0000_UUID        3.2T       15.4G        3.1T   1% /mnt/lscratch[MDT:0]\nlscratch-MDT0001_UUID        3.2T        3.8G        3.2T   1% /mnt/lscratch[MDT:1]\nlscratch-OST0000_UUID       57.4T       16.7T       40.2T  30% /mnt/lscratch[OST:0]\nlscratch-OST0001_UUID       57.4T       18.8T       38.0T  34% /mnt/lscratch[OST:1]\nlscratch-OST0002_UUID       57.4T       17.6T       39.3T  31% /mnt/lscratch[OST:2]\nlscratch-OST0003_UUID       57.4T       16.6T       40.3T  30% /mnt/lscratch[OST:3]\nlscratch-OST0004_UUID       57.4T       16.5T       40.3T  30% /mnt/lscratch[OST:4]\nlscratch-OST0005_UUID       57.4T       16.5T       40.3T  30% /mnt/lscratch[OST:5]\nlscratch-OST0006_UUID       57.4T       16.3T       40.6T  29% /mnt/lscratch[OST:6]\nlscratch-OST0007_UUID       57.4T       17.0T       39.9T  30% /mnt/lscratch[OST:7]\nlscratch-OST0008_UUID       57.4T       16.8T       40.0T  30% /mnt/lscratch[OST:8]\nlscratch-OST0009_UUID       57.4T       13.2T       43.6T  24% /mnt/lscratch[OST:9]\nlscratch-OST000a_UUID       57.4T       13.2T       43.7T  24% /mnt/lscratch[OST:10]\nlscratch-OST000b_UUID       57.4T       13.3T       43.6T  24% /mnt/lscratch[OST:11]\nlscratch-OST000c_UUID       57.4T       14.0T       42.8T  25% /mnt/lscratch[OST:12]\nlscratch-OST000d_UUID       57.4T       13.9T       43.0T  25% /mnt/lscratch[OST:13]\nlscratch-OST000e_UUID       57.4T       14.4T       42.5T  26% /mnt/lscratch[OST:14]\nlscratch-OST000f_UUID       57.4T       12.9T       43.9T  23% /mnt/lscratch[OST:15]\n\nfilesystem_summary:       919.0T      247.8T      662.0T  28% /mnt/lscratch\n</code></pre>"},{"location":"filesystems/lfs/#file-striping","title":"File striping","text":"<p>File striping permits to increase the throughput of operations by taking advantage of several OSSs and OSTs, by allowing one or more clients to read/write different parts of the same file in parallel. On the other hand, striping small files can decrease the performance.</p> <p>File striping allows file sizes larger than a single OST, large files MUST be striped over several OSTs in order to avoid filling a single OST and harming the performance for all users. There is default stripe configuration for ULHPC Lustre filesystems (see below). However, users can set the following stripe parameters for their own directories or files to get optimum I/O performance. You can tune file striping using 3 properties:</p> Property Effect Default Accepted values Advised values stripe_size Size of the file stripes in bytes 1048576 (1m) &gt; 0 &gt; 0 stripe_count Number of OST to stripe across 1 -1 (use all the OSTs), 1-16 -1 stripe_offset Index of the OST where the first stripe of files will be written -1 (automatic) -1, 0-15 -1 <p>Note: With regards <code>stripe_offset</code> (the index of the OST where the first stripe is to be placed); the default is -1 which results in random selection and using a non-default value is NOT recommended.</p> <p>Note</p> <p>Setting stripe size and stripe count correctly for your needs may significantly affect the I/O performance.</p> <ul> <li>Use the <code>lfs getstripe</code> command for getting the stripe parameters.</li> <li>Use <code>lfs setstripe</code> for setting the stripe parameters to get optimal I/O performance. The correct stripe setting depends on your needs and file access patterns.<ul> <li>Newly created files and directories will inherit these parameters from their parent directory. However, the parameters cannot be changed on an existing file.</li> </ul> </li> </ul> <pre><code>$ lfs getstripe dir|filename\n$ lfs setstripe -s &lt;stripe_size&gt; -c &lt;stripe_count&gt; -o &lt;stripe_offset&gt; dir|filename\n    usage: lfs setstripe -d &lt;directory&gt;   (to delete default striping from an existing directory)\n    usage: lfs setstripe [--stripe-count|-c &lt;stripe_count&gt;]\n                         [--stripe-index|-i &lt;start_ost_idx&gt;]\n                         [--stripe-size|-S &lt;stripe_size&gt;]  &lt;directory|filename&gt;\n</code></pre> <p>Example:</p> <pre><code>$ lfs getstripe $SCRATCH\n/scratch/users/&lt;login&gt;/\nstripe_count:   1 stripe_size:    1048576 stripe_offset:  -1\n[...]\n$ lfs setstripe -c -1 $SCRATCH\n$ lfs getstripe $SCRATCH\n/scratch/users/&lt;login&gt;/\nstripe_count:  -1 stripe_size:   1048576 pattern:       raid0 stripe_offset: -1\n</code></pre> <p>In this example, we view the current stripe setting of the <code>$SCRATCH</code> directory. The stripe count is changed to all OSTs and verified. All files written to this directory will be striped over the maximum number of OSTs (16). Use <code>lfs check osts</code> to see the number and status of active OSTs for each filesystem on the cluster. Learn more by reading the man page:</p> <pre><code>$ lfs check osts\n$ man lfs\n</code></pre>"},{"location":"filesystems/lfs/#file-stripping-examples","title":"File stripping Examples","text":"<ul> <li>Set the striping parameters for a directory containing only small files (&lt; 20MB)</li> </ul> <pre><code>$ cd $SCRATCH\n$ mkdir test_small_files\n$ lfs getstripe test_small_files\ntest_small_files\nstripe_count:   1 stripe_size:    1048576 stripe_offset:  -1 pool:\n$ lfs setstripe --stripe-size 1M --stripe-count 1 test_small_files\n$ lfs getstripe test_small_files\ntest_small_files\nstripe_count:   1 stripe_size:    1048576 stripe_offset:  -1\n</code></pre> <ul> <li>Set the striping parameters for a directory containing only large files between 100MB and 1GB</li> </ul> <pre><code>$ mkdir test_large_files\n$ lfs setstripe --stripe-size 2M --stripe-count 2 test_large_files\n$ lfs getstripe test_large_files\ntest_large_files\nstripe_count:   2 stripe_size:    2097152 stripe_offset:  -1\n</code></pre> <ul> <li>Set the striping parameters for a directory containing files larger than 1GB</li> </ul> <pre><code>$ mkdir test_larger_files\n$ lfs setstripe --stripe-size 4M --stripe-count 6 test_larger_files\n$ lfs getstripe test_larger_files\ntest_larger_files\nstripe_count:   6 stripe_size:    4194304 stripe_offset:  -1\n</code></pre> <p>Big Data files management on Lustre</p> <p>Using a large stripe size can improve performance when accessing very large files</p> <p>Large stripe size allows each client to have exclusive access to its own part of a file. However, it can be counterproductive in some cases if it does not match your I/O pattern. The choice of stripe size has no effect on a single-stripe file.</p> <p>Note that these are simple examples, the optimal settings defer depending on the application (concurrent threads accessing the same file, size of each write operation, etc).</p>"},{"location":"filesystems/lfs/#lustre-best-practices","title":"Lustre Best practices","text":"<p>Parallel I/O on the same file</p> <p>Increase the <code>stripe_count</code> for parallel I/O to the same file.</p> <p>When multiple processes are writing blocks of data to the same file in parallel, the I/O performance for large files will improve when the <code>stripe_count</code> is set to a larger value. The stripe count sets the number of OSTs to which the file will be written. By default, the stripe count is set to 1. While this default setting provides for efficient access of metadata (for example to support the <code>ls -l</code> command), large files should use stripe counts of greater than 1. This will increase the aggregate I/O bandwidth by using multiple OSTs in parallel instead of just one. A rule of thumb is to use a stripe count approximately equal to the number of gigabytes in the file.</p> <p>Another good practice is to make the stripe count be an integral factor of the number of processes performing the write in parallel, so that you achieve load balance among the OSTs. For example, set the stripe count to 16 instead of 15 when you have 64 processes performing the writes. For more details, you can read the following external resources:</p> <ul> <li>Reference Documentation:  Managing File Layout (Striping) and Free Space</li> <li>Lustre Wiki</li> <li>Lustre Best Practices - Nasa HECC</li> <li>I/O and Lustre Usage - NISC</li> </ul>"},{"location":"filesystems/lustre/","title":"Lustre (<code>$SCRATCH</code>)","text":""},{"location":"filesystems/lustre/#introduction","title":"Introduction","text":"<p>The Lustre file system is an open-source, parallel file system that supports many requirements of leadership class HPC simulation environments.</p> <p>It is available as a global high-performance file system on all ULHPC computational systems through a DDN ExaScaler system.</p> <p>It is meant to host temporary scratch data within your jobs. In terms of raw storage capacities, it represents more than 1.6PB.</p>"},{"location":"filesystems/lustre/#scratch-directory-scratch","title":"Scratch directory <code>(${SCRATCH})</code>","text":"<p>The scratch area is a Lustre-based file system that provides high performance temporary storage of large files and is accessible across all cluster nodes. Use scratch to store working files and temporary large data files.</p> <ul> <li>The scratch file system is not fully redundant, so do not use scratch to store files that cannot be recreated. For instance store only simulation output that can be recalculated.</li> </ul> <p>Small file and random I/O</p> <p>The scratch is best used to write large files in a continuous manner. Even though the Lustre file system can handle small file and random I/O better that our GPFS system, it still slows down considerably as the number of I/O operations increases. Typical example of operations with a lot random and small file I/O operations in the parallel compilation of large projects.</p> <p>Prefer the locally mounted file system in <code>/tmp</code> for small file and random I/O.</p> Origin of the term scratch <p>The term scratch originates from scratch data tapes. People uses scratch tapes to write and read data that did not fit into the main memory, and since it was a tape, it could only perform continuous I/O. The term scratch is a bit abused in modern times as most storage systems nowadays support random access. In the case of the lustre system in UL HPC, the terms scratch serves as a reminder that the file system is best used for contiguous I/O, even though it supports random access quite well.</p> <p>The environment variable <code>${SCRATCH}</code> (which expands to <code>/scratch/users/$(whoami)</code>) points to a users scratch directory. The absolute path may change, but the value of <code>${SCRATCH}</code> will always be correct.</p> <p>ULHPC <code>$SCRATCH</code> quotas and backup</p> <p>Extended ACLs are provided for sharing data with other users using fine-grained control. See quotas for detailed information about inode, space quotas, and file system policies. In particular, your SCRATCH directory is NOT backuped according to the policy detailed in the ULHPC backup policies.</p> A short history of Lustre <p>Lustre was initiated &amp; funded by the U.S. Department of Energy Office of Science &amp; National Nuclear Security Administration laboratories in mid 2000s. Developments continue through the Cluster File Systems (ClusterFS) company founded in 2001. Sun Microsystems acquired ClusterFS in 2007 with the intent to bring Lustre technologies to Sun's ZFS file system and the Solaris operating system. In 2010, Oracle bought Sun and began to manage and release Lustre, however the company was not known for HPC. In December 2010, Oracle announced that they would cease Lustre 2.x development and place Lustre 1.8 into maintenance-only support, creating uncertainty around the future development of the file system. Following this announcement, several new organizations sprang up to provide support and development in an open community development model, including Whamcloud, Open Scalable File Systems  (OpenSFS,  a nonprofit organization promoting the Lustre file system to ensure Lustre remains vendor-neutral, open, and free), Xyratex or DDN. By the end of 2010, most Lustre developers had left Oracle.</p> <p>WhamCloud was bought by Intel in 2011 and Xyratex took over the Lustre trade mark, logo, related assets (support) from Oracle. In June 2018, the Lustre team and assets were acquired from Intel by DDN. DDN organized the new acquisition as an independent division, reviving the Whamcloud name for the new division.</p>"},{"location":"filesystems/lustre/#general-architecture","title":"General Architecture","text":"<p>A Lustre file system has three major functional units:</p> <ul> <li>One or more MetaData Servers (MDS) nodes (here two) that have one or more MetaData Target (MDT) devices per Lustre filesystem that stores namespace metadata, such as filenames, directories, access permissions, and file layout. The MDT data is stored in a local disk filesystem. However, unlike block-based distributed filesystems, such as GPFS/SpectrumScale and PanFS, where the metadata server controls all of the block allocation, the Lustre metadata server is only involved in pathname and permission checks, and is not involved in any file I/O operations, avoiding I/O scalability bottlenecks on the metadata server.</li> <li>One or more Object Storage Server (OSS) nodes that store file data on one or more Object Storage Target (OST) devices.<ul> <li>The capacity of a Lustre file system is the sum of the capacities provided by the OSTs.</li> <li>OSSs do most of the work and thus require as much RAM as possible<ul> <li>Rule of thumb: ~2 GB base memory + 1 GB / OST</li> <li>Failover configurations: ~2 GB / OST</li> </ul> </li> <li>OSSs should have as much CPUs as possible, but it is not as much critical as on MDS</li> </ul> </li> <li>Client(s) that access and use the data. Lustre presents all clients with a unified namespace for all of the files and data in the filesystem, using standard POSIX semantics, and allows concurrent and coherent read and write access to the files in the filesystem.</li> </ul> Lustre general features and numbers <p>Lustre brings a modern architecture within an Object based file system with the following features:</p> <ul> <li>Adaptable: supports wide range of networks and storage hardware</li> <li>Scalable: Distributed file object handling for 100.000 clients and more</li> <li>Stability: production-quality stability and failover</li> <li>Modular: interfaces for easy adaption</li> <li>Highly Available: no single point of failure when configured with HA software</li> <li>BIG and exapandable: allow for multiple PB in one namespace</li> <li>Open-source and community driven.</li> </ul> <p>Lustre provides a POSIX compliant layer supported on most Linux flavours. In terms of raw number capabilities for the Lustre:</p> <ul> <li>Max system size: about 64PB</li> <li>Max number of OSTs: 8150</li> <li>Max number of MDTs: multiple per filesystem supported since Lustre 2.4</li> <li>Files per directory: 25 Millions (**don't run <code>ls -al</code>)</li> <li>Max stripes: 2000 since Lustre 2.2</li> <li>Stripe size: Min 64kB -- Max 2TB</li> <li>Max object size: 16TB(<code>ldiskfs</code>) 256PB (ZFS)</li> <li>Max file size: 31.35PB (<code>ldiskfs</code>) 8EB (ZFS)</li> </ul> When to use Lustre? <ul> <li>Lustre is optimized for:<ul> <li>Large files</li> <li>Sequential throughput</li> <li>Parallel applications writing to different parts of a file</li> </ul> </li> <li>Lustre will not perform well for<ul> <li>Lots of small files</li> <li>High number of meta data requests, improved on new versions</li> <li>Waste of space on the OSTs</li> </ul> </li> </ul> <ul> <li>Understanding the Lustre Filesystems</li> </ul>"},{"location":"filesystems/lustre/#storage-system-implementation","title":"Storage System Implementation","text":"<p>The way the ULHPC Lustre file system is implemented is depicted on the below figure.</p> <p></p> <p>Acquired as part of RFP 170035, the ULHPC configuration is based upon:</p> <ul> <li>a set of 2x EXAScaler Lustre building blocks that each consist of:<ul> <li>1x DDN SS7700 base enclosure and its controller pair with 4x FDR ports</li> <li>1x DDN SS8460 disk expansion enclosure (84-slot drive enclosures)</li> </ul> </li> <li>OSTs: 160x SEAGATE disks (7.2K RPM HDD, 8TB, Self Encrypted Disks (SED))<ul> <li>configured over 16 RAID6 (8+2) pools and extra disks in spare pools</li> </ul> </li> <li>MDTs: 18x  HGST disks (10K RPM HDD, 1.8TB,  Self Encrypted Disks (SED))<ul> <li>configured over 8 RAID1 pools and extra disks in spare pools</li> </ul> </li> <li>Two redundant MDS servers<ul> <li>Dell R630,   2x Intel Xeon E5-2667v4 @ 3.20GHz [8c], 128GB RAM</li> </ul> </li> <li>Two redundant OSS servers<ul> <li>Dell R630XL, 2x Intel Xeon E5-2640v4 @ 2.40GHz [10c], 128GB RAM</li> </ul> </li> </ul> Criteria Value Power (nominal) 6.8 KW Power (idle) 5.5 KW Weight 432 kg Rack Height 22U <p>LNet is configured to be performed with OST based balancing.</p>"},{"location":"filesystems/lustre/#filesystem-performance","title":"Filesystem Performance","text":"<p>The performance of the ULHPC Lustre filesystem is expected to be in the range of at least 15GB/s for large sequential read and writes.</p>"},{"location":"filesystems/lustre/#ior","title":"IOR","text":"<p>Upon release of the system, performance measurement by IOR, a synthetic benchmark for testing the performance of distributed filesystems, was run for an increasing number of clients as well as with 1kiB, 4kiB, 1MiB and 4MiB transfer sizes.</p> <p></p> <p>As can be seen, aggregated writes and reads exceed 15 GB/s (depending on the test) which meets the minimum requirement.</p>"},{"location":"filesystems/lustre/#fio","title":"FIO","text":"<p>Random IOPS benchmark was performed using FIO with 20 and 40 GB file size over 8 jobs, leading to the following total size of 160GB and 320 GB</p> <ul> <li>320 GB is &gt; 2\\times RAM size of the OSS node (128 GB RAM)</li> <li>160 GB is &gt; 1\\times RAM size of the OSS node (128 GB RAM)</li> </ul> <p></p>"},{"location":"filesystems/lustre/#mdtest","title":"MDTEST","text":"<p>Mdtest (based on the <code>7c0ec41</code> on September 11 , 2017 (based on v1.9.3)) was used to benchmark the metadata capabilities of the delivered system. HT was turned on to be able to run 32 threads.</p> <p></p> <p>Mind the logarithmic Y-Axis. Tests on 4 clients with up to 20 threads have been included as well to show the scalability of the system.</p>"},{"location":"filesystems/lustre/#lustre-usage","title":"Lustre Usage","text":""},{"location":"filesystems/lustre/#understanding-lustre-io","title":"Understanding Lustre I/O","text":"<p>When a client (a compute node from your job) needs to create or access a file, the client queries the metadata server (MDS) and the metadata target (MDT) for the layout and location of the file's stripes. Once the file is opened and the client obtains the striping information, the MDS is no longer involved in the file I/O process. The client interacts directly with the object storage servers (OSSes) and OSTs to perform I/O operations such as locking, disk allocation, storage, and retrieval.</p> <p>If multiple clients try to read and write the same part of a file at the same time, the Lustre distributed lock manager enforces coherency, so that all clients see consistent results.</p>"},{"location":"filesystems/lustre/#discover-mdts-and-osts","title":"Discover MDTs and OSTs","text":"<p>ULHPC's Lustre file systems look and act like a single logical storage, but a large files on Lustre can be divided into multiple chunks (stripes) and stored across over OSTs. This technique is called file striping. The stripes are distributed among the OSTs in a round-robin fashion to ensure load balancing. It is thus important to know the number of OST on your running system.</p> <p>As mentioned in the Lustre implementation section, the ULHPC Lustre infrastructure is composed of 2 MDS servers (2 MDT), 2 OSS servers and 16 OSTs. You can list the MDTs and OSTs with the command <code>lfs df</code>:</p> <pre><code>$ cds      # OR: cd $SCRATCH\n$ lfs df -h\nUUID                       bytes        Used   Available Use% Mounted on\nlscratch-MDT0000_UUID        3.2T       15.4G        3.1T   1% /mnt/lscratch[MDT:0]\nlscratch-MDT0001_UUID        3.2T        3.8G        3.2T   1% /mnt/lscratch[MDT:1]\nlscratch-OST0000_UUID       57.4T       16.7T       40.2T  30% /mnt/lscratch[OST:0]\nlscratch-OST0001_UUID       57.4T       18.8T       38.0T  34% /mnt/lscratch[OST:1]\nlscratch-OST0002_UUID       57.4T       17.6T       39.3T  31% /mnt/lscratch[OST:2]\nlscratch-OST0003_UUID       57.4T       16.6T       40.3T  30% /mnt/lscratch[OST:3]\nlscratch-OST0004_UUID       57.4T       16.5T       40.3T  30% /mnt/lscratch[OST:4]\nlscratch-OST0005_UUID       57.4T       16.5T       40.3T  30% /mnt/lscratch[OST:5]\nlscratch-OST0006_UUID       57.4T       16.3T       40.6T  29% /mnt/lscratch[OST:6]\nlscratch-OST0007_UUID       57.4T       17.0T       39.9T  30% /mnt/lscratch[OST:7]\nlscratch-OST0008_UUID       57.4T       16.8T       40.0T  30% /mnt/lscratch[OST:8]\nlscratch-OST0009_UUID       57.4T       13.2T       43.6T  24% /mnt/lscratch[OST:9]\nlscratch-OST000a_UUID       57.4T       13.2T       43.7T  24% /mnt/lscratch[OST:10]\nlscratch-OST000b_UUID       57.4T       13.3T       43.6T  24% /mnt/lscratch[OST:11]\nlscratch-OST000c_UUID       57.4T       14.0T       42.8T  25% /mnt/lscratch[OST:12]\nlscratch-OST000d_UUID       57.4T       13.9T       43.0T  25% /mnt/lscratch[OST:13]\nlscratch-OST000e_UUID       57.4T       14.4T       42.5T  26% /mnt/lscratch[OST:14]\nlscratch-OST000f_UUID       57.4T       12.9T       43.9T  23% /mnt/lscratch[OST:15]\n\nfilesystem_summary:       919.0T      247.8T      662.0T  28% /mnt/lscratch\n</code></pre>"},{"location":"filesystems/lustre/#file-striping","title":"File striping","text":"<p>File striping permits to increase the throughput of operations by taking advantage of several OSSs and OSTs, by allowing one or more clients to read/write different parts of the same file in parallel. On the other hand, striping small files can decrease the performance.</p> <p>File striping allows file sizes larger than a single OST, large files MUST be striped over several OSTs in order to avoid filling a single OST and harming the performance for all users. There is default stripe configuration for ULHPC Lustre filesystems (see below). However, users can set the following stripe parameters for their own directories or files to get optimum I/O performance. You can tune file striping using 3 properties:</p> Property Effect Default Accepted values Advised values stripe_size Size of the file stripes in bytes 1048576 (1m) &gt; 0 &gt; 0 stripe_count Number of OST to stripe across 1 -1 (use all the OSTs), 1-16 -1 stripe_offset Index of the OST where the first stripe of files will be written -1 (automatic) -1, 0-15 -1 <p>Note: With regards <code>stripe_offset</code> (the index of the OST where the first stripe is to be placed); the default is -1 which results in random selection and using a non-default value is NOT recommended.</p> <p>Note</p> <p>Setting stripe size and stripe count correctly for your needs may significantly affect the I/O performance.</p> <ul> <li>Use the <code>lfs getstripe</code> command for getting the stripe parameters.</li> <li>Use <code>lfs setstripe</code> for setting the stripe parameters to get optimal I/O performance. The correct stripe setting depends on your needs and file access patterns.<ul> <li>Newly created files and directories will inherit these parameters from their parent directory. However, the parameters cannot be changed on an existing file.</li> </ul> </li> </ul> <pre><code>$ lfs getstripe dir|filename\n$ lfs setstripe -s &lt;stripe_size&gt; -c &lt;stripe_count&gt; -o &lt;stripe_offset&gt; dir|filename\n    usage: lfs setstripe -d &lt;directory&gt;   (to delete default striping from an existing directory)\n    usage: lfs setstripe [--stripe-count|-c &lt;stripe_count&gt;]\n                         [--stripe-index|-i &lt;start_ost_idx&gt;]\n                         [--stripe-size|-S &lt;stripe_size&gt;]  &lt;directory|filename&gt;\n</code></pre> <p>Example:</p> <pre><code>$ lfs getstripe $SCRATCH\n/scratch/users/&lt;login&gt;/\nstripe_count:   1 stripe_size:    1048576 stripe_offset:  -1\n[...]\n$ lfs setstripe -c -1 $SCRATCH\n$ lfs getstripe $SCRATCH\n/scratch/users/&lt;login&gt;/\nstripe_count:  -1 stripe_size:   1048576 pattern:       raid0 stripe_offset: -1\n</code></pre> <p>In this example, we view the current stripe setting of the <code>$SCRATCH</code> directory. The stripe count is changed to all OSTs and verified. All files written to this directory will be striped over the maximum number of OSTs (16). Use <code>lfs check osts</code> to see the number and status of active OSTs for each filesystem on the cluster. Learn more by reading the man page:</p> <pre><code>$ lfs check osts\n$ man lfs\n</code></pre>"},{"location":"filesystems/lustre/#file-stripping-examples","title":"File stripping Examples","text":"<ul> <li>Set the striping parameters for a directory containing only small files (&lt; 20MB)</li> </ul> <pre><code>$ cd $SCRATCH\n$ mkdir test_small_files\n$ lfs getstripe test_small_files\ntest_small_files\nstripe_count:   1 stripe_size:    1048576 stripe_offset:  -1 pool:\n$ lfs setstripe --stripe-size 1M --stripe-count 1 test_small_files\n$ lfs getstripe test_small_files\ntest_small_files\nstripe_count:   1 stripe_size:    1048576 stripe_offset:  -1\n</code></pre> <ul> <li>Set the striping parameters for a directory containing only large files between 100MB and 1GB</li> </ul> <pre><code>$ mkdir test_large_files\n$ lfs setstripe --stripe-size 2M --stripe-count 2 test_large_files\n$ lfs getstripe test_large_files\ntest_large_files\nstripe_count:   2 stripe_size:    2097152 stripe_offset:  -1\n</code></pre> <ul> <li>Set the striping parameters for a directory containing files larger than 1GB</li> </ul> <pre><code>$ mkdir test_larger_files\n$ lfs setstripe --stripe-size 4M --stripe-count 6 test_larger_files\n$ lfs getstripe test_larger_files\ntest_larger_files\nstripe_count:   6 stripe_size:    4194304 stripe_offset:  -1\n</code></pre> <p>Big Data files management on Lustre</p> <p>Using a large stripe size can improve performance when accessing very large files</p> <p>Large stripe size allows each client to have exclusive access to its own part of a file. However, it can be counterproductive in some cases if it does not match your I/O pattern. The choice of stripe size has no effect on a single-stripe file.</p> <p>Note that these are simple examples, the optimal settings defer depending on the application (concurrent threads accessing the same file, size of each write operation, etc).</p>"},{"location":"filesystems/lustre/#lustre-best-practices","title":"Lustre Best practices","text":"<p>Parallel I/O on the same file</p> <p>Increase the <code>stripe_count</code> for parallel I/O to the same file.</p> <p>When multiple processes are writing blocks of data to the same file in parallel, the I/O performance for large files will improve when the <code>stripe_count</code> is set to a larger value. The stripe count sets the number of OSTs to which the file will be written. By default, the stripe count is set to 1. While this default setting provides for efficient access of metadata (for example to support the <code>ls -l</code> command), large files should use stripe counts of greater than 1. This will increase the aggregate I/O bandwidth by using multiple OSTs in parallel instead of just one. A rule of thumb is to use a stripe count approximately equal to the number of gigabytes in the file.</p> <p>Another good practice is to make the stripe count be an integral factor of the number of processes performing the write in parallel, so that you achieve load balance among the OSTs. For example, set the stripe count to 16 instead of 15 when you have 64 processes performing the writes. For more details, you can read the following external resources:</p> <ul> <li>Reference Documentation:  Managing File Layout (Striping) and Free Space</li> <li>Lustre Wiki</li> <li>Lustre Best Practices - Nasa HECC</li> <li>I/O and Lustre Usage - NISC</li> </ul>"},{"location":"filesystems/quotas/","title":"Quotas","text":""},{"location":"filesystems/quotas/#overview","title":"Overview","text":"<p>Users and project groups are assigned a fixed amount of storage. There are quota limits in terms of storage space and number of files (inodes). When a quota limit is reached writes to the relevant directories will fail. The storage limits are described below.</p> <p>Quota limits in cluster file systems</p> Directory Default space quota Default inode quota <code>${HOME}</code> 500 GB 1 M <code>${SCRATCH}</code> 10 TB 1 M <code>${PROJECTHOME}/&lt;project name&gt;</code> 1 TB<sup>[1]</sup> 1 M<sup>[1]</sup> <code>/mnt/isilon/projects/&lt;project name&gt;</code> 1.14 PB globally<sup>[2]</sup> - <ol> <li>This is the default and free of charge allocation for projects; requests for more space may incur charges.</li> <li>On Isilon all projects share one global quota limit and the HPC Platform team sets up individual project quotas. Unfortunately it is not currently possible for users to see the quota status on Isilon.</li> </ol>"},{"location":"filesystems/quotas/#storage-usage-information","title":"Storage usage information","text":"<p>The UL HPC systems provide the <code>df-ulhpc</code> command on the cluster login nodes, which displays current usage, soft quota, hard quota and grace period. Any directories that have exceeded the quota will be highlighted in red.</p> <ul> <li>Check current space quota status:   <pre><code>df-ulhpc\n</code></pre></li> <li>Check current inode quota status:   <pre><code>df-ulhpc -i\n</code></pre></li> </ul> <p>Quota limits are applied over 2 time periods. Once you reach the soft quota you can still write data until the grace period expires (7 days) or you reach the hard quota. After you reach the end of the grace period or the hard quota, you have to reduce your usage to below the soft quota to be able to write data again.</p> <p>Warning</p> <p>Do not forget that inodes are also limited! If you are not exceeding the space quota limits according to the output of <code>df-ulhpc</code> and you cannot write files, try <code>df-ulhpc -i</code> to check is you exceed the inode limits.</p> <p>Quota on Isilon</p> <p>On Isilon all projects share one global quota limit and the HPC Platform team sets up individual project quotas. Unfortunately it is not currently possible for users to see the quota status on Isilon with the <code>df-ulhpc</code> command.</p> <p>If you notice that writes on directories stored on Isilon fail, this is probably due to storage exceeding the assigned quota. Contact the UL HPC team for further instructions.</p>"},{"location":"filesystems/quotas/#detail-information-about-storage-usage","title":"Detail information about storage usage","text":"<p>Quite often you exceed the quota limits, but you don't know exactly which files and directories contribute more towards the quota numbers. To detect the exact source of storage and inode usage, you can use the <code>du</code> command.</p> <ul> <li>To print information about space usage:   <pre><code>du --max-depth=&lt;depth&gt; --human-readable &lt;directory&gt;\n</code></pre></li> <li>To print information about inode usage:   <pre><code>du --max-depth=&lt;depth&gt; --human-readable --inodes &lt;directory&gt;\n</code></pre></li> </ul> <p>The flag options and the arguments used are:</p> <ul> <li>depth: the resource (space or inodes) usage for any file from depth and bellow is summed in the report for the directory at level depth in which the file belongs, and</li> <li>directory: the directory for which the analysis is curried out; leaving empty performs the analysis in the current working directory.</li> </ul> <p>For a more graphical approach, use <code>ncdu</code>. With the <code>c</code> option <code>ncdu</code> displays the aggregate inode number for the directories in the current working directory.</p>"},{"location":"filesystems/quotas/#information-about-global-storage-usage","title":"Information about global storage usage","text":"<p>Sometimes it may be useful to inspect the global file system usage. For instance, the <code>${SCRATCH}</code> space is over subscribed, and you may experience a slowdown if the storage is reaching its limits.</p> <ul> <li>Check free space on all file systems: <pre><code>df -h\n</code></pre></li> <li>Check free space on the file system containing a given path: <pre><code>df -h &lt;path&gt;\n</code></pre></li> </ul>"},{"location":"filesystems/quotas/#increasing-quota","title":"Increasing quota","text":"<p>Quotas for <code>${HOME}</code> and <code>${SCRATCH}</code> are fixed on a per user and cannot change.</p> <p>If your project needs additional space or inodes for a specific project directory you may request it via ServiceNow (HPC \u2192 Storage &amp; projects \u2192 Extend quota).</p>"},{"location":"filesystems/quotas/#troubleshooting","title":"Troubleshooting","text":"<p>The quotas on project directories are based on the group. Be aware that the quota for the default user group <code>clusterusers</code> is 0. If you get a quota error, but <code>df-ulhpc</code> and <code>df-ulhpc -i</code> confirm that the quota is not expired, you are most likely trying to write a file with the group <code>clusterusers</code> instead of the project group.</p> <p>To avoid this issue, check out the <code>newgrp</code> command or set the <code>s</code> mode bit (\"set group ID\") on the directory with <code>chmod g+s &lt;directory&gt;</code>. The <code>s</code> bit means that any file or folder created below will inherit the group.</p> <p>To transfer data with <code>rsync</code> into a project directory, please check the data transfer documentation.</p>"},{"location":"filesystems/unix-file-permissions/","title":"Unix File Permissions","text":""},{"location":"filesystems/unix-file-permissions/#brief-overview","title":"Brief Overview","text":"<p>Every file (and directory) has an owner, an associated Unix group, and a set of permission flags that specify separate read, write, and execute permissions for the \"user\" (owner), \"group\", and \"other\".  Group permissions apply to all users who belong to the group associated with the file.  \"Other\" is also sometimes known as \"world\" permissions, and applies to all users who can login to the system.  The command <code>ls -l</code> displays the permissions and associated group for any file.  Here is an example of the output of this command:</p> <pre><code>drwx------ 2 elvis elvis  2048 Jun 12 2012  private\n-rw------- 2 elvis elvis  1327 Apr  9 2012  try.f90\n-rwx------ 2 elvis elvis 12040 Apr  9 2012  a.out\ndrwxr-x--- 2 elvis bigsci 2048 Oct 17 2011  share\ndrwxr-xr-x 3 elvis bigsci 2048 Nov 13 2011  public\n</code></pre> <p>From left to right, the fields above represent:</p> <ol> <li>set of ten permission flags</li> <li>link count (irrelevant to this topic)</li> <li>owner</li> <li>associated group</li> <li>size</li> <li>date of last modification</li> <li>name of file</li> </ol> <p>The permission flags from left to right are:</p> Position Meaning 1 \"d\" if a directory, \"-\" if a normal file 2, 3, 4 read, write, execute permission for user (owner) of file 5, 6, 7 read, write, execute permission for group 8, 9, 10 read, write, execute permission for other (world) <p>and have the following meanings:</p> Value Meaning <code>-</code> Flag is not set. <code>r</code> File is readable. <code>w</code> File is writable. For directories, files may be created or removed. <code>x</code> File is executable. For directories, files may be listed. <code>s</code> Set group ID (sgid). For directories, files created therein will be associated with the same group as the directory, rather than default group of the user.  Subdirectories created therein will not only have the same group, but will also inherit the sgid setting. <p>These definitions can be used to interpret the example output of <code>ls -l</code> presented above:</p> <pre><code>drwx------ 2 elvis elvis  2048 Jun 12 2012  private\n</code></pre> <p>This is a directory named \"private\", owned by user elvis and associated with Unix group elvis.  The directory has read, write, and execute permissions for the owner, and no permissions for any other user.</p> <pre><code>-rw------- 2 elvis elvis  1327 Apr  9 2012  try.f90\n</code></pre> <p>This is a normal file named \"try.f90\", owned by user elvis and associated with group elvis.  It is readable and writable by the owner, but is not accessible to any other user.</p> <pre><code>-rwx------ 2 elvis elvis 12040 Apr  9 2012  a.out\n</code></pre> <p>This is a normal file named \"a.out\", owned by user elvis and associated with group elvis.  It is executable, as well as readable and writable, for the owner only.</p> <pre><code>drwxr-x--- 2 elvis bigsci 2048 Oct 17 2011  share\n</code></pre> <p>This is a directory named \"share\", owned by user elvis and associated with group bigsci.  The owner can read and write the directory; all members of the file group bigsci can list the contents of the directory.  Presumably, this directory would contain files that also have \"group read\" permissions.</p> <pre><code>drwxr-xr-x 3 elvis bigsci 2048 Nov 13 2011  public\n</code></pre> <p>This is a directory named \"public\", owned by user elvis and associated with group bigsci.  The owner can read and write the directory; all other users can only read the contents of the directory.  A directory such as this would most likely contain files that have \"world read\" permissions.</p>"},{"location":"filesystems/unix-file-permissions/#useful-file-permission-commands","title":"Useful File Permission Commands","text":""},{"location":"filesystems/unix-file-permissions/#umask","title":"umask","text":"<p>When a file is created, the permission flags are set according to the file mode creation mask, which can be set using the <code>umask</code> command. The file mode creation mask (sometimes referred to as \"the umask\") is a three-digit octal value whose nine bits correspond to fields 2-10 of the permission flags. The resulting permissions are calculated via the bitwise AND of the unary complement of the argument (using bitwise NOT) and the default permissions specified by the shell (typically 666 for files and 777 for directories). Common useful values are:</p> umask value File Permissions Directory Permissions <code>002</code> <code>-rw-rw-r--</code> <code>drwxrwxr-x</code> <code>007</code> <code>-rw-rw----</code> <code>drwxrwx---</code> <code>022</code> <code>-rw-r--r--</code> <code>drwxr-xr-x</code> <code>027</code> <code>-rw-r-----</code> <code>drwxr-x---</code> <code>077</code> <code>-rw-------</code> <code>drwx------</code> <p>Note that at ULHPC, the default umask is left unchanged (022), yet it can be redefined in your <code>~/.bash_profile</code> configuration file if needed.</p>"},{"location":"filesystems/unix-file-permissions/#chmod","title":"<code>chmod</code>","text":"<p>The <code>chmod</code> (\"change mode\") command is used to change the permission flags on existing files. It can be applied recursively using the \"-R\" option. It can be invoked with either octal values representing the permission flags, or with symbolic representations of the flags. The octal values have the following meaning:</p> Octal Digit Binary Representation (<code>rwx</code>) Permission <code>0</code> <code>000</code> none <code>1</code> <code>001</code> execute only <code>2</code> <code>010</code> write only <code>3</code> <code>011</code> write and execute <code>4</code> <code>100</code> read only <code>5</code> <code>101</code> read and execute <code>6</code> <code>110</code> read and write <code>7</code> <code>111</code> read, write, and execute (full permissions) <p>Here is an example of <code>chmod</code> using octal values:</p> <pre><code>$ umask\n0022\n$ touch foo\n$ ls -l foo\n-rw-r--r--. 1 elvis elvis 0 Nov 19 14:49 foo\n$ chmod 755 foo\n$ ls -l foo\n-rwxr-xr-x. 1 elvis elvis 0 Nov 19 14:49 foo\n</code></pre> <p>In the above example, the umask for user elvis results in a file that is read-write for the user, and read for group and other. The <code>chmod</code> command specifies read-write-execute permissions for the user, and read-execute permissions for group and other.</p> <p>Here is the format of the <code>chmod</code> command when using symbolic values:</p> <pre><code>chmod [-R] [classes][operator][modes] file ...\n</code></pre> <p>The classes determine to which combination of user/group/other the operation will apply, the operator specifies whether permissions are being added or removed, and the modes specify the permissions to be added or removed. Classes are formed by combining one or more of the following letters:</p> Letter Class Description <code>u</code> user Owner of the file <code>g</code> group Users who are members of the file's group <code>o</code> other Users who are not the owner of the file or members of the file's group <code>a</code> all All of the above (equivalent to <code>ugo</code>) <p>The following operators are supported:</p> Operator Description <code>+</code> Add the specified modes to the specified classes. <code>-</code> Remove the specified modes from the specified classes. <code>=</code> The specified modes are made the exact modes for the specified classes. <p>The modes specify which permissions are to be added to or removed from the specified classes. There are three primary values which correspond to the basic permissions, and two less frequently-used values that are useful in specific circumstances:</p> Mode Name Description <code>r</code> read Read a file or list a directory's contents. <code>w</code> write Write to a file or directory. <code>x</code> execute Execute a file or traverse a directory. <code>X</code> \"special\" execute This is a slightly more restrictive version of \"x\".  It applies execute permissions to directories in all cases, and to files only if at least one execute permission bit is already set.  It is typically used with the \"+\" operator and the \"-R\" option, to give group and/or other access to a large directory tree, without setting execute permissions on normal (non-executable) files (e.g., text files).  For example, <code>chmod -R go+rx bigdir</code> would set read and execute permissions on every file (including text files) and directory in the bigdir directory, recursively, for group and other.  The command <code>chmod -R go+rX bigdir</code> would set read and execute permissions on every directory, and would set group and other read and execute permissions on files that were already executable by the owner. <code>s</code> setgid or sgid This setting is typically applied to directories.  If set, any file created in that directory will be associated with the directory's group, rather than with the default file group of the owner. This is useful in setting up directories where many users share access.  This setting is sometimes referred to as the \"sticky bit\", although that phrase has a historical meaning unrelated to this context. <p>Sets of class/operator/mode may separated by commas. Using the above definitions, the previous (octal notation) example can be done symbolically:</p> <pre><code>$ umask\n0022\n$ touch foo\n$ ls -l foo\n-rw-r--r--. 1 elvis elvis 0 Nov 19 14:49 foo\n$ chmod u+x,go+rx foo\n$ ls -l foo\n-rwxr-xr-x. 1 elvis elvis 0 Nov 19 14:49 foo\n</code></pre>"},{"location":"filesystems/unix-file-permissions/#unix-file-groups","title":"Unix File Groups","text":"<p>Unix file groups provide a means to control access to shared data on disk and tape.</p>"},{"location":"filesystems/unix-file-permissions/#overview-of-unix-groups","title":"Overview of Unix Groups","text":"<p>Every user on a Unix system is a member of one or more Unix groups, including their primary or default group. Every file (or directory) on the system has an owner and an associated group. When a user creates a file, the file's associated group will be the user's default group. The user (owner) has the ability to change the associated group to any of the groups to which the user belongs. Unix groups can be defined that allow users to share data with other users who belong to the same group.</p>"},{"location":"filesystems/unix-file-permissions/#unix-groups-at-ulhpc","title":"Unix Groups at ULHPC","text":"<p>All user's default group is <code>clusterusers</code>. Users usually belong to several other groups, including groups associated with specific research projects.</p> <p>Groups are used to shared file between project members, and can be created on request. See the page about Project Data Management for more information.</p>"},{"location":"filesystems/unix-file-permissions/#useful-unix-group-commands","title":"Useful Unix Group Commands","text":"Command Description <code>groups username</code> List group membership <code>id username</code> List group membership with group ids <code>ls -l</code> List group associated with file or directory <code>chgrp</code> Change group associated with file or directory <code>newgrp</code> Create new shell with different default group <code>sg</code> Execute command with different default group"},{"location":"g5k/getting-started-g5k/","title":"Getting started","text":"<p> Presentation slides [pdf]</p>"},{"location":"g5k/getting-started-g5k/#introduction","title":"Introduction","text":"<p>Grid'5000 is a large-scale and flexible testbed for experiment-driven research in all areas of computer science, with a focus on parallel and distributed computing, cloud, HPC, Big Data and AI.</p> <p>It provides:</p> <ul> <li>access to a large amount of resources (about 25000 cores, 800 compute nodes, 550 GPUs...)</li> <li>highly configurable and controllable hardware</li> <li>advanced monitoring and measurement features</li> <li>Reproducible Research and Open Science support</li> <li>a large community of 500+ users</li> </ul> <p>In this tutorial, we'll cover the basics of Grid'5000 usage. At the end of this session, you should be able to log into the platform, reserve computing resources, and perform basic experiments on them.</p>"},{"location":"g5k/getting-started-g5k/#grid5000-platform","title":"Grid'5000 platform","text":"<p>Grid'5000 is a distributed platform, composed of multiple sites scattered all over France, Luxembourg and Belgium. To use Grid'5000, a user will log into a given site (e.g., Luxembourg, Lyon, Grenoble, Toulouse, \u2026) and reserve computing resources located at this site in order to deploy their experimentation (or reserve resources across several sites to perform multi-site experimentation).</p> <p></p>"},{"location":"g5k/getting-started-g5k/#discovering-grid5000-resources","title":"Discovering Grid'5000 resources","text":"<p>Grid'5000 resources are organized in clusters. A cluster is a named group of nodes with homogeneous hardware (CPU, RAM, disk space, GPU...): for example larochette in Luxembourg, neowise in Lyon, dahu in Grenoble...</p> <p>A node is a bare-metal resource (server, or edge-class machine) composed of several cores. A node is named by appending its ID to the cluster name, for example: larochette-1 and larochette-2 are nodes belonging to the larochette cluster.</p> <p>Finally, a core (CPU core) is the smallest resource size that can be reserved on Grid'5000. Each node is composed of several cores that can be reserved individually.</p>"},{"location":"g5k/getting-started-g5k/#grid5000-wiki","title":"Grid'5000 wiki","text":""},{"location":"g5k/getting-started-g5k/#resources-list","title":"Resources list","text":"<p>The Grid'5000 wiki provides a lot of information regarding the platform. On this website, we'll find the Global Hardware page which contains the list of Grid'5000 computing resources. You can filter your search by using the links on the page header to filter resources by site.</p>"},{"location":"g5k/getting-started-g5k/#resources-availability","title":"Resources availability","text":"<p>The Status Page provides links to check status of Grid'5000 and its resources. The \"Resources reservations (OAR) status\" section provides availability of Grid'5000 nodes on each site (among other types of resources, which are beyond the scope of this tutorial).</p> <p>By clicking on the <code>nodes</code> link of a given site, a user can check the resources' availability for a given amount of time on a drawgantt chart. This can help users see what type of resources are immediately free to use.</p>"},{"location":"g5k/getting-started-g5k/#status-page","title":"Status page","text":"<p>Maintenances, incidents, and other unpredicted events can alter the platform usability, resources' availability, or experiments' reproducibility. You can check the list of events currently ongoing, planned, and terminated at the following link: Grid'5000 platform status.</p>"},{"location":"g5k/getting-started-g5k/#sites-descriptions","title":"Sites descriptions","text":"<p>Each Grid'5000 site also has a Home page where links related to the site are aggregated and additional information can be found. The more interesting links on these pages are the hardware and network page links. Here are some links for the biggest sites:</p> <ul> <li>grenoble: hardware | network</li> <li>lille: hardware | network</li> <li>luxembourg: hardware | network</li> <li>lyon: hardware | network</li> <li>nancy: hardware | network</li> <li>nantes: hardware | network</li> <li>rennes hardware | network</li> </ul>"},{"location":"g5k/getting-started-g5k/#logging-into-a-site","title":"Logging into a site","text":"<p>In order to reserve resources located on a given site, users must log into the related frontend. A frontend is a virtual machine which has a software environment very close to the one deployed on the nodes by default. There, users will be able to reserve resources, start deployment, and more.</p> <p>If you haven't done it before, generate an SSH key and put your public key on Grid'5000 User Management System. This is done with the <code>ssh-keygen</code> command:</p> <pre><code>user@pc: ssh-keygen -t ed25519\n</code></pre> <p>The choice of how you configure your SSH key (passphrase, location...) is yours. Just remember that this key will be used each time you log in to Grid'5000.</p> <p>When the SSH key is generated, copy the public key:</p> <pre><code>user@pc: cat .ssh/id_ed25519.pub\n# Copy the output of this command.\n</code></pre> <p>Then go to the following link: https://api.grid5000.fr/stable/users/. Paste the content of your public key in the <code>SSH Keys</code> tab on the right.</p>"},{"location":"g5k/getting-started-g5k/#first-log-in-on-grid5000","title":"First log in on Grid'5000","text":"<p>As seen in the schema describing the Grid'5000 platform, users must pass through <code>access.grid5000.fr</code> to gain access to the platform. Once on this machine, they can start another SSH connection to the site frontend they want to reach:</p> <pre><code>user@pc:~$ ssh &lt;g5k_login&gt;@access.grid5000.fr\nuser@access-north:~$ ssh luxembourg\nuser@fluxembourg:~$\n</code></pre> <p>You should be greeted by a message similar to below:</p> <pre><code>Linux fluxembourg 5.10.0-35-amd64 #1 SMP Debian 5.10.237-1 (2025-05-19) x86_64\n------ Welcome to SLICES-FR - Luxembourg ------\n\nAs a member of the 'ulhpc' group, you can access the following clusters in queue 'default' on this site:\n - petitprince (2012): 11 nodes (2 CPUs Intel Xeon E5-2630L, 6 cores/CPU, 32GB RAM, 232GB HDD, 2 x 10Gb Ethernet)\n - larochette  (2024): 7 nodes (2 CPUs Intel Xeon Platinum 8568Y+, 48 cores/CPU, 4 GPUs Instinct MI210, 512GB RAM, 2980GB SSD, 1 x 25Gb Ethernet)\n* with exotic job type:\n - vianden     (2024): 1 node (2 CPUs Intel Xeon Platinum 8470, 52 cores/CPU, 8 GPUs AMD Instinct MI300X, 2048GB RAM, 2980GB SSD, 1 x 100Gb Ethernet, 2 x 10Gb Ethernet)\n\nMore resources are available in other sites and queues. See &lt;https://api.grid5000.fr/explorer&gt;\n\n** Other SLICES-FR sites: grenoble lille lyon nancy nantes rennes sophia strasbourg toulouse\n** Other groups you are a member of: g5k-staff\n\n** 1 bug could affect your experiment (see https://www.grid5000.fr/status/artifact/)\n--&gt; Inconsistent BIOS configuration details on petitprince\n    https://intranet.grid5000.fr/bug/10647\n\n** Useful links:\n - users home: https://www.grid5000.fr/w/Users_Home\n - usage policy: https://www.grid5000.fr/w/Grid5000:UsagePolicy\n - account management (password change): https://api.grid5000.fr/ui/account\n - support: https://www.grid5000.fr/w/Support\nLast login: Tue Sep  2 16:43:50 2025 from 192.168.66.33\n[user@fluxembourg|~]$\n</code></pre> <p>The Message of the Day provides a little summary of the site:</p> <ul> <li>resources provided by the site</li> <li>other sites list</li> <li>Events and bugs that could affect experiments</li> <li>links to the wiki</li> </ul>"},{"location":"g5k/getting-started-g5k/#efficient-ssh-connection-with-config-file","title":"Efficient SSH connection with config file","text":"<p>Currently we need to manually connect to <code>access.grid5000.fr</code> via SSH and then start another SSH connection to a site. This is tedious and error prone, so we'll configure our SSH client to automatically start an SSH connection to <code>access.grid5000.fr</code> when we're trying to access the Grid'5000 network.</p> <p>Open the file where your SSH user configuration is stored on your local machine. It is usually <code>~/.ssh/config</code>. Add the following lines to this file, and replace <code>&lt;g5k_login&gt;</code> with your Grid'5000 login:</p> <pre><code># Alias for the gateway (not really needed, but convenient)\nHost g5k\n  User &lt;g5k_login&gt;\n  Hostname access.grid5000.fr\n  ForwardAgent no\n\n# Direct connection to hosts within Grid'5000 which are not reachable directly\nHost *.g5k\n  User &lt;g5k_login&gt;\n  ProxyCommand ssh g5k -W \"$(basename %h .g5k):%p\"\n  ForwardAgent no\n</code></pre> <p>The ProxyCommand given might not work with shells other than bash on the Grid'5000 side.</p> <p>Now, we should be able to log into any Grid'5000 site \"directly\" with this command:</p> <pre><code>user@pc:~$ ssh luxembourg.g5k\nuser@fluxembourg:~$\n</code></pre>"},{"location":"g5k/getting-started-g5k/#first-resource-reservation-with-oar","title":"First resource reservation with OAR","text":"<p>OAR is the tool used by Grid'5000 to manage its resources.</p> <p>In this part of the tutorial, we'll connect to a Grid'5000 site and reserve resources to learn how users can perform experiments on the platform.</p> <p>Before logging into a Grid'5000 site, it's best to already know what resources we'd like to reserve. Usually, users will explore the Hardware pages mentioned earlier and choose resources that fit their experiment's requirements. For this tutorial, we'll use the clervaux cluster in Luxembourg.</p>"},{"location":"g5k/getting-started-g5k/#reserving-resources","title":"Reserving resources","text":"<p>We are now ready to reserve resources. Grid'5000's resources are managed by the OAR resources and tasks manager. Hence, the command to reserve a resource is <code>oarsub</code>. A basic <code>oarsub</code> command will allocate the first resource available for a default duration (wall time) of 1 hour:</p> <pre><code>[user@fluxembourg|~]$ oarsub -r now -q testing\n# Filtering out exotic resources (vianden).\n# Set walltime to default (3600 s).\nOAR_JOB_ID=256908\n# Advance reservation request: waiting for validation...\n# Reservation valid --&gt; OK\n</code></pre> <p>Our reservation has been assigned a job ID. Our reservation should be visible on the drawgantt of the site we're currently logged into. It may not have started yet, so it may be worth checking our reservation's state with the <code>oarstat</code> command:</p> <pre><code>[user@fluxembourg|~]$ oarstat -u\nJob id     Name           User           Submission Date     S Queue\n---------- -------------- -------------- ------------------- - ----------\n1894417                   user           2025-06-08 12:19:52 R default\n</code></pre> <p>(<code>-u</code> option is used to show reservations from the current user only)</p> <p>The state of the reservation will be shown in the <code>S</code> column. There are the following job states:</p> <ul> <li><code>W</code>: waiting - OAR must wait before scheduling the job, because the requested resources are not available yet</li> <li><code>L</code>: launching - the job is starting, resources will be available to use soon</li> <li><code>R</code>: running - The job is currently running</li> <li><code>E</code>: error - There was an error with the job</li> <li><code>F</code>: finished - The job has terminated</li> </ul> <p>Check regularly the state of your job until its state is <code>R</code>.</p>"},{"location":"g5k/getting-started-g5k/#information-about-a-reservation","title":"Information about a reservation","text":"<p>After submitting a reservation, it can be useful to check related information, such as the node(s) allocated, with the <code>oarstat</code> command.</p> <pre><code>user@fluxembourg:~$ oarstat -f -j 1894417\nid: 1894417\n    array_id = 1894417\n    array_index = 1\n    name =\n    project = g5k-staff\n    owner = user\n    state = Error\n    wanted_resources = -l \"{(type = 'default') AND exotic = 'NO'}/host=1,walltime=0:59:49\"\n    types = monitor=prom_.*default_metrics\n    dependencies =\n    assigned_resources = 2762+2763+2764+2765+2766+2767+2768+2769+2770+2771+2772+2773\n    assigned_hostnames = clervaux-11.luxembourg.grid5000.fr\n    queue = default\n    command =\n    launching_directory = /home/user\n    stdout_file = &lt;interactive shell&gt;\n    stderr_file = &lt;interactive shell&gt;\n    type = INTERACTIVE\n    properties = maintenance = 'NO'\n    reservation = Scheduled\n    walltime = 0:59:49\n    submission_time = 2025-06-08 12:19:52\n    start_time = 2025-06-08 12:20:02\n    stop_time = 2025-06-08 12:45:26\n    cpuset_name = user_1894417\n    initial_request = oarsub -r now\n    message = R=12,W=0:59:49,J=R,P=g5k-staff,T=monitor=prom_.*default_metrics\n    scheduled_start = no prediction\n    resubmit_job_id = 0\n    events =\n</code></pre> <p>If needed, <code>oarstat</code> can output in YAML format with the <code>-Y</code> parameter, or JSON with the <code>-J</code> parameter.</p> <p>The node allocated to our reservation can be found by looking at the <code>assigned_hostnames</code> property of the oarstat output. This will help us log into our resource during the next step.</p>"},{"location":"g5k/getting-started-g5k/#connecting-to-the-reservation","title":"Connecting to the reservation","text":"<p>Now that we have a resource reserved, we can connect to the node allocated and use it. This can be done using SSH:</p> <pre><code>[user@fluxembourg|~]$ ssh clervaux-11\nuser@clervaux-11:~$\n</code></pre> <p>You can also use the following command to get connected to the node:</p> <pre><code>[user@fluxembourg|~]$ oarsub -C\nuser@clervaux-11:~$\n</code></pre> <p><code>oarsub -C</code> will work only if you have one reservation currently ongoing at the moment. If you have several reservations running at the same time, <code>oarsub</code> can't decide which node it will send you to, and it will ask you to enter the OAR Job ID like this: <code>oarsub -C &lt;jobid&gt;</code></p> <p>You should see that your prompt changed: it now displays the current Grid'5000 node on which you are logged.</p>"},{"location":"g5k/getting-started-g5k/#information-about-storage-on-grid5000","title":"Information about storage on Grid'5000","text":"<p>On Grid'5000 infrastructure, users have access to several means of storing their files. First, each user is allocated one homedir directory per site, with a 25GB soft quota (and 100GB hard quota). These homes are mounted by default on the nodes that are used by an experiment.</p> <p>Apart from this homedir mounted via NFS, computing nodes have their own filesystem on disk by default. This means that if someone is writing files into <code>/tmp</code> or <code>/var</code> they are writing directly to the nodes' disk, thus having quicker IOs than if they were writing in their home via NFS. However, disks are not persistent on Grid'5000, so any file written on a node's disk will be lost eventually. Users still need to bring back their files to their home at the end of their experiment.</p> <p>Finally, users can benefit from storage groups. Storage groups are big dedicated volumes located on NFS servers which volume is around several terabytes. These volumes can be associated with a dedicated user group, ensuring that only a subset of users are allowed to access.</p> <p>Each of these storage options is described in the Storage Guide.</p>"},{"location":"g5k/getting-started-g5k/#node-usage-example","title":"Node usage example","text":"<p>At this time, users start prototyping their experiments. For example, we could clone a git repository, build it, and then execute the code. Let's assume we wanted to run a benchmark. We'll get the NAS Parallel Benchmarks (NPB), compile and run them. Once done, we'll export the result to our home.</p> <p>Let's start by going into the <code>/tmp/</code> directory and downloading the benchmarks' source code:</p> <pre><code>user@clervaux-11:~$ cd /tmp/\nuser@clervaux-11:~$ wget 'https://www.nas.nasa.gov/assets/npb/NPB3.4.3.tar.gz'\nuser@clervaux-11:~$ tar -xzvf NPB3.4.3.tar.gz\n</code></pre> <p>It's important to keep in mind that the <code>/tmp/</code> directory is located on the disk's filesystem, while our current home directory is mounted via NFS from a storage server. Working in <code>/tmp/</code> reduces network I/O overhead and reduces pressure on the storage servers.</p> <p>Now we'll use the default template and Makefile to compile the benchmarks:</p> <pre><code>user@clervaux-11:~$ cd NPB3.4.3/NPB3.4-OMP\nuser@clervaux-11:/tmp/NPB3.4.3/NPB3.4-OMP$ cp config/suite.def.template config/suite.def\nuser@clervaux-11:/tmp/NPB3.4.3/NPB3.4-OMP$ cp config/make.def.template config/make.def\nuser@clervaux-11:/tmp/NPB3.4.3/NPB3.4-OMP$ make -j$(nproc) suite\n</code></pre> <p>Now that everything is compiled, we can run the benchmarks. The following commands create a directory where the benchmarks output will be stored. The benchmarks are run with the default configuration, and once they have completed their output is copied to our home directory.</p> <pre><code>user@clervaux-11:/tmp/NPB3.4.3/NPB3.4-OMP$ mkdir /tmp/benchs\nuser@clervaux-11:/tmp/NPB3.4.3/NPB3.4-OMP$ for bench in $(ls bin); do ./bin/$bench | tee /tmp/benchs/$bench.txt; done\nuser@clervaux-11:/tmp/NPB3.4.3/NPB3.4-OMP$ cp -R /tmp/benchs ~/benchs-$OAR_JOBID\n</code></pre> <p>The benchmarks output should have been copied to our home directory. Each benchmark output can now be read with <code>cat</code> or <code>less</code>, and will stay in our home even when our reservation is over.</p> <pre><code>user@clervaux-11:/tmp/NPB3.4.3/NPB3.4-OMP$ cd\nuser@clervaux-11:~$ cat benchs-$OAR_JOBID/lu.S.x.txt\n</code></pre> <p>In the example we use the <code>OAR_JOBID</code> variable because we are still \"in\" our reservation. If someone wishes to come back to their results later, they'll have to enter the reservation's job id manually.</p>"},{"location":"g5k/getting-started-g5k/#deeper-customization-by-gaining-root-access-with-sudo-g5k","title":"Deeper customization by gaining root access with sudo-g5k","text":"<p>One of the key features of Grid'5000 is that you are allowed to become root under certain conditions to fully customize your experiment setup. One way of becoming root is by reserving a whole computing node as we did and using the <code>sudo-g5k</code> command. This command is a wrapper for <code>sudo</code> that will perform safety checks before granting you the right to use the default sudo and become root.</p> <p>Once logged into their computing resource, users can ask for root access with the following command:</p> <pre><code>user@clervaux-11:~$ sudo-g5k [command-to-execute]\n</code></pre> <p>Providing a command while using <code>sudo-g5k</code> is not mandatory.</p> <p>Once <code>sudo-g5k</code> has run, the platform is aware that the user is going to perform non-trivial operations on the node. When their job is finished, the default environment will be redeployed to provide a clean copy for future users.</p> <p>After running <code>sudo-g5k</code>, it is possible to run root commands with <code>sudo</code> or to become root directly:</p> <pre><code>user@clervaux-11:~$ sudo -iu root\nroot@clervaux-11:~#\n</code></pre> <p>Note that the <code>$</code> at the end of the prompt has changed to <code>#</code>, indicating you are indeed root.</p> <p>Once a user is allowed to use the root account, they can perform any operation that would be forbidden as a normal user:</p> <pre><code>root@clervaux-11:~# apt update\nroot@clervaux-11:~# apt upgrade\nroot@clervaux-11:~# apt install ninvaders\nroot@clervaux-11:~# exit\nuser@clervaux-11:~$ ninvaders\n</code></pre> <p>You can quit ninvaders by using <code>Ctrl</code>+<code>c</code> shortcut if the lecturer or your thesis professor is coming. </p> <p>Be aware that using <code>sudo-g5k</code> will trigger a full redeployment of the node at the end of your reservation. <code>sudo-g5k</code> is a very great tool for prototyping, but it should not be used intensively, especially on automatic and short reservations. By using <code>sudo-g5k</code> in short reservations performed by a script you might end up submitting reservations faster than the node can be redeployed, causing a starvation phenomenon for other users.</p>"},{"location":"g5k/getting-started-g5k/#releasing-resources","title":"Releasing resources","text":"<p>When work is finished on the node we reserved, we can log out and delete our job with the <code>oardel</code> command:</p> <pre><code>user@clervaux-11:~$ logout\nConnection to clervaux-11.luxembourg.grid5000.fr closed.\nDisconnected from OAR job 1894417.\n[user@fluxembourg|~]$ oardel 1894417\nDeleting the job = 1894417 ...REGISTERED.\nThe job(s) [ 1894417 ] will be deleted in the near future.\n</code></pre> <p>If we don't do the <code>oardel</code> command, the node will sit idle for approximately one hour (due to the fact that our reservation has a default duration, also called \"job walltime\", of 1 hour).</p> <p>It is important to delete jobs that are holding unused resources to make them available to other users.</p>"},{"location":"g5k/getting-started-g5k/#plan-a-reservation-on-resources","title":"Plan a reservation on resources","text":"<p>Users can reserve resources in advance for a given date and a given amount of time. This can be done with the <code>oarsub</code> command by specifying the start date and end date of the job:</p> <pre><code>user@fluxembourg:~$ oarsub -r \"2025-06-10 10:00, 2025-06-10 12:00\"\n# Filtering out exotic resources (vianden).\nOAR_JOB_ID=256910\n# Advance reservation request: waiting for validation...\n# Reservation valid --&gt; OK\n</code></pre> <p>Here, a node has been allocated on 2025-06-10 between 10AM and 12PM (French time). The job will start automatically at the given date, and we'll be able to log in when it starts.</p> <p>Delete this job, since we are not going to use it:</p> <pre><code>user@fluxembourg:~$ oardel 256910\nDeleting the job = 256910 ...REGISTERED.\nThe job(s) [ 256910 ] will be deleted in the near future.\n</code></pre>"},{"location":"g5k/getting-started-g5k/#advanced-resource-management","title":"Advanced resource management","text":"<p>For now, we have been reserving resources without specifically asking for anything. This part of the tutorial will present some ways to select resources according to given criteria.</p>"},{"location":"g5k/getting-started-g5k/#filter-resources-with-p-option","title":"Filter resources with -p option","text":""},{"location":"g5k/getting-started-g5k/#cluster-name","title":"Cluster name","text":"<p>With the <code>-p</code> option of the <code>oarsub</code> command we can ask for a given cluster. For example:</p> <pre><code>user@fluxembourg:~$ oarsub -r now -p larochette\n</code></pre>"},{"location":"g5k/getting-started-g5k/#cluster-properties","title":"Cluster properties","text":"<p>The <code>-p</code> option can also be used to specify hardware specs we want to get for our reservation. For example, <code>-p \"core_count &gt; 8\"</code> will let us reserve computing resources with more than 8 cores.</p> <p>The string passed with the <code>-p</code> option follows SQL syntax, so we can write selections like this:</p> <pre><code>user@fluxembourg:~$ oarsub -r now -p \"cputype LIKE 'Intel Xeon%'\"\n</code></pre> <p>This <code>oarsub</code> command will reserve one node having an Intel Xeon CPU.</p> <p>For more properties, consult the OAR properties page of the wiki. The OAR Syntax Simplification page also describes how to ease the use of the <code>-p</code> parameter by avoiding having to write SQL syntax.</p>"},{"location":"g5k/getting-started-g5k/#exotic-type","title":"Exotic type","text":"<p>By default, OAR will exclude some resources from the pool of resources that can be allocated to users. These resources are called exotic. A resource can be declared exotic for several reasons:</p> <ul> <li>It is rare on the platform (only a few machines with the same particular characteristics are available in Grid'5000)</li> <li>Its usage is complex: a relevant usage of these resources requires specific knowledge</li> <li>It has characteristics too different from regular Grid'5000 resources to be allocated as a \"standard\" resource</li> </ul> <p>For all the reasons mentioned above, some resources won't be allocated unless users specify the <code>-t exotic</code> parameter in their oarsub command.</p> <p>The list of exotic resources available on Grid'5000 can be found here: https://www.grid5000.fr/w/Exotic.</p> <p>Here is an oarsub command that will reserve the vianden node in Luxembourg:</p> <pre><code>user@fluxembourg:~$ oarsub -r now -p vianden -t exotic\n</code></pre>"},{"location":"g5k/getting-started-g5k/#resources-quantity","title":"Resources quantity","text":"<p>The <code>-l</code> parameter can be used to alter various parameters regarding the reservation:</p> <ul> <li>walltime of the job, which corresponds to reservation duration</li> <li>number of resources requested (nodes, cpu, cores, gpu...)</li> </ul> <p>By using these options, you can modify the granularity of your reservation and extend the time your resources will be allocated to you.</p> <p>The following command will book 2 nodes for a duration of 4 hours:</p> <pre><code>user@fluxembourg:~$ oarsub -r now -l nodes=2,walltime=4:00\n</code></pre> <p>Once again, when reserving several nodes at the same time, the oarstat command can provide the list of reserved computing resources. Here is a command that formats the output of <code>oarstat</code> into JSON and prints the assigned_network_address of the nodes allocated:</p> <pre><code>user@fluxembourg:~$ oarstat -j 1899660 -fJ | jq '.\"1899660\".assigned_network_address'\n[\n  \"clervaux-1.luxembourg.grid5000.fr\",\n  \"clervaux-10.luxembourg.grid5000.fr\"\n]\n</code></pre>"},{"location":"g5k/getting-started-g5k/#extend-a-reservation-with-oarwalltime","title":"Extend a reservation with oarwalltime","text":"<p>Once the walltime of a reservation is reached, users are logged out of their node and their processes are killed. To avoid this issue, users can extend the walltime of their reservation while it is running with the command <code>oarwalltime</code>. Note that <code>oarwalltime</code> will not be able to extend the reservation duration every time; if someone is in the queue for the same resources, based on several parameters oarwalltime might refuse to extend the walltime.</p> <p>The following command extends by 1 hour the walltime of a reservation:</p> <pre><code>user@fluxembourg:~$ oarwalltime &lt;oar_job_id&gt; +1:00\n</code></pre>"},{"location":"g5k/getting-started-g5k/#basics-of-system-deployment-with-kadeploy3","title":"Basics of system deployment with kadeploy3","text":"<p>One of the key features of Grid'5000 is that users are allowed to deploy another operating system on the computing nodes they reserved and become root on their resources. Operating system deployment is done with kadeploy3.</p> <p>With kadeploy3, users can deploy software environments (usually GNU/Linux distributions) on their computing resources and customize the system to set up a complex experiment.</p>"},{"location":"g5k/getting-started-g5k/#grid5000-environments","title":"Grid'5000 environments","text":"<p>The Grid'5000/SLICES-FR &amp; ABACA technical teams maintain several environments that can be deployed. Here is a partial list of the environments available:</p> environment quick description debian11-min Debian 11 distribution with standard system utilities debian11-nfs debian11-min with nfs configuration for user home mounting debian11-big debian11-nfs with partial support of modules providing for scientific software debian12-min Debian 12 distribution with standard system utilities debian12-nfs debian12-min with user home mounted on the computing node debian12-big debian12-nfs with partial support of modules providing for scientific software ubuntu2404-min ubuntu 24.04 with standard system utilities ubuntu2404-nfs ubuntu2404-min with nfs configuration for user home mounting <p>The full list of environments can be found on the following page.</p> <p>On a site frontend, the command <code>kaenv3</code> can be used to print the list of available environments.</p> <pre><code>user@fluxembourg:~$ kaenv3\n</code></pre>"},{"location":"g5k/getting-started-g5k/#make-a-reservation-for-deployment","title":"Make a reservation for deployment","text":""},{"location":"g5k/getting-started-g5k/#-t-deploy-parameter","title":"\"-t deploy\" parameter","text":"<p>In order to be deployed, nodes must be reserved with the <code>-t deploy</code> parameter. This notifies OAR that we are about to change the operating system of the node, and that it will have to redeploy the node back to its original environment when our job is over.</p> <p>Let's reserve a node with this parameter:</p> <pre><code>user@fluxembourg:~$ oarsub -r now -t deploy -q testing\n</code></pre>"},{"location":"g5k/getting-started-g5k/#environment-deployment","title":"Environment deployment","text":"<p>Once the job has started, we can start the deployment using:</p> <p><pre><code>user@fluxembourg:~$ kadeploy3 -e debian12-min -m clervaux-3.luxembourg.grid5000.fr\n</code></pre> (<code>-m</code> can be given multiple times to deploy several nodes at once)</p> <p>Alternatively, you can use your reservation's job ID to perform the deployment:</p> <pre><code>user@fluxembourg:~$ oarsub -C 1895784\nuser@fluxembourg:~$ kadeploy3 -e debian12-min\n</code></pre> <p>Once the deployment is over, we can log in as root on our newly-deployed node: <pre><code>user@fluxembourg:~$ ssh root@clervaux-3\nroot@clervaux-3:~#\n</code></pre></p> <p>To ensure we have changed the distribution running on the node, we can take a look at the file <code>/etc/os-release</code>, or install <code>lsb_release</code>:</p> <pre><code>root@clervaux-3:~# apt update &amp;&amp; apt install -y lsb-release\nroot@clervaux-3:~# lsb_release -a\n</code></pre>"},{"location":"g5k/getting-started-g5k/#customize-environment","title":"Customize environment","text":"<p>Now that we are logged in as root on the node, we can fully customize the system. For example, install upgrades, install a newer kernel, drivers, toolchain etc.</p> <p>Once a user is happy with their customization, they should think about saving their environment to be able to deploy it later without having to reconfigure it. The Environment Creation page describes two methods for storing Grid'5000 environments:</p> <ul> <li>saving the current customized environment to a compressed tarball with tgz-g5k</li> <li>write a reproducible recipe and generate the environment with Kameleon</li> </ul>"},{"location":"g5k/getting-started-g5k/#serial-console-with-kaconsole3","title":"Serial console with kaconsole3","text":"<p>Now that we gained root access to a computing node, the risk of committing a regrettable mistake increases: some modification performed as root may disconnect the node from the network, or prevent it from booting.</p> <p>To prevent users from being stuck in situations where they lost access to their resource and are forced to redeploy a full environment, Grid'5000 provides a way to access the nodes via their serial console.</p> <p>This is done by using the <code>kaconsole3</code> command.</p> <p>The following command will give access to a node's serial console, if we reserved it via a -t deploy job:</p> <pre><code>user@fluxembourg:~$ kaconsole3 -m clervaux-3\n</code></pre> <p>Since you are accessing a serial console, it is possible that you don't see anything at first. Pressing <code>ENTER</code> should make information appear, such as the login prompt. To exit a kaconsole3 session, type <code>&amp;.</code>, or press <code>ESC</code> 4 times.</p> <p>This tool is also very useful for debugging boot problems. Let's reboot the node and look at the kaconsole3 output to see the boot logs of the node. Rebooting a node reserved via a <code>-t deploy</code> job can be done with the <code>kareboot3</code> command. Open a new shell, connect to Grid'5000 and enter the following command. Next, go to your kaconsole3 session and look at the logs outputted by the kernel on serial TTY.</p> <pre><code>user@fluxembourg:~$ kareboot3 simple -m clervaux-3\nuser@fluxembourg:~$ kaconsole3 -m clervaux-3\n</code></pre>"},{"location":"g5k/getting-started-g5k/#example-of-network-mistake-repaired-with-kaconsole3","title":"Example of network mistake repaired with kaconsole3","text":"<p>A common use case of Grid'5000 is to deploy network topologies to study distributed systems. In this context, a mistake in the network configuration can be costly: a misconfigured node might not be reachable anymore, thus being unconfigurable.</p> <p>We'll simulate this mistake by harshly deactivating the network interface of our node:</p> <pre><code>user@fluxembourg:~$ ssh root@clervaux-3\n</code></pre> <p>Use the <code>ip a</code> command to see which interface has an IP address. On the clervaux cluster, this interface is <code>enp1s0f0np0</code>. Let's shutdown this interface:</p> <pre><code>root@clervaux-4:~# ifdown enp1s0f0np0\n</code></pre> <p>Our terminal is hanging! This means we have cut the branch we were sitting on: the node is not reachable via the network. Let's log into Grid'5000 in another terminal and try to ping our node:</p> <pre><code>user@pc:~$ ssh luxembourg.g5k\nuser@fluxembourg:~$ ping clervaux-3\nPING clervaux-3.luxembourg.grid5000.fr (172.16.180.3) 56(84) bytes of data.\nFrom fluxembourg.luxembourg.grid5000.fr (172.16.191.109) icmp_seq=1 Destination Host Unreachable\nFrom fluxembourg.luxembourg.grid5000.fr (172.16.191.109) icmp_seq=2 Destination Host Unreachable\nFrom fluxembourg.luxembourg.grid5000.fr (172.16.191.109) icmp_seq=3 Destination Host Unreachable\n^C\n--- clervaux-3.luxembourg.grid5000.fr ping statistics ---\n6 packets transmitted, 0 received, +3 errors, 100% packet loss, time 5113ms\npipe 4\n</code></pre> <p>Pings don't reach our computing resource. It is really disconnected from the network. We'll use <code>kaconsole3</code> to gain access to the serial console of the node and then put back up the network interface of the node:</p> <p>Reminder: the default password for the root account in Grid'5000 environments is <code>grid5000</code>.</p> <pre><code>user@fluxembourg:~$ kaconsole3 -m clervaux-3\nroot\npassword:\n[...]\nroot@clervaux-4:~# ifup enp1s0f0np0\n</code></pre> <p>We have restored the network connection! The previous SSH connection should resume a few moments later.</p>"},{"location":"g5k/getting-started-g5k/#experiment-automation","title":"Experiment automation","text":""},{"location":"g5k/getting-started-g5k/#grid5000-experimentation-workflow","title":"Grid'5000 experimentation workflow","text":"<p>On Grid'5000, there is a distinction between interactive usage and non-interactive usage. For now, we've been using resources in 'interactive mode', meaning that we were performing every action by ourselves. 'Non-interactive mode' is when someone lets a script (bash, python, ruby...) use the resources that were allocated. Even resource reservations can be scripted.</p> <p>Grid'5000 usage is split between day and night: the day is intended to let people use computing nodes interactively, while the night (and full weekends) are used to run longer and automated reservations. Grid'5000 Usage Policy provides more information about this day/night concept.</p>"},{"location":"g5k/getting-started-g5k/#submitting-a-command","title":"Submitting a command","text":"<p>For simple tasks, users can submit a command that will be executed when the job starts. When we made our previous reservations for this tutorial, there was actually a command that was executed by default: <code>sleep infinity</code>. That was this command that was holding our job in place. By providing our own command, we can start a script at the beginning of our reservation that will handle our experiment (for example, start a deployment, setup a software stack on the node, and then run a series of benchmarks).</p> <p>Let's submit a simple command to experiment with this feature:</p> <pre><code>user@fluxembourg:~$ oarsub -r now \"lscpu &gt; lscpu.txt\"\n</code></pre> <p>Let this job be scheduled and executed, and then take a look at the <code>lscpu.txt</code> file. You'll see that this file has been populated with the output of lscpu, and this output suggests that it has been executed on the computing node that was allocated to you.</p> <p>If your non-deploy reservation contains several nodes, the given command will be executed on the first allocated node, which is called the head node. A deploy reservation will have its command executed on the site's frontend. When this command terminates, the reservation is over, and any remaining connection will be closed. When no command is provided (as we did until now), a default <code>sleep infinity</code> command is started. This command will never complete, so it ensures that the job will only terminate when the reservation's walltime has been exceeded.</p> <p>Instead of submitting a basic command like we did, we could submit a command launching a script of our own and performing a whole experiment: fetching source code, building, running, extracting relevant metrics.</p>"},{"location":"g5k/getting-started-g5k/#script-execution","title":"Script execution","text":"<p>While specifying a command of their own with their reservation, users can execute a script of their own to handle their experiment. Here is a small bash script that will deploy an environment and run a stress command:</p> <pre><code>#!/bin/bash\n\n# bash strict mode configuration\nset -euo pipefail\nIFS=$'\\n\\t'\n\nENV=debian12-min\nNODES_FILE=~/.nodes\n\nif [ -z \"$OAR_NODEFILE\" ]; then\n    echo \"OAR_NODEFILE variable not defined.\"\n    echo \"Please submit a reservation and rerun this script.\"\n    exit 1\nfi\n\ncat $OAR_NODEFILE | sort | uniq &gt; $NODES_FILE\nkadeploy3 -f $OAR_NODEFILE -e $ENV\nclush -l root --hostfile=$NODES_FILE \"apt-get update &amp;&amp; apt-get install -y stress\"\nclush -l root --hostfile=$NODES_FILE 'stress -c $(nproc) -t 60 &gt; /tmp/stress.txt'\n\nmkdir -p stress\nclush -l root --hostfile=$NODES_FILE --rcopy /tmp/stress.txt --dest stress\n\nrm $NODES_FILE\n</code></pre> <p>This script deploys a <code>-min</code> environment. Min environments don't include NFS tools and configuration allowing automagic homedir mounting. This is why we must use scp to retrieve the data generated by our experiment at the end of the job. In a similar way, users often need to clone git repositories or send source code to their reserved nodes at the beginning of their reservation.</p> <p>Let's save this script as <code>experiment.sh</code> in our home directory. Be careful about setting the <code>x</code> permission, as we'll need to execute it:</p> <pre><code>user@fluxembourg:~$ chmod +x experiment.sh\nuser@fluxembourg:~$ oarsub -t deploy -r now \"./experiment.sh\" -q testing\n</code></pre> <p>We can track the progress of the job by looking at the job's state with the <code>oarstat</code> command, or by looking at <code>OAR.&lt;jobid&gt;.stdout</code> and <code>OAR.&lt;jobid&gt;.stderr</code> content. Once the reservation is over, we should see that a <code>stress</code> directory was created in our home, and a .txt file has been synced inside.</p> <p>Let's find the job id of our reservation and then monitor our experiment with the multitail command:</p> <pre><code>user:~$ multitail -i OAR.&lt;jobid&gt;.stdout -i OAR.&lt;jobid&gt;.stderr\n</code></pre>"},{"location":"g5k/getting-started-g5k/#fully-scripted-usage-of-grid5000-through-the-rest-api","title":"Fully scripted usage of Grid'5000 through the REST API","text":""},{"location":"g5k/getting-started-g5k/#programmatically-discovering-resources","title":"Programmatically discovering resources","text":"<p>Grid'5000 provides a REST API that allows users to automatically discover resources and allows monitoring their evolution over time. Using tools like curl, or a simple web browser, users can obtain a lot of information such as:</p> <ul> <li>Grid'5000 sites list and information: https://api.grid5000.fr/stable/sites/<ul> <li>luxembourg site description: https://api.grid5000.fr/stable/sites/luxembourg/</li> </ul> </li> <li>Clusters information:<ul> <li>luxembourg clusters information: https://api.grid5000.fr/stable/sites/luxembourg/clusters/</li> </ul> </li> <li>Nodes information:<ul> <li>luxembourg vianden-1 node information: https://api.grid5000.fr/stable/sites/luxembourg/clusters/vianden/nodes/vianden-1</li> </ul> </li> </ul> <p>These endpoints allow users to explore Grid'5000 resources programmatically. Here is a bash script that will search for any cluster having more than 2 network interfaces:</p> <pre><code>#!/bin/bash\n\nmapfile -t sites &lt; &lt;(curl -s https://api.grid5000.fr/stable/sites | jq -r \".items[] | .uid\")\n\nfor site in \"${sites[@]}\"; do\n    mapfile -t clusters &lt; &lt;(curl -s https://api.grid5000.fr/stable/sites/$site/clusters | jq -r \".items[] | .uid\")\n    for cluster in \"${clusters[@]}\"; do\n        count=$(curl -s https://api.grid5000.fr/stable/sites/$site/clusters/$cluster/nodes.json | jq \".items[0].kavlan | length\")\n        if [ $(($count)) -ge 2 ]; then\n            echo \"$site: $cluster has $count network interfaces\"\n        fi\n    done\ndone\n</code></pre> <p>This script must be executed from within Grid'5000. We'll see in the next steps how we can log into the platform. Users wanting to reach the API from outside Grid'5000 will have to authenticate themselves using their Grid'5000 account.</p>"},{"location":"g5k/getting-started-g5k/#job-submission","title":"Job submission","text":"<p>Since Grid'5000 provides a REST API, each API call can be automatized via a high-level script (python, ruby, bash...). This allows users to script and reproduce all their experiments: from the reservation to the results saving.</p> <p>Let's write a python script that submits the reservation for our previous stress reservation. Below is a script that will ask users to specify the Grid'5000 site and cluster where to perform the experiment, and will then submit a reservation executing the <code>experiment.sh</code> shown above.</p> <p>This python script assumes that there is a bash script named <code>experiment.sh</code> located at the root of your home directory and executable! If you want to launch the experiment on a different site you must ensure that the <code>experiment.sh</code> script has been copied to the homedir of this site.</p> <pre><code>import os\nimport requests\nfrom time import sleep\n\n# When querying the API from the frontends, users are already identified.\n# However, if using this script from outside of Grid'5000, g5k_auth\n# must be a tuple of strings: (username, password)\ng5k_auth = None\n\nsite_id = input(\"Enter a site: \")\ncluster = input(\"Enter a cluster name: \")\n\napi_job_url = f\"https://api.grid5000.fr/stable/sites/{site_id}/jobs\"\n\npayload = {\n    'resources': 'nodes=1',\n    'types': ['deploy'],\n    'command': './experiment.sh',\n    'properties': f\"cluster='{cluster}'\",\n    'name': 'slices-fr-school-2025'\n}\n\njob = requests.post(api_job_url, data=payload, auth=g5k_auth).json()\njob_id = job['uid']\n\nprint(f\"Job submitted ({job_id})\")\n</code></pre> <p>While executing this script, you will be asked for a site and a cluster name:</p> <pre><code>user@fluxembourg:~$ python api.py\nEnter a site: luxembourg\nEnter a cluster: clervaux\nJob submitted (1900199)\n</code></pre> <p>Then, we can monitor our job with the <code>oarstat</code> and <code>kaconsole3</code> commands. At some point, we should see a new file appear in the <code>stress</code> folder located in our home directory.</p> <p>This python script has been kept simple for teaching purposes, but it could be far more complex. For example, kadeploy3 deployment can also be automatized through the Grid'5000 API. A user could also monitor the state of the reservation (<code>waiting</code>, <code>launching</code>, <code>running</code>...) to provide more information as the reservation is handled by OAR.</p> <p>Many libraries are already available to reduce the amount of code needed to script a Grid'5000 experiment. They are listed on the Experiment scripting tutorial page.</p>"},{"location":"g5k/getting-started-g5k/#to-go-further-links-and-tutorials","title":"To go further: links and tutorials","text":"<p>This tutorial introduced a very basic usage of Grid'5000 and its resources.</p>"},{"location":"g5k/getting-started-g5k/#links","title":"Links","text":"<ul> <li>Advanced resources reservation with OAR</li> <li>OAR Syntax Simplification</li> <li> <p>oarsub manual (online)</p> </li> <li> <p>Grid'5000 nodes bare-metal deployment with kadeploy3</p> </li> <li> <p>System environment creation guide</p> </li> <li> <p>Experiment scripting tutorial</p> </li> <li>REST API</li> <li> <p>API tutorial</p> </li> <li> <p>KaVLAN introduction</p> </li> <li>KaVLAN advanced usage</li> <li>Network reconfiguration tutorial</li> <li>Network Emulation</li> <li>IPv6</li> <li> <p>Reconfigurable firewall</p> </li> <li> <p>Monitoring with Kwollect</p> </li> <li>Energy consumption tutorial</li> </ul>"},{"location":"help/","title":"Support","text":"<p>ULHPC strives to support in a user friendly way your [super]computing needs. Note however that we are not here to make your PhD at your place ;)</p> <p> Service Now HPC Support Portal</p>"},{"location":"help/#faqtroubleshooting","title":"FAQ/Troubleshooting","text":"<ul> <li>Password reset</li> <li>Connection issues</li> <li>File Permissions<ul> <li>Access rights to project directory</li> <li>Quotas</li> </ul> </li> </ul>"},{"location":"help/#read-the-friendly-manual","title":"Read the Friendly Manual","text":"<p>We have always maintained an extensive documentation and tutorials available online, which aims at being the most up-to-date and comprehensive.</p> <p>So please, read the documentation first if you have a question of problem -- we probably provide detailed instructions here</p>"},{"location":"help/#help-desk","title":"Help Desk","text":"<p>The online help desk Service is the preferred method for contacting ULHPC.</p> <p>Tips</p> <p>Before reporting a problem or and issue, kindly remember that:</p> <ol> <li>Your issue is probably documented here on the ULHPC Technical documentation</li> <li>An event may be on-going:<ul> <li>Planned maintenance are announced at least 2 weeks in advance - -- see Maintenance and Downtime Policy</li> <li>The proper SSH banner is displayed during planned downtime</li> </ul> </li> <li>check the state of your nodes and jobs<ul> <li>Joining/monitoring running jobs</li> <li>Monitoring post-mortem Job status and efficiency</li> </ul> </li> </ol> <p> Service Now HPC Support Portal</p> <p>You can make code snippets, shell outputs, etc in your ticket much more readable by inserting a line with: <pre><code>[code]&lt;pre&gt;\n</code></pre> before the snippet, and another line with: <pre><code>&lt;/pre&gt;[/code]\n</code></pre> after it. For a full list of formatting options, see this ServiceNow article.</p> <p>Be as precise and complete as possible</p> <p>ULHPC team handle thousands of support requests per year. In order to ensure efficient timely resolution of issues, ensure that:</p> <ol> <li>you select the appropriate category (left menu)</li> <li>you include as much of the following as possible when making a request:<ul> <li>Who?  - Name and user id (login), eventually project name</li> <li>When? - When did the problem occur?</li> <li>Where? - Which cluster ? Which node ? Which job ?<ul> <li>Really include Job IDs</li> <li>Location of relevant files<ul> <li>input/output, job launcher scripts, source code, executables etc.</li> </ul> </li> </ul> </li> <li>What? - What happened? What exactly were you doing or trying to do ?<ul> <li>include Error messages - kindly report system or software messages literally and exactly.</li> <li>output of <code>module list</code></li> <li>any steps you have tried</li> <li>Steps to reproduce</li> </ul> </li> <li>Any part of this technical documentation you checked before opening the ticket</li> </ul> </li> </ol> <p>Access to the online help system requires logging in with your Uni.lu username, password, and eventually one-time password. If you are an existing user unable to log in, you can send us an email.</p> <p>Availability and Response Time</p> <p>HPC support is provided on a volunteer basis by UL HPC staff and associated UL experts working at normal business hours. We offer no guarantee on response time except with paid support contracts.</p>"},{"location":"help/#email-support","title":"Email support","text":"<p>You can contact us by mail to the ULHPC Team Email (ONLY if you cannot login/access the HPC Support helpdesk portal : <code>hpc-team@uni.lu</code></p> <p>You may also ask the help of other ULHPC users using the HPC User community mailing list: (moderated): <code>hpc-users@uni.lu</code></p>"},{"location":"interconnect/ethernet/","title":"Ethernet Network","text":"<p>Having a single high-bandwidth and low-latency network as the local Fast IB interconnect network to support efficient HPC and Big data workloads would not provide the necessary flexibility brought by the Ethernet protocol. Especially applications that are not able to employ the native protocol foreseen for that network and thus forced to use an IP emulation layer will benefit from the flexibility of Ethernet-based networks.</p> <p>An additional, Ethernet-based network offers the robustness and resiliency needed for management tasks inside the system in such cases</p> <p>Outside the Fast IB interconnect network used inside the clusters, we maintain an Ethernet network organized as a 2-layer topology:</p> <ol> <li>one upper level (Gateway Layer) with routing, switching features, network isolation and filtering (ACL) rules and meant to interconnect only switches.<ul> <li>This layer is handled by a redundant set of site routers (ULHPC gateway routers).</li> <li>it allows to interface the University network for both internal (LAN) and external (WAN) communications</li> </ul> </li> <li>one bottom level (Switching Layer) composed by the [stacked] core switches as well as the TOR (Top-the-rack) switches, meant to interface the HPC servers and compute nodes.</li> </ol> <p>An overview of this topology is provided in the below figure.</p> <p></p> <p>ACM PEARC'22 article</p> <p>If you are interested to get more details on the implemented Ethernet network, you can refer to the following article published to the ACM PEARC'22 conference (Practice and Experience in Advanced Research Computing) in Boston, USA on July 13, 2022.</p> <p>ACM Reference Format | ORBilu entry | OpenAccess | slides  Sebastien Varrette, Hyacinthe Cartiaux, Teddy Valette, and Abatcha Olloh. 2022. Aggregating and Consolidating two High Performant Network Topologies: The ULHPC Experience. In Practice and Experience in Advanced Research Computing (PEARC '22). Association for Computing Machinery, New York, NY, USA, Article 61, 1\u20136. https://doi.org/10.1145/3491418.3535159</p>"},{"location":"interconnect/ib/","title":"Fast Local Interconnect Network","text":"<p>High Performance Computing (HPC) encompasses advanced computation over parallel processing, enabling faster execution of highly compute intensive tasks. The execution time of a given simulation depends upon many factors, such as the number of CPU/GPU cores and their utilisation factor and the interconnect performance, efficiency, and scalability. HPC interconnect technologies can be nowadays divided into three categories: Ethernet, InfiniBand, and vendor specific interconnects. While Ethernet is established as the dominant interconnect standard for mainstream commercial computing requirements, the underlying protocol has inherent limitations preventing low-latency deployments expected in real HPC environment. When in need of high-bandwidth and low-latency as required in efficient high performance computing systems, better options have emerged and are considered:</p> <ul> <li>InfiniBand technologies<sup>2</sup>. See Introduction to High-Speed InfiniBand Interconnect.</li> <li>Vendor specific interconnects, which currently correspond to the technology provided by three main HPC vendors: Cray/HPC Slingshot, Intel's EOL Omni-Path Architecture (OPA)  or, to a minor measure, Bull BXI.</li> </ul> <p>Within the ULHPC facility, the InfiniBand solution was preferred as the predominant interconnect technology in the HPC market, tested against the largest set of HPC workloads. In practice:</p> <ul> <li>Iris relies on a EDR Infiniband (IB) Fabric in a Fat-Tree Topology</li> <li>Aion relies on a HDR100 Infiniband (IB) Fabric in a Fat-Tree Topology</li> </ul> <p>ACM PEARC'22 article</p> <p>If you are interested to understand the architecture and the solutions designed upon Aion acquisition to expand and consolidate the previously existing IB networks beyond its seminal capacity limits (while keeping at best their Bisection bandwidth), you can refer to the following article published to the ACM PEARC'22 conference (Practice and Experience in Advanced Research Computing) in Boston, USA on July 13, 2022.</p> <p>ACM Reference Format | ORBilu entry | OpenAccess | slides  Sebastien Varrette, Hyacinthe Cartiaux, Teddy Valette, and Abatcha Olloh. 2022. Aggregating and Consolidating two High Performant Network Topologies: The ULHPC Experience. In Practice and Experience in Advanced Research Computing (PEARC '22). Association for Computing Machinery, New York, NY, USA, Article 61, 1\u20136. https://doi.org/10.1145/3491418.3535159</p>"},{"location":"interconnect/ib/#ulhpc-ib-topology","title":"ULHPC IB Topology","text":"<p>One of the most significant differentiators between HPC systems and lesser performing systems is, apart from the interconnect technology deployed, the supporting topology. There are several topologies commonly used in large-scale HPC deployments (Fat-Tree, 3D-Torus, Dragonfly+ etc.). Fat-tree remains the widely used topology in HPC clusters due to its versatility, high bisection bandwidth and well understood routing. For this reason, each production clusters of the ULHPC facility rely on Fat-Tree topology.</p> <p>To minimize the number of switches per nodes while keeping a good Bisection bandwidth and allowing to interconnect Iris and Aion IB networks, the following configuration has been implemented:</p> <p></p> <p>For more details:  Iris IB Interconnect  Aion IB Interconnect</p> <p>The tight integration of I/O and compute in the ULHPC supercomputer architecture gives a very robust, time critical production systems. The selected routing algorithms also provides a dedicated and fast path to the IO targets, avoiding congestion on the high-speed network and mitigating the risk of runtime \"jitter\" for time critical jobs.</p>"},{"location":"interconnect/ib/#ib-fabric-diagnostic-utilities","title":"IB Fabric Diagnostic Utilities","text":"<p>An InfiniBand fabric is composed of switches and channel adapter (HCA/Connect-X cards) devices. To identify devices in a fabric (or even in one switch system), each device is given a GUID (a MAC address equivalent). Since a GUID is a non-user-friendly string of characters, we alias it to a meaningful, user-given name. There are a few IB diagnostic tools (typically installed by the <code>infiniband-diags</code> package) using these names. The ULHPC team is using them to diagnose Infiniband Fabric Information<sup>1</sup> -- see also InfiniBand Guide by Atos/Bull (PDF)</p> Tools Description <code>ibnodes</code> Show Infiniband nodes in topology <code>ibhosts</code> Show InfiniBand host nodes in topology <code>ibswitches</code> Show InfiniBand switch nodes in topology <code>ibnetdiscover</code> Discover InfiniBand topology <code>ibdiag</code> Scans the fabric using directed route packets and extracts all the available information(connectivity, devices) <code>perfquery</code> find errors on a particular or number of HCA\u2019s and switch ports <code>sminfo</code> Get InfiniBand Subnet Manager Info"},{"location":"interconnect/ib/#mellanox-equipment-fw-update","title":"Mellanox Equipment FW Update","text":"<p>An InfiniBand fabric is composed of switches and channel adapter (HCA/Connect-X cards) devices. Both should be kept up-to-date to mitigate potential security issues.</p>"},{"location":"interconnect/ib/#mellanox-connectx-hca-cards","title":"Mellanox ConnectX HCA cards","text":"<p>The Mellanox HCA firmware updater tool: <code>mlxup</code>, can be downloaded from mellanox.com. A Typical workflow applied within the ULHPC team to update the firmware of the Connect-X cards :</p> <ol> <li> <p>Query specific device or all devices (if no device were supplied)</p> <pre><code>mlxup --query\n</code></pre> </li> <li> <p>Go to https://support.mellanox.com/s/downloads-center then click on <code>Adapter &gt; ConnectX-&lt;N&gt; &gt; All downloads</code> (select any OS, it will redirect you to the same page)</p> <ul> <li>Click on \"Firmware\" tab and enter the PSID number obtained from <code>mlxup --query</code></li> <li> <p>Download the latest firmware version</p> <p>wget http://content.mellanox.com/firmware/fw-ConnectX[...].bin.zip</p> </li> </ul> </li> <li> <p>Unzip the downloaded file: <code>unzip [...]</code></p> </li> <li> <p>Burn device with latest firmware</p> <pre><code>mlxup -d &lt;PCI-device-name&gt; -i &lt;unzipped-image-file&gt;.bin\n</code></pre> </li> <li> <p>Reboot</p> </li> </ol>"},{"location":"interconnect/ib/#mellanox-ib-switches","title":"Mellanox IB Switches","text":"<ul> <li>Reference documentation<ul> <li>You need to download from Mellanox Download Center<ul> <li>BEWARE of the processor architecture (X86 vs. PPC) when selecting the images</li> </ul> </li> <li>select the switch model and download the proposed images -- Pay attention to the download path</li> </ul> </li> </ul> <ol> <li> <p>Most require priviledged (root) right and thus are not available for ULHPC end users.\u00a0\u21a9</p> </li> <li> <p>Originated in 1999 to specifically address workload requirements that were not adequately addressed by Ethernet and designed for scalability, using a switched fabric network topology together with Remote Direct Memory Access (RDMA) to reduce CPU overhead. Although InfiniBand is backed by a standards organisation (InfiniBand Trade Association with formal and open multi-vendor processes, the InfiniBand market is currently dominated by a single significant vendor Mellanox recently acquired by NVidia, which also dominates the non-Ethernet market segment across HPC deployments.\u00a0\u21a9</p> </li> </ol>"},{"location":"jobs/affinity_and_pinning/","title":"Affinity and pinning of processes and threads","text":"<p>HPC systems consist of multiple components that provide access to various resources. A process or thread running in some core of the system rarely has the same quality of access to all the resources. Take for instance a CPU node of Iris.</p> <p>Topology of an Iris CPU node</p> <p> </p> <p>An Iris CPU node; the node contains 2 CPU sockets, each socket contains a single NUMA<sup>1</sup> node, each NUMA node consists of a group of 14 cores sharing a single physical L3 cache, and each core provides a single PU<sup>2</sup>.</p> <p>A typical HPC cluster node is composed of computing cores that are integrated in an recursive manner in more complex structures to implement the functionality of the node. In Iris CPU nodes for instance,</p> <ul> <li>compute nodes contain 2 sockets,</li> <li>each socket contains a single NUMA<sup>1</sup>,</li> <li>each NUMA node consists of a group of 14 cores sharing a single physical L3 cache, and</li> <li>each core provides a single PU (processor unit)<sup>2</sup><sup>3</sup>.</li> </ul> <p>The CPU is thus organized in a number of levels with actual threads running on processor units, and processes consist of one or more threads. Communication between threads and thus processes, involves sending information from one thread to another. Depending on how many levels of the node architecture hierarchy the message has to pass the slower the communication will be. Also, threads and thus processes that are close in the architectural hierarchy tend to share memory. If you want more memory per thread you may have to spread your threads across processor units that are distant with respect to communication.</p> <p>Typically the libraries used in HPC applications provide a high degree of control of process and thread placement. In order to extract the best performance from the HPC system you often need to specify the placement of threads and processes.</p> TL; DR <p>For regular CPU nodes these configurations work well for the majority of applications.</p> Iris CPU nodesAion nodes <p>The recommended setting for optimal performance for most applications when using full nodes of Iris is to add the options:</p> <ul> <li><code>--ntasks-per-socket=1</code>,</li> <li><code>--cpus-per-task=14</code>, and</li> <li><code>--distribution=block:block</code></li> </ul> <p>in every call of <code>srun</code>. This is an example submission script.</p> <pre><code>#!/bin/bash --login\n#SBATCH --job-name=stress_test\n#SBATCH --partition=batch\n#SBATCH --qos=normal\n#SBATCH --nodes=4\n#SBATCH --time=00:15:00\n#SBATCH --output=%x-%j.out\n#SBATCH --error=%x-%j.err\n#SBATCH --exclusive\n\ndeclare stress_test_duration=160\n\nsrun --ntasks-per-socket=1 --cpus-per-task=14 --distribution=block:block stress-ng --cpu 14 --timeout \"${stress_test_duration}\"\n</code></pre> <p>The recommended setting for optimal performance for most applications when using full nodes of Aion is to add the options:</p> <ul> <li><code>--ntasks-per-socket=1</code>,</li> <li><code>--cpus-per-task=16</code>, and</li> <li><code>--distribution=block:block</code></li> </ul> <p>in every call of <code>srun</code>. This is an example submission script.</p> <pre><code>#!/bin/bash --login\n#SBATCH --job-name=stress_test\n#SBATCH --partition=batch\n#SBATCH --qos=normal\n#SBATCH --nodes=4\n#SBATCH --time=00:15:00\n#SBATCH --output=%x-%j.out\n#SBATCH --error=%x-%j.err\n#SBATCH --exclusive\n\ndeclare stress_test_duration=160\n\nsrun --ntasks-per-socket=1 --cpus-per-task=16 --distribution=block:block stress-ng --cpu 16 --timeout \"${stress_test_duration}\"\n</code></pre>"},{"location":"jobs/affinity_and_pinning/#process-placement","title":"Process placement","text":"<p>A call to <code>sbatch</code> or <code>salloc</code> in clusters with the Slurm scheduler allocates the resources for a job. To pin processes of your job to specific cores within each node you need to set binding option in each step of your job with <code>srun</code> or in raw calls to <code>mpirun</code>.</p>"},{"location":"jobs/affinity_and_pinning/#binding-processes","title":"Binding processes","text":"<p>Process (tasks) are bound to cores with the <code>--cpu-bind=[verbose],&lt;bind type&gt;</code> option flag of <code>srun</code>. The bind option assigns a process to fixed group of processor units for the duration of a job step. Some common options for <code>&lt;bind type&gt;</code> are</p> <ul> <li><code>threads</code>: binds processes to threads,</li> <li><code>cores</code>: binds processes to cores,</li> <li><code>sockets</code>: binds processes to sockets,</li> <li><code>map_cpu:&lt;list&gt;</code>: binds processes to processor unit ranks given in a comma separated <code>&lt;list&gt;</code>, and</li> <li><code>mask_cpu:&lt;list&gt;</code>: binds processes to groups of processing units defined by masks upon ranks given in a comma separated <code>&lt;list&gt;</code>.</li> </ul> <p>You can optionally add the <code>verbose</code> option in debugging jobs to print process bindings before the job step runs.</p> <p>Like with other resources, Slurm is using the control group feature of Linux to allocate processor units to processes. The base option that controls the allocation of cores is the <code>mask_cpu</code>. The other allocation options are converted to mask according to groupings of cores with similar access to resources by the hardware locality tool.</p>"},{"location":"jobs/affinity_and_pinning/#mask-binding","title":"Mask binding","text":"<p>The basic binding option is the mask option, <code>mask_cpu:&lt;list&gt;</code>, all other options are converted to masks according to the site configuration of Slurm.</p> <ul> <li>The <code>&lt;mask&gt;</code> is a bit field over the processor unit ranks (as reported by hardware locality) that marks with 1 the processor units that are available to a process running within a job step.</li> <li>The <code>&lt;list&gt; == &lt;mask&gt;[,&lt;mask&gt;]*</code> option of the binding directive is a comma separated list of masks, each consecutive process of the <code>srun</code> command uses the next available mask.</li> <li>The list is reused in a round-robin manner if there are more processes than masks.</li> <li>Processes are allowed to move freely within the allocated processor units.</li> </ul> Hexadecimal notation for masks <p>Masks are written in hexadecimal notation. For instance, assume an allocation on some machine with 2 sockets, 16 cores per socket, and one hardware thread per core. Such an allocation contains 32 processor units (cores). Then, the mask <pre><code>mask_cpu:0xff0000,0xff000000\n</code></pre> maps 4 processes to the following bit fields</p> Process number Processor unit availability <code>0</code> <code>0000 0000 1111 1111 0000 0000 0000 0000</code> <code>1</code> <code>1111 1111 0000 0000 0000 0000 0000 0000</code> <code>2</code> <code>0000 0000 1111 1111 0000 0000 0000 0000</code> <code>3</code> <code>1111 1111 0000 0000 0000 0000 0000 0000</code> <p>that in turn constrain</p> <ul> <li>task 0 and 2 to run on processor units (hardware threads) 16 to 23 of the node, and</li> <li>task 1 and 3 to run on processor units (hardware threads) 24 to 31 of the node.</li> </ul> <p>For convenience, you can add leading zeros to a mask so that all masks are of the same length, so, <code>0x00ff0000 = 0xff0000</code>.</p> <p>As an example consider an I/O throughput test for Aion local storage. In Aion there are 8 NUMA nodes per compute node, 4 CCXs per NUMA node, and 4 cores sharing a single L3 cache per CCX. Every digit in the hexadecimal mask controls the availability of a single CCX. To launch an I/O stress test in 4 parallel processes, with</p> <ul> <li>processes 0 and 2 pinned on CCX 0 of virtual socket 0, and</li> <li>processes 1 and 3 pinned on CCX 0 of virtual socket 1</li> </ul> <p>use the following command.</p> <pre><code>salloc --nodes=1 --exclusive --partition=batch --qos=normal --time=4:00:00\nsrun --nodes=1 --ntasks-per-node=4 --cpu-bind=verbose,mask_cpu:0xf,0xf0000 stress-ng --hdd 1 --timeout 240\n</code></pre> Context switching within binding regions <p>The HDD stress test processes are I/O heavy and are often interrupted by thread and process context switching. You can login to the running job from another terminal using the <code>sjoin</code> alias available in UL HPC systems, and with <code>htop</code> you may see the load moving among the allocated cores.</p>"},{"location":"jobs/affinity_and_pinning/#map-binding","title":"Map binding","text":"<p>The map binding binds processes to specific processor units (PUs). The <code>&lt;list&gt;==&lt;PU rank&gt;[,&lt;PU rank&gt;]*</code> option of the binding directive is a list of processing unit ranks, as reported by hardware locality.</p> <ul> <li>Processes launched within a job step use the processor unit with corresponding rank.</li> <li>Ranks may appear more that once if processing units are shared between processes.</li> <li>The list is reused if the available ranks are exhausted.</li> </ul> <p>Limitations of map binding</p> <p>Note that with the map binding each process is assigned a single processor unit. If your process needs more that one processor unit, as is the case with multithreaded processes, uss another binging option like mask.</p> <p>As an example consider an I/O throughput test for Aion local storage. In Aion there are 8 NUMA nodes per compute node, with 16 cores per NUMA node. Every digit in the map maps a process to the processor unit (PU) with the corresponding rank. To launch an I/O stress test in 4 parallel processes, with</p> <ul> <li>processes 0 and 1 pinned on cores 0 and 2 of virtual socket 0, and</li> <li>processes 2 and 3 pinned on cores 16 and 18 of virtual socket 1</li> </ul> <p>use the following command.</p> <pre><code>salloc --nodes=1 --exclusive --partition=batch --qos=normal --time=4:00:00\nsrun --nodes=1 --ntasks-per-node=4 --cpu-bind=verbose,map_cpu:0,2,16,18 stress-ng --hdd 1 --timeout 240\n</code></pre>"},{"location":"jobs/affinity_and_pinning/#automatically-generated-masks-binding","title":"Automatically generated masks binding","text":"<p>The Slurm scheduler provides options to generate bindings automatically. Many systems as MUMPS rely on simple allocations of one MPI process per NUMA node, and OpenMP thread parallelism within NUMA nodes. Automatically generated masks can describe such simple binding efficiently and the resulting description is most often portable between systems.</p> <p>When using automatic binging users may want to inspect the resulting binding mask. The binding is implemented using control groups, so the cores allocated for the process can be inspected with</p> <ul> <li>the <code>vorbose</code> option of binding,</li> <li>the <code>taskset</code> command, or</li> <li>directly reading the <code>Cpus_allowed_list</code> in <code>/proc/self/status</code>.</li> </ul> <p>As an example consider binding 2 processes per node for 2 nodes with the various available options. The <code>taskset</code> command is used to report the core bindings, and the verbose option is enabled to print the same information in a format closer to the internal system representation. Start by creating an allocation with exclusive access to 2 nodes:</p> <pre><code>salloc --exclusive --nodes=2 --ntasks-per-node=2 --time=1:00:00\n</code></pre> ThreadsCoresSocketsCustom binding <p>Simultaneous multi-threading is disabled in our systems, so the <code>threads</code> option is not relevant. The core and processor unit (threads) object types of hardware locality coincide.</p> <p>In this example the processes are pinned to cores of the CPUs.</p> <ul> <li> <p>Allow processes to share sockets.   <pre><code>$ srun --nodes=2 --ntasks-per-node=2 --cpu-bind=verbose,cores bash -c 'echo -n \"task ${SLURM_PROCID} (node ${SLURM_NODEID}): \"; taskset --cpu-list --pid ${BASHPID}' | sort\ncpu-bind=MASK - aion-0014, task  0  0 [3431378]: mask 0x1 set\ncpu-bind=MASK - aion-0014, task  1  1 [3431379]: mask 0x2 set\ncpu-bind=MASK - aion-0220, task  2  0 [2549677]: mask 0x1 set\ncpu-bind=MASK - aion-0220, task  3  1 [2549678]: mask 0x2 set\ntask 0 (node 0): pid 3431378's current affinity list: 0\ntask 1 (node 0): pid 3431379's current affinity list: 1\ntask 2 (node 1): pid 2549677's current affinity list: 0\ntask 3 (node 1): pid 2549678's current affinity list: 1\n</code></pre></p> </li> <li> <p>Processes pinned with one process per socket.   <pre><code>$ srun --nodes=2 --ntasks-per-node=2 --ntasks-per-socket=1 --cpu-bind=verbose,cores bash -c 'echo -n \"task ${SLURM_PROCID} (node ${SLURM_NODEID}): \"; taskset --cpu-list --pid ${BASHPID}' | sort\ncpu-bind=MASK - aion-0014, task  0  0 [3431621]: mask 0x1 set\ncpu-bind=MASK - aion-0014, task  1  1 [3431622]: mask 0x10000 set\ncpu-bind=MASK - aion-0220, task  2  0 [2549899]: mask 0x1 set\ncpu-bind=MASK - aion-0220, task  3  1 [2549900]: mask 0x10000 set\ntask 0 (node 0): pid 3431621's current affinity list: 0\ntask 1 (node 0): pid 3431622's current affinity list: 16\ntask 2 (node 1): pid 2549899's current affinity list: 0\ntask 3 (node 1): pid 2549900's current affinity list: 16\n</code></pre></p> </li> </ul> <p>In this example the processes are pinned to sockets of the processesing nodes.</p> <ul> <li> <p>Allow processes to share sockets (one processor unit per process by default).   <pre><code>$ srun --nodes=2 --ntasks-per-node=2 --cpu-bind=verbose,sockets bash -c 'echo -n \"task ${SLURM_PROCID} (node ${SLURM_NODEID}): \"; taskset --cpu-list --pid ${BASHPID}' | sort\ncpu-bind=MASK - aion-0014, task  0  0 [3431515]: mask 0xffff set\ncpu-bind=MASK - aion-0014, task  1  1 [3431516]: mask 0xffff set\ncpu-bind=MASK - aion-0220, task  2  0 [2549712]: mask 0xffff set\ncpu-bind=MASK - aion-0220, task  3  1 [2549713]: mask 0xffff set\ntask 0 (node 0): pid 3431515's current affinity list: 0-15\ntask 1 (node 0): pid 3431516's current affinity list: 0-15\ntask 2 (node 1): pid 2549712's current affinity list: 0-15\ntask 3 (node 1): pid 2549713's current affinity list: 0-15\n</code></pre></p> </li> <li> <p>Processes pinned with one process per socket.   <pre><code>$ srun --nodes=2 --ntasks-per-node=2 --ntasks-per-socket=1 --cpu-bind=verbose,sockets bash -c 'echo -n \"task ${SLURM_PROCID} (node ${SLURM_NODEID}): \"; taskset --cpu-list --pid ${BASHPID}' | sort\ncpu-bind=MASK - aion-0014, task  0  0 [3431908]: mask 0xffff set\ncpu-bind=MASK - aion-0014, task  1  1 [3431909]: mask 0xffff0000 set\ncpu-bind=MASK - aion-0220, task  2  0 [2550128]: mask 0xffff set\ncpu-bind=MASK - aion-0220, task  3  1 [2550129]: mask 0xffff0000 set\ntask 0 (node 0): pid 3431908's current affinity list: 0-15\ntask 1 (node 0): pid 3431909's current affinity list: 16-31\ntask 2 (node 1): pid 2550128's current affinity list: 0-15\ntask 3 (node 1): pid 2550129's current affinity list: 16-31\n</code></pre></p> </li> </ul> <p>With custom bindings we specify exactly where processes are bound.</p> <ul> <li> <p>Pin (single threaded) processes to sockets.   <pre><code>$ srun --cpu-bind=verbose,mask_cpu:0xffff,0xffff0000 bash -c 'echo -n \"task ${SLURM_PROCID} (node ${SLURM_NODEID})\"; taskset --cpu-list --pid ${BASHPID}' | sort\ncpu-bind=MASK - aion-0001, task  0  0 [3535103]: mask 0xffff set\ncpu-bind=MASK - aion-0001, task  1  1 [3535104]: mask 0xffff0000 set\ncpu-bind=MASK - aion-0339, task  2  0 [3595625]: mask 0xffff set\ncpu-bind=MASK - aion-0339, task  3  1 [3595626]: mask 0xffff0000 set\ntask 0 (node 0)pid 3535103's current affinity list: 0-15\ntask 1 (node 0)pid 3535104's current affinity list: 16-31\ntask 2 (node 1)pid 3595625's current affinity list: 0-15\ntask 3 (node 1)pid 3595626's current affinity list: 16-31\n</code></pre></p> </li> <li> <p>Pin processes to cores.   <pre><code>$ srun --cpu-bind=verbose,mask_cpu:0x1,0x10000 bash -c 'echo -n \"task ${SLURM_PROCID} (node ${SLURM_NODEID})\"; taskset --cpu-list --pid ${BASHPID}' | sort\ncpu-bind=MASK - aion-0014, task  0  0 [3435289]: mask 0x1 set\ncpu-bind=MASK - aion-0014, task  1  1 [3435290]: mask 0x10000 set\ncpu-bind=MASK - aion-0220, task  2  0 [2553408]: mask 0x1 set\ncpu-bind=MASK - aion-0220, task  3  1 [2553409]: mask 0x10000 set\ntask 0 (node 0)pid 3435289's current affinity list: 0\ntask 1 (node 0)pid 3435290's current affinity list: 16\ntask 2 (node 1)pid 2553408's current affinity list: 0\ntask 3 (node 1)pid 2553409's current affinity list: 16\n</code></pre></p> </li> </ul> <p>Printing the binding information however, demonstrates how processes are spread across nodes. By default, the tasks are distributed in blocks, where first the available slots in the first node of the allocation are filled, before moving to the next.</p> <p>An exhaustive list of reporting features for binding configurations can be found in the administrator's guide of Slurm.</p> Resources <ol> <li>Configuration options managing the allocation of CPU resources</li> <li>Configuration of the control group plugin of Slurm (<code>cgroup.conf</code>)</li> </ol>"},{"location":"jobs/affinity_and_pinning/#process-distribution","title":"Process distribution","text":"<p>The processes of a job step are distributed along the compute nodes of a job. The distribution of the process is controlled with the <code>--distribution</code> option flag of <code>srun</code>, and the distribution mechanism is based on hardware locality and control groups, like the binding mechanism. The basic options for the distribution option flag are <code>--distribution={*|block|cyclic|arbitrary}[:{*|block|cyclic|fcyclic}[:{*|block|cyclic|fcyclic}]]</code> and their meaning is the following.</p> <ul> <li>Level 0 (default <code>block</code>): Determines the distributing of processes across nodes (depth 0 of hardware locality objects).<ul> <li><code>*</code>: Use the default method</li> <li><code>block</code>: Distribute processes in a balanced manner across nodes so that if not enough node are available consecutive tasks share a node.</li> <li><code>cyclic</code>: Distribute processes across nodes in a round-robin manner, so that consecutive processes are placed on consecutive nodes of the allocation node list (<code>${SLURM_NODELIST}</code>).</li> <li><code>arbitrary</code>: Used in conjunction with <code>--nodelist=&lt;node&gt;[,&lt;nodes&gt;]*</code> to place processes in consecutive nodes in the provided node list in a round-robin manner.</li> </ul> </li> <li>Level 1 (default <code>block</code>): Determines the distributing of processes across sockets (depth 1 of hardware locality objects).<ul> <li><code>*</code>: Use the default method</li> <li><code>block</code>: Distribute processes for binding in a balanced manner across sockets of the node so that consecutive processes are places in consecutive cores.</li> <li><code>cyclic</code>: Distribute processes across sockets of a node in a round-robin manner so that consecutive processes are placed in consecutive sockets, and allocate cores in for each process in a consecutive manner within the socket.</li> <li><code>fcyclic</code>: Distribute processes across sockets of a node in a round-robin manner over processes so that a core of the 1<sup>st</sup> non-fully allocated process is placed on the first socket with an available core, then a core of the 2<sup>nd</sup> non-fully allocated process is placed on the first socket with an unallocated core, and so on.</li> </ul> </li> <li>Level 2 (default <code>block</code>): Determines the distributing of processes across cores in a socket (depth 7 of hardware locality objects).<ul> <li><code>*</code>: Use the default method</li> <li><code>block</code>: Distribute processes for binding in a balanced manner across processor units (hardware threads) of the core so that consecutive processes are places in consecutive processor units.</li> <li><code>cyclic</code>: Distribute processes in a round robin manner across cores so that consecutive processes are placed in consecutive cores, and allocate processor units for each processes in a consecutive manner within the core.</li> <li><code>fcyclic</code>: Distribute processes across processor units of a core in a round-robin manner over processes so that a processor unit of the 1<sup>st</sup> non-fully allocated process is places on the first core with an available processor unit, then a processor unit of the 2<sup>nd</sup> non-fully allocated process is place in the first core with an unallocated processor unit, and so on.</li> </ul> </li> </ul> <p>Relevant options for Aion and Iris</p> <p>In Aion and Iris the simultaneous multithreading (SMT) is disabled, and all cores contain a single processor unit (hardware thread). Thus, the level 2 options for the <code>--distribution</code> flag are redundant.</p> Difference between the <code>cyclic</code> and <code>fcyclic</code> options <p>Consider a system with</p> <ul> <li>2 sockets (<code>S[0-1]</code>)</li> <li>4 cores per socket (<code>C[0-3]</code>), and</li> <li>2 processor units per core (<code>P[0-1]</code>).</li> </ul> <p>If a job step with 16 processes with 1 processor unit per process are allocated as follows.</p> Distribution (<code>--distribution</code>) Allocation <code>*:cyclic:cyclic</code> <code>S0C0P0 S0C0P1 S0C1P0 S0C1P1 S0C2P0 S0C2P1 S0C3P0 S0C3P1 S1C0P0 S1C0P1 S1C1P0 S1C1P1 S1C2P0 S1C2P1 S1C3P0 S1C3P1</code> <code>*:cyclic:fcyclic</code> <code>S0C0P0 S0C1P0 S0C2P0 S0C3P0 S0C0P1 S0C1P1 S0C2P1 S0C3P1 S1C0P0 S1C1P0 S1C2P0 S1C3P0 S1C0P1 S1C1P1 S1C2P1 S1C3P1</code> <code>*:fcyclic:cyclic</code> <code>S0C0P0 S1C0P0 S0C0P1 S1C0P1 S0C1P0 S1C1P0 S0C1P1 S1C1P1 S0C2P0 S1C2P0 S0C2P1 S1C2P1 S3C1P0 S1C3P0 S0C3P1 S1C3P1</code> <code>*:fcyclic:fcyclic</code> <code>S0C0P0 S1C0P0 S0C1P0 S1C1P0 S0C0P1 S1C0P1 S0C1P1 S1C1P1 S0C2P0 S1C2P0 S0C3P0 S1C3P0 S0C2P1 S1C2P1 S0C3P1 S1C3P1</code> <p>An easy way to remember the order is that <code>f</code> prefix flips the order of significance between adjacent objects of the system starting from right to left on the original significance order. The relation is depicted in the following table.</p> Distribution (<code>--distribution</code>) Order of object significance <code>*:cyclic:cyclic</code> <code>SCP</code> <code>*:cyclic:fcyclic</code> <code>SPC</code> <code>*:fcyclic:cyclic</code> <code>CPS</code> <code>*:fcyclic:fcyclic</code> <code>PCS</code>"},{"location":"jobs/affinity_and_pinning/#automatic-distributions","title":"Automatic distributions","text":"<p>Typical distributions of processes can be achieved using the typical automatic options of the distribution flag. Consider for instance an allocation of 2 nodes in Aion.</p> <pre><code>salloc --exclusive --nodes=2\n</code></pre> <p>Then, the processes of a job step with 8 processes can be distributed using the automatic options for nodes (level 0) in 2 different manners.</p> blockcyclic <pre><code>$ srun --ntasks=8 --distribution=block bash -c 'echo -n \"task $SLURM_PROCID (node $SLURM_NODEID): \"; taskset --cpu-list --pid ${BASHPID}' | sort --key=2 --numeric-sort\ntask 0 (node 0): pid 3584429's current affinity list: 0\ntask 1 (node 0): pid 3584430's current affinity list: 1\ntask 2 (node 0): pid 3584431's current affinity list: 2\ntask 3 (node 0): pid 3584432's current affinity list: 3\ntask 4 (node 1): pid 3628427's current affinity list: 0\ntask 5 (node 1): pid 3628428's current affinity list: 1\ntask 6 (node 1): pid 3628429's current affinity list: 2\ntask 7 (node 1): pid 3628430's current affinity list: 3\n</code></pre> <pre><code>$ srun --ntasks=8 --distribution=cyclic bash -c 'echo -n \"task $SLURM_PROCID (node $SLURM_NODEID): \"; taskset --cpu-list --pid ${BASHPID}' | sort --key=2 --numeric-sort\ntask 0 (node 0): pid 3584499's current affinity list: 0\ntask 1 (node 1): pid 3628579's current affinity list: 0\ntask 2 (node 0): pid 3584500's current affinity list: 1\ntask 3 (node 1): pid 3628580's current affinity list: 1\ntask 4 (node 0): pid 3584501's current affinity list: 2\ntask 5 (node 1): pid 3628581's current affinity list: 2\ntask 6 (node 0): pid 3584502's current affinity list: 3\ntask 7 (node 1): pid 3628582's current affinity list: 3\n</code></pre> <p>Consider another example where a job step with 32 processes of 4 threads each needs to be distributed over 2 nodes. This is a typical situation for Aion where every 4 cores share a physically coherent cache. Many applications are optimized to take advantage of L3 cache for shared memory parallelism and use message passing parallelism for communication across physically coherent L3 cache regions. A combination of the <code>--cpus-per-task</code> flag that generates binding automatically, and the automatic options for the distribution flag is usually sufficient for most such jobs.</p> block:blockblock:cycliccyclic:blockcyclic:cyclic <pre><code>$ srun --ntasks=32 --cpus-per-task=4 --distribution=block:block bash -c 'echo -n \"task $SLURM_PROCID (node $SLURM_NODEID): \"; taskset --cpu-list --pid ${BASHPID}' | sort --key=2 --numeric-sort\ntask 0 (node 0): pid 3585148's current affinity list: 0-3\ntask 1 (node 0): pid 3585149's current affinity list: 4-7\ntask 2 (node 0): pid 3585150's current affinity list: 8-11\ntask 3 (node 0): pid 3585151's current affinity list: 12-15\ntask 4 (node 0): pid 3585152's current affinity list: 16-19\ntask 5 (node 0): pid 3585153's current affinity list: 20-23\ntask 6 (node 0): pid 3585154's current affinity list: 24-27\ntask 7 (node 0): pid 3585155's current affinity list: 28-31\ntask 8 (node 0): pid 3585156's current affinity list: 32-35\ntask 9 (node 0): pid 3585157's current affinity list: 36-39\ntask 10 (node 0): pid 3585158's current affinity list: 40-43\ntask 11 (node 0): pid 3585159's current affinity list: 44-47\ntask 12 (node 0): pid 3585160's current affinity list: 48-51\ntask 13 (node 0): pid 3585161's current affinity list: 52-55\ntask 14 (node 0): pid 3585162's current affinity list: 56-59\ntask 15 (node 0): pid 3585163's current affinity list: 60-63\ntask 16 (node 1): pid 3629024's current affinity list: 0-3\ntask 17 (node 1): pid 3629025's current affinity list: 4-7\ntask 18 (node 1): pid 3629026's current affinity list: 8-11\ntask 19 (node 1): pid 3629027's current affinity list: 12-15\ntask 20 (node 1): pid 3629028's current affinity list: 16-19\ntask 21 (node 1): pid 3629029's current affinity list: 20-23\ntask 22 (node 1): pid 3629030's current affinity list: 24-27\ntask 23 (node 1): pid 3629031's current affinity list: 28-31\ntask 24 (node 1): pid 3629032's current affinity list: 32-35\ntask 25 (node 1): pid 3629033's current affinity list: 36-39\ntask 26 (node 1): pid 3629034's current affinity list: 40-43\ntask 27 (node 1): pid 3629035's current affinity list: 44-47\ntask 28 (node 1): pid 3629036's current affinity list: 48-51\ntask 29 (node 1): pid 3629037's current affinity list: 52-55\ntask 30 (node 1): pid 3629038's current affinity list: 56-59\ntask 31 (node 1): pid 3629039's current affinity list: 60-63\n</code></pre> <pre><code>$ srun --ntasks=32 --cpus-per-task=4 --distribution=block:cyclic bash -c 'echo -n \"task $SLURM_PROCID (node $SLURM_NODEID): \"; taskset --cpu-list --pid ${BASHPID}' | sort --key=2 --numeric-sort\ntask 0 (node 0): pid 3585968's current affinity list: 0-3\ntask 1 (node 0): pid 3585969's current affinity list: 16-19\ntask 2 (node 0): pid 3585970's current affinity list: 32-35\ntask 3 (node 0): pid 3585971's current affinity list: 48-51\ntask 4 (node 0): pid 3585972's current affinity list: 64-67\ntask 5 (node 0): pid 3585973's current affinity list: 80-83\ntask 6 (node 0): pid 3585974's current affinity list: 96-99\ntask 7 (node 0): pid 3585975's current affinity list: 112-115\ntask 8 (node 0): pid 3585976's current affinity list: 4-7\ntask 9 (node 0): pid 3585977's current affinity list: 20-23\ntask 10 (node 0): pid 3585978's current affinity list: 36-39\ntask 11 (node 0): pid 3585979's current affinity list: 52-55\ntask 12 (node 0): pid 3585980's current affinity list: 68-71\ntask 13 (node 0): pid 3585981's current affinity list: 84-87\ntask 14 (node 0): pid 3585982's current affinity list: 100-103\ntask 15 (node 0): pid 3585983's current affinity list: 116-119\ntask 16 (node 1): pid 3629593's current affinity list: 0-3\ntask 17 (node 1): pid 3629594's current affinity list: 16-19\ntask 18 (node 1): pid 3629595's current affinity list: 32-35\ntask 19 (node 1): pid 3629596's current affinity list: 48-51\ntask 20 (node 1): pid 3629597's current affinity list: 64-67\ntask 21 (node 1): pid 3629598's current affinity list: 80-83\ntask 22 (node 1): pid 3629599's current affinity list: 96-99\ntask 23 (node 1): pid 3629600's current affinity list: 112-115\ntask 24 (node 1): pid 3629601's current affinity list: 4-7\ntask 25 (node 1): pid 3629602's current affinity list: 20-23\ntask 26 (node 1): pid 3629603's current affinity list: 36-39\ntask 27 (node 1): pid 3629604's current affinity list: 52-55\ntask 28 (node 1): pid 3629605's current affinity list: 68-71\ntask 29 (node 1): pid 3629606's current affinity list: 84-87\ntask 30 (node 1): pid 3629607's current affinity list: 100-103\ntask 31 (node 1): pid 3629608's current affinity list: 116-119\n</code></pre> <pre><code>$ srun --ntasks=32 --cpus-per-task=4 --distribution=cyclic:block bash -c 'echo -n \"task $SLURM_PROCID (node $SLURM_NODEID): \"; taskset --cpu-list --pid ${BASHPID}' | sort --key=2 --numeric-sort\ntask 0 (node 0): pid 3586174's current affinity list: 0-3\ntask 1 (node 1): pid 3629673's current affinity list: 0-3\ntask 2 (node 0): pid 3586175's current affinity list: 4-7\ntask 3 (node 1): pid 3629674's current affinity list: 4-7\ntask 4 (node 0): pid 3586176's current affinity list: 8-11\ntask 5 (node 1): pid 3629675's current affinity list: 8-11\ntask 6 (node 0): pid 3586177's current affinity list: 12-15\ntask 7 (node 1): pid 3629676's current affinity list: 12-15\ntask 8 (node 0): pid 3586178's current affinity list: 16-19\ntask 9 (node 1): pid 3629677's current affinity list: 16-19\ntask 10 (node 0): pid 3586179's current affinity list: 20-23\ntask 11 (node 1): pid 3629678's current affinity list: 20-23\ntask 12 (node 0): pid 3586180's current affinity list: 24-27\ntask 13 (node 1): pid 3629679's current affinity list: 24-27\ntask 14 (node 0): pid 3586181's current affinity list: 28-31\ntask 15 (node 1): pid 3629680's current affinity list: 28-31\ntask 16 (node 0): pid 3586182's current affinity list: 32-35\ntask 17 (node 1): pid 3629681's current affinity list: 32-35\ntask 18 (node 0): pid 3586183's current affinity list: 36-39\ntask 19 (node 1): pid 3629682's current affinity list: 36-39\ntask 20 (node 0): pid 3586184's current affinity list: 40-43\ntask 21 (node 1): pid 3629683's current affinity list: 40-43\ntask 22 (node 0): pid 3586185's current affinity list: 44-47\ntask 23 (node 1): pid 3629684's current affinity list: 44-47\ntask 24 (node 0): pid 3586186's current affinity list: 48-51\ntask 25 (node 1): pid 3629685's current affinity list: 48-51\ntask 26 (node 0): pid 3586187's current affinity list: 52-55\ntask 27 (node 1): pid 3629686's current affinity list: 52-55\ntask 28 (node 0): pid 3586188's current affinity list: 56-59\ntask 29 (node 1): pid 3629687's current affinity list: 56-59\ntask 30 (node 0): pid 3586189's current affinity list: 60-63\ntask 31 (node 1): pid 3629688's current affinity list: 60-63\n</code></pre> <pre><code>$ srun --ntasks=32 --cpus-per-task=4 --distribution=cyclic:cyclic bash -c 'echo -n \"task $SLURM_PROCID (node $SLURM_NODEID): \"; taskset --cpu-list --pid ${BASHPID}' | sort --key=2 --numeric-sort\ntask 0 (node 0): pid 3586295's current affinity list: 0-3\ntask 1 (node 1): pid 3629775's current affinity list: 0-3\ntask 2 (node 0): pid 3586296's current affinity list: 16-19\ntask 3 (node 1): pid 3629776's current affinity list: 16-19\ntask 4 (node 0): pid 3586297's current affinity list: 32-35\ntask 5 (node 1): pid 3629777's current affinity list: 32-35\ntask 6 (node 0): pid 3586298's current affinity list: 48-51\ntask 7 (node 1): pid 3629778's current affinity list: 48-51\ntask 8 (node 0): pid 3586299's current affinity list: 64-67\ntask 9 (node 1): pid 3629779's current affinity list: 64-67\ntask 10 (node 0): pid 3586300's current affinity list: 80-83\ntask 11 (node 1): pid 3629780's current affinity list: 80-83\ntask 12 (node 0): pid 3586301's current affinity list: 96-99\ntask 13 (node 1): pid 3629781's current affinity list: 96-99\ntask 14 (node 0): pid 3586302's current affinity list: 112-115\ntask 15 (node 1): pid 3629782's current affinity list: 112-115\ntask 16 (node 0): pid 3586303's current affinity list: 4-7\ntask 17 (node 1): pid 3629783's current affinity list: 4-7\ntask 18 (node 0): pid 3586304's current affinity list: 20-23\ntask 19 (node 1): pid 3629784's current affinity list: 20-23\ntask 20 (node 0): pid 3586305's current affinity list: 36-39\ntask 21 (node 1): pid 3629785's current affinity list: 36-39\ntask 22 (node 0): pid 3586306's current affinity list: 52-55\ntask 23 (node 1): pid 3629786's current affinity list: 52-55\ntask 24 (node 0): pid 3586307's current affinity list: 68-71\ntask 25 (node 1): pid 3629787's current affinity list: 68-71\ntask 26 (node 0): pid 3586308's current affinity list: 84-87\ntask 27 (node 1): pid 3629788's current affinity list: 84-87\ntask 28 (node 0): pid 3586309's current affinity list: 100-103\ntask 29 (node 1): pid 3629789's current affinity list: 100-103\ntask 30 (node 0): pid 3586310's current affinity list: 116-119\ntask 31 (node 1): pid 3629790's current affinity list: 116-119\n</code></pre> <p>Automatic options for the distribution flag are simple to define reducing the chance of typos, and are portable between systems. However, sometimes like in the case of system benchmarking, very accurate control of process placement is required.</p>"},{"location":"jobs/affinity_and_pinning/#manual-distribution","title":"Manual distribution","text":"<p>Very precise placement of processes is afforded using the node list (<code>--nodelist</code>) argument, the <code>arbitrary</code> distribution, and CPU masks. The automatic options themselves are converted into node lists and CPU masks in the background. This is demonstrated in the following example.</p> <ul> <li> <p>Start by allocating a job with 2 nodes:   <pre><code>salloc --exclusive --nodes=2\n</code></pre></p> </li> <li> <p>Then read the allocated nodes:   <pre><code>$ echo $SLURM_NODELIST\naion-[0001,0339]\n</code></pre></p> </li> <li> <p>Finally, use a combination of the <code>--nodelist</code> option to place processes into nodes and bind them to the desired group of cores.   <pre><code>$ srun --nodelist=aion-[0001,0339,0001,0339,0339,0339,0339,0339] --distribution=arbitrary --cpu-bind=verbose,mask_cpu:0xf,0xf0,0xf00,0xf000,0xf0000,0xf00000,0xf000000,0xf0000000 bash -c 'echo -n \"task $SLURM_PROCID (node $SLURM_NODEID): \"; taskset --cpu-list --pid ${BASHPID}' | sort\n\ncpu-bind=MASK - aion-0001, task  0  0 [3576513]: mask 0xf set\ncpu-bind=MASK - aion-0001, task  2  1 [3576514]: mask 0xf0 set\ncpu-bind=MASK - aion-0339, task  1  0 [3623221]: mask 0xf set\ncpu-bind=MASK - aion-0339, task  3  1 [3623222]: mask 0xf0 set\ncpu-bind=MASK - aion-0339, task  4  2 [3623223]: mask 0xf00 set\ncpu-bind=MASK - aion-0339, task  5  3 [3623224]: mask 0xf000 set\ncpu-bind=MASK - aion-0339, task  6  4 [3623225]: mask 0xf0000 set\ncpu-bind=MASK - aion-0339, task  7  5 [3623226]: mask 0xf00000 set\ntask 0 (node 0): pid 3576513's current affinity list: 0-3\ntask 1 (node 1): pid 3623221's current affinity list: 0-3\ntask 2 (node 0): pid 3576514's current affinity list: 4-7\ntask 3 (node 1): pid 3623222's current affinity list: 4-7\ntask 4 (node 1): pid 3623223's current affinity list: 8-11\ntask 5 (node 1): pid 3623224's current affinity list: 12-15\ntask 6 (node 1): pid 3623225's current affinity list: 16-19\n</code></pre></p> </li> </ul> <p>With the <code>arbitrary</code> option of the distribution flag, the launcher will launch one process in every entry of the node list (repeated entries allowed). If no node list (<code>--nodelist</code>) is provided, then the distribution method defaults to <code>block</code>.</p>"},{"location":"jobs/affinity_and_pinning/#resources","title":"Resources","text":"<ol> <li>Hardware Locality (hwloc)</li> <li>Distribution and binding options - LUMI</li> <li>Process and Thread Distribution and Binding - LUMI trainings</li> </ol> <ol> <li> <p>NUMA: Non-Uniform Memory Access\u00a0\u21a9\u21a9</p> </li> <li> <p>PU: Processor Unit\u00a0\u21a9\u21a9</p> </li> <li> <p>Simultaneous Multi Threading (SMT), also known as Hyper-Threading for Intel CPUs, is disabled in our cluster nodes because it impacts negatively numerical calculations that comprise the vast majority of our workloads.\u00a0\u21a9</p> </li> </ol>"},{"location":"jobs/arrays/","title":"Job arrays in HPC systems","text":"<p>Job arrays is a mechanism for submitting and managing collections of similar jobs. All jobs in an array have the same options in terms of scheduler resources, as they are submitted with the same <code>sbatch</code> options and directives.</p> <p>When to use job arrays</p> <p>A naive way to submit multiple jobs is to programmatically submit the jobs with a custom script. This however can quickly hit the maximum job limit (<code>MaxJobPU</code>), which is <code>100</code> jobs per user in the <code>normal</code> QoS. If your jobs share the same options, then consider using job arrays. Job arrays create job records for task progressively, so they will help you keep within <code>MaxJobPU</code> while reducing the load for the scheduler.</p> <p>When not to use job arrays</p> <p>Every job in a job array requires an allocation from the scheduler, which is an expensive operation. If you plan to submit many small jobs in an array that require the allocation of more than 10 jobs per minute, please use GNU parallel to batch multiple tasks in a single job allocation.</p>"},{"location":"jobs/arrays/#using-job-arrays","title":"Using job arrays","text":"<p>The job array feature of Slurm groups similar jobs and provides functionality for managing the group of jobs. A fundamental concept for managing and organizing the is the task index value (task ID). Every task in the job array is a Slurm job and is assigned a task ID which is used to refer to the Slurm job in the context of the job array. The following environment variables are set in job array tasks on top of the usual <code>SLURM_JOB_ID</code> that is available on all jobs.</p> <ul> <li><code>SLURM_ARRAY_JOB_ID</code> is the Slurm job ID of the whole job array.</li> <li><code>SLURM_ARRAY_TASK_ID</code> is the task ID of the current Slurm job.</li> <li><code>SLURM_ARRAY_TASK_COUNT</code> is the total number of tasks in the job array.</li> <li><code>SLURM_ARRAY_TASK_MAX</code> is the largest task ID in the job array</li> <li><code>SLURM_ARRAY_TASK_MIN</code> is the smallest task ID in the job array.</li> </ul> Inner workings of jobs arrays and the job ID of the whole array <p>When a job array is submitted to Slurm, only one job record is created. The <code>SLURM_JOB_ID</code> of this initial job will then be the <code>SLURM_ARRAY_JOB_ID</code> of the whole array. Additional jobs records are then created by the initial job. The <code>squeue</code> command shows that additional jobs appear in the queue as their records are created by the initial job, and the initial job ID string changes to reflect the progress of the job array execution. This gradual submission of jobs ensures that the user remains within the limits specified by the Slurm configuration. For instance in a job array with <code>400</code> jobs, up to 100 jobs will be launched in parallel in the <code>normal</code> QoS that has a limit (<code>MaxJobPU</code>) of 100 jobs.</p> <p>Typically the Slurm job with <code>SLURM_ARRAY_JOB_ID</code> will also execute the last task in the array before terminating, but this is implementation dependent and not part of the job array interface.</p> <p>The task ID takes values from <code>0</code> up to some maximum value determined by the <code>MaxArraySize</code> variable in the Slurm configuration. The maximum available task ID is <code>(MaxArraySize - 1)</code>, and limits the maximum possible size of a job array.</p> <p>Maximum size of job arrays in UL HPC systems</p> <p>The <code>MaxArraySize</code> in our site is set to the default value from Slurm at <code>1001</code>. Job arrays allow the submission of many jobs very quickly, and purely configured scripts can easily overload the scheduler.</p> <p>If you are affected by the <code>MaxArraySize</code> limit, please consider using GNU parallel.</p>"},{"location":"jobs/arrays/#submitting-a-job-array","title":"Submitting a job array","text":"<p>A job array is submitted with the <code>--array</code> (<code>-a</code> short form) option of <code>sbatch</code>. The option is available both in the command line and as script <code>#SBATCH</code> directive, as usual. The <code>--array</code> option takes as argument a list of tasks that will be run. The simples for of list is a simple range,</p> <pre><code>sbatch --array=0-31 job_array_script.sh\n</code></pre> <p>where the <code>--array</code> is used to control how many Slurm jobs are created. Inside the <code>job_array_script.sh</code> the <code>SLURM_ARRAY_TASK_ID</code> can be used to control to differentiate the operation of the script. The number of jobs that runs in parallel is controlled using the suffix <pre><code>sbatch --array=&lt;task list&gt;%&lt;number of parallel jobs&gt; job_script.sh\n</code></pre> where <code>&lt;number of parallel jobs&gt;</code> is the maximum number of jobs that will run in parallel.</p> Advances task list specifications <p>During debugging or testing it's often convenient to specify a subrange of tasks to execute. The task list supports a rich syntax. The types of entries in the task list are</p> <ul> <li>single task task IDs: <code>&lt;task ID&gt;</code>,</li> <li>ranges of task IDs: <code>&lt;task ID:begin&gt;-&lt;task ID:end&gt;</code> where <code>&lt;task ID:begin&gt; &lt;= &lt;task ID:end&gt;</code>, and</li> <li>stepped ranges of task IDs: <code>&lt;task ID:begin&gt;-&lt;task ID:end&gt;:&lt;step&gt;</code> where <code>&lt;task ID:begin&gt; &lt;= &lt;task ID:end&gt;</code> and <code>&lt;step&gt; &gt; 0</code>.</li> </ul> <p>Any comma separated list of entries is a task list and repeated entries are ignored.</p> <p>For instance,</p> <ul> <li><code>--array=1-4</code> is equivalent to <code>--array=1,2,3,4</code>,</li> <li><code>--array=1-7:2</code> is equivalent to <code>--array=1,3,5,7</code>,</li> <li><code>--array=1-7:2,0-6:2</code> is equivalent to <code>--array=1,3,5,7,0,2,4,6</code>, and</li> <li><code>--array=1-4,1-7:2</code> is equivalent to <code>--array=1,2,3,4,5,7</code>.</li> </ul> <p>A task list is valid if all task IDs in the list are in the range <code>0-(MaxArraySize-1)</code>.</p> <p>If you job specification has a syntax error or lists tasks with ID outside the range <code>0-(MaxArraySize-1)</code>, then the array job submission fails immediately with an error message.</p> <p>Job execution order</p> <p>The task ID simply provides a way to differentiate the job array tasks and their behavior, there is no guaranty in which order tasks will run. If you need your job array tasks to run in a particular order consider using job dependencies.</p>"},{"location":"jobs/arrays/#managing-tasks-and-arrays","title":"Managing tasks and arrays","text":"<p>A combination of the <code>${SLURM_ARRAY_TASK_ID}</code> and <code>${SLURM_ARRAY_JOB_ID}</code> can replace the <code>${SLURM_JOB_ID}</code> of job array tasks in Slurm commands such as <code>scontrol</code> and <code>squeue</code>.</p> <ul> <li>Use the <code>${SLURM_ARRAY_JOB_ID}_${SLURM_ARRAY_TASK_ID}</code> to refer to a task of the job array.</li> <li>Use the <code>${SLURM_ARRAY_JOB_ID}</code> to refer to all the tasks of the job array collectively.</li> </ul> <p>Each array job, is associated with a job ID string that contains information about the status of the array. The job ID string is formatted using the task list as follows. <pre><code>&lt;${SLURM_ARRAY_JOB_ID}&gt;_[&lt;task list&gt;]\n</code></pre></p> <p>As the execution of the job array with job <code>${SLURM_ARRAY_JOB_ID}</code> progresses, the job ID string is updated to reflect the progress.</p> <p>The job array ID string</p> <p>Assume that a job <pre><code>sbatch --array=0-399%4 job_script.sh\n</code></pre> is submitted and gets assigned <code>SLURM_ARRAY_JOB_ID=625449</code>.</p> <ul> <li>The initial job ID string is: <code>9625449_[0-399%4]</code></li> <li>After tasks <code>0-23</code> are executed, the new ID string is: <code>9625449_[24-399%4]</code></li> </ul> <p>A few example with the most representative use for job ID strings cases follow.</p>"},{"location":"jobs/arrays/#canceling-job-arrays-and-job-array-tasks","title":"Canceling job arrays and job array tasks","text":"<p>With <code>scancel</code> some of the array tasks or all the array can be cancelled. Assume that array with <code>SLURM_ARRAY_JOB_ID=9624577</code> is running.</p> <ul> <li>To cancel the whole job array use:   <pre><code>scancel 9624577\n</code></pre></li> <li>To cancel the job array task with <code>SLURM_ARRAY_TASK_ID=197</code> in particular use:   <pre><code>scancel 9624577_197\n</code></pre></li> </ul> <p>Syntax shortcuts for job ID strings</p> <p>When addressing a single task ID, the square brackets in the job ID string can be dropped. For instance, <pre><code>9624577_[197]\n</code></pre> is equivalent to <pre><code>9624577_197\n</code></pre> in all cases where job ID strings appear.</p>"},{"location":"jobs/arrays/#viewing-array-job-status","title":"Viewing array job status","text":"<p>The <code>squeue</code> can access the job ID string for the whole array and task ID strings of individual tasks. Assume that array with <code>SLURM_ARRAY_JOB_ID=9624577</code> is running.</p> <ul> <li>To view the status of the whole job array use:   <pre><code>squeue --job=9624577\n</code></pre></li> <li>To view the status of a job array task with <code>SLURM_ARRAY_TASK_ID=197</code> in particular use:   <pre><code>squeue --job=9624577_197\n</code></pre></li> </ul> <p>Viewing the status of all steps of a submitted array</p> <p>When submitting a job array job records for the tasks of the array are created progressively. If you would like to view all submitted tasks, provide the <code>--array</code> (<code>-r</code> short format) option to <code>squeue</code>.</p> Formatting the output of <code>squeue</code> to extract array status information <p>The job ID string printed by <code>squeue</code> contains information about the status of the array job, but the ID string may be printed with insufficient number of digit to extract any useful information. Use the <code>--format</code> or <code>--Format</code> options of <code>squeue</code> to control how information is printed. The <code>--format</code> option supports greater flexibility in formatting whereas <code>--Format</code> supports access to all fields; the 2 interfaces cannot be used in parallel.</p> <p>The following option values print the ID string with sufficiently many digits:</p> <code>--format</code> style<code>--Format</code> style <pre><code>squeue --format='%.25i %.9P %.9q %.15j %.15u %.2t %.6D %.10M %.10L %12Q %R'\n</code></pre> <pre><code>squeue --Format='JobID:10,ArrayTaskID:20,Partition:10,QOS:10,Name:30,UserName:20,NumNodes:6,State:15,TimeUsed:6,TimeLeft:10,PriorityLong:10,ReasonList:30'\n</code></pre> <p>The format commands are compatible with the <code>sq</code> wrapper script for <code>squeue</code> that is available in UL HPC systems.</p> <p>You can also set environment variables to avoid having to specify the format string every time.</p> <ul> <li>Set <code>SQUEUE_FORMAT</code> for old interface <code>--format</code> type scripts.</li> <li>Set <code>SQUEUE_FORMAT2</code> for new interface <code>--Format</code> type scripts.</li> </ul> <p>This variables can be set in the <code>${HOME}/.bashrc</code> script to be loaded automatically in your environment.</p> <code>--format</code> style<code>--Format</code> style <pre><code>export SQUEUE_FORMAT='%.25i %.9P %.9q %.15j %.15u %.2t %.6D %.10M %.10L %12Q %R'\n</code></pre> <pre><code>export SQUEUE_FORMAT2='JobID:10,ArrayTaskID:20,Partition:10,QOS:10,Name:30,UserName:20,NumNodes:6,State:15,TimeUsed:6,TimeLeft:10,PriorityLong:10,ReasonList:30'`\n</code></pre>"},{"location":"jobs/arrays/#modifying-job-array-tasks","title":"Modifying job array tasks","text":"<p>Even though job array tasks are submitted with the exact same scheduler options, individual jobs can be modified at any point before completion with the <code>scotrol</code> command. For instance, you can increase the runtime of the task of a job array that has already been submitted.</p> <p>Consider submitting the following job.</p> <p><code>stress_test.sh</code></p> <pre><code>#!/bin/bash --login\n#SBATCH --job-name=array_script\n#SBATCH --array=0-400%4\n#SBATCH --partition=batch\n#SBATCH --qos=normal\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=16\n#SBATCH --time=00:10:00\n\ndeclare test_duration=12m\n\nsrun \\\n  stress-ng \\\n    --cpu ${SLURM_CPUS_PER_TASK} \\\n    --timeout \"${test_duration}\"\n</code></pre> <p>The tasks in <code>stress_test.sh</code> do not have sufficient time to finish. After submission the <code>TimeLimit</code> can be raised to 15min to allow tasks sufficient time to finish. Assume that <code>SLURM_ARRAY_JOB_ID=9625003</code>.</p> <ul> <li>Update all tasks with:   <pre><code>scontrol update jobid=9625003 TimeLimit=00:15:00\n</code></pre>   For complete tasks you may get a warning, like:   <pre><code>9625003_6-11: Job has already finished\n</code></pre></li> <li>Update individual tasks:   <pre><code>scontrol update jobid=9625003_4 TimeLimit=00:15:00\n</code></pre></li> </ul>"},{"location":"jobs/arrays/#job-array-scripts","title":"Job array scripts","text":"<p>Consider a job array script designed to stress test a set of network file systems mounted on <code>${FILE_SYSTEM_PATH_PREFIX}_0</code> to <code>${FILE_SYSTEM_PATH_PREFIX}_255</code>. The job array launch script is the following.</p> <p>io_test.sh</p> <pre><code>#!/bin/bash --login\n#SBATCH --job-name=array_script\n#SBATCH --array=0-255%16\n#SBATCH --partition=batch\n#SBATCH --qos=normal\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=16\n#SBATCH --time=00:30:00\n#SBATCH --output=%x-%A_%a.out\n#SBATCH --error=%x-%A_%a.err\n\ndeclare test_duration=20m\n\nsrun \\\n  stress-ng \\\n    --timeout \"${test_duration}\" \\\n    --iomix \"${SLURM_CPUS_PER_TASK}\" \\\n    --temp-path \"${FILE_SYSTEM_PATH_PREFIX}_${SLURM_ARRAY_TASK_ID}\" \\\n    --verify \\\n    --metrics\n</code></pre> <p>This job script a job array with <code>256</code> tasks, where up to <code>16</code> tasks will run in parallel. Job arrays provide two extra filename patterns that can be used to name output files (defined with the <code>--output</code> and <code>--error</code> options). This patterns are,</p> <ul> <li><code>%A</code> that contains the master job allocation number <code>SLURM_ARRAY_JOB_ID</code>, and</li> <li><code>%a</code> that contains the task index number <code>SLURM_ARRAY_TASK_ID</code>.</li> </ul>"},{"location":"jobs/arrays/#launch-rate-calculations","title":"Launch rate calculations","text":"<p>The <code>io_test.sh</code> script launches 16 jobs in parallel, and each job has a duration of 20 minutes. This results in a job launch rate of</p>    \\frac{16 ~ \\text{jobs}}{20 ~ \\text{min}} = 0.8 ~ \\text{jobs per minute}  <p>that is lower than the rule of thumb limit of 10 jobs per minute. Imagine for instance that we do not limit the maximum number of tasks that can run in parallel by overriding the <code>--array</code> option. <pre><code>sbatch --array=0-255 io_test.sh\n</code></pre> Then, up to all 256 can run in parallel and each job has a duration of 20 minutes, which would result in a peak allocation rate of</p>    \\frac{256}{20} = 12.8 ~ \\text{jobs per minute}  <p>a lunch rate that is momentarily above the rule of rule of thumbs limit of 10 jobs per minute. Therefore, a limit in the maximum number of parallel running jobs should be considered.</p> <p>Limiting the job launch rate</p> <p>The <code>MaxArraySize</code> limit in UL HPC systems makes it difficult to exceed the suggested limit of job launches per minute. However, in case you need to launch more that 1000 jobs or you expect a job launch rate that is more that 10 jobs per minute, please consider using GNU parallel.</p>"},{"location":"jobs/arrays/#writing-launch-scripts","title":"Writing launch scripts","text":"<p>Array indices can be used to differentiate the input of a task. In the following example, a script creates programmatically a job array to run a parametric investigation on a 2-dimensional input, and then launches the job array.</p> <p><code>launch_parammetric_analysis.sh</code></p> <pre><code>#!/usr/bin/bash --login\n\ndeclare max_parallel_tasks=16\ndeclare speed_step=0.01\n\ngenerate_commands() {\n  local filename=\"${1}\"\n\n  echo -n &gt; ${filename}\n  declare nx ny vx vy\n  for nx in $(seq 1 10); do\n    for ny in $(seq 1 10); do\n      vx=\"$(echo \"${nx}\"*\"${speed_step}\" | bc --mathlib)\"\n      vy=\"$(echo \"${ny}\"*\"${speed_step}\" | bc --mathlib)\"\n      echo \"simulate_with_drift.py '${vx}' '${vy}' --output-file='speed_idx_${nx}_${ny}.dat'\" &gt;&gt; ${filename}\n    done\n  done\n}\n\ngenerate_submission_script() {\n  local submission_script=\"${1}\"\n  local command_script=\"${2}\"\n\n  local n_commands=\"$(cat ${command_script} | wc --lines)\"\n  local max_task_id=\"$((${n_commands} - 1))\"\n\n  cat &gt; job_array_script.sh &lt;&lt;EOF\n#!/bin/bash --login\n#SBATCH --job-name=parametric_analysis\n#SBATCH --array=0-${max_task_id}%${max_parallel_tasks}\n#SBATCH --partition=batch\n#SBATCH --qos=normal\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=16\n#SBATCH --time=0-10:00:00\n#SBATCH --output=%x-%A_%a.out\n#SBATCH --error=%x-%A_%a.err\n\nmodule load lang/Python\n\ndeclade command=\"\\$(sed \"\\${SLURM_ARRAY_TASK_ID}\"'!d' ${command_script})\"\n\necho \"Running commnand: \\${command}\"\neval \"srun python \\${command}\"\nEOF\n}\n\ngenerate_commands 'commands.sh'\ngenerate_submission_script 'job_array_script.sh' 'commands.sh'\n\nsbatch job_array_script.sh\n</code></pre> <p>Run the <code>launch_parammetric_analysis.sh</code> script with the bash command.</p> <pre><code>bash launch_parammetric_analysis.sh\n</code></pre> Avoiding script generation <p>Script generation is a complex and error prone command. In this example script generation is unavoidable, as the whole parametric analysis cannot run in a single job of the <code>normal</code> QoS which has the default maximum wall time (<code>MaxWall</code>) of 2 days. The expected runtime on each simulation would be about 0.25 to 0.5 of the maximum wall time (<code>--time</code>) which is set at 10 hours.</p> <p>If all the parametric analysis can run within the 2 day limit, then consider running the analysis in a single allocation using GNU parallel. You can then generate the command file and lauch the simulation all from a single script in a single job allocation.</p>"},{"location":"jobs/best-effort/","title":"Best-effort Jobs","text":"<p>Best-effort jobs are preemptible jobs that can be interrupted by higher priority jobs. Job preemption is the act of \"stopping\" one or more \"low-priority\" jobs to let a \"high-priority\" job run. Job preemption is implemented as a variation of Slurm's Gang Scheduling logic.</p> <ul> <li>Preemptible jobs can be scheduled for requeuing upon preemption with the <code>--requeue</code> option of <code>sbatch</code>.</li> <li>When a job is requeued, the batch script is initiated from its beginning.</li> <li>The <code>besteffort</code> QoS is the only preemptible QoS in UL HPC systems.</li> </ul> <p>To submit a job in the <code>besteffort</code> QoS set the <code>--qos</code> option as follows.</p> <pre><code>sbatch --partition={batch|gpu|bigmem} --qos=besteffort [...]\n</code></pre> <p>A few examples for submitting <code>besteffort</code> OoS are the following in the various compute node types are summarized in the following table.</p> Node Type Slurm command regular <code>sbatch [--account=&lt;project&gt;] --partition=batch          --qos=besteffort [--constraints={broadwell,skylake}] [...]</code> gpu <code>sbatch [--account=&lt;project&gt;] --partition={gpu,hopper}   --qos=besteffort --gpus=1 [--constraint=volta{16,32}] [...]</code> bigmem <code>sbatch [--account=&lt;project&gt;] --partition=bigmem         --qos=besteffort [...]</code> <p>Why use preemtible jobs?</p> <p>Best-effort (preemptible) jobs allow an efficient usage of the platform resources. Usually when jobs are scheduled, some compute nodes are left unexploited. Backfilling can be used to schedule some lower priority jobs in leftover nodes, but these jobs must fill in the \"time\" gaps left by higher priority jobs. Preemptible jobs relax the time constrains, by allowing the scheduler to schedule them in \"time\" gaps where they do not fit, knowing that they can be interrupted at any time to schedule a higher priority job.</p> <p>As a result of their scheduling flexibility, <code>besteffort</code> QoS have less constraints than the other QoS (for instance, you can submit more jobs).</p> <p>Computing with best-effort jobs</p> <p>Best-effort jobs with the <code>--requeue</code> option are launched from the beginning each time they are requeued.</p> <ul> <li>The user must ensure that checkpoint-restart or another progress tracking mechanism is used to ensure that best-effort jobs can be stop and resumed without losing any computation progress.</li> <li>A best-effort job with the <code>--requeue</code> option is requeued until the job exits on its own (successfully or otherwise).</li> </ul>"},{"location":"jobs/billing/","title":"Job Accounting and Billing","text":"<p> Usage Charging Policy</p>"},{"location":"jobs/billing/#billing-rates","title":"Billing rates","text":""},{"location":"jobs/billing/#ulhpc-usage-charging-policy","title":"ULHPC Usage Charging Policy","text":"<p>The advertised prices are for internal partners only</p> <p>The price list and all other information of this page are meant for internal partners, i.e., not for external companies.  If you are not an internal partner, please contact us at hpc-partnership@uni.lu. Alternatively, you can contact LuxProvide, the national HPC center which aims at serving the private sector for HPC needs.</p>"},{"location":"jobs/billing/#how-to-estimate-hpc-costs-for-projects","title":"How to estimate HPC costs for projects?","text":"<p>You can use the following excel document to estimate the cost of your HPC usage:</p> <p> UL HPC Cost Estimates for Project Proposals [xlsx] </p> <p>Note that there are two sheets offering two ways to estimate based on your specific situation. Please read the red sections to ensure that you are using the correct estimation sheet.</p> <p>Note that even if you plan for large-scale experiments on PRACE/EuroHPC supercomputers through computing credits granted by Call for Proposals for Project Access, you should plan for ULHPC costs since you will have to demonstrate the scalability of your code -- the University's facility is ideal for that. You can contact hpc-partnership@uni.lu for more details about this.</p>"},{"location":"jobs/billing/#hpc-price-list-2022-10-01","title":"HPC price list - 2022-10-01","text":"<p>Note that ULHPC price list has been updated, see below.</p>"},{"location":"jobs/billing/#compute","title":"Compute","text":"Compute type Description \u20ac (excl. VAT) / node-hour CPU - small 28 cores, 128 GB RAM 0.25\u20ac CPU - regular 128 cores, 256 GB RAM 1.25\u20ac CPU - big mem 112 cores, 3 TB RAM 6.00\u20ac GPU 4 V100, 28 cores, 768 GB RAM 5.00\u20ac <p>The prices above correspond to a full-node cost. However, jobs can use a fraction of a node and the price of the job will be computed based on that fraction. Please find below the core-hour / GPU-hour costs and how we compute how much to charge:</p> Compute type Unit \u20ac (excl. VAT) CPU - small Core-hour 0.0089\u20ac CPU - regular Core-hour 0.0097\u20ac CPU - big mem Core-hour 0.0535\u20ac GPU GPU-hour 1.25\u20ac <p>For CPU nodes, the fraction correspond to the number of requested cores, e.g. 64 cores on a CPU - regular node corresponds to 50% of the available cores and thus will be charged 50% of 1.25\u20ac. </p> <p>Regarding the RAM of a job, if you do not override the default behaviour, you will receive a percentage of the RAM corresponding to the amount of requested cores, e.g, 128G of RAM for the 64 cores example from above (50% of a CPU - regular node). If you override the default behaviour and request more RAM, we will re-compute the equivalent number of cores, e.g. if you request 256G of RAM and 64 cores, we will charge 128 cores.</p> <p>For GPU nodes, the fraction considers the number of GPUs. There are 4 GPUs, 28 cores and 768G of RAM on one machine. This means that for each GPU, you can have up to 7 cores and 192G of RAM. If you request more than those default, we will re-compute the GPU equivalent, e.g. if you request 1 GPU and 8 cores, we will charge 2 GPUs.</p>"},{"location":"jobs/billing/#storage","title":"Storage","text":"Storage type \u20ac (excl. VAT) / GB / Month Additional information Home Free 500 GB Project 0.02\u20ac 1 TB free Scratch Free 10 TB <p>Note that for project storage, we charge the quota and not the used storage.</p>"},{"location":"jobs/billing/#hpc-resource-allocation-for-ul-internal-rd-and-training","title":"HPC Resource allocation for UL internal R&amp;D and training","text":"<p>ULHPC resources are free of charge for UL staff for their internal work and training activities. Principal Investigators (PI) will nevertheless receive on a regular basis a usage report of their team activities on the UL HPC platform. The corresponding accumulated price will be provided even if this amount is purely indicative and won't be charged back.</p> <p>Any other activities will be reviewed with the rectorate and are a priori subjected to be billed.</p>"},{"location":"jobs/billing/#submit-project-related-jobs","title":"Submit project related jobs","text":"<p>To allow the ULHPC team to keep track of the jobs related to a project, use the <code>-A &lt;projectname&gt;</code> flag in Slurm, either in the Slurm directives preamble of your script, e.g.,</p> <pre><code>#SBATCH -A myproject\n</code></pre> <p>or on the command line when you submit your job, e.g., <code>sbatch -A myproject /path/to/launcher.sh</code></p>"},{"location":"jobs/billing/#trackable-resources-tres-billing-weights","title":"Trackable RESources (TRES) Billing Weights","text":"<p>The above policy is in practice implemented through the Slurm Trackable RESources (TRES) and remains an important factor for the Fairsharing score calculation.</p> <p>As explained in the ULHPC Usage Charging Policy, we set TRES for CPU, GPU, and Memory usage according to weights defined as follows:</p> Weight Description \\alpha_{cpu} Normalized relative performance of CPU processor core (ref.: skylake 73.6 GFlops/core) \\alpha_{mem} Inverse of the average available memory size per core \\alpha_{GPU} Weight per GPU accelerator <p>Each partition has its own weights (combined into <code>TRESBillingWeight</code>) you can check with</p> <pre><code># /!\\ ADAPT &lt;partition&gt; accordingly\nscontrol show partition &lt;partition&gt;\n</code></pre>"},{"location":"jobs/dependencies/","title":"Job dependencies","text":"<p>Interdependent jobs can be submitted in Slurm systems to perform tasks with dependencies between their various steps. Job dependencies are useful for instance in managing data. </p>"},{"location":"jobs/dependencies/#specifying-job-dependencies","title":"Specifying job dependencies","text":"<p>Job dependencies are inserted with the <code>--dependency</code> (<code>-d</code> in short format) option flag.</p> <pre><code>$ sbatch --dependency=&lt;dependency_list&gt; script.sh\n</code></pre> <p>The <code>&lt;dependency_list&gt;</code> object is composed by a number of dependencies in a comma (<code>,</code>) separated list <pre><code>&lt;dependency_list&gt; = &lt;dependency&gt;[,&lt;dependency&gt;...]\n</code></pre> if all dependencies must be satisfied, or question mark (<code>?</code>) separated list <pre><code>&lt;dependency_list&gt; = &lt;dependency&gt;[?&lt;dependency&gt;...]\n</code></pre> if any of the dependencies is sufficient. Only one separator may be used.</p> <p>When a job with dependencies is queued, the job is not considered for execution until its dependencies are satisfied. The scheduler takes into account the end time of dependencies to reserve resources for depended jobs.</p> <p>Job dependencies</p> Dependency Description <code>after:job_id[+time][:jobid[+time]...]</code> Enable after the listed jobs start or are canceled; wait <code>time</code> minutes before starting, start imediatelly if no <code>time</code> is specified). <code>afterany:job_id[:jobid...]</code><sup>[1]</sup> Enable after the specified jobs have terminated. <code>afterburstbuffer:job_id[:jobid...]</code> Enable after the specified jobs have terminated and any associated burst buffer stage out operations have completed. <code>afternotok:job_id[:jobid...]</code> Enable after the specified jobs have terminated in some failed state (non-zero exit code, node failure, timed out, etc). <code>afterok:job_id[:jobid...]</code> Enable after the specified jobs have successfully executed (ran to completion with an exit code of zero). <code>singleton</code> Enable after any previously launched jobs sharing the same job name and user have terminated. In other words, only one job by that name and owned by that user can be running or suspended at any point in time. <code>aftercorr:job_id[:jobid...]</code><sup>[2]</sup> Enable after the corresponding task ID in <code>job_id</code> array has completed successfully (ran to completion with an exit code of zero). <ol> <li>The default dependency type.</li> <li>Applicable only to job arrays.</li> </ol>"},{"location":"jobs/dependencies/#submitting-jobs-with-dependencies","title":"Submitting jobs with dependencies","text":"<p>For instance, is you want <code>second_job.sh</code> to start after <code>first_job.sh</code> has completed successfully, then issue the commands:</p> <pre><code>$ first_job_id=$(sbatch --parsable first_job,sh)\n$ sbatch --dependency=afterok:${first_job_id} second_job.sh\n</code></pre> <p>If you want <code>dependant_job.sh</code> to start after <code>job_0.sh</code> and <code>job_1.sh</code> have completed successfully, then issue the commands:</p> <pre><code>$ job_0_id=$(sbatch --parsable job_0.sh)\n$ job_1_id=$(sbatch --parsable job_1.sh)\n$ sbatch --dependency=afterok:${job_0_id},afterok:${job_1_id} dependant_job.sh\n</code></pre> <p>As the number of dependent job increases, it pays off to create a submission script with all the dependency information. For instance:</p> <p>Contents of <code>submit_jobs.sh</code></p> <pre><code>#!/bin/bash --login\n\ndeclare copy_job_id=$(sbatch --parsable copy_data.sh)\ndeclare analysis_job_0_id=$(sbatch --parsable --dependency=afterok:${copy_job_id} analysis_job_0.sh)\ndeclare analysis_job_1_id=$(sbatch --parsable --dependency=afterok:${copy_job_id} analysis_job_1.sh)\nsbatch --dependency=afterok:${analysis_job_0_id},${analysis_job_1_id} cleanup_job.sh\n</code></pre> <p>Then all the jobs are submitted with the command:</p> <pre><code>$ bash submit_jobs.sh\n</code></pre> The <code>--parsable</code> option of <code>sbatch</code> <p>With the option <code>--parsable</code> the <code>sbatch</code> command return a single string with the job ID that can be stored in a shell variable. For instance without <code>--parsable</code>, the output of the batch submission is <pre><code>$ sbatch job.sh\nSubmitted batch job 12345\n</code></pre> and with <code>--parsable</code> the output is <pre><code>$ sbatch --parsable job.sh\n12345\n</code></pre> that is only the job ID.</p>"},{"location":"jobs/dependencies/#examples","title":"Examples","text":"<p>These are 2 cases that appear often in our systems. The first is an example of transferring data between the available storage tiers. The second involves running a light database to support a serries of jobs that are submitted automatically for the duration of a project.</p>"},{"location":"jobs/dependencies/#using-afterok-dependencies-to-transfer-data","title":"Using <code>afterok</code> dependencies to transfer data","text":"<p>When using the scratch data typically needs to be transferred from a project directory or the long term storage to the scratch before the jobs starts, and then the results must be transferred back and the scratch cleared after the job finishes. With job dependencies someone can schedule</p> <ul> <li>a job to perform the data transfer to scratch,</li> <li>a job that runs their program with data in scratch that starts after the data transfer completes successfully,</li> <li>a job to transfer the results back and clean the scratch that starts after their program execution has finished successfully.</li> </ul>"},{"location":"jobs/dependencies/#using-singleton-dependency-to-run-a-lightweight-database","title":"Using <code>singleton</code> dependency to run a lightweight database","text":"<p>Job dependencies can be used to provide a service that runs continuously for the duration of some experiment. Imagine for instance that</p> <ul> <li>you maintain an SQLite database in the Isilon file system containing measurement data and analysis results, and</li> <li>that you run an application in a external machine that periodically writes data files in Isilon, and submits a job to analyse the data and store them in the database.</li> </ul> <p>For this setup, you need a script that runs an SQLite database in the cluster at all times.</p> <p>The singleton job provides an appropriate method for maintaining a jobs running in manner accounted by the Slurm scheduler. The singleton dependency ensure that there is single job running in the cluster for each combination of job name (<code>--job-name</code>) and user id. If you submit multiple job with the same name and the <code>--dependency=singleton</code> they will run one at a time.</p>"},{"location":"jobs/dependencies/#the-database-job-service","title":"The database job service","text":"<p>There are 2 options for running a database to collect processed results, run for a fixed amount of time, or for until a specific time instance. The following scripts assume that the database engine is contained in a singularity container.</p> Run for a fixed durationRun until a specific time instance <p>Running the database for a fixed duration is the simplest option. Split your job in jobs of smaller duration, and submit all the jobs at once. For instance this script splits the job in one day chunks.</p> <p>Database script <code>run_database.sh</code></p> <pre><code>#!/bin/bash --login\n\n#SBATCH --job-name=database_service\n#SBATCH --mail-type=all\n#SBATCH --mail-user=name.surname@uni.lu\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=16\n#SBATCH --time=1-00:05:00\n#SBATCH --partition=batch\n#SBATCH --qos=normal\n#SBATCH --output=%x-%j.out\n#SBATCH --error=%x-%j.err\n\nmodule load tools/Apptainer\napptainer run ${PROJECTHOME}/project_name/containers/database.sif &amp;\n\nsleep $((24*60*60)) # 1 day in sec\n</code></pre> <p>If you want to run your job for <code>${N}</code> days, submit <code>${N}</code> copies: <pre><code>for i in $(seq \"${N}\"); do sbatch run_database.sh; done; unset i\n</code></pre></p> <p>Running a job until a specific time instance is also possible but more involved. The following script takes as argument the end date and time of the service.</p> <p>Database script <code>run_database.sh</code></p> <pre><code>#!/bin/bash --login\n\n#SBATCH --job-name=database_service\n#SBATCH --mail-type=all\n#SBATCH --mail-user=name.surname@uni.lu\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=16\n#SBATCH --time=1-00:05:00\n#SBATCH --partition=batch\n#SBATCH --qos=normal\n#SBATCH --output=%x-%j.out\n#SBATCH --error=%x-%j.err\n\ndeclare end_time=\"${1}\"\n\nconvert_to_unix_time() {\n  local time=\"${1}\"\n  date --date=\"${time}\" '+%s'\n}\n\nget_slurm_job_script() {\n  local job_id=\"${1}\"\n  scontrol show job \"${job_id}\" | awk 'BEGIN {FS=\"=\"} /^[[:space:]]*Command=/{print $2}'\n}\n\ndeclare time_now=$(date '+%s')\nif [ -n \"${end_time}\" ] &amp;&amp; [ \"${time_now}\" -lt \"$(convert_to_unix_time \"${end_time}\")\" ]; then\n  sbatch --dependency=singleton \"$(get_slurm_job_script ${SLURM_JOBID})\" \"${end_time}\"\nfi\n\nmodule load tools/Apptainer\napptainer run ${PROJECTHOME}/project_name/containers/database.sif &amp;\n\nsleep $((24*60*60)) # 1 day in sec\n</code></pre> <p>Submit the script providing the end date and time of your job with the command <pre><code>sbatch run_database.sh \"${end_time}\"\n</code></pre> where <code>${end_time}</code> should be a string interpretable by the <code>date</code> command. A very flexible format is ISO 8601; for instance, <code>Wed Apr 23 04:49:36 PM CEST 2025</code> becomes <code>2025-04-23T16:49:36+02:00</code> in ISO 8601. The <code>date</code> can parse both aforementioned strings.</p> <p>Avoid fork-bombs</p> <p>If you forget to specify <code>--dependency=singleton</code> in the command <pre><code>sbatch --dependency=singleton \"$(get_slurm_job_script ${SLURM_JOBID})\" \"${end_time}\"\n</code></pre> then <code>run_database.sh</code> jobs are recursively queued, and can crush of the scheduler. This is the equivalent of an accidental fork bomb for the Slurm scheduler.</p> <p>Ensure proper scheduling</p> <p>If the service is scheduled after the command is called, <pre><code>apptainer run ${PROJECTHOME}/project_name/containers/database.sif\nsbatch --dependency=singleton \"$(get_slurm_job_script ${SLURM_JOBID})\" \"${end_time}\"\n</code></pre> where you first wait for the process to finish (note the absence of <code>&amp;</code>) and then schedule the next job, there may be a long wait for resources and thus an unacceptably long interruption of your service.</p> <p>The job scripts for the database, <code>run_database.sh</code>, schedule the future database job, launch a containerized database job in the background, and wait for the job for a fixed amount of time, a bit smaller that the job maximum duration. Thus, the scheduler has enough information an leeway to schedule the next instance of the database container close to the time the first job ends. Thus, given that enough resources are available, there will be a small gap between the two instances of the database process. Any application that is designed to be resilient to small interruptions in the database service will ride through the relaunch of the database without issues.</p> <p>Providing services with HPC systems</p> <p>The HPC system is not designed to provide continuously running services. The provided script provides constraints for the service running time, however, the constraints are not visible to the Slurm scheduler which reduces the effectiveness of the scheduling algorithm. Please use the provided method only for lightweight services.</p> <p>If you need to run large services for unspecified amounts of time, consider setting up a virtual machine.</p>"},{"location":"jobs/dependencies/#submitting-jobs-that-use-the-database","title":"Submitting jobs that use the database","text":"<p>To periodically submit jobs that use the database it is assumed that the analysis procedure is contained in a Singularity container and that the job is configured to access the database. For instance, the database machine IP and the port used by the database may be stored and read from a specific location in the cluster file system. It is assumed that the single input to the analysis is a binary <code>dat</code> file.</p> <p>Job script <code>run_analysis</code></p> <pre><code>#!/bin/bash --login\n\n#SBATCH --job-name=data_analysis\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=128\n#SBATCH --time=0-00:30:00\n#SBATCH --partition=batch\n#SBATCH --qos=normal\n#SBATCH --output=%x-%j.out\n#SBATCH --error=%x-%j.err\n\nlocal data=\"${1}\"\n\nmodule load tools/Apptainer\napptainer run ${PROJECTHOME}/project_name/containers/analysis.sif \"${data}\"\n</code></pre> <p>With the database job running, the data analysis job can be submitted remotely for a server with the command <pre><code>ssh aion-cluster \"sbatch ${PROJECTHOME}/project_name/scripts/run_analysis.sh /mnt/isilon/projects/project_name/data/datafile_${id}.dat\"\n</code></pre> given that the SSH configuration of the server contains the <code>aion-cluster</code> entry for the Aion cluster.</p>"},{"location":"jobs/dependencies/#resources","title":"Resources","text":"<ol> <li>Documentation for <code>sbatch</code></li> </ol>"},{"location":"jobs/gnu-parallel/","title":"GNU parallel in HPC systems","text":"<p>Job campaigns that allocate many small jobs quickly, either using job arrays or custom launcher scripts, should use GNU parallel to reduce the scheduler load. The Slurm scheduler performs 2 jobs,</p> <ul> <li>allocates resources for a job,</li> <li>launches the job steps.</li> </ul> <p>Slurm is designed to allocate resources in an allocation loop that runs periodically, usually every 30-180s, depending on its configuration. If many small jobs are in the queue, then operations triggered during the allocation loop, such as backfilling, become expensive. As a result, the scheduling loop can delay past its period, causing the scheduler to appear slow and unresponsive. GNU parallel executes multiple commands in parallel in a single allocation or even in a single job step, removing the need to allocate resources and reducing the scheduler load.</p> <p>When should GNU parallel be used?</p> <p>As a rule of thumb,</p> <ul> <li>if you are planning to execute jobs campaigns that require more than 10 job allocations per minute, then please consider grouping jobs with GNU parallel;</li> <li>if you are planning to execute jobs that cumulatively launch more than 1000 job steps per minute, then please consider grouping job and job steps using GNU parallel.</li> </ul>"},{"location":"jobs/gnu-parallel/#using-gnu-parallel-in-hpc-jobs","title":"Using GNU parallel in HPC jobs","text":"<p>GNU parallel (command <code>parallel</code>)is a shell tool for executing jobs in parallel using one or more computers. A job can be a single command or a small script that has to be run for each of the lines in the input.</p> <ul> <li>The jobs are forked from the main job when the <code>parallel</code> command executes.</li> <li>The parallel command blocks until all forked processes exit.</li> <li>You can limit the number of jobs that run in parallel; <code>parallel</code> implements a form of process pull, where a limited number of processes running in parallel executes the jobs.</li> </ul>"},{"location":"jobs/gnu-parallel/#grouping-allocations-with-gnu-parallel","title":"Grouping allocations with GNU parallel","text":"<p>The scheduler is much more efficient in lunching job steps within a job, where resources have been allocated and there is no need to interact with the resource allocation loop. Job steps are lunched within a job with call to the blocking <code>srun</code> command, so the executable needs to be launched with the <code>srun</code> command within GNU parallel.</p> <pre><code>#!/bin/bash --login\n#SBATCH --job-name=single_job_step\n#SBATCH --partition=batch\n#SBATCH --qos=normal\n#SBATCH --nodes=4\n#SBATCH --ntasks-per-node=8\n#SBATCH --cpus-per-task=16\n#SBATCH --time=02:00:00\n#SBATCH --output=%x-%j.out\n#SBATCH --error=%x-%j.err\n\ndeclare test_duration=60\n\nparallel \\\n  --max-procs \"${SLURM_NTASKS}\" \\\n  --max-args 0 \\\n  srun \\\n    --nodes=1 \\\n    --ntasks=1 \\\n    stress-ng \\\n      --cpu ${SLURM_CPUS_PER_TASK} \\\n      --timeout \"${test_duration}\" \\\n      ::: {0..1023}\n</code></pre> <p>This example script launches a single job step per GNU parallel job. The executable <code>stress-ng</code> is a stress test program.</p> <ul> <li>The number of GNU parallel jobs (<code>--max-procs</code>) is limited to the number of tasks, <code>SLURM_NTASKS</code>, so that there is no overallocation of GNU parallel jobs. Note that if a jobs step is launched without sufficient resources, then the job step fails.</li> <li>Each job step is consuming a single task (<code>--ntasks=1</code>) that has access to <code>SLURM_CPUS_PER_TASK</code> CPUs. The <code>stress-ng</code> program requires the explicit specification of the number of CPUs to use for the CPU benchmark with the <code>--cpu</code> flag.</li> </ul> Slurm environment variables (<code>SLURM_*</code>) <p>Upon starting a job step, <code>srun</code> reads the options defined in a set of environment variables, most of the starting with the prefix <code>SLURM_</code>. The scheduler it self will set many of these variables with an allocation for a job is created. Some useful variables set and their corresponding allocation (<code>sbatch</code>/<code>salloc</code>) flags are the following.</p> <ul> <li><code>--nodes</code>: <code>SLURM_NNODES</code></li> <li><code>--ntasks</code>: <code>SLURM_NTASKS</code></li> <li><code>--ntasks-per-node</code>: <code>SLURM_NTASKS_PER_NODE</code></li> <li><code>--cpus-per-task</code>: <code>SLURM_CPUS_PER_TASK</code></li> </ul> <p>Note that some environment variables are evaluated implicitly even if the corresponding option is not defined for the allocation. For instance in the example above we define <code>--nodes</code> and <code>--ntasks-per-node</code>, but not <code>--ntasks</code>, yet the <code>SLURM_NTASKS</code> is set to</p> <pre><code>SLURM_NTASKS = SLURM_NNODES * SLURM_NTASKS_PER_NODE = 4 * 8 = 24\n</code></pre> <p>and its value can be used in the submission script without defining <code>--nodes</code>.</p> <p>Job allocation rate calculation</p> <ul> <li>In this example jobs steps of <code>1 min</code> duration (<code>test_duration</code>) are being launched in parallel from <code>4</code> nodes (<code>--nodes</code>) and <code>8</code> tasks per node (<code>--ntasks-per-node</code>), for a total of <code>32</code> tasks.</li> <li>If <code>32</code> tasks where launched every minute in distinct jobs the job allocation rate would be above the empirical limit of <code>10</code> jobs per minute.</li> <li>Thus, the use of GNU parallel is justified.</li> </ul>"},{"location":"jobs/gnu-parallel/#grouping-allocations-and-job-steps-with-gnu-parallel","title":"Grouping allocations and job steps with GNU parallel","text":"<p>If a jobs contains a massive amounts of very small job steps, it can be limited by the rate with which job steps can be launched. The scheduler stores some information for each job step in a database, and with multiple small steps launching in parallel the throughput limits of the database system can exceeded affecting every job in the cluster. In such extreme cases, GNU parallel can limit the number of job steps by grouping multiple GNU parallel jobs in a single job step.</p> <p>There are 2 options when lunching multiple GNU parallel jobs in a single jobs step, the GNU parallel jobs of the step can be launched in a script or in a function.</p> GNU parallel jobs in a functionGNU parallel jobs in a script <p>Submission script</p> <pre><code>#!/bin/bash --login\n#SBATCH --job-name=multi_job_step_in_function\n#SBATCH --partition=batch\n#SBATCH --qos=normal\n#SBATCH --nodes=4\n#SBATCH --ntasks-per-node=8\n#SBATCH --cpus-per-task=16\n#SBATCH --time=02:00:00\n#SBATCH --output=%x-%j.out\n#SBATCH --error=%x-%j.err\n\ndeclare test_duration=5\ndeclare substeps=$((1024*48+11))\ndeclare substeps_per_step=48\ndeclare cpus_per_substep=4\n\ndeclare steps=$(( ${substeps} / ${substeps_per_step} ))\ndeclare remainder_steps=$(( ${substeps} % ${substeps_per_step} ))\nif [ ! \"${remainder_steps}\" -eq \"0\" ]; then\n  steps=$(( ${steps} + 1 ))\nfi\n\nrun_step() {\n  local substeps=\"${1}\"\n  local substeps_per_step=\"${2}\"\n  local test_duration=\"${3}\"\n  local cpus_per_substep=\"${4}\"\n  local step_idx=\"${5}\"\n\n  local initial_substep=$(( ${step_idx} * ${substeps_per_step} ))\n  local final_substep=$(( ${initial_substep} + ${substeps_per_step} ))\n  if [ \"${final_substep}\" -gt \"${substeps}\" ]; then\n    final_substep=${substeps}\n  fi\n  final_substep=$(( ${final_substep} - 1 ))\n\n  local max_parallel_substeps=$(( ${SLURM_CPUS_PER_TASK} / ${cpus_per_substep} ))\n\n  parallel \\\n    --max-procs \"${max_parallel_substeps}\" \\\n    --max-args 0 \\\n      stress-ng \\\n        --cpu \"${cpus_per_substep}\" \\\n        --timeout \"${test_duration}\" \\\n        ::: $(seq ${initial_substep} \"${final_substep}\")\n}\n\nexport -f run_step\n\ndeclare final_step=$(( ${steps} - 1 ))\n\nparallel \\\n  --max-procs \"${SLURM_NTASKS}\" \\\n  --max-args 1 \\\n  srun \\\n    --nodes=1 \\\n    --ntasks=1 \\\n    bash \\\n      -c \"\\\"run_step ${substeps} ${substeps_per_step} ${test_duration} ${cpus_per_substep} {1}\\\"\" \\\n      ::: $(seq 0 ${final_step})\n</code></pre> <p>When running the GNU parallel job steps in a function, make sure that the function is exported to the environment of <code>srun</code> with the <code>export -f</code> bash builtin command.</p> <p>Submission script</p> <pre><code>#!/bin/bash --login\n#SBATCH --job-name=multi_job_step_in_script\n#SBATCH --partition=batch\n#SBATCH --qos=normal\n#SBATCH --nodes=4\n#SBATCH --ntasks-per-node=8\n#SBATCH --cpus-per-task=16\n#SBATCH --time=02:00:00\n#SBATCH --output=%x-%j.out\n#SBATCH --error=%x-%j.err\n\ndeclare test_duration=5\ndeclare substeps=$((1024*48+11))\ndeclare substeps_per_step=48\ndeclare cpus_per_substep=4\n\ndeclare steps=$(( ${substeps} / ${substeps_per_step} ))\ndeclare remainder_steps=$(( ${substeps} % ${substeps_per_step} ))\nif [ ! \"${remainder_steps}\" -eq \"0\" ]; then\n  steps=$(( ${steps} + 1 ))\nfi\n\ndeclare final_step=$(( ${steps} - 1 ))\n\nparallel \\\n  --max-procs \"${SLURM_NTASKS}\" \\\n  --max-args 1 \\\n  srun \\\n    --nodes=1 \\\n    --ntasks=1 \\\n    run_job_step \\\n      \"${substeps}\" \\\n      \"${substeps_per_step}\" \\\n      \"${test_duration}\" \\\n      \"${cpus_per_substep}\" \\\n      ::: $(seq 0 ${final_step})\n</code></pre> <p>Ensure that the external script <code>run_job_step</code> is accessible from submission script, for instance, place both scripts in the same directory.</p> <p>External job step execution script <code>run_job_step</code></p> <pre><code>#!/bin/bash --login\n\ndeclare substeps=\"${1}\"\ndeclare substeps_per_step=\"${2}\"\ndeclare test_duration=\"${3}\"\ndeclare cpus_per_substep=\"${4}\"\ndeclare step_idx=\"${5}\"\n\ndeclare initial_substep=$(( ${step_idx} * ${substeps_per_step} ))\ndeclare final_substep=$(( ${initial_substep} + ${substeps_per_step} ))\nif [ \"${final_substep}\" -gt \"${substeps}\" ]; then\n  final_substep=${substeps}\nfi\nfinal_substep=$(( ${final_substep} - 1 ))\n\ndeclare max_parallel_substeps=$(( ${SLURM_CPUS_PER_TASK} / ${cpus_per_substep} ))\n\nparallel \\\n  --max-procs \"${max_parallel_substeps}\" \\\n  --max-args 0 \\\n    stress-ng \\\n      --cpu \"${cpus_per_substep}\" \\\n      --timeout \"${test_duration}\" \\\n      ::: $(seq ${initial_substep} \"${final_substep}\")\n</code></pre> <p>Finally, make sure that the <code>run_job_step</code> is executable. The command <pre><code>chmod u+x run_job_step\n</code></pre> is usually required to provide the user with execution permission.</p> <p>Job step launch rate calculation</p> <ul> <li>Each node contains <code>128</code> CPUs, and each job requires <code>4</code> CPUs (<code>cpus_per_substep</code>); thus each node has <code>128/4 = 32</code> slots to execute jobs in parallel.</li> <li>There are <code>4</code> nodes (<code>--nodes</code>), so with <code>32</code> slots per node, there are in total <code>128</code> slots to launch job steps in parallel.</li> <li>Each jobs has a duration of <code>5 sec</code>, so each slot runs <code>12</code> jobs per minute.</li> <li>If every parallel job was a job step, then <code>12*128 = 1536</code> jobs steps per minute are launched.</li> <li>This is above the empirical threshold of <code>1000</code> job steps per minute, so grouping steps is justified.</li> <li>Grouping creates groups of <code>48</code> GNU parallel jobs (<code>substeps_per_step</code>) reducing the job step launch rate to <code>1536/48 = 32</code> that is safely below the empirical limit of <code>1000</code> per minute.</li> </ul>"},{"location":"jobs/gpu/","title":"ULHPC GPU Nodes","text":"<p>Each GPU node provided as part of the <code>gpu</code> partition feature 4x Nvidia V100 SXM2 (with either 16G or 32G memory) interconnected by the NVLink 2.0 architecture</p> <p>NVlink was designed as an alternative solution to PCI Express with higher bandwidth and additional features (e.g., shared memory) specifically designed to be compatible with Nvidia's own GPU ISA for multi-GPU systems -- see wikichip article.</p> <p></p> <p>Because of the hardware organization, you MUST follow the below recommendations:</p> <ol> <li>Do not run jobs on GPU nodes if you have no use of GPU accelerators!, i.e. if you are not using any of the software compiled against the <code>{foss,intel}cuda</code> toolchain.</li> <li>Avoid using more than 4 GPUs, ideally within the same node.</li> <li>Dedicated \u00bc of the available CPU cores for the management of each GPU card reserved.</li> </ol> <p>Thus your typical GPU launcher would match the AI/DL launcher example:</p> <pre><code>#!/usr/bin/bash --login\n\n#SBATCH --job-name=gpu_example\n#SBATCH --output=%x-%j.out\n#SBATCH --error=%x-%j.out\n\n### Request one GPU tasks for 4 hours - dedicate 1/4 of available cores for its management\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=7\n#SBATCH --gpus-per-task=1\n#SBATCH --time=0-04:00:00\n\n### Submit to the `gpu` partition of Iris\n#SBATCH --partition=gpu\n#SBATCH --qos=normal\n\nprint_error_and_exit() { echo \"***ERROR*** $*\"; exit 1; }\n\nmodule purge || print_error_and_exit \"No 'module' command available\"\nmodule load numlib/cuDNN   # Example using the cuDNN module\n\n[...]\n</code></pre> <p>Interactive jobs</p> <p>In the UL HPC systems you can use the <code>si-gpu</code>, a wrapper for the <code>salloc</code> command, that allocates interactive job in a GPU node with sensible default options.</p>"},{"location":"jobs/hwloc/","title":"Examining the architecture of compute nodes","text":"<p>You can extract detailed information about the architecture of cluster nodes using the Portable Hardware Locality (hwloc) package. The hardware locality modules are provided in UL HPC clusters by the <code>system/hwloc</code> modules. Let's examine the output of hardware locality for the various types of nodes in Aion and Iris and how it is interpreted.</p>"},{"location":"jobs/hwloc/#using-hardware-locality","title":"Using hardware locality","text":"<p>Running the hardware locality is as simple as loading the module and calling the hardware locality program <code>hwloc-ls</code>.</p> Iris CPU nodesIris GPU nodesIris Bigmem nodesAion CPU nodes <ol> <li>Allocate a full node in Iris.    <pre><code>salloc --partition=batch --qos=normal --nodes=1 --ntasks-per-node=1 --cpus-per-task=28\n</code></pre></li> <li>Load the hardware locality module.    <pre><code>module load system/hwloc\n</code></pre></li> <li>Run the hardware locality program <code>hwloc-ls</code>.    <pre><code>hwloc-ls\n</code></pre></li> </ol> The output of <code>hwloc-ls</code> <pre><code>$ hwloc-ls\nMachine (126GB total)\n  Package L#0\n    NUMANode L#0 (P#0 63GB)\n    L3 L#0 (35MB)\n      L2 L#0 (256KB) + L1d L#0 (32KB) + L1i L#0 (32KB) + Core L#0 + PU L#0 (P#0)\n      L2 L#1 (256KB) + L1d L#1 (32KB) + L1i L#1 (32KB) + Core L#1 + PU L#1 (P#2)\n      L2 L#2 (256KB) + L1d L#2 (32KB) + L1i L#2 (32KB) + Core L#2 + PU L#2 (P#4)\n      L2 L#3 (256KB) + L1d L#3 (32KB) + L1i L#3 (32KB) + Core L#3 + PU L#3 (P#6)\n      L2 L#4 (256KB) + L1d L#4 (32KB) + L1i L#4 (32KB) + Core L#4 + PU L#4 (P#8)\n      L2 L#5 (256KB) + L1d L#5 (32KB) + L1i L#5 (32KB) + Core L#5 + PU L#5 (P#10)\n      L2 L#6 (256KB) + L1d L#6 (32KB) + L1i L#6 (32KB) + Core L#6 + PU L#6 (P#12)\n      L2 L#7 (256KB) + L1d L#7 (32KB) + L1i L#7 (32KB) + Core L#7 + PU L#7 (P#14)\n      L2 L#8 (256KB) + L1d L#8 (32KB) + L1i L#8 (32KB) + Core L#8 + PU L#8 (P#16)\n      L2 L#9 (256KB) + L1d L#9 (32KB) + L1i L#9 (32KB) + Core L#9 + PU L#9 (P#18)\n      L2 L#10 (256KB) + L1d L#10 (32KB) + L1i L#10 (32KB) + Core L#10 + PU L#10 (P#20)\n      L2 L#11 (256KB) + L1d L#11 (32KB) + L1i L#11 (32KB) + Core L#11 + PU L#11 (P#22)\n      L2 L#12 (256KB) + L1d L#12 (32KB) + L1i L#12 (32KB) + Core L#12 + PU L#12 (P#24)\n      L2 L#13 (256KB) + L1d L#13 (32KB) + L1i L#13 (32KB) + Core L#13 + PU L#13 (P#26)\n    HostBridge\n      PCIBridge\n        PCI 01:00.0 (InfiniBand)\n          Net \"ib0\"\n          OpenFabrics \"mlx5_0\"\n      PCIBridge\n        PCIBridge\n          PCIBridge\n            PCIBridge\n              PCI 08:00.0 (VGA)\n      PCI 00:1f.2 (SATA)\n        Block(Disk) \"sda\"\n  Package L#1\n    NUMANode L#1 (P#1 63GB)\n    L3 L#1 (35MB)\n      L2 L#14 (256KB) + L1d L#14 (32KB) + L1i L#14 (32KB) + Core L#14 + PU L#14 (P#1)\n      L2 L#15 (256KB) + L1d L#15 (32KB) + L1i L#15 (32KB) + Core L#15 + PU L#15 (P#3)\n      L2 L#16 (256KB) + L1d L#16 (32KB) + L1i L#16 (32KB) + Core L#16 + PU L#16 (P#5)\n      L2 L#17 (256KB) + L1d L#17 (32KB) + L1i L#17 (32KB) + Core L#17 + PU L#17 (P#7)\n      L2 L#18 (256KB) + L1d L#18 (32KB) + L1i L#18 (32KB) + Core L#18 + PU L#18 (P#9)\n      L2 L#19 (256KB) + L1d L#19 (32KB) + L1i L#19 (32KB) + Core L#19 + PU L#19 (P#11)\n      L2 L#20 (256KB) + L1d L#20 (32KB) + L1i L#20 (32KB) + Core L#20 + PU L#20 (P#13)\n      L2 L#21 (256KB) + L1d L#21 (32KB) + L1i L#21 (32KB) + Core L#21 + PU L#21 (P#15)\n      L2 L#22 (256KB) + L1d L#22 (32KB) + L1i L#22 (32KB) + Core L#22 + PU L#22 (P#17)\n      L2 L#23 (256KB) + L1d L#23 (32KB) + L1i L#23 (32KB) + Core L#23 + PU L#23 (P#19)\n      L2 L#24 (256KB) + L1d L#24 (32KB) + L1i L#24 (32KB) + Core L#24 + PU L#24 (P#21)\n      L2 L#25 (256KB) + L1d L#25 (32KB) + L1i L#25 (32KB) + Core L#25 + PU L#25 (P#23)\n      L2 L#26 (256KB) + L1d L#26 (32KB) + L1i L#26 (32KB) + Core L#26 + PU L#26 (P#25)\n      L2 L#27 (256KB) + L1d L#27 (32KB) + L1i L#27 (32KB) + Core L#27 + PU L#27 (P#27)\n    HostBridge\n      PCIBridge\n        PCI 81:00.0 (Ethernet)\n          Net \"eno1\"\n        PCI 81:00.1 (Ethernet)\n          Net \"eno2\"\n</code></pre> <p>From the output you can see the following in an Iris CPU node.</p> <ul> <li>There are 2 sockets in a node (<code>Package</code>).</li> <li>There is a single NUMA node with <code>63GB</code> of RAM and a single L3 cache per socket.</li> <li>There are 14 cores per L3 cache group.</li> <li>There is a single processor unit (<code>PU</code>), also known as hardware thread, per core.</li> <li>The storage (<code>sda</code>) and the fast interconnect adaptor (<code>mlx5_0</code>) are attached to socket 0 (<code>Package L#0</code>).</li> </ul> <ol> <li>Allocate a full node in Iris.    <pre><code>salloc --partition=gpu --qos=normal --nodes=1 --ntasks-per-node=1 --cpus-per-task=28 --gpus-per-task=4\n</code></pre></li> <li>Load the hardware locality module.    <pre><code>module load system/hwloc\n</code></pre></li> <li>Run the hardware locality program <code>hwloc-ls</code>.    <pre><code>hwloc-ls\n</code></pre></li> </ol> The output of <code>hwloc-ls</code> <pre><code>Machine (755GB total)\n  Package L#0\n    NUMANode L#0 (P#0 377GB)\n    L3 L#0 (19MB)\n      L2 L#0 (1024KB) + L1d L#0 (32KB) + L1i L#0 (32KB) + Core L#0 + PU L#0 (P#0)\n      L2 L#1 (1024KB) + L1d L#1 (32KB) + L1i L#1 (32KB) + Core L#1 + PU L#1 (P#2)\n      L2 L#2 (1024KB) + L1d L#2 (32KB) + L1i L#2 (32KB) + Core L#2 + PU L#2 (P#4)\n      L2 L#3 (1024KB) + L1d L#3 (32KB) + L1i L#3 (32KB) + Core L#3 + PU L#3 (P#6)\n      L2 L#4 (1024KB) + L1d L#4 (32KB) + L1i L#4 (32KB) + Core L#4 + PU L#4 (P#8)\n      L2 L#5 (1024KB) + L1d L#5 (32KB) + L1i L#5 (32KB) + Core L#5 + PU L#5 (P#10)\n      L2 L#6 (1024KB) + L1d L#6 (32KB) + L1i L#6 (32KB) + Core L#6 + PU L#6 (P#12)\n      L2 L#7 (1024KB) + L1d L#7 (32KB) + L1i L#7 (32KB) + Core L#7 + PU L#7 (P#14)\n      L2 L#8 (1024KB) + L1d L#8 (32KB) + L1i L#8 (32KB) + Core L#8 + PU L#8 (P#16)\n      L2 L#9 (1024KB) + L1d L#9 (32KB) + L1i L#9 (32KB) + Core L#9 + PU L#9 (P#18)\n      L2 L#10 (1024KB) + L1d L#10 (32KB) + L1i L#10 (32KB) + Core L#10 + PU L#10 (P#20)\n      L2 L#11 (1024KB) + L1d L#11 (32KB) + L1i L#11 (32KB) + Core L#11 + PU L#11 (P#22)\n      L2 L#12 (1024KB) + L1d L#12 (32KB) + L1i L#12 (32KB) + Core L#12 + PU L#12 (P#24)\n      L2 L#13 (1024KB) + L1d L#13 (32KB) + L1i L#13 (32KB) + Core L#13 + PU L#13 (P#26)\n    HostBridge\n      PCI 00:11.5 (SATA)\n      PCIBridge\n        PCI 01:00.0 (Ethernet)\n          Net \"eno3\"\n        PCI 01:00.1 (Ethernet)\n          Net \"eno4\"\n      PCIBridge\n        PCIBridge\n          PCI 03:00.0 (VGA)\n      PCIBridge\n        PCI 04:00.0 (SATA)\n          Block(Disk) \"sda\"\n      PCIBridge\n        PCI 05:00.0 (Ethernet)\n          Net \"eno1\"\n        PCI 05:00.1 (Ethernet)\n          Net \"eno2\"\n    HostBridge\n      PCIBridge\n        PCIBridge\n          PCIBridge\n            PCI 1a:00.0 (3D)\n          PCIBridge\n            PCI 1c:00.0 (3D)\n          PCIBridge\n            PCI 1d:00.0 (3D)\n          PCIBridge\n            PCI 1e:00.0 (3D)\n    HostBridge\n      PCIBridge\n        PCI 5e:00.0 (InfiniBand)\n          Net \"ib0\"\n          OpenFabrics \"mlx5_0\"\n        PCI 5e:00.1 (InfiniBand)\n          Net \"ib1\"\n          OpenFabrics \"mlx5_1\"\n  Package L#1\n    NUMANode L#1 (P#1 378GB)\n    L3 L#1 (19MB)\n      L2 L#14 (1024KB) + L1d L#14 (32KB) + L1i L#14 (32KB) + Core L#14 + PU L#14 (P#1)\n      L2 L#15 (1024KB) + L1d L#15 (32KB) + L1i L#15 (32KB) + Core L#15 + PU L#15 (P#3)\n      L2 L#16 (1024KB) + L1d L#16 (32KB) + L1i L#16 (32KB) + Core L#16 + PU L#16 (P#5)\n      L2 L#17 (1024KB) + L1d L#17 (32KB) + L1i L#17 (32KB) + Core L#17 + PU L#17 (P#7)\n      L2 L#18 (1024KB) + L1d L#18 (32KB) + L1i L#18 (32KB) + Core L#18 + PU L#18 (P#9)\n      L2 L#19 (1024KB) + L1d L#19 (32KB) + L1i L#19 (32KB) + Core L#19 + PU L#19 (P#11)\n      L2 L#20 (1024KB) + L1d L#20 (32KB) + L1i L#20 (32KB) + Core L#20 + PU L#20 (P#13)\n      L2 L#21 (1024KB) + L1d L#21 (32KB) + L1i L#21 (32KB) + Core L#21 + PU L#21 (P#15)\n      L2 L#22 (1024KB) + L1d L#22 (32KB) + L1i L#22 (32KB) + Core L#22 + PU L#22 (P#17)\n      L2 L#23 (1024KB) + L1d L#23 (32KB) + L1i L#23 (32KB) + Core L#23 + PU L#23 (P#19)\n      L2 L#24 (1024KB) + L1d L#24 (32KB) + L1i L#24 (32KB) + Core L#24 + PU L#24 (P#21)\n      L2 L#25 (1024KB) + L1d L#25 (32KB) + L1i L#25 (32KB) + Core L#25 + PU L#25 (P#23)\n      L2 L#26 (1024KB) + L1d L#26 (32KB) + L1i L#26 (32KB) + Core L#26 + PU L#26 (P#25)\n      L2 L#27 (1024KB) + L1d L#27 (32KB) + L1i L#27 (32KB) + Core L#27 + PU L#27 (P#27)\n    HostBridge\n      PCIBridge\n        PCI d8:00.0 (NVMExp)\n          Block(Disk) \"nvme0n1\"\n</code></pre> <p>From the output you can see the following in an Iris CPU node.</p> <ul> <li>There are 2 sockets in a node (<code>Package</code>).</li> <li>There is a single NUMA node with <code>378GB</code> of RAM and a single L3 cache per socket.</li> <li>There are 14 cores per L3 cache group.</li> <li>There is a single processor unit (<code>PU</code>), also known as hardware thread, per core.</li> <li>There are 4 GPUs attached to socket 0 (<code>Package L#0</code>) through PCIe (<code>PCIBridge</code>).</li> <li>The fast interconnect adaptor (<code>mlx5_0</code>) is also attached to socket 0.</li> <li>The storage (<code>nvme0n1</code>) is attached to socket 1.</li> </ul> <ol> <li>Allocate a full node in Iris.    <pre><code>salloc --partition=bigmem --qos=normal --nodes=1 --ntasks-per-node=1 --cpus-per-task=112\n</code></pre></li> <li>Load the hardware locality module.    <pre><code>module load system/hwloc\n</code></pre></li> <li>Run the hardware locality program <code>hwloc-ls</code>.    <pre><code>hwloc-ls\n</code></pre></li> </ol> The output of <code>hwloc-ls</code> <pre><code>Machine (3022GB total)\n  Package L#0\n    NUMANode L#0 (P#0 754GB)\n    L3 L#0 (39MB)\n      L2 L#0 (1024KB) + L1d L#0 (32KB) + L1i L#0 (32KB) + Core L#0 + PU L#0 (P#0)\n      L2 L#1 (1024KB) + L1d L#1 (32KB) + L1i L#1 (32KB) + Core L#1 + PU L#1 (P#4)\n      L2 L#2 (1024KB) + L1d L#2 (32KB) + L1i L#2 (32KB) + Core L#2 + PU L#2 (P#8)\n      L2 L#3 (1024KB) + L1d L#3 (32KB) + L1i L#3 (32KB) + Core L#3 + PU L#3 (P#12)\n      L2 L#4 (1024KB) + L1d L#4 (32KB) + L1i L#4 (32KB) + Core L#4 + PU L#4 (P#16)\n      L2 L#5 (1024KB) + L1d L#5 (32KB) + L1i L#5 (32KB) + Core L#5 + PU L#5 (P#20)\n      L2 L#6 (1024KB) + L1d L#6 (32KB) + L1i L#6 (32KB) + Core L#6 + PU L#6 (P#24)\n      L2 L#7 (1024KB) + L1d L#7 (32KB) + L1i L#7 (32KB) + Core L#7 + PU L#7 (P#28)\n      L2 L#8 (1024KB) + L1d L#8 (32KB) + L1i L#8 (32KB) + Core L#8 + PU L#8 (P#32)\n      L2 L#9 (1024KB) + L1d L#9 (32KB) + L1i L#9 (32KB) + Core L#9 + PU L#9 (P#36)\n      L2 L#10 (1024KB) + L1d L#10 (32KB) + L1i L#10 (32KB) + Core L#10 + PU L#10 (P#40)\n      L2 L#11 (1024KB) + L1d L#11 (32KB) + L1i L#11 (32KB) + Core L#11 + PU L#11 (P#44)\n      L2 L#12 (1024KB) + L1d L#12 (32KB) + L1i L#12 (32KB) + Core L#12 + PU L#12 (P#48)\n      L2 L#13 (1024KB) + L1d L#13 (32KB) + L1i L#13 (32KB) + Core L#13 + PU L#13 (P#52)\n      L2 L#14 (1024KB) + L1d L#14 (32KB) + L1i L#14 (32KB) + Core L#14 + PU L#14 (P#56)\n      L2 L#15 (1024KB) + L1d L#15 (32KB) + L1i L#15 (32KB) + Core L#15 + PU L#15 (P#60)\n      L2 L#16 (1024KB) + L1d L#16 (32KB) + L1i L#16 (32KB) + Core L#16 + PU L#16 (P#64)\n      L2 L#17 (1024KB) + L1d L#17 (32KB) + L1i L#17 (32KB) + Core L#17 + PU L#17 (P#68)\n      L2 L#18 (1024KB) + L1d L#18 (32KB) + L1i L#18 (32KB) + Core L#18 + PU L#18 (P#72)\n      L2 L#19 (1024KB) + L1d L#19 (32KB) + L1i L#19 (32KB) + Core L#19 + PU L#19 (P#76)\n      L2 L#20 (1024KB) + L1d L#20 (32KB) + L1i L#20 (32KB) + Core L#20 + PU L#20 (P#80)\n      L2 L#21 (1024KB) + L1d L#21 (32KB) + L1i L#21 (32KB) + Core L#21 + PU L#21 (P#84)\n      L2 L#22 (1024KB) + L1d L#22 (32KB) + L1i L#22 (32KB) + Core L#22 + PU L#22 (P#88)\n      L2 L#23 (1024KB) + L1d L#23 (32KB) + L1i L#23 (32KB) + Core L#23 + PU L#23 (P#92)\n      L2 L#24 (1024KB) + L1d L#24 (32KB) + L1i L#24 (32KB) + Core L#24 + PU L#24 (P#96)\n      L2 L#25 (1024KB) + L1d L#25 (32KB) + L1i L#25 (32KB) + Core L#25 + PU L#25 (P#100)\n      L2 L#26 (1024KB) + L1d L#26 (32KB) + L1i L#26 (32KB) + Core L#26 + PU L#26 (P#104)\n      L2 L#27 (1024KB) + L1d L#27 (32KB) + L1i L#27 (32KB) + Core L#27 + PU L#27 (P#108)\n    HostBridge\n      PCI 00:11.5 (SATA)\n      PCI 00:17.0 (SATA)\n      PCIBridge\n        PCI 01:00.0 (Ethernet)\n          Net \"eth0\"\n        PCI 01:00.1 (Ethernet)\n          Net \"eth2\"\n      PCIBridge\n        PCIBridge\n          PCI 03:00.0 (VGA)\n    HostBridge\n      PCIBridge\n        PCI 17:00.0 (Ethernet)\n          Net \"eth1\"\n        PCI 17:00.1 (Ethernet)\n          Net \"eth3\"\n    HostBridge\n      PCIBridge\n        PCI 33:00.0 (InfiniBand)\n          Net \"ib0\"\n          OpenFabrics \"mlx5_0\"\n        PCI 33:00.1 (InfiniBand)\n          Net \"ib1\"\n          OpenFabrics \"mlx5_1\"\n  Package L#1\n    NUMANode L#1 (P#1 756GB)\n    L3 L#1 (39MB)\n      L2 L#28 (1024KB) + L1d L#28 (32KB) + L1i L#28 (32KB) + Core L#28 + PU L#28 (P#1)\n      L2 L#29 (1024KB) + L1d L#29 (32KB) + L1i L#29 (32KB) + Core L#29 + PU L#29 (P#5)\n      L2 L#30 (1024KB) + L1d L#30 (32KB) + L1i L#30 (32KB) + Core L#30 + PU L#30 (P#9)\n      L2 L#31 (1024KB) + L1d L#31 (32KB) + L1i L#31 (32KB) + Core L#31 + PU L#31 (P#13)\n      L2 L#32 (1024KB) + L1d L#32 (32KB) + L1i L#32 (32KB) + Core L#32 + PU L#32 (P#17)\n      L2 L#33 (1024KB) + L1d L#33 (32KB) + L1i L#33 (32KB) + Core L#33 + PU L#33 (P#21)\n      L2 L#34 (1024KB) + L1d L#34 (32KB) + L1i L#34 (32KB) + Core L#34 + PU L#34 (P#25)\n      L2 L#35 (1024KB) + L1d L#35 (32KB) + L1i L#35 (32KB) + Core L#35 + PU L#35 (P#29)\n      L2 L#36 (1024KB) + L1d L#36 (32KB) + L1i L#36 (32KB) + Core L#36 + PU L#36 (P#33)\n      L2 L#37 (1024KB) + L1d L#37 (32KB) + L1i L#37 (32KB) + Core L#37 + PU L#37 (P#37)\n      L2 L#38 (1024KB) + L1d L#38 (32KB) + L1i L#38 (32KB) + Core L#38 + PU L#38 (P#41)\n      L2 L#39 (1024KB) + L1d L#39 (32KB) + L1i L#39 (32KB) + Core L#39 + PU L#39 (P#45)\n      L2 L#40 (1024KB) + L1d L#40 (32KB) + L1i L#40 (32KB) + Core L#40 + PU L#40 (P#49)\n      L2 L#41 (1024KB) + L1d L#41 (32KB) + L1i L#41 (32KB) + Core L#41 + PU L#41 (P#53)\n      L2 L#42 (1024KB) + L1d L#42 (32KB) + L1i L#42 (32KB) + Core L#42 + PU L#42 (P#57)\n      L2 L#43 (1024KB) + L1d L#43 (32KB) + L1i L#43 (32KB) + Core L#43 + PU L#43 (P#61)\n      L2 L#44 (1024KB) + L1d L#44 (32KB) + L1i L#44 (32KB) + Core L#44 + PU L#44 (P#65)\n      L2 L#45 (1024KB) + L1d L#45 (32KB) + L1i L#45 (32KB) + Core L#45 + PU L#45 (P#69)\n      L2 L#46 (1024KB) + L1d L#46 (32KB) + L1i L#46 (32KB) + Core L#46 + PU L#46 (P#73)\n      L2 L#47 (1024KB) + L1d L#47 (32KB) + L1i L#47 (32KB) + Core L#47 + PU L#47 (P#77)\n      L2 L#48 (1024KB) + L1d L#48 (32KB) + L1i L#48 (32KB) + Core L#48 + PU L#48 (P#81)\n      L2 L#49 (1024KB) + L1d L#49 (32KB) + L1i L#49 (32KB) + Core L#49 + PU L#49 (P#85)\n      L2 L#50 (1024KB) + L1d L#50 (32KB) + L1i L#50 (32KB) + Core L#50 + PU L#50 (P#89)\n      L2 L#51 (1024KB) + L1d L#51 (32KB) + L1i L#51 (32KB) + Core L#51 + PU L#51 (P#93)\n      L2 L#52 (1024KB) + L1d L#52 (32KB) + L1i L#52 (32KB) + Core L#52 + PU L#52 (P#97)\n      L2 L#53 (1024KB) + L1d L#53 (32KB) + L1i L#53 (32KB) + Core L#53 + PU L#53 (P#101)\n      L2 L#54 (1024KB) + L1d L#54 (32KB) + L1i L#54 (32KB) + Core L#54 + PU L#54 (P#105)\n      L2 L#55 (1024KB) + L1d L#55 (32KB) + L1i L#55 (32KB) + Core L#55 + PU L#55 (P#109)\n    HostBridge\n      PCIBridge\n        PCI 48:00.0 (NVMExp)\n          Block(Disk) \"nvme0n1\"\n  Package L#2\n    NUMANode L#2 (P#2 756GB)\n    L3 L#2 (39MB)\n      L2 L#56 (1024KB) + L1d L#56 (32KB) + L1i L#56 (32KB) + Core L#56 + PU L#56 (P#2)\n      L2 L#57 (1024KB) + L1d L#57 (32KB) + L1i L#57 (32KB) + Core L#57 + PU L#57 (P#6)\n      L2 L#58 (1024KB) + L1d L#58 (32KB) + L1i L#58 (32KB) + Core L#58 + PU L#58 (P#10)\n      L2 L#59 (1024KB) + L1d L#59 (32KB) + L1i L#59 (32KB) + Core L#59 + PU L#59 (P#14)\n      L2 L#60 (1024KB) + L1d L#60 (32KB) + L1i L#60 (32KB) + Core L#60 + PU L#60 (P#18)\n      L2 L#61 (1024KB) + L1d L#61 (32KB) + L1i L#61 (32KB) + Core L#61 + PU L#61 (P#22)\n      L2 L#62 (1024KB) + L1d L#62 (32KB) + L1i L#62 (32KB) + Core L#62 + PU L#62 (P#26)\n      L2 L#63 (1024KB) + L1d L#63 (32KB) + L1i L#63 (32KB) + Core L#63 + PU L#63 (P#30)\n      L2 L#64 (1024KB) + L1d L#64 (32KB) + L1i L#64 (32KB) + Core L#64 + PU L#64 (P#34)\n      L2 L#65 (1024KB) + L1d L#65 (32KB) + L1i L#65 (32KB) + Core L#65 + PU L#65 (P#38)\n      L2 L#66 (1024KB) + L1d L#66 (32KB) + L1i L#66 (32KB) + Core L#66 + PU L#66 (P#42)\n      L2 L#67 (1024KB) + L1d L#67 (32KB) + L1i L#67 (32KB) + Core L#67 + PU L#67 (P#46)\n      L2 L#68 (1024KB) + L1d L#68 (32KB) + L1i L#68 (32KB) + Core L#68 + PU L#68 (P#50)\n      L2 L#69 (1024KB) + L1d L#69 (32KB) + L1i L#69 (32KB) + Core L#69 + PU L#69 (P#54)\n      L2 L#70 (1024KB) + L1d L#70 (32KB) + L1i L#70 (32KB) + Core L#70 + PU L#70 (P#58)\n      L2 L#71 (1024KB) + L1d L#71 (32KB) + L1i L#71 (32KB) + Core L#71 + PU L#71 (P#62)\n      L2 L#72 (1024KB) + L1d L#72 (32KB) + L1i L#72 (32KB) + Core L#72 + PU L#72 (P#66)\n      L2 L#73 (1024KB) + L1d L#73 (32KB) + L1i L#73 (32KB) + Core L#73 + PU L#73 (P#70)\n      L2 L#74 (1024KB) + L1d L#74 (32KB) + L1i L#74 (32KB) + Core L#74 + PU L#74 (P#74)\n      L2 L#75 (1024KB) + L1d L#75 (32KB) + L1i L#75 (32KB) + Core L#75 + PU L#75 (P#78)\n      L2 L#76 (1024KB) + L1d L#76 (32KB) + L1i L#76 (32KB) + Core L#76 + PU L#76 (P#82)\n      L2 L#77 (1024KB) + L1d L#77 (32KB) + L1i L#77 (32KB) + Core L#77 + PU L#77 (P#86)\n      L2 L#78 (1024KB) + L1d L#78 (32KB) + L1i L#78 (32KB) + Core L#78 + PU L#78 (P#90)\n      L2 L#79 (1024KB) + L1d L#79 (32KB) + L1i L#79 (32KB) + Core L#79 + PU L#79 (P#94)\n      L2 L#80 (1024KB) + L1d L#80 (32KB) + L1i L#80 (32KB) + Core L#80 + PU L#80 (P#98)\n      L2 L#81 (1024KB) + L1d L#81 (32KB) + L1i L#81 (32KB) + Core L#81 + PU L#81 (P#102)\n      L2 L#82 (1024KB) + L1d L#82 (32KB) + L1i L#82 (32KB) + Core L#82 + PU L#82 (P#106)\n      L2 L#83 (1024KB) + L1d L#83 (32KB) + L1i L#83 (32KB) + Core L#83 + PU L#83 (P#110)\n  Package L#3\n    NUMANode L#3 (P#3 756GB)\n    L3 L#3 (39MB)\n      L2 L#84 (1024KB) + L1d L#84 (32KB) + L1i L#84 (32KB) + Core L#84 + PU L#84 (P#3)\n      L2 L#85 (1024KB) + L1d L#85 (32KB) + L1i L#85 (32KB) + Core L#85 + PU L#85 (P#7)\n      L2 L#86 (1024KB) + L1d L#86 (32KB) + L1i L#86 (32KB) + Core L#86 + PU L#86 (P#11)\n      L2 L#87 (1024KB) + L1d L#87 (32KB) + L1i L#87 (32KB) + Core L#87 + PU L#87 (P#15)\n      L2 L#88 (1024KB) + L1d L#88 (32KB) + L1i L#88 (32KB) + Core L#88 + PU L#88 (P#19)\n      L2 L#89 (1024KB) + L1d L#89 (32KB) + L1i L#89 (32KB) + Core L#89 + PU L#89 (P#23)\n      L2 L#90 (1024KB) + L1d L#90 (32KB) + L1i L#90 (32KB) + Core L#90 + PU L#90 (P#27)\n      L2 L#91 (1024KB) + L1d L#91 (32KB) + L1i L#91 (32KB) + Core L#91 + PU L#91 (P#31)\n      L2 L#92 (1024KB) + L1d L#92 (32KB) + L1i L#92 (32KB) + Core L#92 + PU L#92 (P#35)\n      L2 L#93 (1024KB) + L1d L#93 (32KB) + L1i L#93 (32KB) + Core L#93 + PU L#93 (P#39)\n      L2 L#94 (1024KB) + L1d L#94 (32KB) + L1i L#94 (32KB) + Core L#94 + PU L#94 (P#43)\n      L2 L#95 (1024KB) + L1d L#95 (32KB) + L1i L#95 (32KB) + Core L#95 + PU L#95 (P#47)\n      L2 L#96 (1024KB) + L1d L#96 (32KB) + L1i L#96 (32KB) + Core L#96 + PU L#96 (P#51)\n      L2 L#97 (1024KB) + L1d L#97 (32KB) + L1i L#97 (32KB) + Core L#97 + PU L#97 (P#55)\n      L2 L#98 (1024KB) + L1d L#98 (32KB) + L1i L#98 (32KB) + Core L#98 + PU L#98 (P#59)\n      L2 L#99 (1024KB) + L1d L#99 (32KB) + L1i L#99 (32KB) + Core L#99 + PU L#99 (P#63)\n      L2 L#100 (1024KB) + L1d L#100 (32KB) + L1i L#100 (32KB) + Core L#100 + PU L#100 (P#67)\n      L2 L#101 (1024KB) + L1d L#101 (32KB) + L1i L#101 (32KB) + Core L#101 + PU L#101 (P#71)\n      L2 L#102 (1024KB) + L1d L#102 (32KB) + L1i L#102 (32KB) + Core L#102 + PU L#102 (P#75)\n      L2 L#103 (1024KB) + L1d L#103 (32KB) + L1i L#103 (32KB) + Core L#103 + PU L#103 (P#79)\n      L2 L#104 (1024KB) + L1d L#104 (32KB) + L1i L#104 (32KB) + Core L#104 + PU L#104 (P#83)\n      L2 L#105 (1024KB) + L1d L#105 (32KB) + L1i L#105 (32KB) + Core L#105 + PU L#105 (P#87)\n      L2 L#106 (1024KB) + L1d L#106 (32KB) + L1i L#106 (32KB) + Core L#106 + PU L#106 (P#91)\n      L2 L#107 (1024KB) + L1d L#107 (32KB) + L1i L#107 (32KB) + Core L#107 + PU L#107 (P#95)\n      L2 L#108 (1024KB) + L1d L#108 (32KB) + L1i L#108 (32KB) + Core L#108 + PU L#108 (P#99)\n      L2 L#109 (1024KB) + L1d L#109 (32KB) + L1i L#109 (32KB) + Core L#109 + PU L#109 (P#103)\n      L2 L#110 (1024KB) + L1d L#110 (32KB) + L1i L#110 (32KB) + Core L#110 + PU L#110 (P#107)\n      L2 L#111 (1024KB) + L1d L#111 (32KB) + L1i L#111 (32KB) + Core L#111 + PU L#111 (P#111)\n</code></pre> <p>From the output you can see the following in an Iris CPU node.</p> <ul> <li>There are 4 sockets in a node (<code>Package</code>).</li> <li>There is a single NUMA node with <code>754GB</code> of RAM and a single L3 cache per socket.</li> <li>There are 28 cores per L3 cache group.</li> <li>There is a single processor unit (<code>PU</code>), also known as hardware thread, per core.</li> <li>There are 2 fast interconnect adaptors (<code>mlx5_0</code> and <code>mlx5_1</code>) attached to socket 0 (<code>Package L#0</code>).</li> <li>The storage (<code>nvme0n1</code>) is attached to socket 1.</li> </ul> <ol> <li>Allocate a full node in Aion.    <pre><code>salloc --partition=batch --qos=normal --nodes=1 --ntasks-per-node=1 --cpus-per-task=128\n</code></pre></li> <li>Load the hardware locality module.    <pre><code>module load system/hwloc\n</code></pre></li> <li>Run the hardware locality program <code>hwloc-ls</code>.    <pre><code>hwloc-ls\n</code></pre></li> </ol> The output of <code>hwloc-ls</code> <pre><code>$ hwloc-ls\nMachine (251GB total)\n  Package L#0\n    Group0 L#0\n      NUMANode L#0 (P#0 31GB)\n      L3 L#0 (16MB)\n        L2 L#0 (512KB) + L1d L#0 (32KB) + L1i L#0 (32KB) + Core L#0 + PU L#0 (P#0)\n        L2 L#1 (512KB) + L1d L#1 (32KB) + L1i L#1 (32KB) + Core L#1 + PU L#1 (P#1)\n        L2 L#2 (512KB) + L1d L#2 (32KB) + L1i L#2 (32KB) + Core L#2 + PU L#2 (P#2)\n        L2 L#3 (512KB) + L1d L#3 (32KB) + L1i L#3 (32KB) + Core L#3 + PU L#3 (P#3)\n      L3 L#1 (16MB)\n        L2 L#4 (512KB) + L1d L#4 (32KB) + L1i L#4 (32KB) + Core L#4 + PU L#4 (P#4)\n        L2 L#5 (512KB) + L1d L#5 (32KB) + L1i L#5 (32KB) + Core L#5 + PU L#5 (P#5)\n        L2 L#6 (512KB) + L1d L#6 (32KB) + L1i L#6 (32KB) + Core L#6 + PU L#6 (P#6)\n        L2 L#7 (512KB) + L1d L#7 (32KB) + L1i L#7 (32KB) + Core L#7 + PU L#7 (P#7)\n      L3 L#2 (16MB)\n        L2 L#8 (512KB) + L1d L#8 (32KB) + L1i L#8 (32KB) + Core L#8 + PU L#8 (P#8)\n        L2 L#9 (512KB) + L1d L#9 (32KB) + L1i L#9 (32KB) + Core L#9 + PU L#9 (P#9)\n        L2 L#10 (512KB) + L1d L#10 (32KB) + L1i L#10 (32KB) + Core L#10 + PU L#10 (P#10)\n        L2 L#11 (512KB) + L1d L#11 (32KB) + L1i L#11 (32KB) + Core L#11 + PU L#11 (P#11)\n      L3 L#3 (16MB)\n        L2 L#12 (512KB) + L1d L#12 (32KB) + L1i L#12 (32KB) + Core L#12 + PU L#12 (P#12)\n        L2 L#13 (512KB) + L1d L#13 (32KB) + L1i L#13 (32KB) + Core L#13 + PU L#13 (P#13)\n        L2 L#14 (512KB) + L1d L#14 (32KB) + L1i L#14 (32KB) + Core L#14 + PU L#14 (P#14)\n        L2 L#15 (512KB) + L1d L#15 (32KB) + L1i L#15 (32KB) + Core L#15 + PU L#15 (P#15)\n      HostBridge\n        PCIBridge\n          PCI 61:00.0 (InfiniBand)\n            Net \"ib0\"\n            OpenFabrics \"mlx5_0\"\n        PCIBridge\n          PCIBridge\n            PCI 63:00.0 (VGA)\n    Group0 L#1\n      NUMANode L#1 (P#1 31GB)\n      L3 L#4 (16MB)\n        L2 L#16 (512KB) + L1d L#16 (32KB) + L1i L#16 (32KB) + Core L#16 + PU L#16 (P#16)\n        L2 L#17 (512KB) + L1d L#17 (32KB) + L1i L#17 (32KB) + Core L#17 + PU L#17 (P#17)\n        L2 L#18 (512KB) + L1d L#18 (32KB) + L1i L#18 (32KB) + Core L#18 + PU L#18 (P#18)\n        L2 L#19 (512KB) + L1d L#19 (32KB) + L1i L#19 (32KB) + Core L#19 + PU L#19 (P#19)\n      L3 L#5 (16MB)\n        L2 L#20 (512KB) + L1d L#20 (32KB) + L1i L#20 (32KB) + Core L#20 + PU L#20 (P#20)\n        L2 L#21 (512KB) + L1d L#21 (32KB) + L1i L#21 (32KB) + Core L#21 + PU L#21 (P#21)\n        L2 L#22 (512KB) + L1d L#22 (32KB) + L1i L#22 (32KB) + Core L#22 + PU L#22 (P#22)\n        L2 L#23 (512KB) + L1d L#23 (32KB) + L1i L#23 (32KB) + Core L#23 + PU L#23 (P#23)\n      L3 L#6 (16MB)\n        L2 L#24 (512KB) + L1d L#24 (32KB) + L1i L#24 (32KB) + Core L#24 + PU L#24 (P#24)\n        L2 L#25 (512KB) + L1d L#25 (32KB) + L1i L#25 (32KB) + Core L#25 + PU L#25 (P#25)\n        L2 L#26 (512KB) + L1d L#26 (32KB) + L1i L#26 (32KB) + Core L#26 + PU L#26 (P#26)\n        L2 L#27 (512KB) + L1d L#27 (32KB) + L1i L#27 (32KB) + Core L#27 + PU L#27 (P#27)\n      L3 L#7 (16MB)\n        L2 L#28 (512KB) + L1d L#28 (32KB) + L1i L#28 (32KB) + Core L#28 + PU L#28 (P#28)\n        L2 L#29 (512KB) + L1d L#29 (32KB) + L1i L#29 (32KB) + Core L#29 + PU L#29 (P#29)\n        L2 L#30 (512KB) + L1d L#30 (32KB) + L1i L#30 (32KB) + Core L#30 + PU L#30 (P#30)\n        L2 L#31 (512KB) + L1d L#31 (32KB) + L1i L#31 (32KB) + Core L#31 + PU L#31 (P#31)\n    Group0 L#2\n      NUMANode L#2 (P#2 31GB)\n      L3 L#8 (16MB)\n        L2 L#32 (512KB) + L1d L#32 (32KB) + L1i L#32 (32KB) + Core L#32 + PU L#32 (P#32)\n        L2 L#33 (512KB) + L1d L#33 (32KB) + L1i L#33 (32KB) + Core L#33 + PU L#33 (P#33)\n        L2 L#34 (512KB) + L1d L#34 (32KB) + L1i L#34 (32KB) + Core L#34 + PU L#34 (P#34)\n        L2 L#35 (512KB) + L1d L#35 (32KB) + L1i L#35 (32KB) + Core L#35 + PU L#35 (P#35)\n      L3 L#9 (16MB)\n        L2 L#36 (512KB) + L1d L#36 (32KB) + L1i L#36 (32KB) + Core L#36 + PU L#36 (P#36)\n        L2 L#37 (512KB) + L1d L#37 (32KB) + L1i L#37 (32KB) + Core L#37 + PU L#37 (P#37)\n        L2 L#38 (512KB) + L1d L#38 (32KB) + L1i L#38 (32KB) + Core L#38 + PU L#38 (P#38)\n        L2 L#39 (512KB) + L1d L#39 (32KB) + L1i L#39 (32KB) + Core L#39 + PU L#39 (P#39)\n      L3 L#10 (16MB)\n        L2 L#40 (512KB) + L1d L#40 (32KB) + L1i L#40 (32KB) + Core L#40 + PU L#40 (P#40)\n        L2 L#41 (512KB) + L1d L#41 (32KB) + L1i L#41 (32KB) + Core L#41 + PU L#41 (P#41)\n        L2 L#42 (512KB) + L1d L#42 (32KB) + L1i L#42 (32KB) + Core L#42 + PU L#42 (P#42)\n        L2 L#43 (512KB) + L1d L#43 (32KB) + L1i L#43 (32KB) + Core L#43 + PU L#43 (P#43)\n      L3 L#11 (16MB)\n        L2 L#44 (512KB) + L1d L#44 (32KB) + L1i L#44 (32KB) + Core L#44 + PU L#44 (P#44)\n        L2 L#45 (512KB) + L1d L#45 (32KB) + L1i L#45 (32KB) + Core L#45 + PU L#45 (P#45)\n        L2 L#46 (512KB) + L1d L#46 (32KB) + L1i L#46 (32KB) + Core L#46 + PU L#46 (P#46)\n        L2 L#47 (512KB) + L1d L#47 (32KB) + L1i L#47 (32KB) + Core L#47 + PU L#47 (P#47)\n    Group0 L#3\n      NUMANode L#3 (P#3 31GB)\n      L3 L#12 (16MB)\n        L2 L#48 (512KB) + L1d L#48 (32KB) + L1i L#48 (32KB) + Core L#48 + PU L#48 (P#48)\n        L2 L#49 (512KB) + L1d L#49 (32KB) + L1i L#49 (32KB) + Core L#49 + PU L#49 (P#49)\n        L2 L#50 (512KB) + L1d L#50 (32KB) + L1i L#50 (32KB) + Core L#50 + PU L#50 (P#50)\n        L2 L#51 (512KB) + L1d L#51 (32KB) + L1i L#51 (32KB) + Core L#51 + PU L#51 (P#51)\n      L3 L#13 (16MB)\n        L2 L#52 (512KB) + L1d L#52 (32KB) + L1i L#52 (32KB) + Core L#52 + PU L#52 (P#52)\n        L2 L#53 (512KB) + L1d L#53 (32KB) + L1i L#53 (32KB) + Core L#53 + PU L#53 (P#53)\n        L2 L#54 (512KB) + L1d L#54 (32KB) + L1i L#54 (32KB) + Core L#54 + PU L#54 (P#54)\n        L2 L#55 (512KB) + L1d L#55 (32KB) + L1i L#55 (32KB) + Core L#55 + PU L#55 (P#55)\n      L3 L#14 (16MB)\n        L2 L#56 (512KB) + L1d L#56 (32KB) + L1i L#56 (32KB) + Core L#56 + PU L#56 (P#56)\n        L2 L#57 (512KB) + L1d L#57 (32KB) + L1i L#57 (32KB) + Core L#57 + PU L#57 (P#57)\n        L2 L#58 (512KB) + L1d L#58 (32KB) + L1i L#58 (32KB) + Core L#58 + PU L#58 (P#58)\n        L2 L#59 (512KB) + L1d L#59 (32KB) + L1i L#59 (32KB) + Core L#59 + PU L#59 (P#59)\n      L3 L#15 (16MB)\n        L2 L#60 (512KB) + L1d L#60 (32KB) + L1i L#60 (32KB) + Core L#60 + PU L#60 (P#60)\n        L2 L#61 (512KB) + L1d L#61 (32KB) + L1i L#61 (32KB) + Core L#61 + PU L#61 (P#61)\n        L2 L#62 (512KB) + L1d L#62 (32KB) + L1i L#62 (32KB) + Core L#62 + PU L#62 (P#62)\n        L2 L#63 (512KB) + L1d L#63 (32KB) + L1i L#63 (32KB) + Core L#63 + PU L#63 (P#63)\n  Package L#1\n    Group0 L#4\n      NUMANode L#4 (P#4 31GB)\n      L3 L#16 (16MB)\n        L2 L#64 (512KB) + L1d L#64 (32KB) + L1i L#64 (32KB) + Core L#64 + PU L#64 (P#64)\n        L2 L#65 (512KB) + L1d L#65 (32KB) + L1i L#65 (32KB) + Core L#65 + PU L#65 (P#65)\n        L2 L#66 (512KB) + L1d L#66 (32KB) + L1i L#66 (32KB) + Core L#66 + PU L#66 (P#66)\n        L2 L#67 (512KB) + L1d L#67 (32KB) + L1i L#67 (32KB) + Core L#67 + PU L#67 (P#67)\n      L3 L#17 (16MB)\n        L2 L#68 (512KB) + L1d L#68 (32KB) + L1i L#68 (32KB) + Core L#68 + PU L#68 (P#68)\n        L2 L#69 (512KB) + L1d L#69 (32KB) + L1i L#69 (32KB) + Core L#69 + PU L#69 (P#69)\n        L2 L#70 (512KB) + L1d L#70 (32KB) + L1i L#70 (32KB) + Core L#70 + PU L#70 (P#70)\n        L2 L#71 (512KB) + L1d L#71 (32KB) + L1i L#71 (32KB) + Core L#71 + PU L#71 (P#71)\n      L3 L#18 (16MB)\n        L2 L#72 (512KB) + L1d L#72 (32KB) + L1i L#72 (32KB) + Core L#72 + PU L#72 (P#72)\n        L2 L#73 (512KB) + L1d L#73 (32KB) + L1i L#73 (32KB) + Core L#73 + PU L#73 (P#73)\n        L2 L#74 (512KB) + L1d L#74 (32KB) + L1i L#74 (32KB) + Core L#74 + PU L#74 (P#74)\n        L2 L#75 (512KB) + L1d L#75 (32KB) + L1i L#75 (32KB) + Core L#75 + PU L#75 (P#75)\n      L3 L#19 (16MB)\n        L2 L#76 (512KB) + L1d L#76 (32KB) + L1i L#76 (32KB) + Core L#76 + PU L#76 (P#76)\n        L2 L#77 (512KB) + L1d L#77 (32KB) + L1i L#77 (32KB) + Core L#77 + PU L#77 (P#77)\n        L2 L#78 (512KB) + L1d L#78 (32KB) + L1i L#78 (32KB) + Core L#78 + PU L#78 (P#78)\n        L2 L#79 (512KB) + L1d L#79 (32KB) + L1i L#79 (32KB) + Core L#79 + PU L#79 (P#79)\n      HostBridge\n        PCIBridge\n          PCI e1:00.0 (Ethernet)\n            Net \"enp225s0f0\"\n          PCI e1:00.1 (Ethernet)\n            Net \"enp225s0f1\"\n    Group0 L#5\n      NUMANode L#5 (P#5 31GB)\n      L3 L#20 (16MB)\n        L2 L#80 (512KB) + L1d L#80 (32KB) + L1i L#80 (32KB) + Core L#80 + PU L#80 (P#80)\n        L2 L#81 (512KB) + L1d L#81 (32KB) + L1i L#81 (32KB) + Core L#81 + PU L#81 (P#81)\n        L2 L#82 (512KB) + L1d L#82 (32KB) + L1i L#82 (32KB) + Core L#82 + PU L#82 (P#82)\n        L2 L#83 (512KB) + L1d L#83 (32KB) + L1i L#83 (32KB) + Core L#83 + PU L#83 (P#83)\n      L3 L#21 (16MB)\n        L2 L#84 (512KB) + L1d L#84 (32KB) + L1i L#84 (32KB) + Core L#84 + PU L#84 (P#84)\n        L2 L#85 (512KB) + L1d L#85 (32KB) + L1i L#85 (32KB) + Core L#85 + PU L#85 (P#85)\n        L2 L#86 (512KB) + L1d L#86 (32KB) + L1i L#86 (32KB) + Core L#86 + PU L#86 (P#86)\n        L2 L#87 (512KB) + L1d L#87 (32KB) + L1i L#87 (32KB) + Core L#87 + PU L#87 (P#87)\n      L3 L#22 (16MB)\n        L2 L#88 (512KB) + L1d L#88 (32KB) + L1i L#88 (32KB) + Core L#88 + PU L#88 (P#88)\n        L2 L#89 (512KB) + L1d L#89 (32KB) + L1i L#89 (32KB) + Core L#89 + PU L#89 (P#89)\n        L2 L#90 (512KB) + L1d L#90 (32KB) + L1i L#90 (32KB) + Core L#90 + PU L#90 (P#90)\n        L2 L#91 (512KB) + L1d L#91 (32KB) + L1i L#91 (32KB) + Core L#91 + PU L#91 (P#91)\n      L3 L#23 (16MB)\n        L2 L#92 (512KB) + L1d L#92 (32KB) + L1i L#92 (32KB) + Core L#92 + PU L#92 (P#92)\n        L2 L#93 (512KB) + L1d L#93 (32KB) + L1i L#93 (32KB) + Core L#93 + PU L#93 (P#93)\n        L2 L#94 (512KB) + L1d L#94 (32KB) + L1i L#94 (32KB) + Core L#94 + PU L#94 (P#94)\n        L2 L#95 (512KB) + L1d L#95 (32KB) + L1i L#95 (32KB) + Core L#95 + PU L#95 (P#95)\n      HostBridge\n        PCIBridge\n          PCI c3:00.0 (SATA)\n            Block(Disk) \"sda\"\n    Group0 L#6\n      NUMANode L#6 (P#6 31GB)\n      L3 L#24 (16MB)\n        L2 L#96 (512KB) + L1d L#96 (32KB) + L1i L#96 (32KB) + Core L#96 + PU L#96 (P#96)\n        L2 L#97 (512KB) + L1d L#97 (32KB) + L1i L#97 (32KB) + Core L#97 + PU L#97 (P#97)\n        L2 L#98 (512KB) + L1d L#98 (32KB) + L1i L#98 (32KB) + Core L#98 + PU L#98 (P#98)\n        L2 L#99 (512KB) + L1d L#99 (32KB) + L1i L#99 (32KB) + Core L#99 + PU L#99 (P#99)\n      L3 L#25 (16MB)\n        L2 L#100 (512KB) + L1d L#100 (32KB) + L1i L#100 (32KB) + Core L#100 + PU L#100 (P#100)\n        L2 L#101 (512KB) + L1d L#101 (32KB) + L1i L#101 (32KB) + Core L#101 + PU L#101 (P#101)\n        L2 L#102 (512KB) + L1d L#102 (32KB) + L1i L#102 (32KB) + Core L#102 + PU L#102 (P#102)\n        L2 L#103 (512KB) + L1d L#103 (32KB) + L1i L#103 (32KB) + Core L#103 + PU L#103 (P#103)\n      L3 L#26 (16MB)\n        L2 L#104 (512KB) + L1d L#104 (32KB) + L1i L#104 (32KB) + Core L#104 + PU L#104 (P#104)\n        L2 L#105 (512KB) + L1d L#105 (32KB) + L1i L#105 (32KB) + Core L#105 + PU L#105 (P#105)\n        L2 L#106 (512KB) + L1d L#106 (32KB) + L1i L#106 (32KB) + Core L#106 + PU L#106 (P#106)\n        L2 L#107 (512KB) + L1d L#107 (32KB) + L1i L#107 (32KB) + Core L#107 + PU L#107 (P#107)\n      L3 L#27 (16MB)\n        L2 L#108 (512KB) + L1d L#108 (32KB) + L1i L#108 (32KB) + Core L#108 + PU L#108 (P#108)\n        L2 L#109 (512KB) + L1d L#109 (32KB) + L1i L#109 (32KB) + Core L#109 + PU L#109 (P#109)\n        L2 L#110 (512KB) + L1d L#110 (32KB) + L1i L#110 (32KB) + Core L#110 + PU L#110 (P#110)\n        L2 L#111 (512KB) + L1d L#111 (32KB) + L1i L#111 (32KB) + Core L#111 + PU L#111 (P#111)\n    Group0 L#7\n      NUMANode L#7 (P#7 31GB)\n      L3 L#28 (16MB)\n        L2 L#112 (512KB) + L1d L#112 (32KB) + L1i L#112 (32KB) + Core L#112 + PU L#112 (P#112)\n        L2 L#113 (512KB) + L1d L#113 (32KB) + L1i L#113 (32KB) + Core L#113 + PU L#113 (P#113)\n        L2 L#114 (512KB) + L1d L#114 (32KB) + L1i L#114 (32KB) + Core L#114 + PU L#114 (P#114)\n        L2 L#115 (512KB) + L1d L#115 (32KB) + L1i L#115 (32KB) + Core L#115 + PU L#115 (P#115)\n      L3 L#29 (16MB)\n        L2 L#116 (512KB) + L1d L#116 (32KB) + L1i L#116 (32KB) + Core L#116 + PU L#116 (P#116)\n        L2 L#117 (512KB) + L1d L#117 (32KB) + L1i L#117 (32KB) + Core L#117 + PU L#117 (P#117)\n        L2 L#118 (512KB) + L1d L#118 (32KB) + L1i L#118 (32KB) + Core L#118 + PU L#118 (P#118)\n        L2 L#119 (512KB) + L1d L#119 (32KB) + L1i L#119 (32KB) + Core L#119 + PU L#119 (P#119)\n      L3 L#30 (16MB)\n        L2 L#120 (512KB) + L1d L#120 (32KB) + L1i L#120 (32KB) + Core L#120 + PU L#120 (P#120)\n        L2 L#121 (512KB) + L1d L#121 (32KB) + L1i L#121 (32KB) + Core L#121 + PU L#121 (P#121)\n        L2 L#122 (512KB) + L1d L#122 (32KB) + L1i L#122 (32KB) + Core L#122 + PU L#122 (P#122)\n        L2 L#123 (512KB) + L1d L#123 (32KB) + L1i L#123 (32KB) + Core L#123 + PU L#123 (P#123)\n      L3 L#31 (16MB)\n        L2 L#124 (512KB) + L1d L#124 (32KB) + L1i L#124 (32KB) + Core L#124 + PU L#124 (P#124)\n        L2 L#125 (512KB) + L1d L#125 (32KB) + L1i L#125 (32KB) + Core L#125 + PU L#125 (P#125)\n        L2 L#126 (512KB) + L1d L#126 (32KB) + L1i L#126 (32KB) + Core L#126 + PU L#126 (P#126)\n        L2 L#127 (512KB) + L1d L#127 (32KB) + L1i L#127 (32KB) + Core L#127 + PU L#127 (P#127)\n</code></pre> <p>From the output you can see the following in an Aion node.</p> <ul> <li>There are 2 physical sockets in a node (<code>Package</code>).</li> <li>There are 8 virtual sockets (<code>Group0</code>) in a node, 4 per physical socket.</li> <li>There is a single NUMA node with <code>32GB</code> of RAM per virtual socket.</li> <li>There are 4 physical L3 caches per NUMA node.</li> <li>There are 4 cores per L3 cache group.</li> <li>There is a single processor unit (<code>PU</code>), also known as hardware thread, per core.</li> <li>The fast interconnect adaptor (<code>mlx5_0</code>) is attached to virtual socket 0 (<code>Group0 L#0</code>).</li> <li>The local storage (<code>sda</code>) is attache to virtual socket 5 (<code>Group0 L#5</code>).</li> </ul>"},{"location":"jobs/hwloc/#hardware-locality-and-cluster-allocations","title":"Hardware locality and cluster allocations","text":"<p>The hardware locality program is aware of the allocation in the cluster. If you request only part of a node, then hardware locality will only display the allocated resources in the node where it is running. For instance allocate a single node in Iris with 2 tasks. <pre><code>salloc --partition=batch --qos=normal --nodes=1 --ntasks-per-node=2 --cpus-per-task=14\n</code></pre> This allocation command allocates a full socket per job by default. Then load the hardware locality module in the allocation for the job. <pre><code>module load system/hwloc\n</code></pre></p> <p>You can now launch hardware locality in a single task of the allocation. <pre><code>srun --ntasks=1 hwloc-ls\n</code></pre></p> Output of hardware locality for a single task on an Iris CPU job <pre><code>$ srun --ntasks=1 hwloc-ls\nMachine (126GB total)\n  Package L#0\n    NUMANode L#0 (P#0 63GB)\n    L3 L#0 (35MB)\n      L2 L#0 (256KB) + L1d L#0 (32KB) + L1i L#0 (32KB) + Core L#0 + PU L#0 (P#0)\n      L2 L#1 (256KB) + L1d L#1 (32KB) + L1i L#1 (32KB) + Core L#1 + PU L#1 (P#2)\n      L2 L#2 (256KB) + L1d L#2 (32KB) + L1i L#2 (32KB) + Core L#2 + PU L#2 (P#4)\n      L2 L#3 (256KB) + L1d L#3 (32KB) + L1i L#3 (32KB) + Core L#3 + PU L#3 (P#6)\n      L2 L#4 (256KB) + L1d L#4 (32KB) + L1i L#4 (32KB) + Core L#4 + PU L#4 (P#8)\n      L2 L#5 (256KB) + L1d L#5 (32KB) + L1i L#5 (32KB) + Core L#5 + PU L#5 (P#10)\n      L2 L#6 (256KB) + L1d L#6 (32KB) + L1i L#6 (32KB) + Core L#6 + PU L#6 (P#12)\n      L2 L#7 (256KB) + L1d L#7 (32KB) + L1i L#7 (32KB) + Core L#7 + PU L#7 (P#14)\n      L2 L#8 (256KB) + L1d L#8 (32KB) + L1i L#8 (32KB) + Core L#8 + PU L#8 (P#16)\n      L2 L#9 (256KB) + L1d L#9 (32KB) + L1i L#9 (32KB) + Core L#9 + PU L#9 (P#18)\n      L2 L#10 (256KB) + L1d L#10 (32KB) + L1i L#10 (32KB) + Core L#10 + PU L#10 (P#20)\n      L2 L#11 (256KB) + L1d L#11 (32KB) + L1i L#11 (32KB) + Core L#11 + PU L#11 (P#22)\n      L2 L#12 (256KB) + L1d L#12 (32KB) + L1i L#12 (32KB) + Core L#12 + PU L#12 (P#24)\n      L2 L#13 (256KB) + L1d L#13 (32KB) + L1i L#13 (32KB) + Core L#13 + PU L#13 (P#26)\n    HostBridge\n      PCIBridge\n        PCI 01:00.0 (InfiniBand)\n          Net \"ib0\"\n          OpenFabrics \"mlx5_0\"\n      PCIBridge\n        PCIBridge\n          PCIBridge\n            PCIBridge\n              PCI 08:00.0 (VGA)\n      PCI 00:1f.2 (SATA)\n        Block(Disk) \"sda\"\n  Package L#1\n    NUMANode L#1 (P#1 63GB)\n    HostBridge\n      PCIBridge\n        PCI 81:00.0 (Ethernet)\n          Net \"eno1\"\n        PCI 81:00.1 (Ethernet)\n          Net \"eno2\"\n</code></pre> <p>In the output of hardware locality only the cores of the running task are available.</p>"},{"location":"jobs/hwloc/#object-types","title":"Object types","text":"<p>The architectural data extracted by hardware locality are not very useful without any information on how to pin software threads on the processor units (PU) of the compute nodes. Fortunately, hardware locality also provides a distance matrix for the communication latency between processor units. Communication latency is reported at the lowest relevant level of an object type hierarchy.</p> <p>The object types are reported in the verbose output of hardware locality.</p> <pre><code>hwloc-ls --verbose\n</code></pre> <p>In hardware locality interface, object types are an abstraction of the architectural units of organization of the CPU. The finest object type is always the processor unit (<code>PU</code>), also known as hardware thread. Each level in the hierarchy consists of objects of the underlying level.</p> <p>Object types</p> Depth Object Description 0 <code>Machine</code> The compute node. 1 <code>Package</code> The physical socket. 2 <code>Group0</code> A group of cores (level 0); usually this is an architectural artifact like a CPU Complex (CCX) in Zen architectures. 3 <code>L3Cache</code> The L3 cache. 4 <code>L2Cache</code> The L2 cache. 5 <code>L1dCache</code> The L1 data cache. 6 <code>L1iCache</code> The L1 instruction cache. 7 <code>Core</code> The physical CPU core. 8 <code>PU</code> The processor unit; corresponds to hardware threads. <p>There are also special object types that correspond to groups of processor units with uniform access to some resource such as memory channels or peripheral devices such as storage or network cards. For instance NUMA nodes are groups of cores that have access to the same memory channels in Zen2 architecture, and <code>PCIDev</code> is a peripheral PCIe device such as a GPU or network card.</p> <p>Special object types</p> Depth Object Description -3 <code>NUMANode</code> A group of cores with access to the same memory channels -4 <code>PCIBridge</code> A group of cores that have direct access to a PCIe connection. -5 <code>PCIDev</code> A generic PCIe device, such as interconnect cards; connects to a <code>PCIBridge</code>."},{"location":"jobs/hwloc/#relative-communication-latency","title":"Relative communication latency","text":"<p>Hardware locality provides an estimate of the relative latency between processor units. For the reporting purposes, processor units are group to the highest level in the object type hierarchy were communication latency is still uniform within the group; cores are usually grouped into NUMA nodes. To output latency information use the <code>--distances</code> option flag.</p> <pre><code>hwloc-ls --distances\n</code></pre> <p>This option produces a matrix of distances between an object type of the architecture. The unit of the reported values is arbitrary, the important quantity is the ratio between the various values.</p> Distance matrix for Aion compute nodes <pre><code>$ hwloc-ls --distances\nRelative latency matrix (name NUMALatency kind 5) between 8 NUMANodes (depth -3) by logical indexes:\n index     0     1     2     3     4     5     6     7\n     0    10    12    12    12    32    32    32    32\n     1    12    10    12    12    32    32    32    32\n     2    12    12    10    12    32    32    32    32\n     3    12    12    12    10    32    32    32    32\n     4    32    32    32    32    10    12    12    12\n     5    32    32    32    32    12    10    12    12\n     6    32    32    32    32    12    12    10    12\n     7    32    32    32    32    12    12    12    10\n</code></pre> Distance matrix for Iris CPU and GPU compute nodes <pre><code>$ hwloc-ls --distances\nRelative latency matrix (name NUMALatency kind 5) between 2 NUMANodes (depth -3) by logical indexes:\n index     0     1\n     0    10    21\n     1    21    10\n</code></pre> Distance matrix for Iris Bigmem compute nodes <pre><code>$ hwloc-ls --distances\nRelative latency matrix (name NUMALatency kind 5) between 4 NUMANodes (depth -3) by logical indexes:\n index     0     1     2     3\n     0    10    21    21    21\n     1    21    10    21    21\n     2    21    21    10    21\n     3    21    21    21    10\n</code></pre> Bios configuration of Aion nodes <p>There are 2 options in the BIOS for configuring Aion login and compute nodes. We can either configure each NUMA node as its own virtual socket, or group all NUMA nodes of a physical socket into a single virtual socket. The latter option necessitates some extra operations to ensure L3 cache coherency between cores in different NUMA nodes on the same physical socket.</p> <p>This is apparent in the distance matrix for Aion compute nodes.</p> <pre><code>$ hwloc-ls --distances\nRelative latency matrix (name NUMALatency kind 5) between 8 NUMANodes (depth -3) by logical indexes:\n index     0     1     2     3     4     5     6     7\n     0    10    12    12    12    32    32    32    32\n     1    12    10    12    12    32    32    32    32\n     2    12    12    10    12    32    32    32    32\n     3    12    12    12    10    32    32    32    32\n     4    32    32    32    32    10    12    12    12\n     5    32    32    32    32    12    10    12    12\n     6    32    32    32    32    12    12    10    12\n     7    32    32    32    32    12    12    12    10\n</code></pre> <p>Cores in different NUMA nodes within the same socket have a distance of 12 versus cores on the same NUMA node with distance of 10.</p> <p>By disabling the extra synchronization operations for L3, we get faster L3 synchronization within a NUMA node at the expense of slower L3 synchronization across NUMA nodes in the same socket. This affects multithreaded applications as threads rely heavily on caches for fast communication. However, HPC applications use message passing (MPI) parallelism between NUMA nodes which is not affected as much by the cache speed.</p> <p>The situation is different in login nodes where conventional applications usually run. These applications rely heavily in multithreading, so it makes sense to ensure a better average L3 cache synchronization speed at the expense of synchronization speed within NUMA nodes. So in the login nodes physical sockets appear as a single NUMA node.</p> <p>The login nodes have CPUs of the same architecture as compute nodes. You can print the distance matrix (hardware locality is already installed on login nodes, no need for modules).</p> <pre><code>$ hwloc-ls --distances \nRelative latency matrix (name NUMALatency kind 5) between 2 NUMANodes (depth -3) by logical indexes:\n  index     0     1\n      0    10    32\n      1    32    10\n</code></pre> <p>Remember, the absolute value of the number is not important, only ratios between entries of the same matrix.</p>"},{"location":"jobs/hwloc/#resources","title":"Resources","text":"<ol> <li>Hardware Locality (hwloc) documentation</li> </ol>"},{"location":"jobs/interactive/","title":"Interactive Jobs","text":"<p>The <code>interactive</code> (floating) partition (exclusively associated to the <code>debug</code> QOS) is to be used for code development, testing, and debugging.</p> <p>Important</p> <p>Production runs are not permitted in interactive jobs. User accounts are subject to suspension if they are determined to be using the <code>interactive</code> partition and the <code>debug</code> QOS for production computing. In particular, interactive job \"chaining\" is not allowed. Chaining is defined as using a batch script to submit another batch script.</p> <p>You can access the different node classes available using the <code>-C &lt;class&gt;</code> flag (see also List of Slurm features on ULHPC nodes), or (better) through the custom helper functions defined for each category of nodes, i.e. <code>si</code>, <code>si-gpu</code> or <code>si-bigmem</code>:</p> Regular Dual-CPU nodeGPU nodeLarge-Memory node <pre><code>### Quick interative job for the default time\n$ si\n# salloc -p interactive --qos debug -C batch\n\n### Explicitly ask for a skylake node\n$ si -C skylake\n# salloc -p interactive --qos debug -C batch -C skylake\n\n### Use 1 full node for 28 tasks\n$ si --ntasks-per-node 28\n# salloc -p interactive --qos debug -C batch --ntasks-per-node 28\n\n### interactive job for 2 hours\n$ si -t 02:00:00\n# salloc -p interactive --qos debug -C batch -t 02:00:00\n\n### interactive job on 2 nodes, 1 multithreaded tasks per node\n$ si -N 2 --ntasks-per-node 1 -c 4\nsi -N 2 --ntasks-per-node 1 -c 4\n# salloc -p interactive --qos debug -C batch -N 2 --ntasks-per-node 1 -c 4\n</code></pre> <pre><code>### Quick interative job for the default time\n$ si-gpu\n# /!\\ WARNING: append -G 1 to really reserve a GPU\n# salloc -p interactive --qos debug -C gpu -G 1\n\n### (Better) Allocate 1/4 of available CPU cores per GPU to manage\n$ si-gpu -G 1 -c 7\n$ si-gpu -G 2 -c 14\n$ si-gpu -G 4 -c 28\n</code></pre> <pre><code>### Quick interative job for the default time\n$ si-bigmem\n# salloc -p interactive --qos debug -C bigmem\n\n### interactive job with 1 multithreaded task per socket available (4 in total)\n$ si-bigmem --ntasks-per-node 4 --ntasks-per-socket 1 -c 28\n# salloc -p interactive --qos debug -C bigmem --ntasks-per-node 4 --ntasks-per-socket 1 -c 4\n\n### interactive job for 1 task but 512G of memory\n$ si-bigmem --mem 512G\n# salloc -p interactive --qos debug -C bigmem --mem 512G\n</code></pre> <p>If you prefer to rely on the regular <code>srun</code>, the below table proposes the equivalent commands run by the helper scripts <code>si*</code>:</p> Node Type Slurm command regular<code>si [...]</code> <code>salloc -p interactive --qos debug -C batch [...]</code><code>salloc -p interactive --qos debug -C batch,broadwell [...]</code><code>salloc -p interactive --qos debug -C batch,skylake [...]</code> gpu<code>si-gpu [...]</code> <code>salloc -p interactive --qos debug -C gpu    [-C volta[32]] -G 1 [...]</code> bigmem<code>si-bigmem [...]</code> <code>salloc -p interactive --qos debug -C bigmem [...]</code> <p>Impact of Interactive jobs implementation over a floating partition</p> <p>We have recently changed the way interactive jobs are served. Since the <code>interactive</code> partition is no longer dedicated but floating above the other partitions, there is NO guarantee to have an interactive job running if the surrounding partition (<code>batch</code>, <code>gpu</code> or <code>bigmem</code>) is full.</p> <p>However, the backfill scheduling in place together with the partition priority set ensure that interactive jobs will be first served upon resource release.</p>"},{"location":"jobs/long/","title":"Long Jobs","text":"<p>If you are confident that your jobs will last more than 2 days while efficiently using the allocated resources, you can use a long QoS.</p> <pre><code>sbatch --partition={batch|gpu|bigmem} --qos=&lt;cluster&gt;-&lt;partition&gt;-long [...]\n</code></pre> <p>Following EuroHPC/PRACE Recommendations, the <code>long</code> QOS allow for an extended Max walltime (<code>MaxWall</code>) set to 14 days.</p> Node Type Cluster Partition Slurm command regular aion batch <code>sbatch [--account=&lt;project&gt;] --partition=batch  --qos=aion-batch-long  [...]</code> regular iris batch <code>sbatch [--account=&lt;project&gt;] --partition=batch  --qos=iris-batch-long  [--constraint={broadwell,skylake}] [...]</code> gpu (v100) iris gpu <code>sbatch [--account=&lt;project&gt;] --partition=gpu    --qos=iris-gpu-long    --gpus=1 [--constraint=volta{16,32}] [...]</code> gpu (h100) iris hopper <code>sbatch [--account=&lt;project&gt;] --partition=hopper --qos=iris-hopper-long --gpus=1 [...]</code> bigmem iris bigmem <code>sbatch [--account=&lt;project&gt;] --partition=bigmem --qos=iris-bigmem-long [...]</code> <p>Important</p> <p>Be aware however that special restrictions apply to long jobs. In sort, the constraints are the following.</p> <ul> <li>There is a per partition limit to the maximum number of concurrent nodes involved in long jobs (call the alias <code>sqos</code> defined in UL HPC systems for details).</li> <li>In <code>batch</code> partitions no more than 8 long jobs per User (<code>MaxJobsPU</code>) are allowed, using no more than 16 nodes per jobs.</li> <li>In <code>gpu</code> partition no more than 4 long jobs per User (<code>MaxJobsPU</code>) are allowed, using no more than 2 nodes per jobs.</li> <li>In <code>bigmem</code> partition no more than 4 long jobs per User (<code>MaxJobsPU</code>) are allowed, using no more than 2 nodes per jobs.</li> </ul>"},{"location":"jobs/priority/","title":"ULHPC Job Prioritization Factors","text":"<p>The ULHPC Slurm configuration rely on the Multifactor Priority Plugin and the Fair tree algorithm to preform Fairsharing among the users<sup>1</sup></p>"},{"location":"jobs/priority/#priority-factors","title":"Priority Factors","text":"<p>There are several factors enabled on ULHPC supercomputers  that influence job priority:</p> <ul> <li>Age: length of time a job has been waiting (PD state) in the queue</li> <li>Fairshare: difference between the portion of the computing resource that has been promised and the amount of resources that has been consumed - see Fairsharing.</li> <li>Partition: factor associated with each node partition, for instance to privilege <code>interactive</code> over <code>batch</code> partitions</li> <li>QOS A factor associated with each Quality Of Service (<code>low</code> \\longrightarrow <code>urgent</code>)</li> </ul> <p>The job's priority at any given time will be a weighted sum of all the factors that have been enabled. Job priority can be expressed as:</p> <pre><code>Job_priority =\n    PriorityWeightAge       * age_factor +\n    PriorityWeightFairshare * fair-share_factor+\n    PriorityWeightPartition * partition_factor +\n    PriorityWeightQOS       * QOS_factor +\n    - nice_factor\n</code></pre> <p>All of the factors in this formula are floating point numbers that range from 0.0 to 1.0. The weights are unsigned, 32 bit integers, you can get with:   <pre><code>$ sprio -w\n# OR, from slurm.conf\n$ scontrol show config | grep -i PriorityWeight\n</code></pre> You can use the <code>sprio</code> to view the factors that comprise a job's scheduling priority and were your (pending) jobs stands in the priority queue.</p> <p>sprio Utility usage</p> <p>Show current weights <pre><code>sprio -w\n</code></pre> List pending jobs, sorted by jobid <pre><code>sprio [-n]     # OR: sp\n</code></pre> List pending jobs, sorted by priority <pre><code>sprio [-n] -S+Y\nsprio [-n] | sort -k 3 -n\nsprio [-n] -l | sort -k 4 -n\n</code></pre></p> <p>Getting the priority given to a job can be done either with <code>squeue</code>:</p> <pre><code># /!\\ ADAPT &lt;jobid&gt; accordingly\nsqueue -o %Q -j &lt;jobid&gt;\n</code></pre>"},{"location":"jobs/priority/#backfill-scheduling","title":"Backfill Scheduling","text":"<p>Backfill is a mechanism by which lower priority jobs can start earlier to fill the idle slots provided they are finished before the next high priority jobs is expected to start based on resource availability.</p> <p>If your job is sufficiently small, it can be backfilled and scheduled in the shadow of a larger, higher-priority job</p> <p>For more details, see official Slurm documentation</p> <ol> <li> <p>All users from a higher priority account receive a higher fair share factor than all users from a lower priority account\u00a0\u21a9</p> </li> </ol>"},{"location":"jobs/reason-codes/","title":"Job Status and Reason Codes","text":"<p>The <code>squeue</code> command details a variety of information on an active job\u2019s status with state and reason codes. Job state codes describe a job\u2019s current state in queue (e.g. pending, completed). Job reason codes describe the reason why the job is in its current state.</p> <p>The following tables outline a variety of job state and reason codes you may encounter when using squeue to check on your jobs.</p>"},{"location":"jobs/reason-codes/#job-state-codes","title":"Job State Codes","text":"Status Code Explaination CANCELLED <code>CA</code> The job was explicitly cancelled by the user or system administrator. COMPLETED <code>CD</code> The job has completed successfully. COMPLETING <code>CG</code> The job is finishing but some processes are still active. DEADLINE <code>DL</code> The job terminated on deadline FAILED <code>F</code> The job terminated with a non-zero exit code and failed to execute. NODE_FAIL <code>NF</code> The job terminated due to failure of one or more allocated nodes OUT_OF_MEMORY <code>OOM</code> The Job experienced an out of memory error. PENDING <code>PD</code> The job is waiting for resource allocation. It will eventually run. PREEMPTED <code>PR</code> The job was terminated because of preemption by another job. RUNNING <code>R</code> The job currently is allocated to a node and is running. SUSPENDED <code>S</code> A running job has been stopped with its cores released to other jobs. STOPPED <code>ST</code> A running job has been stopped with its cores retained. TIMEOUT <code>TO</code> Job terminated upon reaching its time limit. <p>A full list of these Job State codes can be found in <code>squeue</code> documentation. or <code>sacct</code> documentation.</p>"},{"location":"jobs/reason-codes/#job-reason-codes","title":"Job Reason Codes","text":"Reason Code Explaination <code>Priority</code> One or more higher priority jobs is in queue for running. Your job will eventually run. <code>Dependency</code> This job is waiting for a dependent job to complete and will run afterwards. <code>Resources</code> The job is waiting for resources to become available and will eventually run. <code>InvalidAccount</code> The job\u2019s account is invalid. Cancel the job and rerun with correct account. <code>InvaldQoS</code> The job\u2019s QoS is invalid. Cancel the job and rerun with correct account. <code>QOSGrpCpuLimit</code> All CPUs assigned to your job\u2019s specified QoS are in use; job will run eventually. <code>QOSGrpMaxJobsLimit</code> Maximum number of jobs for your job\u2019s QoS have been met; job will run eventually. <code>QOSGrpNodeLimit</code> All nodes assigned to your job\u2019s specified QoS are in use; job will run eventually. <code>PartitionCpuLimit</code> All CPUs assigned to your job\u2019s specified partition are in use; job will run eventually. <code>PartitionMaxJobsLimit</code> Maximum number of jobs for your job\u2019s partition have been met; job will run eventually. <code>PartitionNodeLimit</code> All nodes assigned to your job\u2019s specified partition are in use; job will run eventually. <code>AssociationCpuLimit</code> All CPUs assigned to your job\u2019s specified association are in use; job will run eventually. <code>AssociationMaxJobsLimit</code> Maximum number of jobs for your job\u2019s association have been met; job will run eventually. <code>AssociationNodeLimit</code> All nodes assigned to your job\u2019s specified association are in use; job will run eventually. <p>A full list of these Job Reason Codes can be found in Slurm\u2019s documentation.</p>"},{"location":"jobs/reason-codes/#running-job-statistics-metrics","title":"Running Job Statistics Metrics","text":"<p>The <code>sstat</code> command allows users to easily pull up status information about their currently running jobs. This includes information about CPU usage, task information, node information, resident set size (RSS), and virtual memory (VM). We can invoke the sstat command as such:</p> <pre><code># /!\\ ADAPT &lt;jobid&gt; accordingly\n$ sstat --jobs=&lt;jobid&gt;\n</code></pre> <p>By default, sstat will pull up significantly more information than what would be needed in the commands default output. To remedy this, we can use the <code>--format</code> flag to choose what we want in our output. A chart of some these variables are listed in the table below:</p> Variable Description <code>avecpu</code> Average CPU time of all tasks in job. <code>averss</code> Average resident set size of all tasks. <code>avevmsize</code> Average virtual memory of all tasks in a job. <code>jobid</code> The id of the Job. <code>maxrss</code> Maximum number of bytes read by all tasks in the job. <code>maxvsize</code> Maximum number of bytes written by all tasks in the job. <code>ntasks</code> Number of tasks in a job. <p>For an example, let's print out a job's average job id, cpu time, max rss, and number of tasks. We can do this by typing out the command:</p> <pre><code># /!\\ ADAPT &lt;jobid&gt; accordingly\nsstat --jobs=&lt;jobid&gt; --format=jobid,cputime,maxrss,ntasks\n</code></pre> <p>A full list of variables that specify data handled by sstat can be found with the <code>--helpformat</code> flag or by visiting the slurm page on <code>sstat</code>.</p>"},{"location":"jobs/reason-codes/#past-job-statistics-metrics","title":"Past Job Statistics Metrics","text":"<p>You can use the custom <code>susage</code> function in <code>/etc/profile.d/slurm.sh</code> to collect statistics information.</p> <pre><code>$ susage -h\nUsage: susage [-m] [-Y] [-S YYYY-MM-DD] [-E YYYT-MM-DD]\n  For a specific user (if accounting rights granted):    susage [...] -u &lt;user&gt;\n  For a specific account (if accounting rights granted): susage [...] -A &lt;account&gt;\nDisplay past job usage summary\n</code></pre> <p>But by default, you should use the <code>sacct</code> command allows users to pull up status information about past jobs. This command is very similar to <code>sstat</code>, but is used on jobs that have been previously run on the system instead of currently running jobs.</p> <pre><code># /!\\ ADAPT &lt;jobid&gt; accordingly\n$ sacct [-X] --jobs=&lt;jobid&gt; [--format=metric1,...]\n# OR, for a user, eventually between a Start and End date\n$ sacct [-X] -u $USER  [-S YYYY-MM-DD] [-E YYYY-MM-DD] [--format=metric1,...]\n# OR, for an account - ADAPT &lt;account&gt; accordingly\n$ sacct [-X] -A &lt;account&gt; [--format=metric1,...]\n</code></pre> <p>Use <code>-X</code> to aggregate the statistics relevant to the job allocation itself, not taking job steps into consideration.</p> <p>The main metrics code you may be interested to review are listed below.</p> Variable Description <code>account</code> Account the job ran under. <code>avecpu</code> Average CPU time of all tasks in job. <code>averss</code> Average resident set size of all tasks in the job. <code>cputime</code> Formatted (Elapsed time * CPU) count used by a job or step. <code>elapsed</code> Jobs elapsed time formated as DD-HH:MM:SS. <code>exitcode</code> The exit code returned by the job script or salloc. <code>jobid</code> The id of the Job. <code>jobname</code> The name of the Job. <code>maxdiskread</code> Maximum number of bytes read by all tasks in the job. <code>maxdiskwrite</code> Maximum number of bytes written by all tasks in the job. <code>maxrss</code> Maximum resident set size of all tasks in the job. <code>ncpus</code> Amount of allocated CPUs. <code>nnodes</code> The number of nodes used in a job. <code>ntasks</code> Number of tasks in a job. <code>priority</code> Slurm priority. <code>qos</code> Quality of service. <code>reqcpu</code> Required number of CPUs <code>reqmem</code> Required amount of memory for a job. <code>reqtres</code> Required Trackable RESources (TRES) <code>user</code> Userna <p>A full list of variables that specify data handled by sacct can be found with the <code>--helpformat</code> flag or by visiting the slurm page on <code>sacct</code>.</p>"},{"location":"jobs/steps/","title":"Job steps","text":""},{"location":"jobs/submit/","title":"Regular Jobs","text":"Node Type Slurm command regular <code>sbatch [-A &lt;project&gt;] -p batch  [--qos {high,urgent}] [-C {broadwell,skylake}] [...]</code> gpu <code>sbatch [-A &lt;project&gt;] -p gpu    [--qos {high,urgent}] [-C volta[32]] -G 1 [...]</code> bigmem <code>sbatch [-A &lt;project&gt;] -p bigmem [--qos {high,urgent}] [...]</code> <p> Main Slurm commands  Resource Allocation guide</p>"},{"location":"jobs/submit/#sbatch-pathtolauncher","title":"<code>sbatch [...] /path/to/launcher</code>","text":"<p><code>sbatch</code> is used to submit a batch launcher script for later execution, corresponding to batch/passive submission mode. The script will typically contain one or more <code>srun</code> commands to launch parallel tasks. Upon submission with <code>sbatch</code>, Slurm will:</p> <ul> <li>allocate resources (nodes, tasks, partition, constraints, etc.)</li> <li>runs a single copy of the batch script on the first allocated node<ul> <li>in particular, if you depend on other scripts, ensure you have refer to them with the complete path toward them.</li> </ul> </li> </ul> <p>When you submit the job, Slurm responds with the job's ID, which will be used to identify this job in reports from Slurm.</p> <pre><code># /!\\ ADAPT path to launcher accordingly\n$ sbatch &lt;path/to/launcher&gt;.sh\nSubmitted batch job 864933\n</code></pre>"},{"location":"jobs/submit/#job-submission-option","title":"Job Submission Option","text":"<p>There are several useful environment variables set be Slurm within an allocated job. The most important ones are detailed in the below table which summarizes the main job submission options offered with <code>{sbatch | srun | salloc} [...]</code>:</p> Command-line option Description Example <code>-N &lt;N&gt;</code> <code>&lt;N&gt;</code> Nodes request <code>-N 2</code> <code>--ntasks-per-node=&lt;n&gt;</code> <code>&lt;n&gt;</code> Tasks-per-node request <code>--ntasks-per-node=28</code> <code>--ntasks-per-socket=&lt;s&gt;</code> <code>&lt;s&gt;</code> Tasks-per-socket request <code>--ntasks-per-socket=14</code> <code>-c &lt;c&gt;</code> <code>&lt;c&gt;</code> Cores-per-task request (multithreading) <code>-c 1</code> <code>--mem=&lt;m&gt;GB</code> <code>&lt;m&gt;</code>GB memory per node request <code>--mem 0</code> <code>-t [DD-]HH[:MM:SS]&gt;</code> Walltime request <code>-t 4:00:00</code> <code>-G &lt;gpu&gt;</code> <code>&lt;gpu&gt;</code> GPU(s) request <code>-G 4</code> <code>-C &lt;feature&gt;</code> Feature request (<code>broadwell,skylake...</code>) <code>-C skylake</code> <code>-p &lt;partition&gt;</code> Specify job partition/queue <code>--qos &lt;qos&gt;</code> Specify job qos <code>-A &lt;account&gt;</code> Specify account <code>-J &lt;name&gt;</code> Job name <code>-J MyApp</code> <code>-d &lt;specification&gt;</code> Job dependency <code>-d singleton</code> <code>--mail-user=&lt;email&gt;</code> Specify email address <code>--mail-type=&lt;type&gt;</code> Notify user by email when certain event types occur. <code>--mail-type=END,FAIL</code> <p>At a minimum a job submission script must include number of nodes, time, type of partition and nodes (resource allocation constraint and features), and quality of service (QOS). If a script does not specify any of these options then a default may be applied. The full list of directives is documented in the man pages for the <code>sbatch</code> command (see. <code>man sbatch</code>).</p> <p>Within a job, you aim at running a certain number of tasks, and Slurm allow for a fine-grain control of the resource allocation that must be satisfied for each task.</p> <p>Beware of Slurm terminology in Multicore Architecture!</p> <p></p> <ul> <li>Slurm Node = Physical node, specified with <code>-N &lt;#nodes&gt;</code><ul> <li>Advice: always explicit number of expected number of tasks per node using <code>--ntasks-per-node &lt;n&gt;</code>. This way you control the node footprint of your job.</li> </ul> </li> <li>Slurm Socket = Physical Socket/CPU/Processor<ul> <li>Advice: if possible, explicit also the number of expected number of tasks per socket (processor) using <code>--ntasks-per-socket &lt;s&gt;</code>.<ul> <li>relations between <code>&lt;s&gt;</code> and <code>&lt;n&gt;</code> must be aligned with the physical NUMA characteristics of the node.</li> <li>For instance on aion nodes, <code>&lt;n&gt; = 8*&lt;s&gt;</code></li> <li>For instance on iris regular nodes, <code>&lt;n&gt;=2*&lt;s&gt;</code> when on iris bigmem nodes, <code>&lt;n&gt;=4*&lt;s&gt;</code>.</li> </ul> </li> </ul> </li> <li>(the most confusing): Slurm CPU = Physical CORE<ul> <li>use <code>-c &lt;#threads&gt;</code> to specify the number of cores reserved per task.</li> <li>Hyper-Threading (HT) Technology is disabled on all ULHPC compute nodes. In particular:<ul> <li>assume #cores = #threads, thus when using <code>-c &lt;threads&gt;</code>, you can safely set <pre><code>OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK:-1} # Default to 1 if SLURM_CPUS_PER_TASK not set\n</code></pre> to automatically abstract from the job context</li> <li>you have interest to match the physical NUMA characteristics of the compute node you're running at (Ex: target 16 threads per socket on Aion nodes (as there are 8 virtual sockets per nodes, 14 threads per socket on Iris regular nodes).</li> </ul> </li> </ul> </li> </ul> <p>The total number of tasks defined in a given job is stored in the <code>$SLURM_NTASKS</code> environment variable.</p> <p>The --cpus-per-task option of srun in Slurm 23.11 and later</p> <p>In the latest versions of Slurm <code>srun</code> inherits the <code>--cpus-per-task</code> value requested by <code>salloc</code> or <code>sbatch</code> by reading the value of <code>SLURM_CPUS_PER_TASK</code>, as for any other option. This behavior may differ from some older versions where special handling was required to propagate the <code>--cpus-per-task</code> option to <code>srun</code>.</p> <p>In case you would like to launch multiple programs in a single allocation/batch script, divide the resources accordingly by requesting resources with <code>srun</code> when launching the process, for instance: <pre><code>srun --cpus-per-task &lt;some of the SLURM_CPUS_PER_TASK&gt; --ntasks &lt;some of the SLURM_NTASKS&gt; [...] &lt;program&gt;\n</code></pre></p> <p>We encourage you to always explicitly specify upon resource allocation the number of tasks you want per node/socket (<code>--ntasks-per-node &lt;n&gt; --ntasks-per-socket &lt;s&gt;</code>), to easily scale on multiple nodes with <code>-N &lt;N&gt;</code>. Adapt the number of threads and the settings to match the physical NUMA characteristics of the nodes</p> AionIris (default Dual-CPU)Iris (Bigmem) <p>16 cores per socket and 8 (virtual) sockets (CPUs) per <code>aion</code> node.</p> <ul> <li><code>{sbatch|srun|salloc|si} [-N &lt;N&gt;] --ntasks-per-node &lt;8n&gt; --ntasks-per-socket &lt;n&gt; -c &lt;thread&gt;</code><ul> <li>Total: <code>&lt;N&gt;</code>\\times 8\\times<code>&lt;n&gt;</code> tasks, each on <code>&lt;thread&gt;</code> threads</li> <li>Ensure <code>&lt;n&gt;</code>\\times<code>&lt;thread&gt;</code>= 16</li> <li>Ex: <code>-N 2 --ntasks-per-node 32 --ntasks-per-socket 4 -c 4</code> (Total: 64 tasks)</li> </ul> </li> </ul> <p>14 cores per socket and 2 sockets (physical CPUs) per regular <code>iris</code>.</p> <ul> <li><code>{sbatch|srun|salloc|si} [-N &lt;N&gt;] --ntasks-per-node &lt;2n&gt; --ntasks-per-socket &lt;n&gt; -c &lt;thread&gt;</code><ul> <li>Total: <code>&lt;N&gt;</code>\\times 2\\times<code>&lt;n&gt;</code> tasks, each on <code>&lt;thread&gt;</code> threads</li> <li>Ensure <code>&lt;n&gt;</code>\\times<code>&lt;thread&gt;</code>= 14</li> <li>Ex: <code>-N 2 --ntasks-per-node 4 --ntasks-per-socket 2  -c 7</code> (Total: 8 tasks)</li> </ul> </li> </ul> <p>28 cores per socket and 4 sockets (physical CPUs) per bigmem <code>iris</code></p> <ul> <li><code>{sbatch|srun|salloc|si} [-N &lt;N&gt;] --ntasks-per-node &lt;4n&gt; --ntasks-per-socket &lt;n&gt; -c &lt;thread&gt;</code><ul> <li>Total: <code>&lt;N&gt;</code>\\times 4\\times<code>&lt;n&gt;</code> tasks, each on <code>&lt;thread&gt;</code> threads</li> <li>Ensure <code>&lt;n&gt;</code>\\times<code>&lt;thread&gt;</code>= 28</li> <li>Ex: <code>-N 2 --ntasks-per-node 8 --ntasks-per-socket 2  -c 14</code> (Total: 16 tasks)</li> </ul> </li> </ul>"},{"location":"jobs/submit/#careful-monitoring-of-your-jobs","title":"Careful Monitoring of your Jobs","text":"<p>Bug</p> <p>DON'T LEAVE your jobs running WITHOUT monitoring them and ensure they are not abusing of the computational resources allocated for you!!!</p> <p> ULHPC Tutorial / Getting Started</p> <p>You will find below several ways to monitor the effective usage of the resources allocated (for running jobs) as well as the general efficiency (Average Walltime Accuracy, CPU/Memory efficiency etc.) for past jobs.</p>"},{"location":"jobs/submit/#joiningmonitoring-running-jobs","title":"Joining/monitoring running jobs","text":""},{"location":"jobs/submit/#sjoin","title":"<code>sjoin</code>","text":"<p>At any moment of time, you can join a running job using the custom helper functions <code>sjoin</code> in another terminal (or another screen/tmux tab/window). The format is as follows:</p> <pre><code>sjoin &lt;jobid&gt; [-w &lt;node&gt;]    # Use &lt;tab&gt; to automatically complete &lt;jobid&gt; among your jobs\n</code></pre> <p>Using <code>sjoin</code> to <code>htop</code> your processes</p> <pre><code># check your running job\n(access)$&gt; sq\n# squeue -u $(whoami)\n   JOBID PARTIT       QOS                 NAME       USER NODE  CPUS ST         TIME    TIME_LEFT PRIORITY NODELIST(REASON)\n 2171206  [...]\n# Connect to your running job, identified by its Job ID\n(access)$&gt; sjoin 2171206     # /!\\ ADAPT &lt;jobid&gt; accordingly, use &lt;TAB&gt; to have it autocatically completed\n# Equivalent of: srun --jobid 2171206 --gres=gpu:0 --pty bash -i\n(node)$&gt; htop # view of all processes\n#               F5: tree view\n#               u &lt;name&gt;: filter by process of &lt;name&gt;\n#               q: quit\n</code></pre> <p>On the [impossibility] to monitor passive GPU jobs over <code>sjoin</code></p> <p>If you use <code>sjoin</code> to join a GPU job, you WON'T be able to see the allocated GPU activity with <code>nvidia-smi</code> and all the monitoring tools provided by NVidia. The reason is that currently, there is no way to perform an over-allocation of a Slurm Generic Resource (GRES) as our GPU cards, that means you can't create (e.g. with <code>sjoin</code> or <code>srun --jobid [...]</code>) job steps with access to GPUs which are bound to another step. To keep <code>sjoin</code> working with gres job, you MUST add \"<code>--gres=none</code>\"</p> <p>You can use a direct connection with <code>ssh &lt;node&gt;</code> or <code>clush -w @job:&lt;jobid&gt;</code> for that (see below) but be aware that confined context is NOT maintained that way and that you will see the GPU processes on all 4 GPU cards.</p>"},{"location":"jobs/submit/#clustershell","title":"ClusterShell","text":"<p>Danger</p> <p>Only for VERY Advanced users!!!. You should know what you are doing when using ClusterShell as you can mistakenly generate a huge amount of remote commands across the cluster which, while they will likely fail, still induce an unexpected load that may disturb the system.</p> <p>ClusterShell is a useful Python package for executing arbitrary commands across multiple hosts. On the ULHPC clusters, it provides a relatively simple way for you to run commands on nodes your jobs are running on, and collect the results.</p> <p>Info</p> <p>You can only <code>ssh</code> to, and therefore run <code>clush</code> on, nodes where you have active/running jobs.</p>"},{"location":"jobs/submit/#nodeset","title":"<code>nodeset</code>","text":"<p>The <code>nodeset</code> command enables the easy manipulation of node sets, as well as node groups, at the command line level. It uses <code>sinfo</code>  underneath but has slightly different syntax. You can use it to ask about node states and nodes your job is running on.</p> <p>The nice difference is you can ask for folded (e.g. <code>iris-[075,078,091-092]</code>) or expanded (e.g. <code>iris-075 iris-078 iris-091 iris-092</code>) forms of the node lists.</p> Command description <code>nodeset -L[LL]</code> List all groups  available <code>nodeset -c [...]</code> show number of nodes in nodeset(s) <code>nodeset -e [...]</code> expand nodeset(s) to separate nodes <code>nodeset -f [...]</code> fold nodeset(s) (or separate nodes) into one nodeset Nodeset expansion and folding nodeset -e (expand)nodeset -f (fold) <pre><code># Get list of nodes with issues\n$ sinfo -R --noheader -o \"%N\"\niris-[005-008,017,161-162]\n# ... and expand that list\n$ sinfo -R --noheader -o \"%N\" | nodeset -e\niris-005 iris-006 iris-007 iris-008 iris-017 iris-161 iris-162\n\n# Actually equivalent of (see below)\n$ nodeset -e @state:drained\n</code></pre> <pre><code># List nodes in IDLE state\n$&gt; sinfo -t IDLE --noheader\ninteractive    up    4:00:00      4   idle iris-[003-005,007]\nlong           up 30-00:00:0      2   idle iris-[015-016]\nbatch*         up 5-00:00:00      1   idle iris-134\ngpu            up 5-00:00:00      9   idle iris-[170,173,175-178,181]\nbigmem         up 5-00:00:00      0    n/a\n\n# make out a synthetic list\n$&gt; sinfo -t IDLE --noheader | awk '{ print $6 }' | nodeset -f\niris-[003-005,007,015-016,134,170,173,175-178,181]\n\n# ... actually done when restricting the column to nodelist only\n$&gt; sinfo -t IDLE --noheader -o \"%N\"\niris-[003-005,007,015-016,134,170,173,175-178,181]\n\n# Actually equivalent of (see below)\n$ nodeset -f @state:idle\n</code></pre> Exclusion / intersection  of nodeset Option Description <code>-x &lt;nodeset&gt;</code> exclude from working set <code>&lt;nodeset&gt;</code> <code>-i &lt;nodeset&gt;</code> intersection from working set with <code>&lt;nodeset&gt;</code> <code>-X &lt;nodeset&gt;</code> (<code>--xor</code>) elements that are in exactly one of the working set and <code>&lt;nodeset&gt;</code> <pre><code># Exclusion\n$&gt; nodeset -f iris-[001-010] -x iris-[003-005,007,015-016]\niris-[001-002,006,008-010]\n# Intersection\n$&gt; nodeset -f iris-[001-010] -i iris-[003-005,007,015-016]\niris-[003-005,007]\n# \"XOR\" (one occurrence only)\n$&gt; nodeset -f iris-[001-010] -x iris-006 -X iris-[005-007]\niris-[001-004,006,008-010]\n</code></pre> <p>The groups useful to you that we have configured are <code>@user</code>, <code>@job</code> and <code>@state</code>.</p> List available groupsUser groupJob groupState group <pre><code>$ nodeset -LLL\n# convenient partition groups\n@batch  iris-[001-168] 168\n@bigmem iris-[187-190] 4\n@gpu    iris-[169-186,191-196] 24\n@interactive iris-[001-196] 196\n# conveniente state groups\n@state:allocated [...]\n@state:idle      [...]\n@state:mixed     [...]\n@state:reserved  [...]\n# your individual jobs\n@job:2252046 iris-076 1\n@job:2252050 iris-[191-196] 6\n# all the jobs under your username\n@user:svarrette iris-[076,191-196] 7\n</code></pre> <p>List expanded node names where you have jobs running <pre><code># Similar to: squeue -h -u $USER -o \"%N\"|nodeset -e\n$ nodeset -e @user:$USER\n</code></pre></p> <p>List folded nodes where your job 1234567 is running (use <code>sq</code> to quickly list your jobs): <pre><code>$ similar to squeue -h -j 1234567 -o \"%N\"\nnodeset -f @job:1234567\n</code></pre></p> <p>List expanded node names that are idle according to slurm <pre><code># Similar to: sinfo -t IDLE -o \"%N\"\nnodeset -e @state:idle\n</code></pre></p>"},{"location":"jobs/submit/#clush","title":"<code>clush</code>","text":"<p><code>clush</code> can run commands on multiple nodes at once for instance to monitor you jobs. It uses the node grouping syntax from [<code>nodeset</code>]((https://clustershell.readthedocs.io/en/latest/tools/nodeset.html) to allow you to run commands on those nodes.</p> <p><code>clush</code> uses <code>ssh</code> to connect to each of these nodes. You can use the <code>-b</code> option to gather output from nodes with same output into the same lines. Leaving this out will report on each node separately.</p> Option Description <code>-b</code> gathering output (as when piping to <code>dshbak -c</code>) <code>-w &lt;nodelist&gt;</code> specify remote hosts, incl. node groups with <code>@group</code> special syntax <code>-g &lt;group&gt;</code> similar to <code>-w @&lt;group&gt;</code>, restrict commands to the hosts group <code>&lt;group&gt;</code> <code>--diff</code> show differences between common outputs Monitor CPU usageMonitor GPU usage <p>Show %cpu, memory usage, and command for all nodes running any of your jobs. <pre><code>clush -bw @user:$USER ps -u$USER -o%cpu,rss,cmd\n</code></pre> As above, but only for the nodes reserved with your job <code>&lt;jobid&gt;</code> <pre><code>clush -bw @job:&lt;jobid&gt; ps -u$USER -o%cpu,rss,cmd\n</code></pre></p> <p>Show what's running on all the GPUs on the nodes associated with your job <code>654321</code>. <pre><code>clush -bw @job:654321 bash -l -c 'nvidia-smi --format=csv --query-compute-apps=process_name,used_gpu_memory'\n</code></pre> As above but for all your jobs (assuming you have only GPU nodes with all GPUs) <pre><code>clush -bw @user:$USER bash -l -c 'nvidia-smi --format=csv --query-compute-apps=process_name,used_gpu_memory'\n</code></pre></p> <p>This may be convenient for passive jobs since the <code>sjoin</code> utility does NOT permit to run <code>nvidia-smi</code> (see explaination). However that way you will see unfortunately ALL processes running on the 4 GPU cards -- including from other users sharing your nodes. It's a known bug, not a feature.</p>"},{"location":"jobs/submit/#pestat-cpumem-usage-report","title":"<code>pestat</code>: CPU/Mem usage report","text":"<p>We have deployed the (excellent) Slurm tool <code>pestat</code> (Processor Element status) of Ole Holm Nielsen that you can use to quickly check the CPU/Memory usage of your jobs. Information deserving investigation (too low/high CPU or Memory usage compared to allocation) will be flagged in Red or Magenta</p> <pre><code>pestat [-p &lt;partition&gt;] [-G] [-f]\n</code></pre> <code>pestat</code> output (official sample output) <p></p>"},{"location":"jobs/submit/#general-guidelines","title":"General Guidelines","text":"<p>As mentionned before, always check your node activity with at least <code>htop</code> on the all allocated nodes to ensure you use them as expected. Several cases might apply to your job workflow:</p> Single Node, single coreSingle Node, multi-coreMulti-node <p>You are dealing with an embarrassingly parallel job campaign and this approach is bad and overload the scheduler unnecessarily. You will also quickly cross the limits set in terms of maximum number of jobs. You must aggregate multiples tasks within a single job to exploit fully a complete node. In particular, you MUST consider using GNU Parallel and our generic GNU launcher <code>launcher.parallel.sh</code>.</p> <p> ULHPC Tutorial / HPC Management of Embarrassingly Parallel Jobs</p> <p>If you asked for more than a core in your job (&gt; 1 tasks, <code>-c &lt;threads&gt;</code> where <code>&lt;threads&gt;</code> &gt; 1), there are 3 typical situations you MUST analysed (and <code>pestat</code> or <code>htop</code> are of great help for that):</p> <ol> <li>You cannot see the expected activity (only 1 core seems to be active at 100%), then you should review your workflow as you are under-exploiting (and thus probably waste) the allocated resources.</li> <li> <ul> <li>For instance on regular <code>iris</code> (resp. <code>aion</code>) node, a CPU load above 28 (resp. 128) is suspect.<ul> <li>Note that we use LBNL Node Health Check (NHC) to automatically drain nodes for which the load exceed twice the core capacity</li> </ul> </li> <li>An analogy for a single core load with the amont of cars possible in a single-lane brige or tunnel is illustrated below (source). Like the bridge/tunnel operator, you'd like your cars/processes to never be waiting, otherwise you are harming the system. Imagine this analogy for the amount of cores available on a computing node to better reporesent the situtation on a single core.</li> </ul> <p>you have the expected activity on the requested cores (Ex: the 28 cores were requested, and <code>htop</code> reports a significant usage of all cores) BUT the CPU load of the system exceed the core capacity of the computing node. That means you are forking too many processes and overloading/harming the systems.</p> <p></p> </li> <li> <p>you have the expected activity on the requested cores and the load match your allocation without harming the system: you're good to go!</p> </li> </ol> <p>If you asked for more than ONE node, ensure that you have consider the following questions.</p> <ol> <li>You are running an MPI job: you generally know what you're doing, YET ensure your followed the single node monitoring checks (<code>htop</code> etc. yet across all nodes) to review your core activity on ALL nodes (see 3. below) . Consider also parallel profilers like Arm Forge</li> <li>You are running an embarrasingly parallel job campaign. You should first ensure you correctly exploit a single node using GNU Parallel before attempting to cross multiple nodes</li> <li>You run a distributed framework able to exploit multiple nodes (typically with a master/slave model as for Spark cluster). You MUST assert that your [slave] processes are really run on the over nodes using</li> </ol> <pre><code># check you running job\n$ sq\n# Join **another** node than the first one listed\n$ sjoin &lt;jobid&gt; -w &lt;node&gt;\n$ htop  # view of all processes\n#               F5: tree view\n#               u &lt;name&gt;: filter by process of &lt;name&gt;\n#               q: quit\n</code></pre>"},{"location":"jobs/submit/#monitoring-past-jobs-efficiency","title":"Monitoring past jobs efficiency","text":"<p>Walltime estimation and Job efficiency</p> <p>By default, none of the regular jobs you submit can exceed a walltime of 2 days (<code>2-00:00:00</code>). You have a strong interest to estimate accurately the walltime of your jobs. While it is not always possible, or quite hard to guess at the beginning of a given job campaign where you'll probably ask for the maximum walltime possible, you should look back as your historical usage for the past efficiency and elapsed time of your previously completed jobs using <code>seff</code> or <code>susage</code> utilities. Update the time constraint <code>[#SBATCH] -t [...]</code> of your jobs accordingly. There are two immediate benefits for you:</p> <ol> <li>Short jobs are scheduled faster, and may even be elligible for backfilling</li> <li>You will be more likely elligible for a raw share upgrade of your user account -- see Fairsharing</li> </ol> <p>The below utilities will help you track the CPU/Memory efficiency (<code>seff</code>) or the Average Walltime Accuracy (<code>susage</code>, <code>sacct</code>) of your past jobs</p>"},{"location":"jobs/submit/#seff","title":"<code>seff</code>","text":"<p>Use <code>seff</code> to double check a past job CPU/Memory efficiency. Below examples should be self-speaking:</p> Good CPU Eff.Good Memory Eff.Good CPU and Memory Eff.[Very] Bad efficiency <pre><code>$ seff 2171749\nJob ID: 2171749\nCluster: iris\nUser/Group: &lt;login&gt;/clusterusers\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 28\nCPU Utilized: 41-01:38:14\nCPU Efficiency: 99.64% of 41-05:09:44 core-walltime\nJob Wall-clock time: 1-11:19:38\nMemory Utilized: 2.73 GB\nMemory Efficiency: 2.43% of 112.00 GB\n</code></pre> <pre><code>$ seff 2117620\nJob ID: 2117620\nCluster: iris\nUser/Group: &lt;login&gt;/clusterusers\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 16\nCPU Utilized: 14:24:49\nCPU Efficiency: 23.72% of 2-12:46:24 core-walltime\nJob Wall-clock time: 03:47:54\nMemory Utilized: 193.04 GB\nMemory Efficiency: 80.43% of 240.00 GB\n</code></pre> <pre><code>$ seff 2138087\nJob ID: 2138087\nCluster: iris\nUser/Group: &lt;login&gt;/clusterusers\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 64\nCPU Utilized: 87-16:58:22\nCPU Efficiency: 86.58% of 101-07:16:16 core-walltime\nJob Wall-clock time: 1-13:59:19\nMemory Utilized: 1.64 TB\nMemory Efficiency: 99.29% of 1.65 TB\n</code></pre> <p>This illustrates a very bad job in terms of CPU/memory efficiency (below 4%), which illustrate a case where basically the user wasted 4 hours of computation while mobilizing a full node and its 28 cores. <pre><code>$ seff 2199497\nJob ID: 2199497\nCluster: iris\nUser/Group: &lt;login&gt;/clusterusers\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 28\nCPU Utilized: 00:08:33\nCPU Efficiency: 3.55% of 04:00:48 core-walltime\nJob Wall-clock time: 00:08:36\nMemory Utilized: 55.84 MB\nMemory Efficiency: 0.05% of 112.00 GB\n</code></pre> This is typical of a single-core task can could be drastically improved via GNU Parallel.</p> <p>Note however that demonstrating a CPU good efficiency with <code>seff</code> may not be enough! You may still induce an abnormal load on the reserved nodes if you spawn more processes than allowed by the Slurm reservation. To avoid that, always try to prefix your executions with <code>srun</code> within your launchers. See also Specific Resource Allocations.</p>"},{"location":"jobs/submit/#susage","title":"<code>susage</code>","text":"<p>Use <code>susage</code> to check your past jobs walltime accuracy (<code>Timelimit</code> vs. <code>Elapsed</code>)</p> <pre><code>$ susage -h\nUsage: susage [-m] [-Y] [-S YYYY-MM-DD] [-E YYYT-MM-DD]\n  For a specific user (if accounting rights granted):    susage [...] -u &lt;user&gt;\n  For a specific account (if accounting rights granted): susage [...] -A &lt;account&gt;\nDisplay past job usage summary\n</code></pre> <p>In all cases, if you are confident that your jobs will last more than 2 days while efficiently using the allocated resources, you can use <code>--qos long</code> QOS. Be aware that special restrictions applies for this kind of jobs.</p>"},{"location":"policies/aup/","title":"Acceptable Use Policy (AUP) 2.1","text":"<p>The University of Luxembourg operates since 2007 a large academic HPC facility which remains the reference implementation within the country, offering a cutting-edge research infrastructure to Luxembourg public research while serving as edge access to the upcoming Euro-HPC Luxembourg supercomputer. The University extends access to its HPC resources (including facilities, services and HPC experts) to its students, staff, research partners (including scientific staff of national public organizations and external partners for the duration of joint research projects) and to industrial partners.</p>"},{"location":"policies/aup/#ul-hpc-aup","title":"UL HPC AUP","text":"<p>There are a number of policies which apply to ULHPC users.</p> <p> UL HPC Acceptable Use Policy (AUP) [pdf] </p> <p>Important</p> <p>All users of UL HPC resources and PIs must abide by the UL HPC Acceptable Use Policy (AUP). You should read and keep a signed copy of this document before using the facility.</p> <p>Access and/or usage of any ULHPC system assumes the tacit acknowledgement to this policy.</p> <p>The purpose of this document is to define the rules and terms governing acceptable use of resources (core hours, license hours, data storage capacity as well as network connectivity and technical support), including access, utilization and security of the resources and data.</p>"},{"location":"policies/aup/#crediting-ulhpc-in-your-research","title":"Crediting ULHPC in your research","text":"<p>One of the requirements stemming from the AUP, is to credit and acknowledge the usage of the University of Luxembourg HPC facility for ALL publications and contributions having results and/or contents obtained or derived from that usage.</p>"},{"location":"policies/aup/#publication-tagging","title":"Publication tagging","text":"<p>You are also requested to tag the publication(s) you have produced thanks to the usage of the UL HPC platform upon their registration on Orbilu:</p> <ul> <li>Login on MyOrbiLu</li> <li>Select your publication entry and click on the \"Edit\" button</li> <li>Select the \"2. Enrich\" category at the top of the page</li> <li>In the \"Research center\" field, enter \"ulhpc\" and select the proposition</li> </ul> <p></p> <p>This tag is a very important indicator for us to quantify the concrete impact of the HPC facility on the research performed at the University.</p> <p>  List of publications generated thanks to the UL HPC Platform</p>"},{"location":"policies/maintenance/","title":"Maintenance and Downtime Policy","text":""},{"location":"policies/maintenance/#scheduled-maintenance","title":"Scheduled Maintenance","text":"<p>The ULHPC team  will schedule maintenance in one of three manners:</p> <ol> <li>Rolling reboots    Whenever possible, ULHPC will apply updates and do other maintenance in a rolling fashion in such a manner as to have either no or as little impact as possible to ULHPC services</li> <li>Partial outages    We will do these as needed but in a manner that impacts only some ULHPC services at a time</li> <li>Full outages    These are outages that will affect all ULHPC services, such as outages of core datacenter networking services,  datacenter power of HVAC/cooling system maintenance or global GPFS/Spectrumscale filesystem updates.    Such maintenance windows typically happen on a quarterly basis.    It should be noted that we are not always able to anticipate when these outages are needed.</li> </ol> <p>ULHPC's goal for these downtimes is to have them completed as fast as possible.    However, validation and qualification of the full platform takes typically one working day, and unforeseen or unusual circumstances may occur. So count for such outages a multiple-day downtime.</p>"},{"location":"policies/maintenance/#notifications","title":"Notifications","text":"<p>We normally inform users of cluster maintenance at least 3 weeks in advance by mail using the HPC User community mailing list (moderated): <code>hpc-users@uni.lu</code>. A second reminder is sent a few days prior to actual downtime.</p> <p>The news of the downtimes is also posted on the hpc/support/infra issue tracker/.</p> <p>A colored \"message of the day\" (motd) banner is displayed on all access/login servers such that you can quickly be informed of any incoming maintenance operation upon connection to the cluster. You can see this when you login or (again),any time by issuing the command:</p> <pre><code>cat /etc/motd\n</code></pre> <p>Detecting maintenance... During the maintenance</p> <ul> <li>During the maintenance period, access to the involved cluster access/login serveur is DENIED and any users still logged-in are disconnected at the beginning of the maintenance<ul> <li>you will receive a written message in your terminal</li> <li>if for some reason during the maintenance you urgently need to collect data from your account, please contact the UL HPC Team by sending a mail to: <code>hpc-team@uni.lu</code>.</li> </ul> </li> <li>We will notify you of the end of the maintenance with a summary of the performed operations.</li> </ul>"},{"location":"policies/maintenance/#exceptional-emergency-maintenance","title":"Exceptional \"EMERGENCY\"  maintenance","text":"<p>Unscheduled downtimes can occur for any number of reasons, including:</p> <ul> <li>Loss of cooling and/or power in the data center.</li> <li>Loss of supporting infrastructure (i.e. hardware).</li> <li>Critical need to make changes to hardware or software that negatively impacts performance or access.</li> <li>Application of critical patches that can't wait until the next scheduled maintenance.</li> <li>For safety or security issues that require immediate action.</li> </ul> <p>We will try to notify users in the advent of such event by email.</p> <p>Danger</p> <p>The ULHPC team reserves the right to intervene in user activity without notice when such activity may destabilize the platform and/or is at the expense of other users, and/or to monitor/verify/debug ongoing system activity.</p>"},{"location":"policies/passwords/","title":"Password Policy","text":""},{"location":"policies/passwords/#password-and-account-protection","title":"Password and Account Protection","text":"<p>A user is given a username (also known as a login name) and associated password that permits her/him to access ULHPC resources.  This username/password pair may be used by a single individual only: passwords must not be shared with any other person. Users who share their passwords will have their access to ULHPC disabled.</p> <p>Do not confuse your UL[HPC] password/passphrase and your SSH passphrase</p> <p>We sometimes receive requests to reset your SSH passphrase, which is something you control upon SSH key generation - see SSH documentation.</p> <p>Passwords must be changed as soon as possible after exposure or suspected compromise. Exposure of passwords and suspected compromises must immediately be reported to ULHPC and the University CISO (see below). In all cases, recommendations for the creation of strong passwords is proposed below.</p>"},{"location":"policies/passwords/#password-manager","title":"Password Manager","text":"<p>You are strongly encouraged also to rely on password manager applications to store your different passwords. You may want to use your browser embedded solution but it's not the safest option. Here is a list of recommended applications:</p> <ul> <li>BitWarden - free with no limits ($10 per year for families) - Github</li> <li>Dashlane - free for up to 50 passwords - 40\u20ac per year for premium (60\u20ac for families)</li> <li>LastPass</li> <li>NordPass - free version limited to one device with unlimited number of passwords; 36$ per year for premium plan</li> <li>1Password - paid version only (yet worth it) with 30-day free trial, 36$ per year (60$ for families)</li> <li>Self-Hosted solutions:<ul> <li>KeepassXC</li> <li><code>pass</code>: the Standard Unix Password Manager.     </li> </ul> </li> </ul>"},{"location":"policies/passwords/#forgotten-passwords","title":"Forgotten Passwords","text":"<p>If you forget your password or if it has recently expired, you can simply contact us to initiate the process of resetting your password.</p>"},{"location":"policies/passwords/#login-failures","title":"Login Failures","text":"<p>Your login privileges will be disabled if you have several login failures while entering your password on a ULHPC resource. You do not need a new password in this situation. The login failures will be automatically cleared after a couple of minutes. No additional actions are necessary.</p>"},{"location":"policies/passwords/#how-to-change-your-password-on-ipa","title":"How To Change Your Password on IPA","text":"<p>See IPA documentation</p> <p>Tip</p> <p>Passwords must be changed under any one of the following circumstances:</p> <ul> <li>Immediately after someone else has obtained your password (do NOT give your password to anyone else).</li> <li>As soon as possible, but at least within one business day after a password has been compromised or after you suspect that a password has been compromised.</li> <li>On direction from ULHPC staff, or by IPA password policy requesting to frequently change your password.</li> </ul> <p>Your new password must adhere to ULHPC's password requirements.</p>"},{"location":"policies/passwords/#password-requirements-and-guidelines","title":"Password Requirements and Guidelines","text":"<p>One of the potentially weakest links in computer security is the individual password. Despite the University's and ULHPC's efforts to keep hackers out of your personal files and away from University resources (e.g., email, web files, licensed software), easily-guessed passwords are still a big problem so you should really pay attention to the following guidelines and recommendations.</p> <p>Recently, the National Institute of Standards and Technology (NIST) has updated their Digital Identity Guidelines in Special Publication 800-63B. We have updated our password policy to bring it in closer alignment with this guidelines. In particular, the updated guidance is counter to the long-held philosophy that passwords must be long and complex. In contrast, the new guidelines recommend that passwords should be \"easy to remember\" but \"hard to guess\", allowing for usability and security to go hand-in-hand. Inpired with other password policies and guidelines (Stanford, NERSC), ULHPC thus recommends the usage of  \"pass phrases\" instead of passwords. Pass phrases are longer, but easier to remember than complex passwords, and if well-chosen can provide better protection against hackers. In addition, the following rules based on password length and usage of Multi-Factor Authentication (MFA) must be satisfied:</p> <ul> <li>The enforced minimum length for accounts with MFA enabled is 8 characters. If MFA is not enabled for your account the minimum password length is 14 characters.</li> <li>The ability to use all special characters according to the following guidelines (see also the Stanford Password Requirements Quick Guide) depending on the password length:<ul> <li>8-11: mixed case letters, numbers, &amp; symbols</li> <li>12-15: mixed case letters &amp; numbers</li> <li>16-19: mixed case letters</li> <li>20+: no restrictions</li> <li>illustrating image</li> </ul> </li> <li>Restrict sequential and repetitive characters (e.g. <code>12345</code> or <code>aaaaaa</code>)</li> <li>Restrict context specific passwords (e.g. the name of the site, etc.)</li> <li>Restrict commonly used passwords (e.g. <code>p@ssw0rd</code>, etc.) and dictionary words</li> <li>Restrict passwords obtained from previous breach corpuses</li> <li>Passwords must be changed every six months.</li> </ul> <p>If you are struggling to come up with a good password, you can inspire from the following approach:</p> Creating a pass phrase (source: Stanford password  policy) <p>A pass phrase is basically just a series of words, which can include spaces, that you employ instead of a single pass \"word.\" Pass phrases should be at least 16 to 25 characters in length (spaces count as characters), but no less. Longer is better because, though pass phrases look simple, the increased length provides so many possible permutations that a standard password-cracking program will not be effective. It is always a good thing to disguise that simplicity by throwing in elements of weirdness, nonsense, or randomness. Here, for example, are a couple pass phrase candidates:</p> <p>pizza with crispy spaniels</p> <p>mangled persimmon therapy</p> <p>Punctuate and capitalize your phrase:</p> <p>Pizza with crispy Spaniels!</p> <p>mangled Persimmon Therapy?</p> <p>Toss in a few numbers or symbols from the top row of the keyboard, plus some deliberately misspelled words, and you'll create an almost unguessable key to your account:</p> <p>Pizza w/ 6 krispy Spaniels!</p> <p>mangl3d Persimmon Th3rapy?</p>"},{"location":"policies/usage-charging/","title":"ULHPC Usage Charging Policy","text":"<p>The advertised prices are for internal partners only</p> <p>The price list and all other information of this page are meant for internal partners, i.e., not for external companies.  If you are not an internal partner, please contact us at hpc-partnership@uni.lu. Alternatively, you can contact LuxProvide, the national HPC center which aims at serving the private sector for HPC needs.</p>"},{"location":"policies/usage-charging/#how-to-estimate-hpc-costs-for-projects","title":"How to estimate HPC costs for projects?","text":"<p>You can use the following excel document to estimate the cost of your HPC usage:</p> <p> UL HPC Cost Estimates for Project Proposals [xlsx] </p> <p>Note that there are two sheets offering two ways to estimate based on your specific situation. Please read the red sections to ensure that you are using the correct estimation sheet.</p> <p>Note that even if you plan for large-scale experiments on PRACE/EuroHPC supercomputers through computing credits granted by Call for Proposals for Project Access, you should plan for ULHPC costs since you will have to demonstrate the scalability of your code -- the University's facility is ideal for that. You can contact hpc-partnership@uni.lu for more details about this.</p>"},{"location":"policies/usage-charging/#hpc-price-list-2022-10-01","title":"HPC price list - 2022-10-01","text":"<p>Note that ULHPC price list has been updated, see below.</p>"},{"location":"policies/usage-charging/#compute","title":"Compute","text":"Compute type Description \u20ac (excl. VAT) / node-hour CPU - small 28 cores, 128 GB RAM 0.25\u20ac CPU - regular 128 cores, 256 GB RAM 1.25\u20ac CPU - big mem 112 cores, 3 TB RAM 6.00\u20ac GPU 4 V100, 28 cores, 768 GB RAM 5.00\u20ac <p>The prices above correspond to a full-node cost. However, jobs can use a fraction of a node and the price of the job will be computed based on that fraction. Please find below the core-hour / GPU-hour costs and how we compute how much to charge:</p> Compute type Unit \u20ac (excl. VAT) CPU - small Core-hour 0.0089\u20ac CPU - regular Core-hour 0.0097\u20ac CPU - big mem Core-hour 0.0535\u20ac GPU GPU-hour 1.25\u20ac <p>For CPU nodes, the fraction correspond to the number of requested cores, e.g. 64 cores on a CPU - regular node corresponds to 50% of the available cores and thus will be charged 50% of 1.25\u20ac. </p> <p>Regarding the RAM of a job, if you do not override the default behaviour, you will receive a percentage of the RAM corresponding to the amount of requested cores, e.g, 128G of RAM for the 64 cores example from above (50% of a CPU - regular node). If you override the default behaviour and request more RAM, we will re-compute the equivalent number of cores, e.g. if you request 256G of RAM and 64 cores, we will charge 128 cores.</p> <p>For GPU nodes, the fraction considers the number of GPUs. There are 4 GPUs, 28 cores and 768G of RAM on one machine. This means that for each GPU, you can have up to 7 cores and 192G of RAM. If you request more than those default, we will re-compute the GPU equivalent, e.g. if you request 1 GPU and 8 cores, we will charge 2 GPUs.</p>"},{"location":"policies/usage-charging/#storage","title":"Storage","text":"Storage type \u20ac (excl. VAT) / GB / Month Additional information Home Free 500 GB Project 0.02\u20ac 1 TB free Scratch Free 10 TB <p>Note that for project storage, we charge the quota and not the used storage.</p>"},{"location":"policies/usage-charging/#hpc-resource-allocation-for-ul-internal-rd-and-training","title":"HPC Resource allocation for UL internal R&amp;D and training","text":"<p>ULHPC resources are free of charge for UL staff for their internal work and training activities. Principal Investigators (PI) will nevertheless receive on a regular basis a usage report of their team activities on the UL HPC platform. The corresponding accumulated price will be provided even if this amount is purely indicative and won't be charged back.</p> <p>Any other activities will be reviewed with the rectorate and are a priori subjected to be billed.</p>"},{"location":"policies/usage-charging/#submit-project-related-jobs","title":"Submit project related jobs","text":"<p>To allow the ULHPC team to keep track of the jobs related to a project, use the <code>-A &lt;projectname&gt;</code> flag in Slurm, either in the Slurm directives preamble of your script, e.g.,</p> <pre><code>#SBATCH -A myproject\n</code></pre> <p>or on the command line when you submit your job, e.g., <code>sbatch -A myproject /path/to/launcher.sh</code></p>"},{"location":"services/jupyter/","title":"Jupyter","text":"<p>Jupyter is a large umbrella project that covers many different software offerings and tools, including the popular Jupyter Notebook and JupyterLab web-based notebook authoring and editing applications. The Jupyter project and its subprojects all center around providing tools and standards for interactive computing with computational notebooks. The set of free software, open standards, and web services supported by Jupyter can be used to implement interactive computing in notebooks across any programming language.</p> <p>We strongly recommend using the Jupyter application provided through modules whenever possible. The applications for interacting with Jupyter notebooks that are supported in UL HPC systems through modules are the following.</p> <ul> <li> <p>JupyterLab provided by <code>tools/JupyterLab</code>: a web-based interactive development environment for notebooks, code, and data; typically used to develop notebook documents.</p> </li> <li> <p>Jupyter Notebook provided by <code>tools/JupyterNotebook</code>: a notebook authoring application; notebooks are shareable document that combines computer code, plain language descriptions, data, and visualizations.</p> </li> <li> <p>JupyterHub provided by <code>tools/JupyterHub</code>: a multi-user Hub that spawns and manages multiple instances of the single-user Jupyter notebook server; useful for sharing multiple instances of a notebook to a group of users.</p> </li> </ul> <p>Warning</p> <p>Modules are not allowed on the access servers. To test interactively, remember to ask for an interactive job first using  for instance the <code>si</code> tool.</p>"},{"location":"services/jupyter/#notebooks","title":"Notebooks","text":"<p>Notebooks are documents that contain computer code, data, and rich text elements such as normal test, graphical equations, links, figures, and widgets. The main advantage of Notebooks is that the concentrate human-readable analysis, descriptions, and results, together with executable versions of code data in a single document. As a result, notebooks are particularly popular for exploratory data analysis, where they allow the interactive development of reproducible data analytic pipelines. Notebooks can be shared or converted into static HTML documents, and they are thus a powerful teaching tool too.</p>"},{"location":"services/jupyter/#kernels","title":"Kernels","text":"<p>Notebooks are associated with kernels, processes that actually execute the code of the notebook. Jupyter applications provide a default IPython kernel in the environment where Jupyer lab runs. Other environments can export a kernel to a Jupyter application instances, allowing each instance to launch interactive session inside environments others than the environment where Jupyter application is installed.</p> <p>Whenever possible use the Python module to create your Python environments. Modules have been configured for optimal performance in our systems. If your application requires a different version of Python you can always install one with Conda or other tools. To create a Python environment for your kernel, start by loading the Python module and then create the environment.</p> <pre><code>module load lang/Python\npython -m venv ${HOME}/environments/notebook_venv\n</code></pre> <p>Install the packages that you require in your environment, and then install the IPython Kernel for Jupyter (<code>ipykernel</code>) package.</p> <pre><code>module load lang/Python\nsource ${HOME}/environments/notebook_venv/bin/activate\npip install ipykernel\ndeactivate\n</code></pre> <p>The IPython Kernel for Jupyter (<code>ipykernel</code>) provides Jupyter kernels that work with IPython, a toolkit for using Python interactively.</p> <p>You can then install a kernel for your environment; the kernel is used by Jupyter applications to create notebooks for the environment. Jupyter applications provide a default environment with a kernel and also search some default locations for additional kernels. The user default location is selected with the <code>--user</code> option of <code>ipykernel</code> and is located in:</p> <pre><code>${HOME}/.local/share/jupyter/kernels\n</code></pre> <p>With the command,</p> <pre><code>module load lang/Python\nsource ${HOME}/environments/notebook_venv/bin/activate\npython -m ipykernel install --user --name notebook_venv --display-name \"Notebook\"\ndeactivate\n</code></pre> <p>the kernel will be created in the default user location:</p> <pre><code>${HOME}/.local/share/jupyter/kernels/notebook_venv\n</code></pre> <p>and will appear with the name \"Notebook\" in the list of available kernels in all Jupyter applications launched by the user.</p> Kernels in arbitrary directories <p>Kernels can be installed in arbitrary directories. For instance you store kernels in a project directory to share them with other members of your team. In this case use the <code>--prefix</code> option when creating the kernel.</p> <pre><code>module load lang/Python\nsource ${HOME}/environments/notebook_venv/bin/activate\npython -m ipykernel install --prefix=${PROJECTHOME}/project_name/environments/jupyter_env --name notebook_venv --display-name \"Notebook\"\ndeactivate\n</code></pre> <p>To use a kernel from a custom installation path instruct the Jupyter application to search for environments in the extra path with the <code>--notebook-dir</code> option. For instance with the command</p> <pre><code>module load tools/JupyterLab\njupyter lab --notebook-dir=${PROJECTHOME}/project_name/environments/jupyter_env\n</code></pre> <p>the \"Notebook\" will be listed in the available kernels in the Jupyter lab application.</p> Kernels for Conda environments <p>Some packages may require a specific version of Python. In this case install the required Python version in a Conda environment. Then follow the steps above to create the Python environment while using the Python of the Conda environment. For instance, the commands <pre><code>micromamba create --name conda_notebook conda-forge::python=3.8\nmicromamba run --name conda_notebook python -m venv ${HOME}/environments/conda_notebook_venv\nsource ${HOME}/environments/conda_notebook_vemv/bin/activate\npython -m ipykernel install --user --name conda_notebook_venv --display-name \"Conda notebook\"\ndeactivate\n</code></pre> create a kernel for the <code>conda_notebook_venv</code> environment with Python 3.8.</p> <p>Note that Jupyter does not currently support kernels for Conda environments, so you have to create a Python environment (<code>venv</code>) for your kernel.</p>"},{"location":"services/jupyter/#environments-with-site-packages","title":"Environments with site packages","text":"<p>The UL HPC systems offer optimized Python packages for applications such as PyTorch. You can access the optimized packages in your environments if you build your environment with access to system site packages. For instance, to access the PyTorch packages that have been optimized for GPUs in the Iris GPU nodes create the environment for your notebook as follows.</p> <pre><code>module load ai/PyTorch/2.3.0-foss-2023b-CUDA-12.6.0\npython -m venv --system-site-packages ${HOME}/environments/notebook_venv\nsource ${HOME}/environments/notebook_venv/bin/activate\npip install ipykernel\npython -m ipykernel install --user --name notebook_venv --display-name \"Notebook\"\ndeactivate\n</code></pre> <p>With the <code>--system-site-packages</code> flag, the packages provided by the <code>ai/PyTorch/2.3.0-foss-2023b-CUDA-12.6.0</code> module are accessible inside the <code>notebook_venv</code> environment.</p> <p>Using environments with system site packages</p> <p>Before using an environment with system site packages, remember to load the module that provides the system site packages. For instance, in our example, you need to load the PyTorch module <pre><code>module load ai/PyTorch/2.3.0-foss-2023b-CUDA-12.6.0\n</code></pre> before using the \"Notebook\" kernel.</p>"},{"location":"services/jupyter/#working-with-jupyterlab","title":"Working with JupyterLab","text":"<p>JupyterLab is a web-based interactive development environment for notebooks, code, and data. Typically used to develop notebook documents, is highly extensible, and more feature-rich that the traditional Jupyter Notebook. In UL HPC systems Jupyter lab is provided by the <code>tools/JupyterLab</code> module. Load the Jupyter module with the following command.</p> <pre><code>module load tools/JupyterLab\n</code></pre>"},{"location":"services/jupyter/#starting-a-jupyterlab-session","title":"Starting a JupyterLab session","text":"<p>Jupyter notebooks must be started as slurm jobs. The following script is a template for Jupyter submission scripts that will rarely need modifications. Most often you will need to modify the session duration (<code>--time</code> SBATCH option).</p> <p>Slurm Launcher script for Jupyter Notebook</p> <pre><code>#!/usr/bin/bash --login\n#SBATCH --job-name=Jupyter\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=2   # Change accordingly, note that ~1.7GB RAM is proivisioned per core\n#SBATCH --partition=batch\n#SBATCH --qos=normal\n#SBATCH --output=%x_%j.out  # Print messages to 'Jupyter_&lt;job id&gt;.out\n#SBATCH --error=%x_%j.err   # Print debug messages to 'Jupyter_&lt;job id&gt;.err\n#SBATCH --time=0-01:00:00   # Change maximum allowable jupyter server uptime here\n\nprint_error_and_exit() { echo \"***ERROR*** $*\"; exit 1; }\nmodule purge || print_error_and_exit \"No 'module' command\"\n\n# Load the JupyterLab module\nmodule load tools/JupyterLab\n#######################################################################\n# IF THE KERNEL YOU UARE PLANNING TO USE IS USING SITE PACKAGES, THEN #\n# LOAD ANY OTHER MODULES REQUIRED BY THE KERNEL HERE.                 #\n#######################################################################\n\ndeclare loopback_device=\"127.0.0.1\"\ndeclare port=\"8888\"\ndeclare connection_instructions=\"connection_instructions.log\"\n\njupyter lab --ip=${loopback_device} --port=${port} --no-browser &amp;\ndeclare lab_pid=$!\n\n# Add connection instruction\necho \"# Connection instructions\" &gt; \"${connection_instructions}\"\necho \"\" &gt;&gt; \"${connection_instructions}\"\necho \"To access the jupyter notebook execute on your personal machine:\" &gt;&gt; \"${connection_instructions}\"\necho \"ssh -N -J ${USER}@access-${ULHPC_CLUSTER}.uni.lu:8022 -L ${port}:${loopback_device}:${port} ${USER}@$(hostname -i)\" &gt;&gt; \"${connection_instructions}\"\necho \"\" &gt;&gt; \"${connection_instructions}\"\necho \"To access the jupyter notebook if you have setup a special key (e.g ulhpc_id_ed25519) to connect to cluster nodes execute on your personal machine:\" &gt;&gt; \"${connection_instructions}\"\necho \"ssh -i ~/.ssh/hpc_id_ed25519 -N -J ${USER}@access-${ULHPC_CLUSTER}.uni.lu:8022 -L ${port}:${loopback_device}:${port} ${USER}@$(hostname -i)\" &gt;&gt; \"${connection_instructions}\"\necho \"\" &gt;&gt; \"${connection_instructions}\"\necho \"Then navigate to:\" &gt;&gt; \"${connection_instructions}\"\n\n# Wait for the server to start\nsleep 2s\n# Wait and check that the landing page is available\ncurl \\\n    --connect-timeout 10 \\\n    --retry 5 \\\n    --retry-delay 1 \\\n    --retry-connrefused \\\n    --silent --show-error --fail \\\n    \"http://${loopback_device}:${port}\" &gt; /dev/null\n# Note down the URL\njupyter lab list 2&gt;&amp;1 \\\n    | grep -E '\\?token=' \\\n    | awk 'BEGIN {FS=\"::\"} {gsub(\"[ \\t]*\",\"\",$1); print $1}' \\\n    | sed -r 's/([0-9]{1,3}\\.){3}[0-9]{1,3}/127\\.0\\.0\\.1/g' \\\n    &gt;&gt; \"${connection_instructions}\"\n\n# Save some debug information\necho -e '\\n===\\n'\n\necho \"AVAILABLE LABS\"\necho \"\"\njupyter lab list\n\necho -e '\\n===\\n'\n\necho \"CONFIGURATION PATHS\"\necho \"\"\njupyter --paths\n\necho -e '\\n===\\n'\n\necho \"KERNEL SPECIFICATIONS\"\necho \"\"\njupyter kernelspec list\n\n# Wait for the user to terminate the lab\nwait ${lab_pid}\n</code></pre> <p>Once your job is running (see Joining/monitoring running jobs), you can combine </p> <ul> <li><code>ssh</code> forwarding, and</li> <li>an <code>ssh</code> jump through the login node,</li> </ul> <p>to connect to the notebook from your laptop. Open a terminal on your laptop and copy-paste the ssh command contained in the file <code>connection_instructions.log</code>, and then navigate to the webpage link provided.</p> <p>Example content  of <code>connection_instructions.log</code></p> <pre><code>&gt; cat connection_instructions.log\n# Connection instructions\n\nTo access the jupyter notebook execute on your personal machine:\nssh -N -J gkafanas@access-aion.uni.lu:8022 -L 8888:127.0.0.1:8888 gkafanas@172.21.12.29\n\nTo access the jupyter notebook if you have setup a special key (e.g ulhpc_id_ed25519) to connect to cluster nodes execute on your personal machine:\nssh -i ~/.ssh/ulhpc_id_ed25519 -N -J gkafanas@access-aion.uni.lu:8022 -L 8888:127.0.0.1:8888 gkafanas@172.21.12.29\n\nThen navigate to:\nhttp://127.0.0.1:8888/?token=b7cf9d71d5c89627250e9a73d4f28cb649cd3d9ff662e7e2\n</code></pre> <p>As the instructions suggest, you access the jupyter lab server in the compute node by the following SSH command.</p> <pre><code>ssh -N -J gkafanas@access-aion.uni.lu:8022 -L 8888:127.0.0.1:8888 gkafanas@172.21.12.29\n</code></pre> <p>The above command,</p> <ul> <li>opens a connection to your allocated cluster compute node jumping through the login node (<code>-J gkafanas@access-aion.uni.lu:8022 gkafanas@172.21.12.29</code>), and</li> <li>forwards the port of the Jupyter server running in the compute node to your local machine (<code>-L 8888:127.0.0.1:8888</code>).</li> </ul> <p>Then, open the connection to the browser in your local machine by following the given link: <pre><code>http://127.0.0.1:8888/?token=b7cf9d71d5c89627250e9a73d4f28cb649cd3d9ff662e7e2\n</code></pre></p> <p>The link provides the access token, so you should be able to login without a password.</p> <p>Warning</p> <p>Do not forget to click on the <code>quit</code> button when finished to stop the Jupyter server and release the resources. Note that in the last line of the submission script the job waits for your Jupyter service to finish. </p> <p>If you encounter any issues, have a look in the debug output in <code>Jupyter_&lt;job id&gt;.err</code>. Generic information about the setup of your system is printed in <code>Jupyter_&lt;job id&gt;.out</code>.</p> Typical content of <code>Jupyter_&lt;job id&gt;.err</code> <pre><code>&gt; cat Jupyter_3664038.err \n[I 2024-11-13 23:19:52.538 ServerApp] jupyter_lsp | extension was successfully linked.\n[I 2024-11-13 23:19:52.543 ServerApp] jupyter_server_terminals | extension was successfully linked.\n[I 2024-11-13 23:19:52.547 ServerApp] jupyterlab | extension was successfully linked.\n[I 2024-11-13 23:19:52.766 ServerApp] notebook_shim | extension was successfully linked.\n[I 2024-11-13 23:19:52.808 ServerApp] notebook_shim | extension was successfully loaded.\n[I 2024-11-13 23:19:52.812 ServerApp] jupyter_lsp | extension was successfully loaded.\n[I 2024-11-13 23:19:52.813 ServerApp] jupyter_server_terminals | extension was successfully loaded.\n[I 2024-11-13 23:19:52.814 LabApp] JupyterLab extension loaded from /home/users/gkafanas/environments/jupyter_env/lib/python3.11/site-packages/jupyterlab\n[I 2024-11-13 23:19:52.814 LabApp] JupyterLab application directory is /mnt/aiongpfs/users/gkafanas/environments/jupyter_env/share/jupyter/lab\n[I 2024-11-13 23:19:52.815 LabApp] Extension Manager is 'pypi'.\n[I 2024-11-13 23:19:52.826 ServerApp] jupyterlab | extension was successfully loaded.\n[I 2024-11-13 23:19:52.827 ServerApp] Serving notebooks from local directory: /mnt/aiongpfs/users/gkafanas/support/jupyter\n[I 2024-11-13 23:19:52.827 ServerApp] Jupyter Server 2.14.2 is running at:\n[I 2024-11-13 23:19:52.827 ServerApp] http://127.0.0.1:8888/lab?token=fe665f90872927f5f84be627f54cf9056908c34b3765e17d\n[I 2024-11-13 23:19:52.827 ServerApp]     http://127.0.0.1:8888/lab?token=fe665f90872927f5f84be627f54cf9056908c34b3765e17d\n[I 2024-11-13 23:19:52.827 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n[C 2024-11-13 23:19:52.830 ServerApp] \n\n    To access the server, open this file in a browser:\n        file:///home/users/gkafanas/.local/share/jupyter/runtime/jpserver-2253096-open.html\n    Or copy and paste one of these URLs:\n        http://127.0.0.1:8888/lab?token=fe665f90872927f5f84be627f54cf9056908c34b3765e17d\n        http://127.0.0.1:8888/lab?token=fe665f90872927f5f84be627f54cf9056908c34b3765e17d\n[I 2024-11-13 23:19:52.845 ServerApp] Skipped non-installed server(s): bash-language-server, dockerfile-language-server-nodejs, javascript-typescript-langserver, jedi-language-server, julia-language-server, pyright, python-language-server, python-lsp-server, r-languageserver, sql-language-server, texlab, typescript-language-server, unified-language-server, vscode-css-languageserver-bin, vscode-html-languageserver-bin, vscode-json-languageserver-bin, yaml-language-server\n[I 2024-11-13 23:19:53.824 ServerApp] 302 GET / (@127.0.0.1) 0.47ms\n</code></pre> Typical content of <code>Jupyter_&lt;job id&gt;.err</code> <pre><code>&gt; cat Jupyter_3664038.out\n\n===\n\nAVAILABLE LABS\n\nCurrently running servers:\nhttp://127.0.0.1:8888/?token=fe665f90872927f5f84be627f54cf9056908c34b3765e17d :: /mnt/aiongpfs/users/gkafanas/support/jupyter\n\n===\n\nCONFIGURATION PATHS\n\nconfig:\n    /home/users/gkafanas/environments/jupyter_env/etc/jupyter\n    /mnt/aiongpfs/users/gkafanas/.jupyter\n    /usr/local/etc/jupyter\n    /etc/jupyter\ndata:\n    /home/users/gkafanas/environments/jupyter_env/share/jupyter\n    /home/users/gkafanas/.local/share/jupyter\n    /usr/local/share/jupyter\n    /usr/share/jupyter\nruntime:\n    /home/users/gkafanas/.local/share/jupyter/runtime\n\n===\n\nKERNEL SPECIFICATIONS\n\nAvailable kernels:\n  other_python_env    /home/users/gkafanas/environments/jupyter_env/share/jupyter/kernels/other_python_env\n  python3             /home/users/gkafanas/environments/jupyter_env/share/jupyter/kernels/python3 \n</code></pre> <p>Troubleshooting</p> <p>If there is no connection link in the connection instructions, for instance <pre><code>Then navigate to:\n</code></pre> the consider resetting the setup of <code>Jupyter</code> by deleting <code>~/.jupyter</code>.</p>"},{"location":"services/jupyter/#environment-configuration","title":"Environment configuration","text":"<p>Jupyter generates various files during runtime, and reads configuration files from various locations. You can control these paths using environment variables. For instance, you may set the <code>JUPYTER_RUNTIME_DIR</code> to point somewhere in the <code>/tmp</code> directory for better performance.</p>"},{"location":"services/jupyter/#jupyterlab","title":"JupyterLab","text":"<ul> <li><code>JUPYTER_CONFIG_DIR</code>: set the directory for Jupyter config files; default is <code>${HOME}/.jupyter</code>.</li> <li><code>JUPYTER_PATH</code>: extra directories to search for installable data files, such as kernelspecs and notebook extensions; should contain a series of directories, separated by <code>os.pathsep</code> (<code>;</code> on Windows and <code>:</code> on Unix); directories given in <code>JUPYTER_PATH</code> are searched before other locations.</li> <li><code>JUPYTER_DATA_DIR</code>: set the user data directory which contains files such as kernelspecs, notebook extensions, or voila templates; default is <code>${HOME}/.local/share/jupyter/</code> (respects <code>${XDG_DATA_HOME}</code>).</li> <li><code>JUPYTER_RUNTIME_DIR</code>: set the location where Jupyter stores runtime files, such as connection files, which are only useful for the lifetime of a particular process; default is <code>${JUPYTER_DATA_DIR}/runtime</code>.</li> </ul>"},{"location":"services/jupyter/#ipython-kernel-for-jupyter-ipykernel","title":"IPython Kernel for Jupyter (<code>ipykernel</code>)","text":"<ul> <li><code>IPYTHONDIR</code>: path to a directory where IPython will store user data; IPython will create it if it does not exist.</li> </ul>"},{"location":"services/jupyter/#password-protected-access","title":"Password protected access","text":"<p>You can also set a password when launching the Jupyter lab. To setup your password, open an interactive session and load the <code>tools/JupyterLab</code> module. Then execute the following command.</p> <pre><code>$ jupyter lab password\nEnter password:\nVerify password:\n[JupyterPasswordApp] Wrote hashed password to /mnt/aiongpfs/users/gkafanas/.jupyter/jupyter_server_config.json\n</code></pre> <p>To login to a password protected session, navigate direct you browser to the URL <code>http://127.0.0.1:8888/</code> and provide your password. You may have to logout to clear the page cache if you have logged in previously with a token.</p> <p>Info</p> <p>You may have to logout to clear the page cache if you have logged in previously with a token.</p> Typical content of a password protected login page <p></p> Setting up passwords for Jupyter Notebooks <p>You can setup password protected logins in Jupyter notebooks similarly. Load the <code>tools/JupyterNotebook</code> module and execute the following command.</p> <pre><code>$ jupyter notebook password\nEnter password:\nVerify password:\n[JupyterPasswordApp] Wrote hashed password to /mnt/aiongpfs/users/gkafanas/.jupyter/jupyter_server_config.json\n</code></pre> <p>To login to a running notebook, navigate to <code>http://127.0.0.1:8888/</code> and enter your password. You may have to logout to clear the page cache if you have logged in previously with a token.</p>"},{"location":"services/jupyter/#install-jupyter","title":"Install Jupyter","text":"<p>In case you need some features of JupyterLab that are not available in the versions of JupyterLab provided by modules in our systems, you can install JupyterLab in a Python environment.</p> <pre><code>module load lang/Python #Loading default Python\npython -m venv ${HOME}/environments/jupyter_venv\nsource ${HOME}/environments/jupyter_venv/bin/activate\npip install jupyterlab\n</code></pre> Debugging installation issues <p>If the installation of JupyterLab fails, try updating the version of <code>pip</code> in your environment,</p> <pre><code>source ${HOME}/environments/jupyter_venv/bin/activate\npython -m pip install --upgrade pip\n</code></pre> <p>and try installing JupyterLab again.</p> <p>After installing your required version of JupyterLab, you can install your packages and <code>ipykernel</code></p> <ul> <li>either in a separate Python environment, or</li> <li>at the same python environment where JupyterLab is installed.</li> </ul> Using a separate Python environment <p>To use a separate Python environment create a notebook and export its kernel so that the JupyterLab installed in your custom environment can  locate it. Remember, you only need to load the environment with JupyterLab, to launch JupyterLab, not the environment of the kernel.</p> Using the Python environment of JupyterLab <p>Every environment with JupyterLab already contains <code>ipykernel</code> to provide a default notebook. You can install all your packages directly in the <code>jupyter_venv</code> and use the default kernel. Note that in this manner you can only support one environment, and the packages you install in that environment must be compatible with the packages require by JupyterLab.</p> Installing the classic Notebook <p>If you prefer to install the classic notebook, you also need to install the <code>ipykernel</code> manually. So, replace <pre><code>pip install jupyterlab\n</code></pre> with <pre><code>pip install jupyter ipykernel\n</code></pre> in the creation of the <code>jupyter_env</code> Python environment.</p>"},{"location":"slurm/","title":"Slurm Resource and Job Management System","text":"<p>ULHPC uses Slurm (Simple Linux Utility for Resource Management) for cluster/resource management and job scheduling. This middleware is responsible for allocating resources to users, providing a framework for starting, executing and monitoring work on allocated resources and scheduling work for future execution.</p> <p> Official docs  Official FAQ  ULHPC Tutorial/Getting Started</p> <p></p> <p>IEEE ISPDC22: ULHPC Slurm 2.0</p> <p>If you want more details on the RJMS optimizations performed upon Aion acquisition, check out our IEEE ISPDC22 conference paper (21<sup>st</sup> IEEE Int. Symp. on Parallel and Distributed Computing) presented in Basel (Switzerland) on July 13, 2022.</p> <p>IEEE Reference Format | ORBilu entry | slides  Sebastien Varrette, Emmanuel Kieffer, and Frederic Pinel, \"Optimizing the Resource and Job Management System of an Academic HPC and Research Computing Facility\". In 21<sup>st</sup> IEEE Intl. Symp. on Parallel and Distributed Computing (ISPDC\u201922), Basel, Switzerland, 2022.</p>"},{"location":"slurm/#tldr-slurm-on-ulhpc-clusters","title":"TL;DR Slurm on ULHPC clusters","text":"<p>In its concise form, the Slurm configuration in place on ULHPC supercomputers features the following attributes you should be aware of when interacting with it:</p> <ul> <li>Predefined Queues/Partitions depending on node type<ul> <li><code>batch</code>  (Default Dual-CPU nodes) Max: 64 nodes, 2 days walltime</li> <li><code>gpu</code>    (GPU nodes nodes)        Max: 4 nodes, 2 days walltime</li> <li><code>bigmem</code> (Large-Memory nodes)     Max: 1 node, 2 days walltime</li> <li>In addition: <code>interactive</code> (for quicks tests)  Max: 2 nodes, 2h walltime<ul> <li>for code development, testing, and debugging</li> </ul> </li> </ul> </li> <li>Queue Policy: cross-partition QOS, mainly tied to priority level (<code>low</code> \\rightarrow <code>urgent</code>)<ul> <li><code>long</code> QOS with extended Max walltime (<code>MaxWall</code>) set to 14 days</li> <li>special preemptible QOS for best-effort jobs: <code>besteffort</code>.</li> </ul> </li> <li>Accounts hierarchy associated to supervisors (multiple   associations possible), projects or trainings<ul> <li>you MUST use the proper account as a detailed usage   tracking is performed and reported.</li> </ul> </li> <li>Slurm Federation configuration between <code>iris</code> and <code>aion</code><ul> <li>ensures global policy (coherent job ID, global scheduling, etc.) within ULHPC systems</li> <li>easily submit jobs from one cluster to another using <code>-M, --cluster aion|iris</code></li> </ul> </li> </ul> <p>For more details, see the appropriate pages in the left menu.</p>"},{"location":"slurm/#jobs","title":"Jobs","text":"<p>A job is an allocation of resources such as compute nodes assigned to a user for an certain amount of time. Jobs can be interactive or passive (e.g., a batch script) scheduled for later execution.</p> <p>What characterize a job?</p> <p>A user jobs have the following key characteristics:</p> <ul> <li>set of requested resources:<ul> <li>number of computing resources: nodes (including all their CPUs and cores) or CPUs (including all their cores) or cores</li> <li>amount of memory: either per node or per CPU</li> <li>(wall)time needed for the users tasks to complete their work</li> </ul> </li> <li>a requested node partition (job queue)</li> <li>a requested quality of service (QoS) level which grants users specific accesses</li> <li>a requested account for accounting purposes</li> </ul> <p>Once a job is assigned a set of nodes, the user is able to initiate parallel work in the form of job steps (sets of tasks) in any configuration within the allocation.</p> <p>When you login to a ULHPC system you land on a access/login node. Login nodes are only for editing and preparing jobs: They are not meant for actually running jobs. From the login node you can interact with Slurm to submit job scripts or start interactive jobs, which will be further run on the compute nodes.</p>"},{"location":"slurm/#submit-jobs","title":"Submit Jobs","text":"<p>There are three ways of submitting jobs with slurm, using either <code>sbatch</code>, <code>srun</code> or <code>salloc</code>:</p> sbatch (passive job)srun (interactive job)salloc (request allocation/interactive job) <pre><code>### /!\\ Adapt &lt;partition&gt;, &lt;qos&gt;, &lt;account&gt; and &lt;command&gt; accordingly\nsbatch -p &lt;partition&gt; [--qos &lt;qos&gt;] [-A &lt;account&gt;] [...] &lt;path/to/launcher.sh&gt;\n</code></pre> <p><pre><code>### /!\\ Adapt &lt;partition&gt;, &lt;qos&gt;, &lt;account&gt; and &lt;command&gt; accordingly\nsrun -p &lt;partition&gt; [--qos &lt;qos&gt;] [-A &lt;account&gt;] [...] ---pty bash\n</code></pre> <code>srun</code> is also to be using within your launcher script to initiate a job step.</p> <pre><code># Request interactive jobs/allocations\n### /!\\ Adapt &lt;partition&gt;, &lt;qos&gt;, &lt;account&gt; and &lt;command&gt; accordingly\nsalloc -p &lt;partition&gt; [--qos &lt;qos&gt;] [-A &lt;account&gt;] [...] &lt;command&gt;\n</code></pre>"},{"location":"slurm/#sbatch","title":"<code>sbatch</code>","text":"<p><code>sbatch</code> is used to submit a batch launcher script for later execution, corresponding to batch/passive submission mode. The script will typically contain one or more <code>srun</code> commands to launch parallel tasks. Upon submission with <code>sbatch</code>, Slurm will:</p> <ul> <li>allocate resources (nodes, tasks, partition, constraints, etc.)</li> <li>runs a single copy of the batch script on the first allocated node<ul> <li>in particular, if you depend on other scripts, ensure you have refer to them with the complete path toward them.</li> </ul> </li> </ul> <p>When you submit the job, Slurm responds with the job's ID, which will be used to identify this job in reports from Slurm.</p> <pre><code># /!\\ ADAPT path to launcher accordingly\n$ sbatch &lt;path/to/launcher&gt;.sh\nSubmitted batch job 864933\n</code></pre>"},{"location":"slurm/#srun","title":"<code>srun</code>","text":"<p><code>srun</code> is used to initiate parallel job steps within a job OR to start an interactive job Upon submission with <code>srun</code>, Slurm will:</p> <ul> <li>(eventually) allocate resources (nodes, tasks, partition, constraints, etc.) when run for interactive submission</li> <li>launch a job step that will execute on the allocated resources.</li> </ul> <p>A job can contain multiple job steps executing sequentially or in parallel on independent or shared resources within the job's node allocation.</p>"},{"location":"slurm/#salloc","title":"salloc","text":"<p><code>salloc</code> is used to allocate resources for a job in real time. Typically this is used to allocate resources  (nodes, tasks, partition, etc.) and spawn a shell. The shell is then used to execute srun commands to launch parallel tasks.</p>"},{"location":"slurm/#specific-resource-allocation","title":"Specific Resource Allocation","text":"<p>Within a job, you aim at running a certain number of tasks, and Slurm allow for a fine-grain control of the resource allocation that must be satisfied for each task.</p> <p>Beware of Slurm terminology in Multicore Architecture!</p> <p></p> <ul> <li>Slurm Node = Physical node, specified with <code>-N &lt;#nodes&gt;</code><ul> <li>Advice: always explicit number of expected number of tasks per node using <code>--ntasks-per-node &lt;n&gt;</code>. This way you control the node footprint of your job.</li> </ul> </li> <li>Slurm Socket = Physical Socket/CPU/Processor<ul> <li>Advice: if possible, explicit also the number of expected number of tasks per socket (processor) using <code>--ntasks-per-socket &lt;s&gt;</code>.<ul> <li>relations between <code>&lt;s&gt;</code> and <code>&lt;n&gt;</code> must be aligned with the physical NUMA characteristics of the node.</li> <li>For instance on aion nodes, <code>&lt;n&gt; = 8*&lt;s&gt;</code></li> <li>For instance on iris regular nodes, <code>&lt;n&gt;=2*&lt;s&gt;</code> when on iris bigmem nodes, <code>&lt;n&gt;=4*&lt;s&gt;</code>.</li> </ul> </li> </ul> </li> <li>(the most confusing): Slurm CPU = Physical CORE<ul> <li>use <code>-c &lt;#threads&gt;</code> to specify the number of cores reserved per task.</li> <li>Hyper-Threading (HT) Technology is disabled on all ULHPC compute nodes. In particular:<ul> <li>assume #cores = #threads, thus when using <code>-c &lt;threads&gt;</code>, you can safely set <pre><code>OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK:-1} # Default to 1 if SLURM_CPUS_PER_TASK not set\n</code></pre> to automatically abstract from the job context</li> <li>you have interest to match the physical NUMA characteristics of the compute node you're running at (Ex: target 16 threads per socket on Aion nodes (as there are 8 virtual sockets per nodes, 14 threads per socket on Iris regular nodes).</li> </ul> </li> </ul> </li> </ul> <p>The total number of tasks defined in a given job is stored in the <code>$SLURM_NTASKS</code> environment variable.</p> <p>The --cpus-per-task option of srun in Slurm 23.11 and later</p> <p>In the latest versions of Slurm <code>srun</code> inherits the <code>--cpus-per-task</code> value requested by <code>salloc</code> or <code>sbatch</code> by reading the value of <code>SLURM_CPUS_PER_TASK</code>, as for any other option. This behavior may differ from some older versions where special handling was required to propagate the <code>--cpus-per-task</code> option to <code>srun</code>.</p> <p>In case you would like to launch multiple programs in a single allocation/batch script, divide the resources accordingly by requesting resources with <code>srun</code> when launching the process, for instance: <pre><code>srun --cpus-per-task &lt;some of the SLURM_CPUS_PER_TASK&gt; --ntasks &lt;some of the SLURM_NTASKS&gt; [...] &lt;program&gt;\n</code></pre></p> <p>We encourage you to always explicitly specify upon resource allocation the number of tasks you want per node/socket (<code>--ntasks-per-node &lt;n&gt; --ntasks-per-socket &lt;s&gt;</code>), to easily scale on multiple nodes with <code>-N &lt;N&gt;</code>. Adapt the number of threads and the settings to match the physical NUMA characteristics of the nodes</p> AionIris (default Dual-CPU)Iris (Bigmem) <p>16 cores per socket and 8 (virtual) sockets (CPUs) per <code>aion</code> node.</p> <ul> <li><code>{sbatch|srun|salloc|si} [-N &lt;N&gt;] --ntasks-per-node &lt;8n&gt; --ntasks-per-socket &lt;n&gt; -c &lt;thread&gt;</code><ul> <li>Total: <code>&lt;N&gt;</code>\\times 8\\times<code>&lt;n&gt;</code> tasks, each on <code>&lt;thread&gt;</code> threads</li> <li>Ensure <code>&lt;n&gt;</code>\\times<code>&lt;thread&gt;</code>= 16</li> <li>Ex: <code>-N 2 --ntasks-per-node 32 --ntasks-per-socket 4 -c 4</code> (Total: 64 tasks)</li> </ul> </li> </ul> <p>14 cores per socket and 2 sockets (physical CPUs) per regular <code>iris</code>.</p> <ul> <li><code>{sbatch|srun|salloc|si} [-N &lt;N&gt;] --ntasks-per-node &lt;2n&gt; --ntasks-per-socket &lt;n&gt; -c &lt;thread&gt;</code><ul> <li>Total: <code>&lt;N&gt;</code>\\times 2\\times<code>&lt;n&gt;</code> tasks, each on <code>&lt;thread&gt;</code> threads</li> <li>Ensure <code>&lt;n&gt;</code>\\times<code>&lt;thread&gt;</code>= 14</li> <li>Ex: <code>-N 2 --ntasks-per-node 4 --ntasks-per-socket 2  -c 7</code> (Total: 8 tasks)</li> </ul> </li> </ul> <p>28 cores per socket and 4 sockets (physical CPUs) per bigmem <code>iris</code></p> <ul> <li><code>{sbatch|srun|salloc|si} [-N &lt;N&gt;] --ntasks-per-node &lt;4n&gt; --ntasks-per-socket &lt;n&gt; -c &lt;thread&gt;</code><ul> <li>Total: <code>&lt;N&gt;</code>\\times 4\\times<code>&lt;n&gt;</code> tasks, each on <code>&lt;thread&gt;</code> threads</li> <li>Ensure <code>&lt;n&gt;</code>\\times<code>&lt;thread&gt;</code>= 28</li> <li>Ex: <code>-N 2 --ntasks-per-node 8 --ntasks-per-socket 2  -c 14</code> (Total: 16 tasks)</li> </ul> </li> </ul>"},{"location":"slurm/#job-submission-options","title":"Job submission options","text":"<p>There are several useful environment variables set be Slurm within an allocated job. The most important ones are detailed in the below table which summarizes the main job submission options offered with <code>{sbatch | srun | salloc} [...]</code>:</p> Command-line option Description Example <code>-N &lt;N&gt;</code> <code>&lt;N&gt;</code> Nodes request <code>-N 2</code> <code>--ntasks-per-node=&lt;n&gt;</code> <code>&lt;n&gt;</code> Tasks-per-node request <code>--ntasks-per-node=28</code> <code>--ntasks-per-socket=&lt;s&gt;</code> <code>&lt;s&gt;</code> Tasks-per-socket request <code>--ntasks-per-socket=14</code> <code>-c &lt;c&gt;</code> <code>&lt;c&gt;</code> Cores-per-task request (multithreading) <code>-c 1</code> <code>--mem=&lt;m&gt;GB</code> <code>&lt;m&gt;</code>GB memory per node request <code>--mem 0</code> <code>-t [DD-]HH[:MM:SS]&gt;</code> Walltime request <code>-t 4:00:00</code> <code>-G &lt;gpu&gt;</code> <code>&lt;gpu&gt;</code> GPU(s) request <code>-G 4</code> <code>-C &lt;feature&gt;</code> Feature request (<code>broadwell,skylake...</code>) <code>-C skylake</code> <code>-p &lt;partition&gt;</code> Specify job partition/queue <code>--qos &lt;qos&gt;</code> Specify job qos <code>-A &lt;account&gt;</code> Specify account <code>-J &lt;name&gt;</code> Job name <code>-J MyApp</code> <code>-d &lt;specification&gt;</code> Job dependency <code>-d singleton</code> <code>--mail-user=&lt;email&gt;</code> Specify email address <code>--mail-type=&lt;type&gt;</code> Notify user by email when certain event types occur. <code>--mail-type=END,FAIL</code> <p>At a minimum a job submission script must include number of nodes, time, type of partition and nodes (resource allocation constraint and features), and quality of service (QOS). If a script does not specify any of these options then a default may be applied. The full list of directives is documented in the man pages for the <code>sbatch</code> command (see. <code>man sbatch</code>).</p>"},{"location":"slurm/#sbatch-directives-vs-cli-options","title":"<code>#SBATCH</code> directives vs. CLI options","text":"<p>Each option can be specified either as an <code>#SBATCH [...]</code> directive in the job submission script:</p> <pre><code>#!/bin/bash -l                # &lt;--- DO NOT FORGET '-l'\n### Request a single task using one core on one node for 5 minutes in the batch queue\n#SBATCH -N 2\n#SBATCH --ntasks-per-node=1\n#SBATCH -c 1\n#SBATCH --time=0-00:05:00\n#SBATCH -p batch\n# [...]\n</code></pre> <p>Or as a command line option when submitting the script:</p> <pre><code>$ sbatch -p batch -N 2 --ntasks-per-node=1 -c 1 --time=0-00:05:00 ./first-job.sh\n</code></pre> <p>The command line and directive versions of an option are equivalent and interchangeable: if the same option is present both on the command line and as a directive, the command line will be honored. If the same option or directive is specified twice, the last value supplied will be used. Also, many options have both a long form, eg <code>--nodes=2</code> and a short form, eg <code>-N 2</code>. These are equivalent and interchangable.</p> Common options to <code>sbatch</code> and <code>srun</code> <p>Many options are common to both <code>sbatch</code> and <code>srun</code>, for example <code>sbatch -N 4 ./first-job.sh</code> allocates 4 nodes to <code>first-job.sh</code>, and <code>srun -N 4 uname -n</code> inside the job runs a copy of <code>uname -n</code> on each of 4 nodes.</p> <p>If you don't specify an option in the <code>srun</code> command line, <code>srun</code> will inherit the value of that option from  <code>sbatch</code>. In these cases the default behavior of <code>srun</code> is to assume the same options as were passed to <code>sbatch</code>. This is achieved via environment variables: <code>sbatch</code> sets a number of environment variables with names like <code>SLURM_NNODES</code> and srun checks the values of those variables. This has two important consequences:</p> <ol> <li>Your job script can see the settings it was submitted with by    checking these environment variables</li> <li>You should NOT override these environment variables. Also be aware    that if your job script tries to do certain tricky things, such as using    <code>ssh</code> to launch a command on another node, the environment might not    be propagated and your job may not behave correctly</li> </ol>"},{"location":"slurm/#hw-characteristics-and-slurm-features-of-ulhpc-nodes","title":"HW characteristics and Slurm features of ULHPC nodes","text":"<p>When selecting specific resources allocations, it is crucial to match the hardware characteristics of the computing nodes. Details are provided below:</p> Node (type) #Nodes #Socket / #Cores RAM [GB] Features <code>aion-[0001-0354]</code> 354 8 / 128 256 <code>batch,epyc</code> <code>iris-[001-108]</code> 108 2 / 28 128 <code>batch,broadwell</code> <code>iris-[109-168]</code> 60 2 / 28 128 <code>batch,skylake</code> <code>iris-[169-186]</code>   (GPU) 18 2 / 28 768 <code>gpu,skylake,volta</code> <code>iris-[191-196]</code>   (GPU) 6 2 / 28 768 <code>gpu,skylake,volta32</code> <code>iris-[187-190]</code> (Large-Memory) 4 4 / 112 3072 <code>bigmem,skylake</code> <p>As can be seen, Slurm [features] are associated to ULHPC compute nodes and permits to easily filter with the <code>-C &lt;feature&gt;</code> option the list of nodes.</p> <p>To list available features, use <code>sfeatures</code>:</p> <pre><code>sfeatures\n# sinfo  -o '%20N %.6D %.6c %15F %12P %f'\n# NODELIST              NODES   CPUS NODES(A/I/O/T)  PARTITION    AVAIL_FEATURES\n# [...]\n</code></pre> <p>Always try to align resource specifications for your jobs with physical characteristics</p> <p>The typical format of your Slurm submission should thus probably be: <pre><code>sbatch|srun|... [-N &lt;N&gt;] --ntasks-per-node &lt;n&gt; -c &lt;thread&gt; [...]\nsbatch|srun|... [-N &lt;N&gt;] --ntasks-per-node &lt;#sockets * s&gt; --ntasks-per-socket &lt;s&gt; -c &lt;thread&gt; [...]\n</code></pre> This would define a total of <code>&lt;N&gt;</code>\\times<code>&lt;n&gt;</code> TASKS (first form) or <code>&lt;N&gt;</code>\\times \\#sockets \\times<code>&lt;s&gt;</code> TASKS (second form), each on <code>&lt;thread&gt;</code> threads. :octicons-alert: You MUST ensure that either:</p> <ul> <li><code>&lt;n&gt;</code>\\times<code>&lt;thread&gt;</code> matches the number of cores avaiable on the target computing node (first form), or</li> <li><code>&lt;n&gt;</code>=\\#sockets \\times<code>&lt;s&gt;</code>, and <code>&lt;s&gt;</code>\\times<code>&lt;thread&gt;</code> matches the number of cores per socket available on the target computing node (second form).</li> </ul> Aion (default Dual-CPU)Iris (default Dual-CPU)Iris (Large-Memory) <p>16 cores per socket and 8 virtual sockets (CPUs) per <code>aion</code> node. Depending on the selected form, you MUST ensure that either <code>&lt;n&gt;</code>\\times<code>&lt;thread&gt;</code>=128, or that <code>&lt;n&gt;</code>=8<code>&lt;s&gt;</code> and <code>&lt;s&gt;</code>\\times<code>&lt;thread&gt;</code>=16. <pre><code>### Example 1 - use all cores available\n{sbatch|srun|salloc} -N 2 --ntasks-per-node 32 --ntasks-per-socket 4 -c 4 [...]\n# Total: 64 tasks (spread across 2 nodes), each on 4 cores/threads\n\n### Example 2 - use all cores available\n{sbatch|srun|salloc} --ntasks-per-node 128 -c 1  [...]\n# Total; 128 (single-core) tasks\n\n### Example 3 - use all cores available\n{sbatch|srun|salloc} -N 1 --ntasks-per-node 8 --ntasks-per-socket 1 -c 16 [...]\n# Total: 8 tasks, each on 16 cores/threads\n\n### Example 4 - use all cores available\n{sbatch|srun|salloc} -N 1 --ntasks-per-node 2 -c 64 [...]\n# Total: 2 tasks, each on 64 cores/threads\n</code></pre></p> <p>14 cores per socket and 2 sockets (physical CPUs) per regular <code>iris</code> node. Depending on the selected form, you MUST ensure that either <code>&lt;n&gt;</code>\\times<code>&lt;thread&gt;</code>=28, or that <code>&lt;n&gt;</code>=2<code>&lt;s&gt;</code> and <code>&lt;s&gt;</code>\\times<code>&lt;thread&gt;</code>=14. <pre><code>### Example 1 - use all cores available\n{sbatch|srun|salloc} -N 3 --ntasks-per-node 14 --ntasks-per-socket 7 -c 2 [...]\n# Total: 42 tasks (spread across 3 nodes), each on 2 cores/threads\n\n### Example 2 - use all cores available\n{sbatch|srun|salloc} -N 2 --ntasks-per-node 28 -c 1  [...]\n# Total; 56 (single-core) tasks\n\n### Example 3 - use all cores available\n{sbatch|srun|salloc} -N 2 --ntasks-per-node 2 --ntasks-per-socket 1 -c 14 [...]\n# Total: 4 tasks (spread across 2 nodes), each on 14 cores/threads\n</code></pre></p> <p>28 cores per socket and 4 sockets (physical CPUs) per bigmem <code>iris</code> node. Depending on the selected form, you MUST ensure that either <code>&lt;n&gt;</code>\\times<code>&lt;thread&gt;</code>=112, or that <code>&lt;n&gt;</code>=4<code>&lt;s&gt;</code> and <code>&lt;s&gt;</code>\\times<code>&lt;thread&gt;</code>=28. <pre><code>### Example 1 - use all cores available\n{sbatch|srun|salloc} -N 1 --ntasks-per-node 56 --ntasks-per-socket 14 -c 2 [...]\n# Total: 56 tasks on a single bigmem node, each on 2 cores/threads\n\n### Example 2 - use all cores available\n{sbatch|srun|salloc} --ntasks-per-node 112 -c 1  [...]\n# Total; 112 (single-core) tasks\n\n### Example 3 - use all cores available\n{sbatch|srun|salloc} -N 1 --ntasks-per-node 4 --ntasks-per-socket 1 -c 28 [...]\n# Total: 4 tasks, each on 28 cores/threads\n</code></pre></p>"},{"location":"slurm/#using-slurm-environment-variables","title":"Using Slurm Environment variables","text":"<p>Recall that the Slurm controller will set several <code>SLURM_*</code> variables in the environment of the batch script. The most important are listed in the table below - use them wisely to make your launcher script as flexible as possible to abstract and adapt from the allocation context, \"independently\" of the way the job script has been submitted.</p> Submission option Environment variable Typical usage <code>-N &lt;N&gt;</code> <code>SLURM_JOB_NUM_NODES</code> or <code>SLURM_NNODES</code> <code>--ntasks-per-node=&lt;n&gt;</code> <code>SLURM_NTASKS_PER_NODE</code> <code>--ntasks-per-socket=&lt;s&gt;</code> <code>SLURM_NTASKS_PER_SOCKET</code> <code>-c &lt;c&gt;</code> <code>SLURM_CPUS_PER_TASK</code> <code>OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK}</code> <code>SLURM_NTASKS</code> Total number of tasks <code>srun -n $SLURM_NTASKS [...]</code>"},{"location":"slurm/accounts/","title":"Slurm Account Hierarchy","text":"<p>The ULHPC resources can be reserved and allocated for the execution of jobs scheduled on the platform thanks to a Resource and Job Management Systems (RJMS) - Slurm in practice. This tool is configured to collect accounting information for every job and job step executed -- see SchedMD accounting documentation.</p> ULHPC account (login) vs. Slurm [meta-]account <ul> <li> <p>Your ULHPC account defines the UNIX user you can use to connect to the facility and make you known to our systems. They are managed by IPA and define your <code>login</code>.</p> </li> <li> <p>Slurm accounts, refered to as meta-account in the sequel, are more loosely defined in Slurm, and should be seen as something similar to a UNIX group: it may contain other (set of) slurm account(s), multiple users, or just a single user. A user may belong to multiple slurm accounts, but MUST have a DefaultAccount, which is set to your line manager or principal investigator meta-account.</p> </li> </ul>"},{"location":"slurm/accounts/#ulhpc-account-tree-hierarchy","title":"ULHPC Account Tree Hierarchy","text":"<p>Every user job runs under a group account, granting access to specific QOS levels. Such an account is unique within the account hierarchy. Accounting records are organized as a hierarchical tree according to 3 layers (slurm accounts) as depicted in the below figure (click to enlarge). At the leaf hierarchy stands the End user <code>&lt;login&gt;</code> from the IPA IdM database, bringing a total of 4 levels.</p> <p></p> Level Account Type Description Example L1 meta-account Top-level structure / organizations UL, CRP, Externals, Projects, Trainings L2 meta-account Organizational Unit (Faculty, ICs, External partner, Funding program...) FSTM, LCSB, LIST... L3 meta-account Principal investigators (PIs), project, courses/lectures <code>&lt;firstname&gt;.&lt;lastname&gt;</code>, <code>&lt;acronym&gt;</code>, <code>&lt;course&gt;</code> L4 login End-users (staff, student):  your ULHPC/IPA login <code>yourlogin</code> <p>Extracting your association tree</p> <p>By default, you will be able to see only the account hierarchy you belongs too through the association(s) set with your login. You can extract it with:</p> <pre><code>$ sacctmgr show association where parent=root format=\"account,user%20,Share,QOS%50\" withsubaccounts\n   Account                 User     Share                                                QOS\n---------------------- -------- ----------- --------------------------------------------------\n                 &lt;top&gt;            &lt;L1share&gt;                   besteffort,debug,long,low,normal\n             &lt;orgunit&gt;            &lt;L2share&gt;                   besteffort,debug,long,low,normal\n&lt;firstname&gt;.&lt;lastname&gt;            &lt;L3share&gt;                   besteffort,debug,long,low,normal\n&lt;firstname&gt;.&lt;lastname&gt;  &lt;login&gt;   &lt;L4share&gt;                   besteffort,debug,long,low,normal\n</code></pre> (Admins) Extract the full hierarchy <p>The below commands assumes you have supervision rights on the <code>root</code> account.</p> <p>To list available L1 accounts (Top-level structure / organizations), use <pre><code>sacctmgr show association where parent=root format=\"cluster,account,Share,QOS%50\"\n</code></pre> To list L2 accounts:</p> Under Uni.lu (UL)Under CRPUnder ExternalsUnder ProjectsUnder Trainings <pre><code>sacctmgr show association where parent=UL format=\"cluster,account,Share,QOS%50\"\n</code></pre> <pre><code>sacctmgr show association where parent=CRP format=\"cluster,account,Share,QOS%50\"\n</code></pre> <pre><code>sacctmgr show association where parent=externals format=\"cluster,account,Share,QOS%50\"\n</code></pre> <pre><code>sacctmgr show association where parent=projects format=\"cluster,account,Share,QOS%50\"\n</code></pre> <pre><code>sacctmgr show association where parent=trainings format=\"cluster,account,Share,QOS%50\"\n</code></pre> <p>To quickly list L3 accounts and its subaccounts: <code>sassoc &lt;account&gt;</code>, or <pre><code>sacctmgr show association where accounts=&lt;L3account&gt; format=\"account%20,user%20,Share,QOS%50\"\n</code></pre> To quickly list End User (L4) associations, use <code>sassoc &lt;login&gt;</code>, or <pre><code>sacctmgr show association where users=&lt;login&gt; format=\"account%20,user%20,Share,QOS%50\"\n</code></pre></p> <p>Default account vs. multiple associations</p> <p>A given user <code>&lt;login&gt;</code> can be associated to multiple accounts, but have a single <code>DefaultAccount</code> (a meta-account at L3 level reflecting your line manager (Format: <code>&lt;firstname&gt;.&lt;lastname&gt;</code>).</p> <p>To get information about your account information in the hierarchy, use the custom <code>acct</code> helper function, typically as <code>acct $USER</code>.</p> <p>Get ULHPC account information with <code>acct &lt;login&gt;</code></p> <p><pre><code># /!\\ ADAPT &lt;login&gt; accordingly\n$ acct &lt;login&gt;\n# sacctmgr show user where name=\"&lt;login&gt;\" format=user,account%20,DefaultAccount%20,share,qos%50 withassoc\n     User                Account             Def Acct       Share                                     QOS\n  ------- ----------------------- ----------------------  ------- ---------------------------------------\n  &lt;login&gt;         project_&lt;name1&gt; &lt;firstname&gt;.&lt;lastname&gt;        1        besteffort,debug,long,low,normal\n  &lt;login&gt;         project_&lt;name2&gt; &lt;firstname&gt;.&lt;lastname&gt;        1   besteffort,debug,high,long,low,normal\n  &lt;login&gt;  &lt;firstname&gt;.&lt;lastname&gt; &lt;firstname&gt;.&lt;lastname&gt;        1        besteffort,debug,long,low,normal\n# ==&gt; &lt;login&gt; Default account: &lt;firstname&gt;.&lt;lastname&gt;\n</code></pre> In the above example, the user <code>&lt;login&gt;</code> is associated to 3 meta-accounts at the L3 level of the hierarchy (his PI <code>&lt;firstname&gt;.&lt;lastname&gt;</code> and two projects account), each granting access to potentially different QOS. The account used upon job submission can be set with the <code>-A &lt;account&gt;</code> option. With the above example: <pre><code>$ sbatch|srun|... [...]                     # Use default account: &lt;firstname&gt;.&lt;lastname&gt;\n$ sbatch|srun|... -A project_&lt;name1&gt; [...]  # Use account project_&lt;name1&gt;\n$ sbatch|srun|... -A project_&lt;name2&gt; --qos high [...] # Use account project_&lt;name2&gt;, granting access to high QOS\n$ sbatch|srun|... -A anotheraccount [...]   # Error: non-existing association between &lt;login&gt; and anotheraccount\n</code></pre></p> <p>To list all associations for a given user or meta-account, use the <code>sassoc</code> helper function: <pre><code># /!\\ ADAPT &lt;login&gt; accordingly\n$ sassoc &lt;login&gt;\n</code></pre> You may use more classically the <code>sacctmgr show [...]</code> command:</p> <ul> <li>User information: <code>sacctmgr show user where name=&lt;login&gt; [withassoc]</code> (use the <code>withassoc</code> attribute to list all associations).</li> <li>Default account:  <code>sacctmgr show user where name=\"&lt;login&gt;\" format=DefaultAccount -P -n</code></li> <li>Get the parent account: <code>sacctmgr show account where name=ulhpc format=Org -n -P</code></li> </ul> <p>To get the current association tree: add <code>withsubaccounts</code> to see ALL sub accounts</p> <pre><code># L1,L2 or L3 account /!\\ ADAPT &lt;name&gt; accordingly\nsacctmgr show association tree where accounts=&lt;name&gt; format=account,share\n# End user (L4)\nsacctmgr show association where users=$USER  format=account,User,share,Partition,QOS\n</code></pre> No association, no job! <p>It is mandatory to have your login registered within at least one association toward a meta-account (PI, project name) to be able to schedule jobs on the</p>"},{"location":"slurm/accounts/#impact-on-fairsharing-and-job-accounting","title":"Impact on FairSharing and Job Accounting","text":"<p>Every node in the above-mentioned tree hierarchy is associated with a weight defining its Raw Share in the FairSharing mechanism in place.</p> <p>Different rules are applied to define these weights/shares depending on the level in the hierarchy:</p> <ul> <li>L1 (Organizational Unit): arbitrary shares to dedicate at least 85% of the platform to serve UL needs and projects</li> <li>L2: function of the out-degree of the tree nodes, reflecting also the past year funding</li> <li>L3: a function reflecting the budget contribution of the PI/project (normalized on a per-month basis) for the year in exercise.</li> <li>L4 (ULHPC/IPA login): efficiency score, giving incentives for a more efficient usage of the platform.</li> </ul> <p>More details are given on this page.</p>"},{"location":"slurm/accounts/#default-vs-project-accounts","title":"Default vs. Project accounts","text":"<p>Default account associations are defined as follows:</p> <ul> <li>For UL staff or external partners: your direct Line Manager <code>firstname.lastname</code> within the institution (Faculty, IC, Company) you belong too.</li> <li>For students: the lecture/course they are registered too<ul> <li>Guest student/training accounts are associated to the <code>Students</code> meta-account.</li> </ul> </li> </ul> <p>In addition, your user account (ULHPC login) may be associated to other meta-accounts such as projects or specific training events.</p> <p>To establish job accounting against these extra specific accounts, use:</p> <pre><code>{sbatch|srun} -A project_&lt;name&gt; [...]\n</code></pre> <p>For more details, see Project accounts.</p> <ol> <li> <p>restrictions applies and do not permit to reveal all information for other accounts than yours.\u00a0\u21a9</p> </li> </ol>"},{"location":"slurm/commands/","title":"Main Slurm Commands","text":""},{"location":"slurm/commands/#submit-jobs","title":"Submit Jobs","text":"<p>There are three ways of submitting jobs with slurm, using either <code>sbatch</code>, <code>srun</code> or <code>salloc</code>:</p> sbatch (passive job)srun (interactive job)salloc (request allocation/interactive job) <pre><code>### /!\\ Adapt &lt;partition&gt;, &lt;qos&gt;, &lt;account&gt; and &lt;command&gt; accordingly\nsbatch -p &lt;partition&gt; [--qos &lt;qos&gt;] [-A &lt;account&gt;] [...] &lt;path/to/launcher.sh&gt;\n</code></pre> <p><pre><code>### /!\\ Adapt &lt;partition&gt;, &lt;qos&gt;, &lt;account&gt; and &lt;command&gt; accordingly\nsrun -p &lt;partition&gt; [--qos &lt;qos&gt;] [-A &lt;account&gt;] [...] ---pty bash\n</code></pre> <code>srun</code> is also to be using within your launcher script to initiate a job step.</p> <pre><code># Request interactive jobs/allocations\n### /!\\ Adapt &lt;partition&gt;, &lt;qos&gt;, &lt;account&gt; and &lt;command&gt; accordingly\nsalloc -p &lt;partition&gt; [--qos &lt;qos&gt;] [-A &lt;account&gt;] [...] &lt;command&gt;\n</code></pre>"},{"location":"slurm/commands/#sbatch","title":"<code>sbatch</code>","text":"<p><code>sbatch</code> is used to submit a batch launcher script for later execution, corresponding to batch/passive submission mode. The script will typically contain one or more <code>srun</code> commands to launch parallel tasks. Upon submission with <code>sbatch</code>, Slurm will:</p> <ul> <li>allocate resources (nodes, tasks, partition, constraints, etc.)</li> <li>runs a single copy of the batch script on the first allocated node<ul> <li>in particular, if you depend on other scripts, ensure you have refer to them with the complete path toward them.</li> </ul> </li> </ul> <p>When you submit the job, Slurm responds with the job's ID, which will be used to identify this job in reports from Slurm.</p> <pre><code># /!\\ ADAPT path to launcher accordingly\n$ sbatch &lt;path/to/launcher&gt;.sh\nSubmitted batch job 864933\n</code></pre>"},{"location":"slurm/commands/#srun","title":"<code>srun</code>","text":"<p><code>srun</code> is used to initiate parallel job steps within a job OR to start an interactive job Upon submission with <code>srun</code>, Slurm will:</p> <ul> <li>(eventually) allocate resources (nodes, tasks, partition, constraints, etc.) when run for interactive submission</li> <li>launch a job step that will execute on the allocated resources.</li> </ul> <p>A job can contain multiple job steps executing sequentially or in parallel on independent or shared resources within the job's node allocation.</p>"},{"location":"slurm/commands/#salloc","title":"salloc","text":"<p><code>salloc</code> is used to allocate resources for a job in real time. Typically this is used to allocate resources  (nodes, tasks, partition, etc.) and spawn a shell. The shell is then used to execute srun commands to launch parallel tasks.</p>"},{"location":"slurm/commands/#interactive-jobs-si","title":"Interactive jobs: <code>si*</code>","text":"<p>You should use the helper functions <code>si</code>, <code>si-gpu</code>, <code>si-bigmem</code> to submit an interactive job.</p> <p>For more details, see interactive jobs.</p>"},{"location":"slurm/commands/#collect-job-information","title":"Collect Job Information","text":"Command Description <code>sacct [-X] -j &lt;jobid&gt; [...]</code> display accounting information on jobs. <code>scontrol show [...]</code> view and/or update system, nodes, job, step, partition or reservation status <code>seff &lt;jobid&gt;</code> get efficiency metrics of past job <code>smap</code> graphically show information on jobs, nodes, partitions <code>sprio</code> show factors that comprise a jobs scheduling priority <code>squeue [-u $(whoami)]</code> display jobs[steps] and their state <code>sstat</code> show status of running jobs."},{"location":"slurm/commands/#squeue","title":"<code>squeue</code>","text":"<p>You can  view information about jobs located in the Slurm scheduling queue (partition/qos), eventually filter on specific job state (R:running /PD:pending / F:failed / PR:preempted) with <code>squeue</code>:</p> <pre><code>$ squeue [-u &lt;user&gt;] [-p &lt;partition&gt;] [---qos &lt;qos&gt;] [--reservation &lt;name&gt;] [-t R|PD|F|PR]\n</code></pre> <p>To quickly access your jobs, you can simply use <code>sq</code></p>"},{"location":"slurm/commands/#live-job-statistics","title":"Live job statistics","text":"<p>You can use the <code>scurrent</code> (for current interactive job) or (more generally) <code>scontrol show job &lt;jobid&gt;</code> to collect detailed information for a running job.</p> <code>scontrol show job &lt;jobid&gt;</code> <pre><code>$  scontrol show job 2166371\nJobId=2166371 JobName=bash\n   UserId=&lt;login&gt;(&lt;uid&gt;) GroupId=clusterusers(666) MCS_label=N/A\n   Priority=12741 Nice=0 Account=ulhpc QOS=debug JobState=RUNNING Reason=None\n   [...]\n   SubmitTime=2020-12-07T22:08:25 EligibleTime=2020-12-07T22:08:25\n   StartTime=2020-12-07T22:08:25 EndTime=2020-12-07T22:38:25\n   [...]\n   WorkDir=/mnt/irisgpfs/users/&lt;login&gt;\n</code></pre>"},{"location":"slurm/commands/#past-job-statistics-slist-sreport","title":"Past job statistics: <code>slist</code>, <code>sreport</code>","text":"<p>Use the <code>slist</code> helper for a given job:</p> <pre><code># /!\\ ADAPT &lt;jobid&gt; accordingly\n$ slist &lt;jobid&gt;\n# sacct -j &lt;JOBID&gt; --format User,JobID,Jobname%30,partition,state,time,elapsed,\\\n#              MaxRss,MaxVMSize,nnodes,ncpus,nodelist,AveCPU,ConsumedEnergyRaw\n# seff &lt;jobid&gt;\n</code></pre> <p>You can also use <code>sreport</code> o generate reports of job usage and cluster utilization for Slurm jobs. For instance, to list your usage in CPU-hours since the beginning of the year:</p> <pre><code>$ sreport -t hours cluster UserUtilizationByAccount Users=$USER  Start=$(date +%Y)-01-01\n--------------------------------------------------------------------------------\nCluster/User/Account Utilization 2021-01-01T00:00:00 - 2021-02-13T23:59:59 (3801600 secs)\nUsage reported in CPU Hours\n----------------------------------------------------------------------------\n  Cluster     Login     Proper Name                Account     Used   Energy\n--------- --------- --------------- ---------------------- -------- --------\n     iris   &lt;login&gt;          &lt;name&gt; &lt;firstname&gt;.&lt;lastname&gt;    [...]\n     iris   &lt;login&gt;          &lt;name&gt;      project_&lt;acronym&gt;    [...]\n</code></pre>"},{"location":"slurm/commands/#job-efficiency","title":"Job efficiency","text":""},{"location":"slurm/commands/#seff","title":"<code>seff</code>","text":"<p>Use <code>seff</code> to double check a past job CPU/Memory efficiency. Below examples should be self-speaking:</p> Good CPU Eff.Good Memory Eff.Good CPU and Memory Eff.[Very] Bad efficiency <pre><code>$ seff 2171749\nJob ID: 2171749\nCluster: iris\nUser/Group: &lt;login&gt;/clusterusers\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 28\nCPU Utilized: 41-01:38:14\nCPU Efficiency: 99.64% of 41-05:09:44 core-walltime\nJob Wall-clock time: 1-11:19:38\nMemory Utilized: 2.73 GB\nMemory Efficiency: 2.43% of 112.00 GB\n</code></pre> <pre><code>$ seff 2117620\nJob ID: 2117620\nCluster: iris\nUser/Group: &lt;login&gt;/clusterusers\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 16\nCPU Utilized: 14:24:49\nCPU Efficiency: 23.72% of 2-12:46:24 core-walltime\nJob Wall-clock time: 03:47:54\nMemory Utilized: 193.04 GB\nMemory Efficiency: 80.43% of 240.00 GB\n</code></pre> <pre><code>$ seff 2138087\nJob ID: 2138087\nCluster: iris\nUser/Group: &lt;login&gt;/clusterusers\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 64\nCPU Utilized: 87-16:58:22\nCPU Efficiency: 86.58% of 101-07:16:16 core-walltime\nJob Wall-clock time: 1-13:59:19\nMemory Utilized: 1.64 TB\nMemory Efficiency: 99.29% of 1.65 TB\n</code></pre> <p>This illustrates a very bad job in terms of CPU/memory efficiency (below 4%), which illustrate a case where basically the user wasted 4 hours of computation while mobilizing a full node and its 28 cores. <pre><code>$ seff 2199497\nJob ID: 2199497\nCluster: iris\nUser/Group: &lt;login&gt;/clusterusers\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 28\nCPU Utilized: 00:08:33\nCPU Efficiency: 3.55% of 04:00:48 core-walltime\nJob Wall-clock time: 00:08:36\nMemory Utilized: 55.84 MB\nMemory Efficiency: 0.05% of 112.00 GB\n</code></pre> This is typical of a single-core task can could be drastically improved via GNU Parallel.</p> <p>Note however that demonstrating a CPU good efficiency with <code>seff</code> may not be enough! You may still induce an abnormal load on the reserved nodes if you spawn more processes than allowed by the Slurm reservation. To avoid that, always try to prefix your executions with <code>srun</code> within your launchers. See also Specific Resource Allocations.</p>"},{"location":"slurm/commands/#susage","title":"<code>susage</code>","text":"<p>Use <code>susage</code> to check your past jobs walltime accuracy (<code>Timelimit</code> vs. <code>Elapsed</code>)</p> <pre><code>$ susage -h\nUsage: susage [-m] [-Y] [-S YYYY-MM-DD] [-E YYYT-MM-DD]\n  For a specific user (if accounting rights granted):    susage [...] -u &lt;user&gt;\n  For a specific account (if accounting rights granted): susage [...] -A &lt;account&gt;\nDisplay past job usage summary\n</code></pre>"},{"location":"slurm/commands/#official-sacct-command","title":"Official <code>sacct</code> command","text":"<p>Alternatively, you can use <code>sacct</code> (use <code>sacct --helpformat</code> to get the list of) for COMPLETED or TIMEOUT jobs (see Job State Codes).</p> using <code>sacct -X -S &lt;start&gt; [...] --format [...],time,elapsed,[...]</code> <p>ADAPT <code>-S &lt;start&gt;</code> and <code>-E &lt;end&gt;</code> dates accordingly - Format: <code>YYYY-MM-DD</code>. hint: <code>$(date +%F)</code> will return today's date in that format, <code>$(date +%Y)</code> return the current year, so the below command will list your completed (or timeout jobs) since the beginning of the month: <pre><code>$ sacct -X -S $(date +%Y)-01-01 -E $(date +%F) --partition batch,gpu,bigmem --state CD,TO --format User,JobID,partition%12,qos,state,time,elapsed,nnodes,ncpus,allocGRES\n     User        JobID    Partition        QOS      State  Timelimit    Elapsed   NNodes      NCPUS    AllocGRES\n--------- ------------ ------------ ---------- ---------- ---------- ---------- -------- ---------- ------------\n &lt;login&gt; 2243517             batch     normal    TIMEOUT 2-00:00:00 2-00:00:05        4        112\n &lt;login&gt; 2243518             batch     normal    TIMEOUT 2-00:00:00 2-00:00:05        4        112\n &lt;login&gt; 2244056               gpu     normal    TIMEOUT 2-00:00:00 2-00:00:12        1         16        gpu:2\n &lt;login&gt; 2246094               gpu       high    TIMEOUT 2-00:00:00 2-00:00:29        1         16        gpu:2\n &lt;login&gt; 2246120               gpu       high  COMPLETED 2-00:00:00 1-02:18:00        1         16        gpu:2\n &lt;login&gt; 2247278            bigmem     normal  COMPLETED 2-00:00:00 1-05:59:21        1         56\n &lt;login&gt; 2250178             batch     normal  COMPLETED 2-00:00:00   10:04:32        1          1\n &lt;login&gt; 2251232               gpu     normal  COMPLETED 1-00:00:00   12:05:46        1          6        gpu:1\n</code></pre></p>"},{"location":"slurm/commands/#platform-status","title":"Platform Status","text":""},{"location":"slurm/commands/#sinfo","title":"<code>sinfo</code>","text":"<p><code>sinfo</code> allow to view information about partition status (<code>-p &lt;partition&gt;</code>),  problematic nodes (<code>-R</code>), reservations (<code>-T</code>), eventually in a summarized form (<code>-s</code>),</p> <pre><code>sinfo [-p &lt;partition&gt;] {-s | -R | -T |...}\n</code></pre> <p>We are providing a certain number of helper functions based on <code>sinfo</code>:</p> Command Description <code>nodelist</code> List available nodes <code>allocnodes</code> List currently allocated nodes <code>idlenodes</code> List currently idle nodes <code>deadnodes</code> List dead nodes per partition (hopefully none ;)) <code>sissues</code> List nodes with issues/problems, with reasons <code>sfeatures</code> List available node features"},{"location":"slurm/commands/#cluster-partition-and-qos-usage-stats","title":"Cluster, partition and QOS usage stats","text":"<p>We have defined several custom ULHPC Slurm helpers defined in <code>/etc/profile.d/slurm.sh</code> to facilitate access to account/parition/qos/usage information. They are listed below.</p> Command Description <code>acct &lt;name&gt;</code> Get information on user/account holder <code>&lt;name&gt;</code> in Slurm accounting DB <code>irisstat</code>, <code>aionstat</code> report cluster status (utilization, partition and QOS live stats) <code>listpartitionjobs &lt;part&gt;</code> List jobs (and current load) of the slurm partition <code>&lt;part&gt;</code> <code>pload [-a] i/b/g/m</code> Overview of the Slurm partition load <code>qload [-a]  &lt;qos&gt;</code> Show current load of the slurm QOS <code>&lt;qos&gt;</code> <code>sbill &lt;jobid&gt;</code> Display job charging / billing summary <code>sjoin [-w &lt;node&gt;]</code> join a running job <code>sassoc &lt;name&gt;</code> Show Slurm association information for <code>&lt;name&gt;</code> (user or account) <code>slist &lt;jobid&gt; [-X]</code> List statistics of a past job <code>sqos</code> Show QOS information and limits <code>susage [-m] [-Y] [...]</code> Display past job usage summary"},{"location":"slurm/commands/#updating-jobs","title":"Updating jobs","text":"Command Description <code>scancel &lt;jobid&gt;</code> cancel a job or set of jobs. <code>scontrol update jobid=&lt;jobid&gt; [...]</code> update pending job definition <code>scontrol hold &lt;jobid&gt;</code> Hold job <code>scontrol resume &lt;jobid&gt;</code> Resume held job <p>The <code>scontrol</code> command allows certain charactistics of a job to be updated while it is still queued (i.e. not running ), with the syntax <code>scontrol update jobid=&lt;jobid&gt; [...]</code></p> <p>Important</p> <p>Once the job is running, most changes requested with <code>scontrol update jobid=[...]</code> will NOT be applied.</p>"},{"location":"slurm/commands/#change-timelimit","title":"Change timelimit","text":"<pre><code># /!\\ ADAPT &lt;jobid&gt; and new time limit accordingly\nscontrol update jobid=&lt;jobid&gt; timelimit=&lt;[DD-]HH:MM::SS&gt;\n</code></pre>"},{"location":"slurm/commands/#change-qos-or-reservation","title":"Change QOS or Reservation","text":"<pre><code># /!\\ ADAPT &lt;jobid&gt;, &lt;qos&gt;, &lt;resname&gt; accordingly\nscontrol update jobid=&lt;jobid&gt; qos=&lt;qos&gt;\nscontrol update jobid=&lt;jobid&gt; reservationname=&lt;resname&gt;\n</code></pre>"},{"location":"slurm/commands/#change-account","title":"Change account","text":"<p>If you forgot to specify the expected project account:</p> <pre><code># /!\\ ADAPT &lt;jobid&gt;, &lt;account&gt; accordingly\nscontrol update jobid=&lt;jobid&gt; account=&lt;account&gt;\n</code></pre> <p>The new account must be eligible to run the job. See Account Hierarchy for more details.</p>"},{"location":"slurm/commands/#hold-and-resume-jobs","title":"Hold and Resume jobs","text":"<p>Prevent a pending job from being started:</p> <pre><code># /!\\ ADAPT &lt;jobid&gt;  accordingly\nscontrol hold &lt;jobid&gt;\n</code></pre> <p>Allow a held job to accrue priority and run:</p> <pre><code># /!\\ ADAPT &lt;jobid&gt;  accordingly\nscontrol release &lt;jobid&gt;\n</code></pre>"},{"location":"slurm/commands/#cancel-jobs","title":"Cancel jobs","text":"<p>Cancel a specific job:</p> <pre><code># /!\\ ADAPT &lt;jobid&gt; accordingly\nscancel &lt;jobid&gt;\n</code></pre> Cancel all jobs owned by a user (you) <p><pre><code>scancel -u $USER\n</code></pre> This only applies to jobs which are associated with your accounts.</p>"},{"location":"slurm/fairsharing/","title":"Fairsharing and Job Accounting","text":"<ul> <li>Resources:<ul> <li>Slurm Priority, Fairshare and Fair Tree (PDF)</li> <li>SchedMD Slurm documentation: Multifactor Priority Plugin</li> <li>Fair tree algorithm, FAS RC docs, Official <code>sshare</code> documentation</li> </ul> </li> </ul> <p>Fairshare allows past resource utilization information to be taken into account into job feasibility and priority decisions to ensure a fair allocation of the computational resources between the all ULHPC users. A difference with a equal scheduling is illustrated in the side picture (source).</p> <p></p> <p>Essentially fairshare is a way of ensuring that users get their appropriate portion of a system. Sadly this term is also used confusingly for different parts of fairshare listed below, so for the sake of clarity, the following terms will be used:</p> <ul> <li>[Raw] Share: portion of the system users have been granted</li> <li>[Raw] Usage: amount of the system users have actually used so far<ul> <li>The fairshare score is the value the system calculates based on the usage    and the share (see below)</li> </ul> </li> <li>Priority: the priority that users are assigned based off of their fairshare score.</li> </ul> <p>Demystifying Fairshare</p> <p>While fairshare may seem complex and confusing, it is actually quite logical once you think about it. The scheduler needs some way to adjudicate who gets what resources when different groups on the cluster have been granted different resources and shares for various reasons (see Account Hierarchy).</p> <p>In order to serve the great variety of groups and needs on the cluster, a method of fairly adjudicating job priority is required. This is the goal of Fairshare. Fairshare allows those users who have not fully used their resource grant to get higher priority for their jobs on the cluster, while making sure that those groups that have used more than their resource grant do not overuse the cluster.</p> <p>The ULHPC supercomputers are a limited shared resource, and Fairshare ensures everyone gets a fair opportunity to use it regardless of how big or small the group is.</p>"},{"location":"slurm/fairsharing/#fairtree-algorithm","title":"FairTree Algorithm","text":"<p>There exists several fairsharing algorithms implemented in Slurm:</p> <ul> <li>Classic Fairshare</li> <li>Depth-Oblivious Fair-share</li> <li>Fair Tree (now implemented on   ULHPC since Oct 2020)</li> </ul> <p>What is Fair Tree?</p> <p>The Fair Tree algorithm prioritizes users such that if accounts A and B are siblings and A has a higher fairshare factor than B, then all children of A will have higher fairshare factors than all children of B.</p> <p>This is done through a rooted plane tree (PDF), also known as a rooted ordered tree, which is logically created then sorted by fairshare with the highest fairshare values on the left. The tree is then visited in a depth-first traversal way. Users are ranked in pre-order as they are found. The ranking is used to create the final fairshare factor for the user. Fair Tree Traversal Illustrated - initial post</p> <p>Some of the benefits include:</p> <ul> <li>All users from a higher priority account receive a higher fair share factor than all users from a lower priority account.</li> <li>Users are sorted and ranked to prevent errors due to precision loss. Ties are allowed.</li> <li>Account coordinators cannot accidentally harm the priority of their users relative to users in other accounts.</li> <li>Users are extremely unlikely to have exactly the same fairshare factor as another user due to loss of precision in calculations.</li> <li>New jobs are immediately assigned a priority.</li> </ul> <p> Overview of Fair Tree for End Users  Level Fairshare Calculation</p>"},{"location":"slurm/fairsharing/#shares","title":"Shares","text":"<p>On ULHPC facilities, each user is associated by default to a meta-account reflecting its direct Line Manager  within the institution (Faculty, IC, Company) you belong too -- see ULHPC Account Hierarchy. You may have other account associations (typically toward projects accounts, granting access to different QOS for instance), and each accounts have Shares granted to them. These Shares determine how much of the cluster that group/account has been granted. Users when they run are charged back for their runs against the account used upon job submission -- you can use <code>sbatch|srun|... -A &lt;account&gt; [...]</code> to change that account.</p> <p> ULHPC Usage Charging Policy</p> <p>Different rules are applied to define these weights/shares depending on the level in the hierarchy:</p> <ul> <li>L1 (Organizational Unit): arbitrary shares to dedicate at least 85% of the platform to serve UL needs and projects</li> <li>L2: function of the out-degree of the tree nodes, reflecting also the past year funding</li> <li>L3: a function reflecting the budget contribution of the PI/project (normalized on a per-month basis) for the year in exercise.</li> <li>L4 (ULHPC/IPA login): efficiency score, giving incentives for a more efficient usage of the platform.</li> </ul>"},{"location":"slurm/fairsharing/#fair-share-factor","title":"Fair Share Factor","text":"<p>The Fairshare score is the value Slurm calculates based off of user's usage reflecting the difference between the portion of the computing resource that has been promised (share) and the amount of resources that has been consumed. It thus influences the order in which a user's queued jobs are scheduled to run based on the portion of the computing resources they have been allocated and the resources their jobs have already consumed.</p> <p>In practice, Slurm's fair-share factor is a floating point number between 0.0 and 1.0 that reflects the shares of a computing resource that a user has been allocated and the amount of computing resources the user's jobs have consumed.</p> <ul> <li>The higher the value, the higher is the placement in the queue of jobs waiting to be scheduled.</li> <li>Reciprocally, the more resources the users is consuming, the lower the fair share factor will be which will result in lower priorities.</li> </ul>"},{"location":"slurm/fairsharing/#ulhpcshare-helper","title":"<code>ulhpcshare</code> helper","text":"<p>Listing the ULHPC shares: <code>ulhpcshare</code> helper</p> <p><code>sshare</code> can be used to view the fair share factors and corresponding promised and actual usage for all users. However, you are encouraged to use the <code>ulhpcshare</code> helper function: <pre><code># your current shares and fair-share factors among your associations\nulhpcshare\n# as above, but for user '&lt;login&gt;'\nulhpcshare -u &lt;login&gt;\n# as above, but for account '&lt;account&gt;'\nulhpcshare -A &lt;account&gt;\n</code></pre> The column that contains the actual factor is called \"FairShare\".</p>"},{"location":"slurm/fairsharing/#official-sshare-utility","title":"Official <code>sshare</code> utility","text":"<p><code>ulhpcshare</code> is a wrapper around the official <code>sshare</code> utility. You can quickly see your score with <pre><code>$ sshare  [-A &lt;account&gt;] [-l] [--format=Account,User,RawShares,NormShares,EffectvUsage,LevelFS,FairShare]\n</code></pre> It will show the Level Fairshare value as <code>Level FS</code>. The field shows the value for each association, thus allowing users to see the results of the fairshare calculation at each level.</p> <p>Note: Unlike the Effective Usage, the Norm Usage is not used by Fair Tree but is still displayed in this case.</p>"},{"location":"slurm/fairsharing/#slurm-parameter-definitions","title":"Slurm Parameter Definitions","text":"<p>In this part some of the set slurm parameters are explained which are used to set up the Fair Tree Fairshare Algorithm. For a more detailed explanation please consult the official documentation</p> <ul> <li><code>PriorityCalcPeriod=HH:MM::SS</code>: frequency in minutes that job half-life decay and Fair Tree calculations are performed.</li> <li><code>PriorityDecayHalfLife=[number of days]-[number of hours]</code>: the time, of which the resource consumption is taken into account for the Fairshare Algorithm, can be set by this.</li> <li><code>PriorityMaxAge=[number of days]-[number of hours]</code>: the maximal queueing time which counts for the priority calculation. Note that queueing times above are possible but do not contribute to the priority factor.</li> </ul> <p>A quick way to check the currently running configuration is:</p> <pre><code>scontrol show config | grep -i priority\n</code></pre>"},{"location":"slurm/fairsharing/#trackable-resources-tres-billing-weights","title":"Trackable RESources (TRES) Billing Weights","text":"<p>Slurm saves accounting data for every job or job step that the user submits. On ULHPC facilities, Slurm Trackable RESources (TRES) is enabled to allow for the scheduler to charge back users for how much they have used of different features (i.e. not only CPU) on the cluster -- see Job Accounting and Billing. This is important as the usage of the cluster factors into the Fairshare calculation.</p> <p>As explained in the ULHPC Usage Charging Policy, we set TRES for CPU, GPU, and Memory usage according to weights defined as follows:</p> Weight Description \\alpha_{cpu} Normalized relative performance of CPU processor core (ref.: skylake 73.6 GFlops/core) \\alpha_{mem} Inverse of the average available memory size per core \\alpha_{GPU} Weight per GPU accelerator <p>Each partition has its own weights (combined into <code>TRESBillingWeight</code>) you can check with</p> <pre><code># /!\\ ADAPT &lt;partition&gt; accordingly\nscontrol show partition &lt;partition&gt;\n</code></pre>"},{"location":"slurm/fairsharing/#ulhpc-usage-charging-policy","title":"ULHPC Usage Charging Policy","text":"<p>The advertised prices are for internal partners only</p> <p>The price list and all other information of this page are meant for internal partners, i.e., not for external companies.  If you are not an internal partner, please contact us at hpc-partnership@uni.lu. Alternatively, you can contact LuxProvide, the national HPC center which aims at serving the private sector for HPC needs.</p>"},{"location":"slurm/fairsharing/#how-to-estimate-hpc-costs-for-projects","title":"How to estimate HPC costs for projects?","text":"<p>You can use the following excel document to estimate the cost of your HPC usage:</p> <p> UL HPC Cost Estimates for Project Proposals [xlsx] </p> <p>Note that there are two sheets offering two ways to estimate based on your specific situation. Please read the red sections to ensure that you are using the correct estimation sheet.</p> <p>Note that even if you plan for large-scale experiments on PRACE/EuroHPC supercomputers through computing credits granted by Call for Proposals for Project Access, you should plan for ULHPC costs since you will have to demonstrate the scalability of your code -- the University's facility is ideal for that. You can contact hpc-partnership@uni.lu for more details about this.</p>"},{"location":"slurm/fairsharing/#hpc-price-list-2022-10-01","title":"HPC price list - 2022-10-01","text":"<p>Note that ULHPC price list has been updated, see below.</p>"},{"location":"slurm/fairsharing/#compute","title":"Compute","text":"Compute type Description \u20ac (excl. VAT) / node-hour CPU - small 28 cores, 128 GB RAM 0.25\u20ac CPU - regular 128 cores, 256 GB RAM 1.25\u20ac CPU - big mem 112 cores, 3 TB RAM 6.00\u20ac GPU 4 V100, 28 cores, 768 GB RAM 5.00\u20ac <p>The prices above correspond to a full-node cost. However, jobs can use a fraction of a node and the price of the job will be computed based on that fraction. Please find below the core-hour / GPU-hour costs and how we compute how much to charge:</p> Compute type Unit \u20ac (excl. VAT) CPU - small Core-hour 0.0089\u20ac CPU - regular Core-hour 0.0097\u20ac CPU - big mem Core-hour 0.0535\u20ac GPU GPU-hour 1.25\u20ac <p>For CPU nodes, the fraction correspond to the number of requested cores, e.g. 64 cores on a CPU - regular node corresponds to 50% of the available cores and thus will be charged 50% of 1.25\u20ac. </p> <p>Regarding the RAM of a job, if you do not override the default behaviour, you will receive a percentage of the RAM corresponding to the amount of requested cores, e.g, 128G of RAM for the 64 cores example from above (50% of a CPU - regular node). If you override the default behaviour and request more RAM, we will re-compute the equivalent number of cores, e.g. if you request 256G of RAM and 64 cores, we will charge 128 cores.</p> <p>For GPU nodes, the fraction considers the number of GPUs. There are 4 GPUs, 28 cores and 768G of RAM on one machine. This means that for each GPU, you can have up to 7 cores and 192G of RAM. If you request more than those default, we will re-compute the GPU equivalent, e.g. if you request 1 GPU and 8 cores, we will charge 2 GPUs.</p>"},{"location":"slurm/fairsharing/#storage","title":"Storage","text":"Storage type \u20ac (excl. VAT) / GB / Month Additional information Home Free 500 GB Project 0.02\u20ac 1 TB free Scratch Free 10 TB <p>Note that for project storage, we charge the quota and not the used storage.</p>"},{"location":"slurm/fairsharing/#hpc-resource-allocation-for-ul-internal-rd-and-training","title":"HPC Resource allocation for UL internal R&amp;D and training","text":"<p>ULHPC resources are free of charge for UL staff for their internal work and training activities. Principal Investigators (PI) will nevertheless receive on a regular basis a usage report of their team activities on the UL HPC platform. The corresponding accumulated price will be provided even if this amount is purely indicative and won't be charged back.</p> <p>Any other activities will be reviewed with the rectorate and are a priori subjected to be billed.</p>"},{"location":"slurm/fairsharing/#submit-project-related-jobs","title":"Submit project related jobs","text":"<p>To allow the ULHPC team to keep track of the jobs related to a project, use the <code>-A &lt;projectname&gt;</code> flag in Slurm, either in the Slurm directives preamble of your script, e.g.,</p> <pre><code>#SBATCH -A myproject\n</code></pre> <p>or on the command line when you submit your job, e.g., <code>sbatch -A myproject /path/to/launcher.sh</code></p>"},{"location":"slurm/fairsharing/#faq","title":"FAQ","text":""},{"location":"slurm/fairsharing/#q-my-user-fairshare-is-low-what-can-i-do","title":"Q: My user fairshare is low, what can I do?","text":"<p>We have introduced an efficiency score evaluated on a regular basis (by default, every year) to measure how efficient you use the computational resources of the University according to several measures for completed jobs:</p> <ul> <li>How efficient you were to estimate the walltime of your jobs (Average Walltime Accuracy)</li> <li>How CPU/Memory efficient were your completed jobs (see <code>seff</code>)</li> </ul> <p>Without entering into the details, we combine these metrics to compute an unique score value S_\\text{efficiency} and you obtain a grade: A (very good), B, C, or D (very bad) which can increase your user share.</p>"},{"location":"slurm/fairsharing/#q-my-account-fairshare-is-low-what-can-i-do","title":"Q: My account fairshare is low, what can I do?","text":"<p>There are several things that can be done when your fairshare is low:</p> <ol> <li>Do not run jobs: Fairshare recovers via two routes.<ul> <li>The first is via your group not running any jobs and letting others use the resource.  That allows your fractional usage to decrease which in turn increases your fairshare score.</li> <li>The second is via the half-life we apply to fairshare which ages out old   usage over time.  Both of these method require not action but inaction on the part of your  group.  Thus to recover your fairshare simply stop running jobs until your fairshare  reaches the level you desire.  Be warned this could take several weeks to accomplish depending on your  current usage.</li> </ul> </li> <li>Be patient, as a corollary to the previous point. Even if your    fairshare is low, your job gains priority by sitting the queue (see Job Priority)    The longer it sits the higher priority it gains.  So even if you have very    low fairshare your jobs will eventually run, it just may take several days to    accomplish.</li> <li>Leverage Backfill: Slurm runs in two scheduling loops.<ul> <li>The first loop is the main loop which simply looks at the top of the   priority chain for the partition and tries to schedule that job.  It will   schedule jobs until it hits a job it cannot schedule and then it restarts   the loop.</li> <li>The second loop is the backfill loop. This loop looks through jobs   further down in the queue and asks can I schedule this job now and not   interfere with the start time of the top priority job.  Think of it as the   scheduler playing giant game of three dimensional tetris, where the   dimensions are number of cores, amount of memory, and amount of time.  If   your job will fit in the gaps that the scheduler has it will put your job   in that spot even if it is low priority.  This requires you to be very   accurate in specifying the core, memory, and time usage (typically below   ) of your job.   The better constrained your job is the more likely the scheduler is to   fit you in to these gaps**.   The <code>seff</code> utility is a great way of figuring out your job performance.</li> </ul> </li> <li>Plan: Better planning and knowledge of your historic usage can help you    better budget your time on the cluster. Our clusters are not infinite    resources.  You have been allocated a slice of the cluster, thus it is best    to budget your usage so that you can run high priority jobs when you need to.</li> <li>HPC Budget contribution: If your group has persistent high demand that cannot be met    with your current allocation, serious consideration should be given to    contributing to the ULHPC budget line.<ul> <li>This should be done for funded research projects - see    HPC Resource Allocations for Research Project</li> <li>This can be done by each individual PI, Dean or IC director In all cases, any contribution on year <code>Y</code> grants additional shares for the group starting year <code>Y+1</code>. We apply a consistent (complex) function taking into account depreciation of the investment. Contact us (by mail or by a ticket for more details.</li> </ul> </li> </ol>"},{"location":"slurm/launchers/","title":"Slurm Launcher Examples","text":"<p>  ULHPC Tutorial / Getting Started   ULHPC Tutorial / OpenMP/MPI </p> <p>When setting your default <code>#SBATCH</code> directive, always keep in mind your expected default resource allocation that would permit to submit your launchers</p> <ol> <li>without options <code>sbatch &lt;launcher&gt;</code> (you will be glad in a couple of month not to have to remember the options you need to pass) and</li> <li>try to stick to a single node (to avoid to accidentally induce a huge submission).</li> </ol>"},{"location":"slurm/launchers/#resource-allocation-guidelines","title":"Resource allocation Guidelines","text":"<p>General guidelines</p> <p>Always try to align resource specifications for your jobs with physical characteristics. Always prefer the use of <code>--ntasks-per-{node,socket}</code> over <code>-n</code> when defining your tasks allocation request to automatically scale appropriately upon multi-nodes submission with for instance <code>sbatch -N 2 &lt;launcher&gt;</code>. Launcher template: <pre><code>#!/bin/bash -l # &lt;--- DO NOT FORGET '-l' to facilitate further access to ULHPC modules\n#SBATCH -p &lt;partition&gt;                     #SBATCH -p &lt;partition&gt;\n#SBATCH -N 1                               #SBATCH -N 1\n#SBATCH --ntasks-per-node=&lt;n&gt;              #SBATCH --ntasks-per-node &lt;#sockets * s&gt;\n#SBATCH -c &lt;thread&gt;                        #SBATCH --ntasks-per-socket &lt;s&gt;\n                                           #SBATCH -c &lt;thread&gt;\n</code></pre> This would define by default a total of <code>&lt;n&gt;</code> (left) or \\#sockets \\times<code>&lt;s&gt;</code> (right) tasks per node, each on <code>&lt;thread&gt;</code> threads. You MUST ensure that either:</p> <ul> <li><code>&lt;n&gt;</code>\\times<code>&lt;thread&gt;</code> matches the number of cores avaiable on the target computing node (left), or</li> <li><code>&lt;n&gt;</code>=\\#sockets \\times<code>&lt;s&gt;</code>, and <code>&lt;s&gt;</code>\\times<code>&lt;thread&gt;</code> matches the number of cores per socket available on the target computing node (right).</li> </ul> <p>See Specific Resource Allocation</p> Node (type) #Nodes #Socket / #Cores RAM [GB] Features <code>aion-[0001-0354]</code> 354 8 / 128 256 <code>batch,epyc</code> <code>iris-[001-108]</code> 108 2 / 28 128 <code>batch,broadwell</code> <code>iris-[109-168]</code> 60 2 / 28 128 <code>batch,skylake</code> <code>iris-[169-186]</code>   (GPU) 18 2 / 28 768 <code>gpu,skylake,volta</code> <code>iris-[191-196]</code>   (GPU) 6 2 / 28 768 <code>gpu,skylake,volta32</code> <code>iris-[187-190]</code> (Large-Memory) 4 4 / 112 3072 <code>bigmem,skylake</code> Aion (default Dual-CPU)Iris (default Dual-CPU)Iris (GPU)Iris (Large-Memory) <p>16 cores per socket and 8 (virtual) sockets (CPUs) per <code>aion</code> node. Examples: <pre><code>#SBATCH -p batch                 #SBATCH -p batch                #SBATCH -p batch\n#SBATCH -N 1                     #SBATCH -N 1                    #SBATCH -N 1\n#SBATCH --ntasks-per-node=128    #SBATCH --ntasks-per-node 16    #SBATCH --ntasks-per-node 8\n#SBATCH --ntasks-per-socket 16   #SBATCH --ntasks-per-socket 2   #SBATCH --ntasks-per-socket 1\n#SBATCH -c 1                     #SBATCH -c 8                    #SBATCH -c 16\n</code></pre></p> <p>14 cores per socket and 2 sockets (physical CPUs) per regular <code>iris</code>. Examples: <pre><code>#SBATCH -p batch                #SBATCH -p batch                 #SBATCH -p batch\n#SBATCH -N 1                    #SBATCH -N 1                     #SBATCH -N 1\n#SBATCH --ntasks-per-node=28    #SBATCH --ntasks-per-node 14     #SBATCH --ntasks-per-node 4\n#SBATCH --ntasks-per-socket=14  #SBATCH --ntasks-per-socket 7    #SBATCH --ntasks-per-socket 2\n#SBATCH -c 1                    #SBATCH -c 2                     #SBATCH -c 7\n</code></pre></p> <p>14 cores per socket and 2 sockets (physical CPUs) per gpu <code>iris</code>, 4 GPU accelerator cards per node. You probably want to dedicate 1 task and \\frac{1}{4} of the available cores to the management of each GPU accelerator. Examples: <pre><code>#SBATCH -p gpu                  #SBATCH -p gpu                   #SBATCH -p gpu\n#SBATCH -N 1                    #SBATCH -N 1                     #SBATCH -N 1\n#SBATCH --ntasks-per-node=1     #SBATCH --ntasks-per-node 2      #SBATCH --ntasks-per-node 4\n#SBATCH -c 7                    #SBATCH --ntasks-per-socket 1    #SBATCH --ntasks-per-socket 2\n#SBATCH -G 1                    #SBATCH -c 7                     #SBATCH -c 7\n                                #SBATCH -G 2                     #SBATCH -G 4\n</code></pre></p> <p>28 cores per socket and 4 sockets (physical CPUs) per bigmem <code>iris</code> node. Examples: <pre><code>#SBATCH -p bigmem              #SBATCH -p bigmem                 #SBATCH -p bigmem\n#SBATCH -N 1                   #SBATCH -N 1                      #SBATCH -N 1\n#SBATCH --ntasks-per-node=4    #SBATCH --ntasks-per-node 8       #SBATCH --ntasks-per-node 16\n#SBATCH --ntasks-per-socket=1  #SBATCH --ntasks-per-socket 2     #SBATCH --ntasks-per-socket 4\n#SBATCH -c 28                  #SBATCH -c 14                     #SBATCH -c 7\n</code></pre> You probably want to play with a single task but define the expected memory allocation with <code>--mem=&lt;size[units]&gt;</code> (Default units are megabytes - Different units can be specified using the suffix <code>[K|M|G|T]</code>)</p>"},{"location":"slurm/launchers/#basic-slurm-launcher-examples","title":"Basic Slurm Launcher Examples","text":"Single core taskMultiple Single core tasksMultithreaded parallel tasks <p>1 task per job (Note: prefer GNU Parallel in that case - see below)</p> <pre><code>#!/bin/bash -l                # &lt;--- DO NOT FORGET '-l'\n### Request a single task using one core on one node for 5 minutes in the batch queue\n#SBATCH -N 1\n#SBATCH --ntasks-per-node=1\n#SBATCH -c 1\n#SBATCH --time=0-00:05:00\n#SBATCH -p batch\n\nprint_error_and_exit() { echo \"***ERROR*** $*\"; exit 1; }\n# Safeguard for NOT running this launcher on access/login nodes\nmodule purge || print_error_and_exit \"No 'module' command\"\n# List modules required for execution of the task\nmodule load &lt;...&gt;\n# [...]\n</code></pre> <p>28 single-core tasks per job</p> <pre><code>#!/bin/bash -l\n### Request as many tasks as cores available on a single node for 3 hours\n#SBATCH -N 1\n#SBATCH --ntasks-per-node=28  # On iris; for aion, use --ntasks-per-node=128\n#SBATCH -c 1\n#SBATCH --time=0-03:00:00\n#SBATCH -p batch\n\nprint_error_and_exit() { echo \"***ERROR*** $*\"; exit 1; }\nmodule purge || print_error_and_exit \"No 'module' command\"\nmodule load &lt;...&gt;\n# [...]\n</code></pre> <p>7 multithreaded tasks per job (4 threads each)</p> <pre><code>#!/bin/bash -l\n### Request as many tasks as cores available on a single node for 3 hours\n#SBATCH -N 1\n#SBATCH --ntasks-per-node=7  # On iris; for aion, use --ntasks-per-node=32\n#SBATCH -c 4\n#SBATCH --time=0-03:00:00\n#SBATCH -p batch\n\nprint_error_and_exit() { echo \"***ERROR*** $*\"; exit 1; }\nmodule purge || print_error_and_exit \"No 'module' command\"\nmodule load &lt;...&gt;\n# [...]\n</code></pre>"},{"location":"slurm/launchers/#embarrassingly-parallel-tasks","title":"Embarrassingly Parallel Tasks","text":"<p>For many users, the reason to consider (or being encouraged) to offload their computing executions on a (remote) HPC or Cloud facility is tied to the limits reached by their computing devices (laptop or workstation). It is generally motivated by time constraints</p> <p>\"My computations take several hours/days to complete. On an HPC, it will last a few minutes, no?\"</p> <p>or search-space explorations:</p> <p>\"I need to check my application against a huge number of input pieces (files) - it worked on a few of them locally but takes ages for a single check. How to proceed on HPC?\"</p> <p>In most of the cases, your favorite Java application or R/python (custom) development scripts, iterated again over multiple input conditions, are inherently SERIAL: they are able to use only one core when executed. You thus deal with what is often call a Bag of (independent) tasks, also referred to as embarrassingly parallel tasks.</p> <p>In this case, you MUST NOT overload the job scheduler with a large number of small (single-core) jobs. Instead, you should use GNU Parallel which permits the effective management of such tasks in a way that optimize both the resource allocation and the completion time.</p> <p>More specifically, GNU Parallel is a tool for executing tasks in parallel, typically on a single machine. When coupled with the Slurm command srun, <code>parallel</code> becomes a powerful way of distributing a set of tasks amongst a number of workers. This is particularly useful when the number of tasks is significantly larger than the number of available workers (i.e. <code>$SLURM_NTASKS</code>), and each tasks is independent of the others.</p> <p> ULHPC Tutorial: GNU Parallel launcher for Embarrassingly Parallel Jobs</p> <p>Luckily, we have prepared a generic GNU Parallel launcher that should be straight forward to adapt to your own workflow following our tutorial:</p> <ol> <li>Create a dedicated script <code>run_&lt;task&gt;</code> responsible to run your java/R/Python tasks while taking as argument the parameter of each run. You can inspire from <code>run_stressme</code> for instance.<ul> <li>test it in interactive</li> </ul> </li> <li> <p>rename the generic launcher <code>launcher.parallel.sh</code> to <code>launcher_&lt;task&gt;.sh</code>,</p> <ul> <li>enable <code>#SBATCH --dependency singleton</code></li> <li>set the jobname</li> <li>change TASK to point to the absolute path to <code>run_&lt;task&gt;</code> script</li> <li>set TASKLISTFILE to point to a files with the parameters to pass to your script for each task</li> <li>adapt eventually the <code>#SBATCH --ntasks-per-node [...]</code> and <code>#SBATCH -c [...]</code> to match your needs AND the hardware configs of a single node (28 cores on iris, 128 cores on Aion) -- see guidelines</li> </ul> </li> <li> <p>test a batch run -- stick to a single node to take the best out of one full node.</p> </li> </ol>"},{"location":"slurm/launchers/#serial-task-script-launcher","title":"Serial Task script Launcher","text":"Serial Killer (Generic template)Serial PythonRMatlab <pre><code>#!/bin/bash -l     # &lt;--- DO NOT FORGET '-l'\n#SBATCH -N 1\n#SBATCH --ntasks-per-node=1\n#SBATCH -c 1\n#SBATCH --time=0-01:00:00\n#SBATCH -p batch\n\nprint_error_and_exit() { echo \"***ERROR*** $*\"; exit 1; }\nmodule purge || print_error_and_exit \"No 'module' command\"\n# C/C++: module load toolchain/intel # OR: module load toolchain/foss\n# Java:  module load lang/Java/1.8\n# Ruby/Perl/Rust...:  module load lang/{Ruby,Perl,Rust...}\n# /!\\ ADAPT TASK variable accordingly - absolute path to the (serial) task to be executed\nTASK=${TASK:=${HOME}/bin/app.exe}\nOPTS=$*\n\nsrun ${TASK} ${OPTS}\n</code></pre> <pre><code>#!/bin/bash -l\n#SBATCH -N 1\n#SBATCH --ntasks-per-node=1\n#SBATCH -c 1\n#SBATCH --time=0-01:00:00\n#SBATCH -p batch\n\nprint_error_and_exit() { echo \"***ERROR*** $*\"; exit 1; }\nmodule purge || print_error_and_exit \"No 'module' command\"\n# Python 3.X by default (also on system)\nmodule load lang/Python\n# module load lang/SciPy-bundle\n# and/or: activate the virtualenv &lt;name&gt; you previously generated with\n#     python -m venv &lt;name&gt;\nsource ./&lt;name&gt;/bin/activate\nOPTS=$*\n\nsrun python [...] ${OPTS}\n</code></pre> <pre><code>#!/bin/bash -l\n#SBATCH -N 1\n#SBATCH --ntasks-per-node=1\n#SBATCH -c 28\n#SBATCH --time=0-01:00:00\n#SBATCH -p batch\n\nprint_error_and_exit() { echo \"***ERROR*** $*\"; exit 1; }\nmodule purge || print_error_and_exit \"No 'module' command\"\nmodule load lang/R\nexport OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK:-1}\nOPTS=$*\n\nsrun Rscript &lt;script&gt;.R ${OPTS}  |&amp; tee job_${SLURM_JOB_NAME}.out\n</code></pre> <p>... but why? just use Python or R.</p> <pre><code>#!/bin/bash -l\n#SBATCH -N 1\n#SBATCH --ntasks-per-node=1\n#SBATCH -c 28\n#SBATCH --time=0-01:00:00\n#SBATCH -p batch\n\nprint_error_and_exit() { echo \"***ERROR*** $*\"; exit 1; }\nmodule purge || print_error_and_exit \"No 'module' command\"\nmodule load math/MATLAB\n\nmatlab -nodisplay -nosplash &lt; INPUTFILE.m &gt; OUTPUTFILE.out\n</code></pre>"},{"location":"slurm/launchers/#specialized-bigdatagpu-launchers","title":"Specialized BigData/GPU launchers","text":"<p>BigData/[Large-]memory single-core tasks</p> <pre><code>#!/bin/bash -l\n### Request one sequential task requiring half the memory of a regular iris node for 1 day\n#SBATCH -J MyLargeMemorySequentialJob       # Job name\n#SBATCH --mail-user=Your.Email@Address.lu   # mail me ...\n#SBATCH --mail-type=end,fail                # ... upon end or failure\n#SBATCH -N 1\n#SBATCH --ntasks-per-node=1\n#SBATCH -c 1\n#SBATCH --mem=64GB         # if above 112GB: consider bigmem partition (USE WITH CAUTION)\n#SBATCH --time=1-00:00:00\n#SBATCH -p batch           # if above 112GB: consider bigmem partition (USE WITH CAUTION)\n\nprint_error_and_exit() { echo \"***ERROR*** $*\"; exit 1; }\nmodule purge || print_error_and_exit \"No 'module' command\"\nmodule load &lt;...&gt;\n# [...]\n</code></pre> <p>AI/DL task  tasks</p> <pre><code>#!/bin/bash -l\n### Request one GPU tasks for 4 hours - dedicate 1/4 of available cores for its management\n#SBATCH -N 1\n#SBATCH --ntasks-per-node=1\n#SBATCH -c 7\n#SBATCH -G 1\n#SBATCH --time=04:00:00\n#SBATCH -p gpu\n\nprint_error_and_exit() { echo \"***ERROR*** $*\"; exit 1; }\nmodule purge || print_error_and_exit \"No 'module' command\"\nmodule load &lt;...&gt;    # USE apps compiled against the {foss,intel}cuda toolchain !\n# Ex: \n# module load numlib/cuDNN\n\n# This should report a single GPU (over 4 available per gpu node)\nnvidia-smi\n# [...]\nsrun [...]\n</code></pre>"},{"location":"slurm/launchers/#pthreadsopenmp-launcher","title":"pthreads/OpenMP Launcher","text":"<p>Always set <code>OMP_NUM_THREADS</code> to match <code>${SLURM_CPUS_PER_TASK:-1}</code></p> <p>You MUST enforce the use of <code>-c &lt;threads&gt;</code> in your launcher to ensure the variable <code>$SLURM_CPUS_PER_TASK</code> exists within your launcher scripts. This is the appropriate value to set for <code>OMP_NUM_THREAD</code>, with default to 1 as extra safely which can be obtained with the following affectation:</p> <pre><code>export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK:-1}\n</code></pre> Aion (default Dual-CPU)Iris (default Dual-CPU) <p>Single node, threaded (pthreads/OpenMP) application launcher</p> <pre><code>#!/bin/bash -l\n# Single node, threaded (pthreads/OpenMP) application launcher, using all 128 cores of an aion cluster node\n#SBATCH -N 1\n#SBATCH --ntasks-per-node=1\n#SBATCH -c 128\n#SBATCH --time=0-01:00:00\n#SBATCH -p batch\n\nprint_error_and_exit() { echo \"***ERROR*** $*\"; exit 1; }\nmodule purge || print_error_and_exit \"No 'module' command\"\nmodule load toolchain/foss\n\nexport OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK:-1}\nOPTS=$*\n\nsrun /path/to/your/threaded.app ${OPTS}\n</code></pre> <p>Single node, threaded (pthreads/OpenMP) application launcher</p> <pre><code>#!/bin/bash -l\n# Single node, threaded (pthreads/OpenMP) application launcher, using all 28 cores of an iris cluster node:\n#SBATCH -N 1\n#SBATCH --ntasks-per-node=1\n#SBATCH -c 28\n#SBATCH --time=0-01:00:00\n#SBATCH -p batch\n\nprint_error_and_exit() { echo \"***ERROR*** $*\"; exit 1; }\nmodule purge || print_error_and_exit \"No 'module' command\"\nmodule load toolchain/foss\n\nexport OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK:-1}\nOPTS=$*\n\nsrun /path/to/your/threaded.app ${OPTS}\n</code></pre>"},{"location":"slurm/launchers/#mpi","title":"MPI","text":""},{"location":"slurm/launchers/#intel-mpi-launchers","title":"Intel MPI Launchers","text":"<p>Official Slurm guide for Intel MPI</p> Aion (default Dual-CPU)Iris (default Dual-CPU) <p>Multi-node parallel application IntelMPI launcher</p> <p><pre><code>#!/bin/bash -l\n# Multi-node parallel application IntelMPI launcher, using 256 MPI processes\n\n#SBATCH -N 2\n#SBATCH --ntasks-per-node 128    # MPI processes per node\n#SBATCH -c 1\n#SBATCH --time=0-01:00:00\n#SBATCH -p batch\n\nprint_error_and_exit() { echo \"***ERROR*** $*\"; exit 1; }\nmodule purge || print_error_and_exit \"No 'module' command\"\nmodule load toolchain/intel\nOPTS=$*\n\nsrun -n $SLURM_NTASKS /path/to/your/intel-toolchain-compiled-application ${OPTS}\n</code></pre> Recall to use <code>si-bigmem</code> to request an interactive job when testing your script. </p> <p>Multi-node parallel application IntelMPI launcher</p> <p><pre><code>#!/bin/bash -l\n# Multi-node parallel application IntelMPI launcher, using 56 MPI processes\n\n#SBATCH -N 2\n#SBATCH --ntasks-per-node 28    # MPI processes per node\n#SBATCH -c 1\n#SBATCH --time=0-01:00:00\n#SBATCH -p batch\n\nprint_error_and_exit() { echo \"***ERROR*** $*\"; exit 1; }\nmodule purge || print_error_and_exit \"No 'module' command\"\nmodule load toolchain/intel\nOPTS=$*\n\nsrun -n $SLURM_NTASKS /path/to/your/intel-toolchain-compiled-application ${OPTS}\n</code></pre> Recall to use <code>si-gpu</code> to request an interactive job when testing your script on a GPU node. </p> <p>You may want to use PMIx as MPI initiator -- use <code>srun --mpi=list</code> to list the available implementations (default: pmi2), and <code>srun --mpi=pmix[_v3] [...]</code> to use PMIx.</p>"},{"location":"slurm/launchers/#openmpi-slurm-launchers","title":"OpenMPI Slurm Launchers","text":"<p>Official Slurm guide for Open MPI</p> Aion (default Dual-CPU)Iris (default Dual-CPU) <p>Multi-node parallel application OpenMPI launcher</p> <pre><code>#!/bin/bash -l\n# Multi-node parallel application OpenMPI launcher, using 256 MPI processes\n\n#SBATCH -N 2\n#SBATCH --ntasks-per-node 128    # MPI processes per node\n#SBATCH -c 1\n#SBATCH --time=0-01:00:00\n#SBATCH -p batch\n\nprint_error_and_exit() { echo \"***ERROR*** $*\"; exit 1; }\nmodule purge || print_error_and_exit \"No 'module' command\"\nmodule load toolchain/foss\nmodule load mpi/OpenMPI\nOPTS=$*\n\nsrun -n $SLURM_NTASKS /path/to/your/foss-toolchain-openMPIcompiled-application ${OPTS}\n</code></pre> <p>Multi-node parallel application OpenMPI launcher</p> <pre><code>#!/bin/bash -l\n# Multi-node parallel application OpenMPI launcher, using 56 MPI processes\n\n#SBATCH -N 2\n#SBATCH --ntasks-per-node 28    # MPI processes per node\n#SBATCH -c 1\n#SBATCH --time=0-01:00:00\n#SBATCH -p batch\n\nprint_error_and_exit() { echo \"***ERROR*** $*\"; exit 1; }\nmodule purge || print_error_and_exit \"No 'module' command\"\nmodule load toolchain/foss\nmodule load mpi/OpenMPI\nOPTS=$*\n\nsrun -n $SLURM_NTASKS /path/to/your/foss-toolchain-openMPIcompiled-application ${OPTS}\n</code></pre>"},{"location":"slurm/launchers/#hybrid-intel-mpiopenmp-launcher","title":"Hybrid Intel MPI+OpenMP Launcher","text":"Aion (default Dual-CPU)Iris (default Dual-CPU) <p>Multi-node hybrid parallel application IntelMPI/OpenMP launcher</p> <pre><code>#!/bin/bash -l\n# Multi-node hybrid application IntelMPI+OpenMP launcher, using 16 threads per socket(CPU) on 2 nodes (256 cores):\n\n#SBATCH -N 2\n#SBATCH --ntasks-per-node   8    # MPI processes per node\n#SBATCH --ntasks-per-socket 1    # MPI processes per (virtual) processor\n#SBATCH -c 16\n#SBATCH --time=0-01:00:00\n#SBATCH -p batch\n\nprint_error_and_exit() { echo \"***ERROR*** $*\"; exit 1; }\nmodule purge || print_error_and_exit \"No 'module' command\"\nmodule load toolchain/intel\nexport OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK:-1}\nOPTS=$*\n\nsrun -n $SLURM_NTASKS /path/to/your/parallel-hybrid-app ${OPTS}\n</code></pre> <p>Multi-node hybrid parallel application IntelMPI/OpenMP launcher</p> <pre><code>#!/bin/bash -l\n# Multi-node hybrid application IntelMPI+OpenMP launcher, using 14 threads per socket(CPU) on 2 nodes (56 cores):\n\n#SBATCH -N 2\n#SBATCH --ntasks-per-node   2    # MPI processes per node\n#SBATCH --ntasks-per-socket 1    # MPI processes per processor\n#SBATCH -c 14\n#SBATCH --time=0-01:00:00\n#SBATCH -p batch\n\nprint_error_and_exit() { echo \"***ERROR*** $*\"; exit 1; }\nmodule purge || print_error_and_exit \"No 'module' command\"\nmodule load toolchain/intel\nexport OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK:-1}\nOPTS=$*\n\nsrun -n $SLURM_NTASKS /path/to/your/parallel-hybrid-app ${OPTS}\n</code></pre>"},{"location":"slurm/launchers/#hybrid-openmpiopenmp-launcher","title":"Hybrid OpenMPI+OpenMP Launcher","text":"Aion (default Dual-CPU)Iris (default Dual-CPU) <p>Multi-node hybrid parallel application OpenMPI/OpenMP launcher</p> <pre><code>#!/bin/bash -l\n# Multi-node hybrid application OpenMPI+OpenMP launcher, using 16 threads per socket(CPU) on 2 nodes (256 cores):\n\n#SBATCH -N 2\n#SBATCH --ntasks-per-node   8    # MPI processes per node\n#SBATCH --ntasks-per-socket 1    # MPI processes per processor\n#SBATCH -c 16\n#SBATCH --time=0-01:00:00\n#SBATCH -p batch\n\nprint_error_and_exit() { echo \"***ERROR*** $*\"; exit 1; }\nmodule purge || print_error_and_exit \"No 'module' command\"\nmodule load toolchain/foss\nmodule load mpi/OpenMPI\nexport OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK:-1}\nOPTS=$*\n\nsrun -n $SLURM_NTASKS /path/to/your/parallel-hybrid-app ${OPTS}\n</code></pre> <p>Multi-node hybrid parallel application OpenMPI/OpenMP launcher</p> <pre><code>#!/bin/bash -l\n# Multi-node hybrid application OpenMPI+OpenMP launcher, using 14 threads per socket(CPU) on 2 nodes (56 cores):\n\n#SBATCH -N 2\n#SBATCH --ntasks-per-node   2    # MPI processes per node\n#SBATCH --ntasks-per-socket 1    # MPI processes per processor\n#SBATCH -c 14\n#SBATCH --time=0-01:00:00\n#SBATCH -p batch\n\nprint_error_and_exit() { echo \"***ERROR*** $*\"; exit 1; }\nmodule purge || print_error_and_exit \"No 'module' command\"\nmodule load toolchain/foss\nmodule load mpi/OpenMPI\nexport OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK:-1}\nOPTS=$*\n\nsrun -n $SLURM_NTASKS /path/to/your/parallel-hybrid-app ${OPTS}\n</code></pre>"},{"location":"slurm/partitions/","title":"ULHPC Slurm Partitions","text":"<p>In Slurm multiple nodes can be grouped into partitions which are sets of nodes aggregated by shared characteristics or objectives, with associated limits for wall-clock time, job size, etc. These limits are hard limits for the jobs and can not be overruled.</p> <p>To select a given partition with a Slurm command, use the <code>--partition=&lt;partition&gt;</code> option:</p> <pre><code>srun|sbatch|salloc|sinfo|squeue... --partition=&lt;partition&gt; [...]\n</code></pre> <p>You will find on ULHPC resources the following partitions (mostly matching the 3 types of computing resources)</p> <ul> <li><code>batch</code> is intended for running parallel scientific applications as passive jobs on \"regular\" nodes (Dual CPU, no accelerators, 128 to 256 GB of RAM)</li> <li><code>gpu</code> is intended for running GPU-accelerated scientific applications  as passive jobs on \"gpu\" nodes (Dual CPU, 4 Nvidia accelerators, 768 GB RAM)</li> <li><code>bigmem</code> is dedicated for memory intensive data processing jobs on \"bigmem\" nodes (Quad-CPU, no accelerators, 3072 GB RAM)</li> <li><code>interactive</code>: a floating partition intended for quick interactive jobs, allowing for quick tests and compilation/preparation work.<ul> <li>this is the only partition crossing all type of nodes (thus floating).</li> <li>use <code>si</code>, <code>si-gpu</code> or <code>si-bigmem</code> to submit an interactive job on either a regular, gpu or bigmem node</li> </ul> </li> </ul>"},{"location":"slurm/partitions/#aion","title":"Aion","text":"AION      (type) #Nodes (cores/node) Default/MaxTime MaxNodes PriorityTier <code>interactive</code> (floating) 354 30min - 2h 2 100 <code>batch</code> (default) 354    (128c) 2h    - 48h 64 1"},{"location":"slurm/partitions/#iris","title":"Iris","text":"IRIS       (type) #Nodes (cores/n) Default/MaxTime MaxNodes PriorityTier <code>interactive</code> (floating) 196 30min - 2h 2 100 <code>batch</code> (default) 168     (28c) 2h    - 48h 64 1 <code>gpu</code> 24      (28c) 2h    - 48h 4 1 <code>hopper</code> 1       (112c) 2h    - 48h 1 1 <code>bigmem</code> 4       (112c) 2h    - 48h 1 1"},{"location":"slurm/partitions/#queuespartitions-state-information","title":"Queues/Partitions State Information","text":"<p>For detailed information about all available partitions and their definition/limits: <pre><code>scontrol show partitions [name]\n</code></pre></p>"},{"location":"slurm/partitions/#partition-load-status","title":"Partition load status","text":"<p>You can of course use <code>squeue --partition=&lt;partition&gt;</code> to list the jobs currently scheduled on a given, partition <code>&lt;partition&gt;</code>.</p> <p>As part of the custom ULHPC Slurm helpers defined in <code>/etc/profile.d/slurm.sh</code>, the following commands have been made to facilitate the review of the current load usage of the partitions.</p> Command Description <code>irisstat</code>, <code>aionstat</code> report cluster status (utilization, partition and QOS live stats) <code>pload [-a] i/b/g/m</code> Overview of the Slurm partition load <code>listpartitionjobs &lt;part&gt;</code> List jobs (and current load) of the slurm partition <code>&lt;part&gt;</code> <p>Partition load with <code>pload</code></p> <pre><code>$ pload -h\nUsage: pload [-a] [--no-header] &lt;partition&gt;\n =&gt; Show current load of the slurm partition &lt;partition&gt;, eventually without header\n    &lt;partition&gt; shortcuts: i=interactive b=batch g=gpu m=bigmem\n Options:\n   -a: show all partition\n$ pload -a\n  Partition  CPU Max  CPU Used  CPU Free     Usage[%]\n      batch     4704      4223       481       89.8%\n        gpu      672       412       260       61.3% GPU: 61/96 (63.5%)\n     bigmem      448       431        17       96.2%\n</code></pre>"},{"location":"slurm/partitions/#partition-limits","title":"Partition Limits","text":"<p>At partition level, only the following limits can be enforced:</p> <ul> <li><code>DefaultTime</code>:       Default time limit</li> <li><code>MaxNodes</code>:          Maximum number of nodes per job</li> <li><code>MinNodes</code>:          Minimum number of nodes per job</li> <li><code>MaxCPUsPerNode</code>:    Maximum number of CPUs job can be allocated on any node</li> <li><code>MaxMemPerCPU/Node</code>: Maximum memory job can be allocated on any CPU or node</li> <li><code>MaxTime</code>:           Maximum length of time user's job can run</li> </ul>"},{"location":"slurm/qos/","title":"ULHPC Slurm QoS","text":"<p>Quality of Service or QoS is used to constrain or modify the characteristics that a job can have. This could come in the form of specifying a QoS to request for a longer run time or a high priority queue for a given job.</p> <p>To select a given QoS with a Slurm command, use the <code>--qos=&lt;QoS&gt;</code> option (available onlt in long form):</p> <pre><code>srun|sbatch|salloc|sinfo|squeue... [--partition=&lt;partition&gt;] --qos=&lt;QoS&gt; [...]\n</code></pre> <p>The default QoS of your jobs depends on your account and affiliation. Normally, the <code>--qos=&lt;QoS&gt;</code> directive does not need to be set for most jobs</p> <p>We favor in general cross-partition QoS, mainly tied to priority level (<code>low</code> \\rightarrow <code>urgent</code>). A special preemptible QoS exists for best-effort jobs and is named <code>besteffort</code>.</p>"},{"location":"slurm/qos/#available-qoss","title":"Available QoS's","text":"QoS cluster partition Prio GrpTRES MaxTresPJ MaxJobPU MaxWall <code>besteffort</code> * * 1 300 50-00:00:00 <code>low</code> * * 10 200 <code>normal</code> * * 100 100 <code>high</code> * * 200 50 <code>urgent</code> * * 1000 20 <code>debug</code> * interactive 150 node=50 10 <code>wide</code> * * 100 node=160 10 0-02:00:00 <code>aion-batch-long</code> aion batch 100 node=64 node=16 8 14-00:00:00 <code>iris-batch-long</code> iris batch 100 node=24 node=16 8 14-00:00:00 <code>iris-gpu-long</code> iris gpu 100 node=6 node=2 4 14-00:00:00 <code>iris-bigmem-long</code> iris bigmem 100 node=2 node=2 4 14-00:00:00 <code>iris-hopper</code> iris hopper 100 100 14-00:00:00 <code>iris-hopper-long</code> iris hopper 100 gres/gpu=2 gres/gpu=1 100 14-00:00:00"},{"location":"slurm/qos/#list-qos-limits","title":"List QoS Limits","text":"<p>Use the <code>sqos</code> utility function to list the existing QOS limits.</p> <p>List current ULHPC QOS limits with <code>sqos</code></p> <pre><code>$ sqos\n# sacctmgr show qos  format=\"name%20,preempt,priority,GrpTRES,MaxTresPerJob,MaxJobsPerUser,MaxWall,flags\"\n                Name    Preempt   Priority       GrpTRES       MaxTRES MaxJobsPU     MaxWall                Flags\n-------------------- ---------- ---------- ------------- ------------- --------- ----------- --------------------\n              normal besteffort        100                                   100                      DenyOnLimit\n          besteffort                     1                                   300 50-00:00:00            NoReserve\n                 low besteffort         10                                   200                      DenyOnLimit\n                high besteffort        200                                    50                      DenyOnLimit\n              urgent besteffort       1000                                    20                      DenyOnLimit\n               debug besteffort        150       node=50                      10                      DenyOnLimit\n               admin besteffort       1000                                                            DenyOnLimit\n                wide besteffort        100                    node=160        10    02:00:00          DenyOnLimit\n     aion-batch-long besteffort        100       node=64       node=16         8 14-00:00:00 DenyOnLimit,Partiti+\n     iris-batch-long besteffort        100       node=24       node=16         8 14-00:00:00 DenyOnLimit,Partiti+\n       iris-gpu-long besteffort        100        node=6        node=2         4 14-00:00:00 DenyOnLimit,Partiti+\n    iris-bigmem-long besteffort        100        node=2        node=2         4 14-00:00:00 DenyOnLimit,Partiti+\n         iris-hopper besteffort        100                                   100                      DenyOnLimit\n    iris-hopper-long besteffort        100    gres/gpu=2    gres/gpu=1       100 14-00:00:00 DenyOnLimit,Partiti+\n</code></pre> <p>What are the possible limits set on ULHPC QoS?</p> <p>At the QoS level, the following elements are composed to define the resource limits for our QoS:</p> <ul> <li>Limits on Trackable RESources TRES - a resource (nodes, cpus, gpus, etc.) tracked for usage or used to enforce limits against, in particular:<ul> <li><code>GrpTRES</code>: The total count of TRES able to be used at any given time from all jobs running from the QoS; if this limit is reached new jobs will be queued but only allowed to run after resources have been relinquished from this group.</li> <li><code>MaxTresPerJob</code>: the maximum size in TRES any given job can have from the QoS.</li> </ul> </li> <li><code>MaxJobsPerUser</code>: The maximum number of jobs a user can have running at a given time.</li> <li><code>MaxWall[DurationPerJob]</code>: The maximum wall clock time any individual job can run for in the given QoS.</li> </ul> <p>As explained in the Limits section, there are basically three layers of Slurm limits, from lower to higher precedence:</p> <ol> <li>None</li> <li>Partitions</li> <li>Account associations: Root/Cluster -&gt; Account (ascending the hierarchy) -&gt; User</li> <li>Job/Partition QoS</li> </ol>"},{"location":"software/","title":"List of all softwares","text":"Software Versions Swsets Architectures Clusters Category Description ABAQUS 2018, 2021 2019b, 2020b broadwell, skylake, epyc iris, aion CFD/Finite element modelling Finite Element Analysis software for modeling, visualization and best-in-class implicit and explicit dynamics FEA. ABINIT 9.4.1 2020b epyc aion Chemistry ABINIT is a package whose main program allows one to find the total energy, charge density and electronic structure of systems made of electrons and nuclei (molecules and periodic solids) within Density Functional Theory (DFT), using pseudopotentials and a planewave or wavelet basis. ABySS 2.2.5 2020b broadwell, epyc, skylake aion, iris Biology Assembly By Short Sequences - a de novo, parallel, paired-end sequence assembler ACTC 1.1 2019b, 2020b broadwell, skylake, epyc iris, aion Libraries ACTC converts independent triangles into triangle strips or fans. ANSYS 19.4, 21.1 2019b, 2020b broadwell, skylake, epyc iris, aion Utilities ANSYS simulation software enables organizations to confidently predict how their products will operate in the real world. We believe that every product is a promise of something greater. AOCC 3.1.0 2020b epyc aion Compilers AMD Optimized C/C++ &amp; Fortran compilers (AOCC) based on LLVM 12.0 ASE 3.19.0, 3.20.1, 3.21.1 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Chemistry ASE is a python package providing an open source Atomic Simulation Environment in the Python scripting language. From version 3.20.1 we also include the ase-ext package, it contains optional reimplementations in C of functions in ASE.  ASE uses it automatically when installed. ATK 2.34.1, 2.36.0 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation ATK provides the set of accessibility interfaces that are implemented by other toolkits and applications. Using the ATK interfaces, accessibility tools have full access to view and control running applications. Advisor 2019_update5 2019b broadwell, skylake iris Performance measurements Vectorization Optimization and Thread Prototyping - Vectorize &amp; thread code or performance \u201cdies\u201d - Easy workflow + data + tips = faster code faster - Prioritize, Prototype &amp; Predict performance gain Anaconda3 2020.02, 2020.11 2019b, 2020b broadwell, skylake, epyc iris, aion Programming Languages Built to complement the rich, open source Python community, the Anaconda platform provides an enterprise-ready data analytics platform that empowers companies to adopt a modern open data science analytics architecture. ArmForge 20.0.3 2019b, 2020b broadwell, skylake, epyc iris, aion Utilities The industry standard development package for C, C++ and Fortran high performance code on Linux. Forge is designed to handle the complex software projects - including parallel, multiprocess and multithreaded code. Arm Forge combines an industry-leading debugger, Arm DDT, and an out-of-the-box-ready profiler, Arm MAP. ArmReports 20.0.3 2019b, 2020b broadwell, skylake, epyc iris, aion Utilities Arm Performance Reports - a low-overhead tool that produces one-page text and HTML reports summarizing and characterizing both scalar and MPI application performance. Arm Performance Reports runs transparently on optimized production-ready codes by adding a single command to your scripts, and provides the most effective way to characterize and understand the performance of HPC application runs. Armadillo 10.5.3, 9.900.1 2020b, 2019b broadwell, epyc, skylake aion, iris Numerical libraries Armadillo is an open-source C++ linear algebra library (matrix maths) aiming towards a good balance between speed and ease of use. Integer, floating point and complex numbers are supported, as well as a subset of trigonometric and statistics functions. Arrow 0.16.0 2019b broadwell, skylake iris Data processing Apache Arrow (incl. PyArrow Python bindings)), a cross-language development platform for in-memory data. Aspera-CLI 3.9.1, 3.9.6 2019b, 2020b broadwell, skylake, epyc iris, aion Utilities IBM Aspera Command-Line Interface (the Aspera CLI) is a collection of Aspera tools for performing high-speed, secure data transfers from the command line. The Aspera CLI is for users and organizations who want to automate their transfer workflows. Autoconf 2.69 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development Autoconf is an extensible package of M4 macros that produce shell scripts to automatically configure software source code packages. These scripts can adapt the packages to many kinds of UNIX-like systems without manual user intervention. Autoconf creates a configuration script for a package from a template file that lists the operating system features that the package can use, in the form of M4 macro calls. Automake 1.16.1, 1.16.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development Automake: GNU Standards-compliant Makefile generator Autotools 20180311, 20200321 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development This bundle collect the standard GNU build tools: Autoconf, Automake and libtool BEDTools 2.29.2, 2.30.0 2019b, 2020b broadwell, skylake, epyc iris, aion Biology BEDTools: a powerful toolset for genome arithmetic. The BEDTools utilities allow one to address common genomics tasks such as finding feature overlaps and computing coverage. The utilities are largely based on four widely-used file formats: BED, GFF/GTF, VCF, and SAM/BAM. BLAST+ 2.11.0, 2.9.0 2020b, 2019b broadwell, epyc, skylake aion, iris Biology Basic Local Alignment Search Tool, or BLAST, is an algorithm for comparing primary biological sequence information, such as the amino-acid sequences of different proteins or the nucleotides of DNA sequences. BWA 0.7.17 2019b, 2020b broadwell, skylake, epyc iris, aion Biology Burrows-Wheeler Aligner (BWA) is an efficient program that aligns relatively short nucleotide sequences against a long reference sequence such as the human genome. BamTools 2.5.1 2019b, 2020b broadwell, skylake, epyc iris, aion Biology BamTools provides both a programmer's API and an end-user's toolkit for handling BAM files. Bazel 0.26.1, 0.29.1, 3.7.2 2019b, 2020b gpu, broadwell, skylake, epyc iris, aion Development Bazel is a build tool that builds code quickly and reliably. It is used to build the majority of Google's software. BioPerl 1.7.2, 1.7.8 2019b, 2020b broadwell, skylake, epyc iris, aion Biology Bioperl is the product of a community effort to produce Perl code which is useful in biology. Examples include Sequence objects, Alignment objects and database searching objects. Bison 3.3.2, 3.5.3, 3.7.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages Bison is a general-purpose parser generator that converts an annotated context-free grammar into a deterministic LR or generalized LR (GLR) parser employing LALR(1) parser tables. Boost.Python 1.74.0 2020b broadwell, epyc, skylake aion, iris Libraries Boost.Python is a C++ library which enables seamless interoperability between C++ and the Python programming language. Boost 1.71.0, 1.74.0 2019b, 2020b broadwell, skylake, epyc iris, aion Development Boost provides free peer-reviewed portable C++ source libraries. Bowtie2 2.3.5.1, 2.4.2 2019b, 2020b broadwell, skylake, epyc iris, aion Biology Bowtie 2 is an ultrafast and memory-efficient tool for aligning sequencing reads to long reference sequences. It is particularly good at aligning reads of about 50 up to 100s or 1,000s of characters, and particularly good at aligning to relatively long (e.g. mammalian) genomes. Bowtie 2 indexes the genome with an FM Index to keep its memory footprint small: for the human genome, its memory footprint is typically around 3.2 GB. Bowtie 2 supports gapped, local, and paired-end alignment modes. CGAL 4.14.1, 5.2 2019b, 2020b broadwell, skylake, epyc iris, aion Numerical libraries The goal of the CGAL Open Source Project is to provide easy access to efficient and reliable geometric algorithms in the form of a C++ library. CMake 3.15.3, 3.18.4, 3.20.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development CMake, the cross-platform, open-source build system.  CMake is a family of tools designed to build, test and package software. CPLEX 12.10 2019b broadwell, skylake iris Mathematics IBM ILOG CPLEX Optimizer's mathematical programming technology enables analytical decision support for improving efficiency, reducing costs, and increasing profitability. CRYSTAL 17 2019b broadwell, skylake iris Chemistry The CRYSTAL package performs ab initio calculations of the ground state energy, energy gradient, electronic wave function and properties of periodic systems. Hartree-Fock or Kohn- Sham Hamiltonians (that adopt an Exchange-Correlation potential following the postulates of Density-Functional Theory) can be used. CUDA 10.1.243, 11.1.1 2019b, 2020b gpu iris System-level software CUDA (formerly Compute Unified Device Architecture) is a parallel computing platform and programming model created by NVIDIA and implemented by the graphics processing units (GPUs) that they produce. CUDA gives developers access to the virtual instruction set and memory of the parallel computational elements in CUDA GPUs. CUDAcore 11.1.1 2020b gpu iris System-level software CUDA (formerly Compute Unified Device Architecture) is a parallel computing platform and programming model created by NVIDIA and implemented by the graphics processing units (GPUs) that they produce. CUDA gives developers access to the virtual instruction set and memory of the parallel computational elements in CUDA GPUs. Check 0.15.2 2020b gpu iris Libraries Check is a unit testing framework for C. It features a simple interface for defining unit tests, putting little in the way of the developer. Tests are run in a separate address space, so both assertion failures and code errors that cause segmentation faults or other signals can be caught. Test results are reportable in the following: Subunit, TAP, XML, and a generic logging format. Clang 11.0.1, 9.0.1 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Compilers C, C++, Objective-C compiler, based on LLVM.  Does not include C++ standard library -- use libstdc++ from GCC. CubeGUI 4.4.4 2019b broadwell, skylake iris Performance measurements Cube, which is used as performance report explorer for Scalasca and Score-P, is a generic tool for displaying a multi-dimensional performance space consisting of the dimensions (i) performance metric, (ii) call path, and (iii) system resource. Each dimension can be represented as a tree, where non-leaf nodes of the tree can be collapsed or expanded to achieve the desired level of granularity. This module provides the Cube graphical report explorer. CubeLib 4.4.4 2019b broadwell, skylake iris Performance measurements Cube, which is used as performance report explorer for Scalasca and Score-P, is a generic tool for displaying a multi-dimensional performance space consisting of the dimensions (i) performance metric, (ii) call path, and (iii) system resource. Each dimension can be represented as a tree, where non-leaf nodes of the tree can be collapsed or expanded to achieve the desired level of granularity. This module provides the Cube general purpose C++ library component and command-line tools. CubeWriter 4.4.3 2019b broadwell, skylake iris Performance measurements Cube, which is used as performance report explorer for Scalasca and Score-P, is a generic tool for displaying a multi-dimensional performance space consisting of the dimensions (i) performance metric, (ii) call path, and (iii) system resource. Each dimension can be represented as a tree, where non-leaf nodes of the tree can be collapsed or expanded to achieve the desired level of granularity. This module provides the Cube high-performance C writer library component. DB 18.1.32, 18.1.40 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Utilities Berkeley DB enables the development of custom data management solutions, without the overhead traditionally associated with such custom projects. DB_File 1.855 2020b broadwell, epyc, skylake aion, iris Data processing Perl5 access to Berkeley DB version 1.x. DBus 1.13.12, 1.13.18 2019b, 2020b broadwell, skylake, epyc iris, aion Development D-Bus is a message bus system, a simple way for applications to talk to one another.  In addition to interprocess communication, D-Bus helps coordinate process lifecycle; it makes it simple and reliable to code a \"single instance\" application or daemon, and to launch applications and daemons on demand when their services are needed. DMTCP 2.5.2 2019b broadwell, skylake iris Utilities DMTCP is a tool to transparently checkpoint the state of multiple simultaneous applications, including multi-threaded and distributed applications. It operates directly on the user binary executable, without any Linux kernel modules or other kernel modifications. Dakota 6.11.0, 6.15.0 2019b, 2020b broadwell, skylake iris Mathematics The Dakota project delivers both state-of-the-art research and robust, usable software for optimization and UQ. Broadly, the Dakota software's advanced parametric analyses enable design exploration, model calibration, risk analysis, and quantification of margins and uncertainty with computational models.\" Doxygen 1.8.16, 1.8.20 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development Doxygen is a documentation system for C++, C, Java, Objective-C, Python, IDL (Corba and Microsoft flavors), Fortran, VHDL, PHP, C#, and to some extent D. ELPA 2019.11.001, 2020.11.001 2019b, 2020b broadwell, epyc, skylake iris, aion Mathematics Eigenvalue SoLvers for Petaflop-Applications . EasyBuild 4.3.0, 4.3.3, 4.4.1, 4.4.2, 4.5.4 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities EasyBuild is a software build and installation framework written in Python that allows you to install software in a structured, repeatable and robust way. Eigen 3.3.7, 3.3.8, 3.4.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Mathematics Eigen is a C++ template library for linear algebra: matrices, vectors, numerical solvers, and related algorithms. Elk 6.3.2, 7.0.12 2019b, 2020b broadwell, skylake, epyc iris, aion Physics An all-electron full-potential linearised augmented-plane wave (FP-LAPW) code with many advanced features. Written originally at Karl-Franzens-Universit\u00e4t Graz as a milestone of the EXCITING EU Research and Training Network, the code is designed to be as simple as possible so that new developments in the field of density functional theory (DFT) can be added quickly and reliably. FDS 6.7.1, 6.7.6 2019b, 2020b broadwell, skylake, epyc iris, aion Physics Fire Dynamics Simulator (FDS) is a large-eddy simulation (LES) code for low-speed flows, with an emphasis on smoke and heat transport from fires. FFTW 3.3.8 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Numerical libraries FFTW is a C subroutine library for computing the discrete Fourier transform (DFT) in one or more dimensions, of arbitrary input size, and of both real and complex data. FFmpeg 4.2.1, 4.3.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation A complete, cross-platform solution to record, convert and stream audio and video. FLAC 1.3.3 2020b broadwell, epyc, skylake, gpu aion, iris Libraries FLAC stands for Free Lossless Audio Codec, an audio format similar to MP3, but lossless, meaning that audio is compressed in FLAC without any loss in quality. FLTK 1.3.5 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation FLTK is a cross-platform C++ GUI toolkit for UNIX/Linux (X11), Microsoft Windows, and MacOS X. FLTK provides modern GUI functionality without the bloat and supports 3D graphics via OpenGL and its built-in GLUT emulation. FastQC 0.11.9 2019b, 2020b broadwell, skylake, epyc iris, aion Biology FastQC is a quality control application for high throughput sequence data. It reads in sequence data in a variety of formats and can either provide an interactive application to review the results of several different QC checks, or create an HTML based report which can be integrated into a pipeline. Flask 1.1.2 2020b broadwell, epyc, skylake, gpu aion, iris Libraries Flask is a lightweight WSGI web application framework. It is designed to make getting started quick and easy, with the ability to scale up to complex applications. This module includes the Flask extensions: Flask-Cors Flink 1.11.2 2020b broadwell, epyc, skylake aion, iris Development Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale. FreeImage 3.18.0 2020b broadwell, epyc, skylake aion, iris Visualisation FreeImage is an Open Source library project for developers who would like to support popular graphics image formats like PNG, BMP, JPEG, TIFF and others as needed by today's multimedia applications. FreeImage is easy to use, fast, multithreading safe. FriBidi 1.0.10, 1.0.5 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Programming Languages The Free Implementation of the Unicode Bidirectional Algorithm. GCC 10.2.0, 8.3.0 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Compilers The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Java, and Ada, as well as libraries for these languages (libstdc++, libgcj,...). GCCcore 10.2.0, 8.3.0 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Compilers The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Java, and Ada, as well as libraries for these languages (libstdc++, libgcj,...). GDAL 3.0.2, 3.2.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Data processing GDAL is a translator library for raster geospatial data formats that is released under an X/MIT style Open Source license by the Open Source Geospatial Foundation. As a library, it presents a single abstract data model to the calling application for all supported formats. It also comes with a variety of useful commandline utilities for data translation and processing. GDB 10.1, 9.1 2020b, 2019b broadwell, epyc, skylake aion, iris Debugging The GNU Project Debugger GDRCopy 2.1 2020b gpu iris Libraries A low-latency GPU memory copy library based on NVIDIA GPUDirect RDMA technology. GEOS 3.8.0, 3.9.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Mathematics GEOS (Geometry Engine - Open Source) is a C++ port of the Java Topology Suite (JTS) GLPK 4.65 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Utilities The GLPK (GNU Linear Programming Kit) package is intended for solving large-scale linear programming (LP), mixed integer programming (MIP), and other related problems. It is a set of routines written in ANSI C and organized in the form of a callable library. GLib 2.62.0, 2.66.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation GLib is one of the base libraries of the GTK+ project GMP 6.1.2, 6.2.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Mathematics GMP is a free library for arbitrary precision arithmetic, operating on signed integers, rational numbers, and floating point numbers. GObject-Introspection 1.63.1, 1.66.1 2019b, 2020b broadwell, skylake, epyc iris, aion Development GObject introspection is a middleware layer between C libraries (using GObject) and language bindings. The C library can be scanned at compile time and generate a metadata file, in addition to the actual native C library. Then at runtime, language bindings can read this metadata and automatically provide bindings to call into the C library. GPAW-setups 0.9.20000 2019b broadwell, skylake iris Chemistry PAW setup for the GPAW Density Functional Theory package. Users can install setups manually using 'gpaw install-data' or use setups from this package. The versions of GPAW and GPAW-setups can be intermixed. GPAW 20.1.0 2019b broadwell, skylake iris Chemistry GPAW is a density-functional theory (DFT) Python code based on the projector-augmented wave (PAW) method and the atomic simulation environment (ASE). It uses real-space uniform grids and multigrid methods or atom-centered basis-functions. GROMACS 2019.4, 2019.6, 2020, 2021, 2021.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Biology GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. This is a CPU only build, containing both MPI and threadMPI builds for both single and double precision. It also contains the gmxapi extension for the single precision MPI build. GSL 2.6 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Numerical libraries The GNU Scientific Library (GSL) is a numerical library for C and C++ programmers. The library provides a wide range of mathematical routines such as random number generators, special functions and least-squares fitting. GTK+ 3.24.13, 3.24.23 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation GTK+ is the primary library used to construct user interfaces in GNOME. It provides all the user interface controls, or widgets, used in a common graphical application. Its object-oriented API allows you to construct user interfaces without dealing with the low-level details of drawing and device interaction. Gdk-Pixbuf 2.38.2, 2.40.0 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation The Gdk Pixbuf is a toolkit for image loading and pixel buffer manipulation. It is used by GTK+ 2 and GTK+ 3 to load and manipulate images. In the past it was distributed as part of GTK+ 2 but it was split off into a separate package in preparation for the change to GTK+ 3. Ghostscript 9.50, 9.53.3 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities Ghostscript is a versatile processor for PostScript data with the ability to render PostScript to different targets. It used to be part of the cups printing stack, but is no longer used for that. Go 1.14.1, 1.16.6 2019b, 2020b broadwell, skylake, epyc iris, aion Compilers Go is an open source programming language that makes it easy to build simple, reliable, and efficient software. Guile 1.8.8, 2.2.4 2019b broadwell, skylake iris Programming Languages Guile is a programming language, designed to help programmers create flexible applications that can be extended by users or other programmers with plug-ins, modules, or scripts. Gurobi 9.0.0, 9.1.2 2019b, 2020b broadwell, skylake, epyc iris, aion Mathematics The Gurobi Optimizer is a state-of-the-art solver for mathematical programming. The solvers in the Gurobi Optimizer were designed from the ground up to exploit modern architectures and multi-core processors, using the most advanced implementations of the latest algorithms. HDF5 1.10.5, 1.10.7 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Data processing HDF5 is a data model, library, and file format for storing and managing data. It supports an unlimited variety of datatypes, and is designed for flexible and efficient I/O and for high volume and complex data. HDF 4.2.15 2020b broadwell, epyc, skylake, gpu aion, iris Data processing HDF (also known as HDF4) is a library and multi-object file format for storing and managing data between machines. HTSlib 1.10.2, 1.12 2019b, 2020b broadwell, skylake, epyc iris, aion Biology A C library for reading/writing high-throughput sequencing data. This package includes the utilities bgzip and tabix Hadoop 2.10.0 2020b broadwell, epyc, skylake aion, iris Utilities Hadoop MapReduce by Cloudera HarfBuzz 2.6.4, 2.6.7 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation HarfBuzz is an OpenType text shaping engine. Harminv 1.4.1 2019b broadwell, skylake iris Mathematics Harminv is a free program (and accompanying library) to solve the problem of harmonic inversion - given a discrete-time, finite-length signal that consists of a sum of finitely-many sinusoids (possibly exponentially decaying) in a given bandwidth, it determines the frequencies, decay constants, amplitudes, and phases of those sinusoids. Horovod 0.19.1, 0.22.0 2019b, 2020b broadwell, skylake, gpu iris Utilities Horovod is a distributed training framework for TensorFlow. Hypre 2.20.0 2020b broadwell, epyc, skylake aion, iris Numerical libraries Hypre is a library for solving large, sparse linear systems of equations on massively parallel computers. The problems of interest arise in the simulation codes being developed at LLNL and elsewhere to study physical phenomena in the defense, environmental, energy, and biological sciences. ICU 64.2, 67.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries ICU is a mature, widely used set of C/C++ and Java libraries providing Unicode and Globalization support for software applications. ISL 0.23 2020b broadwell, epyc, skylake aion, iris Mathematics isl is a library for manipulating sets and relations of integer points bounded by linear constraints. ImageMagick 7.0.10-35, 7.0.9-5 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Visualisation ImageMagick is a software suite to create, edit, compose, or convert bitmap images Inspector 2019_update5 2019b broadwell, skylake iris Utilities Intel Inspector XE is an easy to use memory error checker and thread checker for serial and parallel applications JasPer 2.0.14, 2.0.24 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation The JasPer Project is an open-source initiative to provide a free software-based reference implementation of the codec specified in the JPEG-2000 Part-1 standard. Java 1.8.0_241, 11.0.2, 13.0.2, 16.0.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages Java Platform, Standard Edition (Java SE) lets you develop and deploy Java applications on desktops and servers. Jellyfish 2.3.0 2019b broadwell, skylake iris Biology Jellyfish is a tool for fast, memory-efficient counting of k-mers in DNA. JsonCpp 1.9.3, 1.9.4 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries JsonCpp is a C++ library that allows manipulating JSON values, including serialization and deserialization to and from strings. It can also preserve existing comment in unserialization/serialization steps, making it a convenient format to store user input files. Julia 1.4.1, 1.6.2 2019b, 2020b broadwell, skylake, epyc iris, aion Programming Languages Julia is a high-level, high-performance dynamic programming language for numerical computing Keras 2.3.1, 2.4.3 2019b, 2020b gpu, broadwell, epyc, skylake iris, aion Mathematics Keras is a deep learning API written in Python, running on top of the machine learning platform TensorFlow. LAME 3.100 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Data processing LAME is a high quality MPEG Audio Layer III (MP3) encoder licensed under the LGPL. LLVM 10.0.1, 11.0.0, 9.0.0, 9.0.1 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Compilers The LLVM Core libraries provide a modern source- and target-independent optimizer, along with code generation support for many popular CPUs (as well as some less common ones!) These libraries are built around a well specified code representation known as the LLVM intermediate representation (\"LLVM IR\"). The LLVM Core libraries are well documented, and it is particularly easy to invent your own language (or port an existing compiler) to use LLVM as an optimizer and code generator. LMDB 0.9.24 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries LMDB is a fast, memory-efficient database. With memory-mapped files, it has the read performance of a pure in-memory database while retaining the persistence of standard disk-based databases. LibTIFF 4.0.10, 4.1.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries tiff: Library and tools for reading and writing TIFF data files LittleCMS 2.11, 2.9 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Visualisation Little CMS intends to be an OPEN SOURCE small-footprint color management engine, with special focus on accuracy and performance. Lua 5.1.5, 5.4.2 2019b, 2020b broadwell, skylake, epyc iris, aion Programming Languages Lua is a powerful, fast, lightweight, embeddable scripting language. Lua combines simple procedural syntax with powerful data description constructs based on associative arrays and extensible semantics. Lua is dynamically typed, runs by interpreting bytecode for a register-based virtual machine, and has automatic memory management with incremental garbage collection, making it ideal for configuration, scripting, and rapid prototyping. M4 1.4.18 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development GNU M4 is an implementation of the traditional Unix macro processor. It is mostly SVR4 compatible although it has some extensions (for example, handling more than 9 positional parameters to macros). GNU M4 also has built-in functions for including files, running shell commands, doing arithmetic, etc. MATLAB 2019b, 2020a, 2021a 2019b, 2020b broadwell, skylake, epyc iris, aion Mathematics MATLAB is a high-level language and interactive environment that enables you to perform computationally intensive tasks faster than with traditional programming languages such as C, C++, and Fortran. METIS 5.1.0 2019b, 2020b broadwell, skylake, epyc iris, aion Mathematics METIS is a set of serial programs for partitioning graphs, partitioning finite element meshes, and producing fill reducing orderings for sparse matrices. The algorithms implemented in METIS are based on the multilevel recursive-bisection, multilevel k-way, and multi-constraint partitioning schemes. MPC 1.2.1 2020b broadwell, epyc, skylake aion, iris Mathematics Gnu Mpc is a C library for the arithmetic of complex numbers with arbitrarily high precision and correct rounding of the result. It extends the principles of the IEEE-754 standard for fixed precision real floating point numbers to complex numbers, providing well-defined semantics for every operation. At the same time, speed of operation at high precision is a major design goal. MPFR 4.0.2, 4.1.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Mathematics The MPFR library is a C library for multiple-precision floating-point computations with correct rounding. MUMPS 5.3.5 2020b broadwell, epyc, skylake aion, iris Mathematics A parallel sparse direct solver Mako 1.1.0, 1.1.3 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development A super-fast templating language that borrows the best ideas from the existing templating languages Mathematica 12.0.0, 12.1.0 2019b, 2020b broadwell, skylake, epyc iris, aion Mathematics Mathematica is a computational software program used in many scientific, engineering, mathematical and computing fields. Maven 3.6.3 2019b, 2020b broadwell, skylake, epyc iris, aion Development Binary maven install, Apache Maven is a software project management and comprehension tool. Based on the concept of a project object model (POM), Maven can manage a project's build, reporting and documentation from a central piece of information. Meep 1.4.3 2019b broadwell, skylake iris Physics Meep (or MEEP) is a free finite-difference time-domain (FDTD) simulation software package developed at MIT to model electromagnetic systems. Mesa 19.1.7, 19.2.1, 20.2.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation Mesa is an open-source implementation of the OpenGL specification - a system for rendering interactive 3D graphics. Meson 0.51.2, 0.55.3 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities Meson is a cross-platform build system designed to be both as fast and as user friendly as possible. Mesquite 2.3.0 2019b broadwell, skylake iris Mathematics Mesh-Quality Improvement Library NAMD 2.13 2019b broadwell, skylake iris Chemistry NAMD is a parallel molecular dynamics code designed for high-performance simulation of large biomolecular systems. NASM 2.14.02, 2.15.05 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages NASM: General-purpose x86 assembler NCCL 2.4.8, 2.8.3 2019b, 2020b gpu iris Libraries The NVIDIA Collective Communications Library (NCCL) implements multi-GPU and multi-node collective communication primitives that are performance optimized for NVIDIA GPUs. NLopt 2.6.1, 2.6.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Numerical libraries NLopt is a free/open-source library for nonlinear optimization, providing a common interface for a number of different free optimization routines available online as well as original implementations of various other algorithms. NSPR 4.21, 4.29 2019b, 2020b broadwell, skylake, epyc iris, aion Libraries Netscape Portable Runtime (NSPR) provides a platform-neutral API for system level and libc-like functions. NSS 3.45, 3.57 2019b, 2020b broadwell, skylake, epyc iris, aion Libraries Network Security Services (NSS) is a set of libraries designed to support cross-platform development of security-enabled client and server applications. Ninja 1.10.1, 1.9.0 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Utilities Ninja is a small build system with a focus on speed. OPARI2 2.0.5 2019b broadwell, skylake iris Performance measurements OPARI2, the successor of Forschungszentrum Juelich's OPARI, is a source-to-source instrumentation tool for OpenMP and hybrid codes. It surrounds OpenMP directives and runtime library calls with calls to the POMP2 measurement interface. OTF2 2.2 2019b broadwell, skylake iris Performance measurements The Open Trace Format 2 is a highly scalable, memory efficient event trace data format plus support library. It is the new standard trace format for Scalasca, Vampir, and TAU and is open for other tools. OpenBLAS 0.3.12, 0.3.7 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Numerical libraries OpenBLAS is an optimized BLAS library based on GotoBLAS2 1.13 BSD version. OpenCV 4.2.0, 4.5.1 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation OpenCV (Open Source Computer Vision Library) is an open source computer vision and machine learning software library. OpenCV was built to provide a common infrastructure for computer vision applications and to accelerate the use of machine perception in the commercial products. Includes extra modules for OpenCV from the contrib repository. OpenEXR 2.5.5 2020b broadwell, epyc, skylake aion, iris Visualisation OpenEXR is a high dynamic-range (HDR) image file format developed by Industrial Light &amp; Magic for use in computer imaging applications OpenFOAM-Extend 4.1-20200408 2019b broadwell, skylake iris CFD/Finite element modelling OpenFOAM is a free, open source CFD software package. OpenFOAM has an extensive range of features to solve anything from complex fluid flows involving chemical reactions, turbulence and heat transfer, to solid dynamics and electromagnetics. OpenFOAM 8, v1912 2020b, 2019b epyc, broadwell, skylake aion, iris CFD/Finite element modelling OpenFOAM is a free, open source CFD software package. OpenFOAM has an extensive range of features to solve anything from complex fluid flows involving chemical reactions, turbulence and heat transfer, to solid dynamics and electromagnetics. OpenMPI 3.1.4, 4.0.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion MPI The Open MPI Project is an open source MPI-3 implementation. PAPI 6.0.0 2019b, 2020b broadwell, skylake, epyc iris, aion Performance measurements PAPI provides the tool designer and application engineer with a consistent interface and methodology for use of the performance counter hardware found in most major microprocessors. PAPI enables software engineers to see, in near real time, the relation between software performance and processor events. In addition Component PAPI provides access to a collection of components that expose performance measurement opportunites across the hardware and software stack. PCRE2 10.33, 10.35 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Development The PCRE library is a set of functions that implement regular expression pattern matching using the same syntax and semantics as Perl 5. PCRE 8.43, 8.44 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development The PCRE library is a set of functions that implement regular expression pattern matching using the same syntax and semantics as Perl 5. PDT 3.25 2019b broadwell, skylake iris Performance measurements Program Database Toolkit (PDT) is a framework for analyzing source code written in several programming languages and for making rich program knowledge accessible to developers of static and dynamic analysis tools. PDT implements a standard program representation, the program database (PDB), that can be accessed in a uniform way through a class library supporting common PDB operations. PETSc 3.14.4 2020b broadwell, epyc, skylake aion, iris Numerical libraries PETSc, pronounced PET-see (the S is silent), is a suite of data structures and routines for the scalable (parallel) solution of scientific applications modeled by partial differential equations. PGI 19.10 2019b broadwell, skylake iris Compilers C, C++ and Fortran compilers from The Portland Group - PGI PLUMED 2.5.3, 2.7.0 2019b, 2020b broadwell, skylake, epyc iris, aion Chemistry PLUMED is an open source library for free energy calculations in molecular systems which works together with some of the most popular molecular dynamics engines. Free energy calculations can be performed as a function of many order parameters with a particular  focus on biological problems, using state of the art methods such as metadynamics, umbrella sampling and Jarzynski-equation based steered MD. The software, written in C++, can be easily interfaced with both fortran and C/C++ codes. POV-Ray 3.7.0.8 2020b broadwell, epyc, skylake aion, iris Visualisation The Persistence of Vision Raytracer, or POV-Ray, is a ray tracing program which generates images from a text-based scene description, and is available for a variety of computer platforms. POV-Ray is a high-quality, Free Software tool for creating stunning three-dimensional graphics. The source code is available for those wanting to do their own ports. PROJ 6.2.1, 7.2.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries Program proj is a standard Unix filter function which converts geographic longitude and latitude coordinates into cartesian coordinates Pango 1.44.7, 1.47.0 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation Pango is a library for laying out and rendering of text, with an emphasis on internationalization. Pango can be used anywhere that text layout is needed, though most of the work on Pango so far has been done in the context of the GTK+ widget toolkit. Pango forms the core of text and font handling for GTK+-2.x. ParMETIS 4.0.3 2019b broadwell, skylake iris Mathematics ParMETIS is an MPI-based parallel library that implements a variety of algorithms for partitioning unstructured graphs, meshes, and for computing fill-reducing orderings of sparse matrices. ParMETIS extends the functionality provided by METIS and includes routines that are especially suited for parallel AMR computations and large scale numerical simulations. The algorithms implemented in ParMETIS are based on the parallel multilevel k-way graph-partitioning, adaptive repartitioning, and parallel multi-constrained partitioning schemes. ParMGridGen 1.0 2019b broadwell, skylake iris Mathematics ParMGridGen is an MPI-based parallel library that is based on the serial package MGridGen, that implements (serial) algorithms for obtaining a sequence of successive coarse grids that are well-suited for geometric multigrid methods. ParaView 5.6.2, 5.8.1 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation ParaView is a scientific parallel visualizer. Perl 5.30.0, 5.32.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages Larry Wall's Practical Extraction and Report Language This is a minimal build without any modules. Should only be used for build dependencies. Pillow 6.2.1, 8.0.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation Pillow is the 'friendly PIL fork' by Alex Clark and Contributors. PIL is the Python Imaging Library by Fredrik Lundh and Contributors. PyOpenGL 3.1.5 2020b broadwell, epyc, skylake aion, iris Visualisation PyOpenGL is the most common cross platform Python binding to OpenGL and related APIs. PyQt5 5.15.1 2020b broadwell, epyc, skylake aion, iris Visualisation PyQt5 is a set of Python bindings for v5 of the Qt application framework from The Qt Company. This bundle includes PyQtWebEngine, a set of Python bindings for The Qt Company\u2019s Qt WebEngine framework. PyQtGraph 0.11.1 2020b broadwell, epyc, skylake aion, iris Visualisation PyQtGraph is a pure-python graphics and GUI library built on PyQt5/PySide2 and numpy. PyTorch-Geometric 1.6.3 2020b broadwell, epyc, skylake, gpu aion, iris Libraries PyTorch Geometric (PyG) is a geometric deep learning extension library for PyTorch. PyTorch 1.4.0, 1.7.1, 1.8.1, 1.9.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development Tensors and Dynamic neural networks in Python with strong GPU acceleration. PyTorch is a deep learning framework that puts Python first. PyYAML 5.1.2, 5.3.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries PyYAML is a YAML parser and emitter for the Python programming language. Python 2.7.16, 2.7.18, 3.7.4, 3.8.6 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages Python is a programming language that lets you work more quickly and integrate your systems more effectively. Qt5 5.13.1, 5.14.2 2019b, 2020b broadwell, skylake, epyc iris, aion Development Qt is a comprehensive cross-platform C++ application framework. QuantumESPRESSO 6.7 2019b, 2020b broadwell, epyc, skylake iris, aion Chemistry Quantum ESPRESSO  is an integrated suite of computer codes for electronic-structure calculations and materials modeling at the nanoscale. It is based on density-functional theory, plane waves, and pseudopotentials (both norm-conserving and ultrasoft). RDFlib 5.0.0 2020b broadwell, epyc, skylake, gpu aion, iris Libraries RDFLib is a Python library for working with RDF, a simple yet powerful language for representing information. R 3.6.2, 4.0.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages R is a free software environment for statistical computing and graphics. ReFrame 2.21, 3.6.3 2019b, 2020b broadwell, skylake, epyc iris, aion Development ReFrame is a framework for writing regression tests for HPC systems. Ruby 2.7.1, 2.7.2 2019b, 2020b broadwell, skylake, epyc iris, aion Programming Languages Ruby is a dynamic, open source programming language with a focus on simplicity and productivity. It has an elegant syntax that is natural to read and easy to write. Rust 1.37.0 2019b broadwell, skylake iris Programming Languages Rust is a systems programming language that runs blazingly fast, prevents segfaults, and guarantees thread safety. SAMtools 1.10, 1.12 2019b, 2020b broadwell, skylake, epyc iris, aion Biology SAM Tools provide various utilities for manipulating alignments in the SAM format, including sorting, merging, indexing and generating alignments in a per-position format. SCOTCH 6.0.9, 6.1.0 2019b, 2020b broadwell, skylake, epyc iris, aion Mathematics Software package and libraries for sequential and parallel graph partitioning, static mapping, and sparse matrix block ordering, and sequential mesh and hypergraph partitioning. SDL2 2.0.14 2020b broadwell, epyc, skylake aion, iris Libraries SDL: Simple DirectMedia Layer, a cross-platform multimedia library SIONlib 1.7.6 2019b broadwell, skylake iris Libraries SIONlib is a scalable I/O library for parallel access to task-local files. The library not only supports writing and reading binary data to or from several thousands of processors into a single or a small number of physical files, but also provides global open and close functions to access SIONlib files in parallel. This package provides a stripped-down installation of SIONlib for use with performance tools (e.g., Score-P), with renamed symbols to avoid conflicts when an application using SIONlib itself is linked against a tool requiring a different SIONlib version. SLEPc 3.14.2 2020b broadwell, epyc, skylake aion, iris Numerical libraries SLEPc (Scalable Library for Eigenvalue Problem Computations) is a software library for the solution of large scale sparse eigenvalue problems on parallel computers. It is an extension of PETSc and can be used for either standard or generalized eigenproblems, with real or complex arithmetic. It can also be used for computing a partial SVD of a large, sparse, rectangular matrix, and to solve quadratic eigenvalue problems. SQLite 3.29.0, 3.33.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development SQLite: SQL Database Engine in a C Library SWIG 4.0.1, 4.0.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development SWIG is a software development tool that connects programs written in C and C++ with a variety of high-level programming languages. Salmon 1.1.0 2019b broadwell, skylake iris Biology Salmon is a wicked-fast program to produce a highly-accurate, transcript-level quantification estimates from RNA-seq data. Salome 8.5.0, 9.8.0 2019b, 2020b broadwell, skylake, epyc iris, aion CFD/Finite element modelling The SALOME platform is an open source software framework for pre- and post-processing and integration of numerical solvers from various scientific fields. CEA and EDF use SALOME to perform a large number of simulations, typically related to power plant equipment and alternative energy. To address these challenges, SALOME includes a CAD/CAE modelling tool, mesh generators, an advanced 3D visualization tool, etc. ScaLAPACK 2.0.2, 2.1.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Numerical libraries The ScaLAPACK (or Scalable LAPACK) library includes a subset of LAPACK routines redesigned for distributed memory MIMD parallel computers. Scalasca 2.5 2019b broadwell, skylake iris Performance measurements Scalasca is a software tool that supports the performance optimization of parallel programs by measuring and analyzing their runtime behavior. The analysis identifies potential performance bottlenecks -- in particular those concerning communication and synchronization -- and offers guidance in exploring their causes. SciPy-bundle 2019.10, 2020.11 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages Bundle of Python packages for scientific software Score-P 6.0 2019b broadwell, skylake iris Performance measurements The Score-P measurement infrastructure is a highly scalable and easy-to-use tool suite for profiling, event tracing, and online analysis of HPC applications. Singularity 3.6.0, 3.8.1 2019b, 2020b broadwell, skylake, epyc iris, aion Utilities SingularityCE is an open source container platform designed to be simple, fast, and secure.  Singularity is optimized for EPC and HPC workloads, allowing untrusted users to run untrusted containers in a trusted way. Spack 0.12.1 2019b, 2020b broadwell, skylake, epyc iris, aion Development Spack is a package manager for supercomputers, Linux, and macOS. It makes installing scientific software easy. With Spack, you can build a package with multiple versions, configurations, platforms, and compilers, and all of these builds can coexist on the same machine. Spark 2.4.3 2019b broadwell, skylake iris Development Spark is Hadoop MapReduce done in memory Stata 17 2020b broadwell, epyc, skylake aion, iris Mathematics Stata is a complete, integrated statistical software package that provides everything you need for data analysis, data management, and graphics. SuiteSparse 5.8.1 2020b broadwell, epyc, skylake aion, iris Numerical libraries SuiteSparse is a collection of libraries manipulate sparse matrices. Sumo 1.3.1 2019b broadwell, skylake iris Utilities Sumo is an open source, highly portable, microscopic and continuous traffic simulation package designed to handle large road networks. Szip 2.1.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities Szip compression software, providing lossless compression of scientific data Tcl 8.6.10, 8.6.9 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Programming Languages Tcl (Tool Command Language) is a very powerful but easy to learn dynamic programming language, suitable for a very wide range of uses, including web and desktop applications, networking, administration, testing and many more. TensorFlow 1.15.5, 2.1.0, 2.4.1, 2.5.0 2019b, 2020b gpu, broadwell, skylake, epyc iris, aion Libraries An open-source software library for Machine Intelligence Theano 1.0.4, 1.1.2 2019b, 2020b gpu, broadwell, epyc, skylake iris, aion Mathematics Theano is a Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. Tk 8.6.10, 8.6.9 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Visualisation Tk is an open source, cross-platform widget toolchain that provides a library of basic elements for building a graphical user interface (GUI) in many different programming languages. Tkinter 3.7.4, 3.8.6 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Programming Languages Tkinter module, built with the Python buildsystem TopHat 2.1.2 2019b, 2020b broadwell, skylake, epyc iris, aion Biology TopHat is a fast splice junction mapper for RNA-Seq reads. Trinity 2.10.0 2019b broadwell, skylake iris Biology Trinity represents a novel method for the efficient and robust de novo reconstruction of transcriptomes from RNA-Seq data. Trinity combines three independent software modules: Inchworm, Chrysalis, and Butterfly, applied sequentially to process large volumes of RNA-Seq reads. UCX 1.9.0 2020b broadwell, epyc, skylake, gpu aion, iris Libraries Unified Communication X An open-source production grade communication framework for data centric and high-performance applications UDUNITS 2.2.26 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Physics UDUNITS supports conversion of unit specifications between formatted and binary forms, arithmetic manipulation of units, and conversion of values between compatible scales of measurement. ULHPC-bd 2020b 2020b broadwell, epyc, skylake aion, iris System-level software Generic Module bundle for BigData Analytics software in use on the UL HPC Facility ULHPC-bio 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion System-level software Generic Module bundle for Bioinformatics, biology and biomedical software in use on the UL HPC Facility, especially at LCSB ULHPC-cs 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion System-level software Generic Module bundle for Computational science software in use on the UL HPC Facility, including: - Computer Aided Engineering, incl. CFD - Chemistry, Computational Chemistry and Quantum Chemistry - Data management &amp; processing tools - Earth Sciences - Quantum Computing - Physics and physical systems simulations ULHPC-dl 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion System-level software Generic Module bundle for (CPU-version) of AI / Deep Learning / Machine Learning software in use on the UL HPC Facility ULHPC-gpu 2019b, 2020b 2019b, 2020b gpu iris System-level software Generic Module bundle for GPU accelerated User Software in use on the UL HPC Facility ULHPC-math 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion System-level software Generic Module bundle for  High-level mathematical software and Linear Algrebra libraries in use on the UL HPC Facility ULHPC-toolchains 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion System-level software Generic Module bundle that contains all the dependencies required to enable toolchains and building tools/programming language in use on the UL HPC Facility ULHPC-tools 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion System-level software Misc tools, incl. - perf:      Performance tools - tools:     General purpose tools UnZip 6.0 2020b broadwell, epyc, skylake, gpu aion, iris Utilities UnZip is an extraction utility for archives compressed in .zip format (also called \"zipfiles\"). Although highly compatible both with PKWARE's PKZIP and PKUNZIP utilities for MS-DOS and with Info-ZIP's own Zip program, our primary objectives have been portability and non-MSDOS functionality. VASP 5.4.4, 6.2.1 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Physics The Vienna Ab initio Simulation Package (VASP) is a computer program for atomic scale materials modelling, e.g. electronic structure calculations and quantum-mechanical molecular dynamics, from first principles. VMD 1.9.4a51 2020b broadwell, epyc, skylake aion, iris Visualisation VMD is a molecular visualization program for displaying, animating, and analyzing large biomolecular systems using 3-D graphics and built-in scripting. VTK 8.2.0, 9.0.1 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation The Visualization Toolkit (VTK) is an open-source, freely available software system for 3D computer graphics, image processing and visualization. VTK consists of a C++ class library and several interpreted interface layers including Tcl/Tk, Java, and Python. VTK supports a wide variety of visualization algorithms including: scalar, vector, tensor, texture, and volumetric methods; and advanced modeling techniques such as: implicit modeling, polygon reduction, mesh smoothing, cutting, contouring, and Delaunay triangulation. VTune 2019_update8, 2020_update3 2019b, 2020b broadwell, skylake, epyc iris, aion Utilities Intel VTune Amplifier XE is the premier performance profiler for C, C++, C#, Fortran, Assembly and Java. Valgrind 3.15.0, 3.16.1 2019b, 2020b broadwell, skylake, epyc iris, aion Debugging Valgrind: Debugging and profiling tools VirtualGL 2.6.2 2019b broadwell, skylake iris Visualisation VirtualGL is an open source toolkit that gives any Linux or Unix remote display software the ability to run OpenGL applications with full hardware acceleration. Voro++ 0.4.6 2019b broadwell, skylake iris Mathematics Voro++ is a software library for carrying out three-dimensional computations of the Voronoi tessellation. A distinguishing feature of the Voro++ library is that it carries out cell-based calculations, computing the Voronoi cell for each particle individually. It is particularly well-suited for applications that rely on cell-based statistics, where features of Voronoi cells (eg. volume, centroid, number of faces) can be used to analyze a system of particles. Wannier90 3.1.0 2020b broadwell, epyc, skylake aion, iris Chemistry A tool for obtaining maximally-localised Wannier functions X11 20190717, 20201008 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation The X Window System (X11) is a windowing system for bitmap displays XML-LibXML 2.0201, 2.0206 2019b, 2020b broadwell, skylake, epyc iris, aion Data processing Perl binding for libxml2 XZ 5.2.4, 5.2.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities xz: XZ utilities Xerces-C++ 3.2.2 2019b broadwell, skylake iris Libraries Xerces-C++ is a validating XML parser written in a portable subset of C++. Xerces-C++ makes it easy to give your application the ability to read and write XML data. A shared library is provided for parsing, generating, manipulating, and validating XML documents using the DOM, SAX, and SAX2 APIs. Xvfb 1.20.9 2020b broadwell, epyc, skylake, gpu aion, iris Visualisation Xvfb is an X server that can run on machines with no display hardware and no physical input devices. It emulates a dumb framebuffer using virtual memory. YACS 0.1.8 2020b broadwell, epyc, skylake aion, iris Libraries YACS was created as a lightweight library to define and manage system configurations, such as those commonly found in software designed for scientific experimentation. These \"configurations\" typically cover concepts like hyperparameters used in training a machine learning model or configurable model hyperparameters, such as the depth of a convolutional neural network. Yasm 1.3.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages Yasm: Complete rewrite of the NASM assembler with BSD license Z3 4.8.10 2020b broadwell, epyc, skylake, gpu aion, iris Utilities Z3 is a theorem prover from Microsoft Research. Zip 3.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities Zip is a compression and file packaging/archive utility. Although highly compatible both with PKWARE's PKZIP and PKUNZIP utilities for MS-DOS and with Info-ZIP's own UnZip, our primary objectives have been portability and other-than-MSDOS functionality ant 1.10.6, 1.10.7, 1.10.9 2019b, 2020b broadwell, skylake, epyc iris, aion Development Apache Ant is a Java library and command-line tool whose mission is to drive processes described in build files as targets and extension points dependent upon each other. The main known usage of Ant is the build of Java applications. archspec 0.1.0 2019b broadwell, skylake iris Utilities A library for detecting, labeling, and reasoning about microarchitectures arpack-ng 3.7.0, 3.8.0 2019b, 2020b broadwell, skylake, epyc iris, aion Numerical libraries ARPACK is a collection of Fortran77 subroutines designed to solve large scale eigenvalue problems. at-spi2-atk 2.34.1, 2.38.0 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation AT-SPI 2 toolkit bridge at-spi2-core 2.34.0, 2.38.0 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation Assistive Technology Service Provider Interface. binutils 2.32, 2.35 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities binutils: GNU binary utilities bokeh 2.2.3 2020b broadwell, epyc, skylake, gpu aion, iris Utilities Statistical and novel interactive HTML plots for Python bzip2 1.0.8 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities bzip2 is a freely available, patent free, high-quality data compressor. It typically compresses files to within 10% to 15% of the best available techniques (the PPM family of statistical compressors), whilst being around twice as fast at compression and six times faster at decompression. cURL 7.66.0, 7.72.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities libcurl is a free and easy-to-use client-side URL transfer library, supporting DICT, FILE, FTP, FTPS, Gopher, HTTP, HTTPS, IMAP, IMAPS, LDAP, LDAPS, POP3, POP3S, RTMP, RTSP, SCP, SFTP, SMTP, SMTPS, Telnet and TFTP. libcurl supports SSL certificates, HTTP POST, HTTP PUT, FTP uploading, HTTP form based upload, proxies, cookies, user+password authentication (Basic, Digest, NTLM, Negotiate, Kerberos), file transfer resume, http proxy tunneling and more. cairo 1.16.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation Cairo is a 2D graphics library with support for multiple output devices. Currently supported output targets include the X Window System (via both Xlib and XCB), Quartz, Win32, image buffers, PostScript, PDF, and SVG file output. Experimental backends include OpenGL, BeOS, OS/2, and DirectFB cuDNN 7.6.4.38, 8.0.4.30, 8.0.5.39 2019b, 2020b gpu iris Numerical libraries The NVIDIA CUDA Deep Neural Network library (cuDNN) is a GPU-accelerated library of primitives for deep neural networks. dask 2021.2.0 2020b broadwell, epyc, skylake, gpu aion, iris Data processing Dask natively scales Python. Dask provides advanced parallelism for analytics, enabling performance at scale for the tools you love. double-conversion 3.1.4, 3.1.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries Efficient binary-decimal and decimal-binary conversion routines for IEEE doubles. elfutils 0.183 2020b gpu iris Libraries The elfutils project provides libraries and tools for ELF files and DWARF data. expat 2.2.7, 2.2.9 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities Expat is an XML parser library written in C. It is a stream-oriented parser in which an application registers handlers for things the parser might find in the XML document (like start tags) flatbuffers-python 1.12 2020b broadwell, epyc, skylake, gpu aion, iris Development Python Flatbuffers runtime library. flatbuffers 1.12.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development FlatBuffers: Memory Efficient Serialization Library flex 2.6.4 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages Flex (Fast Lexical Analyzer) is a tool for generating scanners. A scanner, sometimes called a tokenizer, is a program which recognizes lexical patterns in text. fontconfig 2.13.1, 2.13.92 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation Fontconfig is a library designed to provide system-wide font configuration, customization and application access. foss 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion Toolchains (software stacks) GNU Compiler Collection (GCC) based compiler toolchain, including OpenMPI for MPI support, OpenBLAS (BLAS and LAPACK support), FFTW and ScaLAPACK. fosscuda 2019b, 2020b 2019b, 2020b gpu iris Toolchains (software stacks) GCC based compiler toolchain with CUDA support, and including OpenMPI for MPI support, OpenBLAS (BLAS and LAPACK support), FFTW and ScaLAPACK. freetype 2.10.1, 2.10.3 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation FreeType 2 is a software font engine that is designed to be small, efficient, highly customizable, and portable while capable of producing high-quality output (glyph images). It can be used in graphics libraries, display servers, font conversion tools, text image generation tools, and many other products as well. gc 7.6.12 2019b broadwell, skylake iris Libraries The Boehm-Demers-Weiser conservative garbage collector can be used as a garbage collecting replacement for C malloc or C++ new. gcccuda 2019b, 2020b 2019b, 2020b gpu iris Toolchains (software stacks) GNU Compiler Collection (GCC) based compiler toolchain, along with CUDA toolkit. gettext 0.19.8.1, 0.20.1, 0.21 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities GNU 'gettext' is an important step for the GNU Translation Project, as it is an asset on which we may build many other steps. This package offers to programmers, translators, and even users, a well integrated set of tools and documentation gflags 2.2.2 2019b broadwell, skylake iris Development The gflags package contains a C++ library that implements commandline flags processing.  It includes built-in support for standard types such as string and the ability to define flags in the source file in which they are used. giflib 5.2.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries giflib is a library for reading and writing gif images. It is API and ABI compatible with libungif which was in wide use while the LZW compression algorithm was patented. git 2.23.0, 2.28.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. glog 0.4.0 2019b broadwell, skylake iris Development A C++ implementation of the Google logging module. gmsh 4.4.0 2019b broadwell, skylake iris CFD/Finite element modelling Salome is an open-source software that provides a generic Pre- and Post-Processing platform for numerical simulation. It is based on an open and flexible architecture made of reusable components. gmsh 4.8.4 2020b broadwell, epyc, skylake aion, iris Mathematics Gmsh is a 3D finite element grid generator with a build-in CAD engine and post-processor. gnuplot 5.2.8, 5.4.1 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation Portable interactive, function plotting utility gocryptfs 1.7.1, 2.0.1 2019b, 2020b broadwell, skylake, epyc iris, aion Utilities Encrypted overlay filesystem written in Go. gocryptfs uses file-based encryption that is implemented as a mountable FUSE filesystem. Each file in gocryptfs is stored as one corresponding encrypted file on the hard disk. gompi 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion Toolchains (software stacks) GNU Compiler Collection (GCC) based compiler toolchain, including OpenMPI for MPI support. gompic 2019b, 2020b 2019b, 2020b gpu iris Toolchains (software stacks) GNU Compiler Collection (GCC) based compiler toolchain along with CUDA toolkit, including OpenMPI for MPI support with CUDA features enabled. googletest 1.10.0 2019b broadwell, skylake iris Development Google's framework for writing C++ tests on a variety of platforms gperf 3.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development GNU gperf is a perfect hash function generator. For a given list of strings, it produces a hash function and hash table, in form of C or C++ code, for looking up a value depending on the input string. The hash function is perfect, which means that the hash table has no collisions, and the hash table lookup needs a single string comparison only. groff 1.22.4 2020b broadwell, epyc, skylake, gpu aion, iris Utilities Groff (GNU troff) is a typesetting system that reads plain text mixed with formatting commands and produces formatted output. gzip 1.10 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Utilities gzip (GNU zip) is a popular data compression program as a replacement for compress h5py 2.10.0, 3.1.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Data processing HDF5 for Python (h5py) is a general-purpose Python interface to the Hierarchical Data Format library, version 5. HDF5 is a versatile, mature scientific software library designed for the fast, flexible storage of enormous amounts of data. help2man 1.47.16, 1.47.4, 1.47.8 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Utilities help2man produces simple manual pages from the '--help' and '--version' output of other commands. hwloc 1.11.12, 2.2.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion System-level software The Portable Hardware Locality (hwloc) software package provides a portable abstraction (across OS, versions, architectures, ...) of the hierarchical topology of modern architectures, including NUMA memory nodes, sockets, shared caches, cores and simultaneous multithreading. It also gathers various system attributes such as cache and memory information as well as the locality of I/O devices such as network interfaces, InfiniBand HCAs or GPUs. It primarily aims at helping applications with gathering information about modern computing hardware so as to exploit it accordingly and efficiently. hypothesis 4.44.2, 5.41.2, 5.41.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities Hypothesis is an advanced testing library for Python. It lets you write tests which are parametrized by a source of examples, and then generates simple and comprehensible examples that make your tests fail. This lets you find more bugs in your code with less work. iccifort 2019.5.281, 2020.4.304 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Compilers Intel C, C++ &amp; Fortran compilers iccifortcuda 2019b, 2020b 2019b, 2020b gpu iris Toolchains (software stacks) Intel C, C++ &amp; Fortran compilers with CUDA toolkit iimpi 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Toolchains (software stacks) Intel C/C++ and Fortran compilers, alongside Intel MPI. iimpic 2019b, 2020b 2019b, 2020b gpu iris Toolchains (software stacks) Intel C/C++ and Fortran compilers, alongside Intel MPI and CUDA. imkl 2019.5.281, 2020.4.304 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Numerical libraries Intel Math Kernel Library is a library of highly optimized, extensively threaded math routines for science, engineering, and financial applications that require maximum performance. Core math functions include BLAS, LAPACK, ScaLAPACK, Sparse Solvers, Fast Fourier Transforms, Vector Math, and more. impi 2018.5.288, 2019.9.304 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion MPI Intel MPI Library, compatible with MPICH ABI intel 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Toolchains (software stacks) Compiler toolchain including Intel compilers, Intel MPI and Intel Math Kernel Library (MKL). intelcuda 2019b, 2020b 2019b, 2020b gpu iris Toolchains (software stacks) Intel Cluster Toolkit Compiler Edition provides Intel C/C++ and Fortran compilers, Intel MPI &amp; Intel MKL, with CUDA toolkit intltool 0.51.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development intltool is a set of tools to centralize translation of many different file formats using GNU gettext-compatible PO files. itac 2019.4.036 2019b broadwell, skylake iris Utilities The Intel Trace Collector is a low-overhead tracing library that performs event-based tracing in applications. The Intel Trace Analyzer provides a convenient way to monitor application activities gathered by the Intel Trace Collector through graphical displays. jemalloc 5.2.1 2019b broadwell, skylake iris Libraries jemalloc is a general purpose malloc(3) implementation that emphasizes fragmentation avoidance and scalable concurrency support. kallisto 0.46.1 2019b broadwell, skylake iris Biology kallisto is a program for quantifying abundances of transcripts from RNA-Seq data, or more generally of target sequences using high-throughput sequencing reads. kim-api 2.1.3 2019b broadwell, skylake iris Chemistry Open Knowledgebase of Interatomic Models. KIM is an API and OpenKIM is a collection of interatomic models (potentials) for atomistic simulations.  This is a library that can be used by simulation programs to get access to the models in the OpenKIM database. This EasyBuild only installs the API, the models can be installed with the package openkim-models, or the user can install them manually by running kim-api-collections-management install user MODELNAME or kim-api-collections-management install user OpenKIM to install them all. libGLU 9.0.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation The OpenGL Utility Library (GLU) is a computer graphics library for OpenGL. libarchive 3.4.3 2020b broadwell, epyc, skylake, gpu aion, iris Utilities Multi-format archive and compression library libcerf 1.13, 1.14 2019b, 2020b broadwell, skylake, epyc iris, aion Mathematics libcerf is a self-contained numeric library that provides an efficient and accurate implementation of complex error functions, along with Dawson, Faddeeva, and Voigt functions. libctl 4.0.0 2019b broadwell, skylake iris Chemistry libctl is a free Guile-based library implementing flexible control files for scientific simulations. libdrm 2.4.102, 2.4.99 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Libraries Direct Rendering Manager runtime library. libepoxy 1.5.4 2019b, 2020b broadwell, skylake, epyc iris, aion Libraries Epoxy is a library for handling OpenGL function pointer management for you libevent 2.1.11, 2.1.12 2019b, 2020b broadwell, skylake, epyc iris, aion Libraries The libevent API provides a mechanism to execute a callback function when a specific event occurs on a file descriptor or after a timeout has been reached.  Furthermore, libevent also support callbacks due to signals or regular timeouts. libffi 3.2.1, 3.3 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries The libffi library provides a portable, high level programming interface to various calling conventions. This allows a programmer to call any function specified by a call interface description at run-time. libgd 2.2.5, 2.3.0 2019b, 2020b broadwell, skylake, epyc iris, aion Libraries GD is an open source code library for the dynamic creation of images by programmers. libgeotiff 1.5.1, 1.6.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries Library for reading and writing coordinate system information from/to GeoTIFF files libglvnd 1.2.0, 1.3.2 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Libraries libglvnd is a vendor-neutral dispatch layer for arbitrating OpenGL API calls between multiple vendors. libgpuarray 0.7.6 2019b, 2020b gpu iris Libraries Library to manipulate tensors on the GPU. libiconv 1.16 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries Libiconv converts from one character encoding to another through Unicode conversion libjpeg-turbo 2.0.3, 2.0.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries libjpeg-turbo is a fork of the original IJG libjpeg which uses SIMD to accelerate baseline JPEG compression and decompression. libjpeg is a library that implements JPEG image encoding, decoding and transcoding. libmatheval 1.1.11 2019b broadwell, skylake iris Libraries GNU libmatheval is a library (callable from C and Fortran) to parse and evaluate symbolic expressions input as text. libogg 1.3.4 2020b broadwell, epyc, skylake, gpu aion, iris Libraries Ogg is a multimedia container format, and the native file and stream format for the Xiph.org multimedia codecs. libpciaccess 0.14, 0.16 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion System-level software Generic PCI access library. libpng 1.6.37 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries libpng is the official PNG reference library libreadline 8.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries The GNU Readline library provides a set of functions for use by applications that allow users to edit command lines as they are typed in. Both Emacs and vi editing modes are available. The Readline library includes additional functions to maintain a list of previously-entered command lines, to recall and perhaps reedit those lines, and perform csh-like history expansion on previous commands. libsndfile 1.0.28 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries Libsndfile is a C library for reading and writing files containing sampled sound (such as MS Windows WAV and the Apple/SGI AIFF format) through one standard library interface. libtirpc 1.3.1 2020b broadwell, epyc, skylake, gpu aion, iris Libraries Libtirpc is a port of Suns Transport-Independent RPC library to Linux. libtool 2.4.6 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries GNU libtool is a generic library support script. Libtool hides the complexity of using shared libraries behind a consistent, portable interface. libunistring 0.9.10 2019b broadwell, skylake iris Libraries This library provides functions for manipulating Unicode strings and for manipulating C strings according to the Unicode standard. libunwind 1.3.1, 1.4.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries The primary goal of libunwind is to define a portable and efficient C programming interface (API) to determine the call-chain of a program. The API additionally provides the means to manipulate the preserved (callee-saved) state of each call-frame and to resume execution at any point in the call-chain (non-local goto). The API supports both local (same-process) and remote (across-process) operation. As such, the API is useful in a number of applications libvorbis 1.3.7 2020b broadwell, epyc, skylake, gpu aion, iris Libraries Ogg Vorbis is a fully open, non-proprietary, patent-and-royalty-free, general-purpose compressed audio format libwebp 1.1.0 2020b broadwell, epyc, skylake aion, iris Libraries WebP is a modern image format that provides superior lossless and lossy compression for images on the web. Using WebP, webmasters and web developers can create smaller, richer images that make the web faster. libxc 4.3.4, 5.1.2 2019b, 2020b broadwell, skylake, epyc iris, aion Chemistry Libxc is a library of exchange-correlation functionals for density-functional theory. The aim is to provide a portable, well tested and reliable set of exchange and correlation functionals. libxml2 2.9.10, 2.9.9 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Libraries Libxml2 is the XML C parser and toolchain developed for the Gnome project (but usable outside of the Gnome platform). libxslt 1.1.34 2019b broadwell, skylake iris Libraries Libxslt is the XSLT C library developed for the GNOME project (but usable outside of the Gnome platform). libyaml 0.2.2, 0.2.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries LibYAML is a YAML parser and emitter written in C. lxml 4.4.2 2019b broadwell, skylake iris Libraries The lxml XML toolkit is a Pythonic binding for the C libraries libxml2 and libxslt. lz4 1.9.2 2020b broadwell, epyc, skylake, gpu aion, iris Libraries LZ4 is lossless compression algorithm, providing compression speed at 400 MB/s per core. It features an extremely fast decoder, with speed in multiple GB/s per core. magma 2.5.1, 2.5.4 2019b, 2020b gpu iris Mathematics The MAGMA project aims to develop a dense linear algebra library similar to LAPACK but for heterogeneous/hybrid architectures, starting with current Multicore+GPU systems. makeinfo 6.7 2020b broadwell, epyc, skylake, gpu aion, iris Development makeinfo is part of the Texinfo project, the official documentation format of the GNU project. matplotlib 3.1.1, 3.3.3 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Visualisation matplotlib is a python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms. matplotlib can be used in python scripts, the python and ipython shell, web application servers, and six graphical user interface toolkits. molmod 1.4.5 2019b broadwell, skylake iris Mathematics MolMod is a Python library with many compoments that are useful to write molecular modeling programs. ncurses 6.0, 6.1, 6.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development The Ncurses (new curses) library is a free software emulation of curses in System V Release 4.0, and more. It uses Terminfo format, supports pads and color and multiple highlights and forms characters and function-key mapping, and has all the other SYSV-curses enhancements over BSD Curses. netCDF-Fortran 4.5.2, 4.5.3 2019b, 2020b broadwell, skylake, epyc iris, aion Data processing NetCDF (network Common Data Form) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. netCDF 4.7.1, 4.7.4 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Data processing NetCDF (network Common Data Form) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. nettle 3.5.1, 3.6 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries Nettle is a cryptographic library that is designed to fit easily in more or less any context: In crypto toolkits for object-oriented languages (C++, Python, Pike, ...), in applications like LSH or GNUPG, or even in kernel space. networkx 2.5 2020b broadwell, epyc, skylake, gpu aion, iris Utilities NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks. nodejs 12.19.0 2020b broadwell, epyc, skylake, gpu aion, iris Programming Languages Node.js is a platform built on Chrome's JavaScript runtime for easily building fast, scalable network applications. Node.js uses an event-driven, non-blocking I/O model that makes it lightweight and efficient, perfect for data-intensive real-time applications that run across distributed devices. nsync 1.24.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development nsync is a C library that exports various synchronization primitives, such as mutexes numactl 2.0.12, 2.0.13 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities The numactl program allows you to run your application program on specific cpu's and memory nodes. It does this by supplying a NUMA memory policy to the operating system before running your program. The libnuma library provides convenient ways for you to add NUMA memory policies into your own program. numba 0.52.0 2020b broadwell, epyc, skylake, gpu aion, iris Programming Languages Numba is an Open Source NumPy-aware optimizing compiler for Python sponsored by Continuum Analytics, Inc. It uses the remarkable LLVM compiler infrastructure to compile Python syntax to machine code. phonopy 2.2.0 2019b broadwell, skylake iris Libraries Phonopy is an open source package of phonon calculations based on the supercell approach. pixman 0.38.4, 0.40.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation Pixman is a low-level software library for pixel manipulation, providing features such as image compositing and trapezoid rasterization. Important users of pixman are the cairo graphics library and the X server. pkg-config 0.29.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development pkg-config is a helper tool used when compiling applications and libraries. It helps you insert the correct compiler options on the command line so an application can use gcc -o test test.c <code>pkg-config --libs --cflags glib-2.0</code> for instance, rather than hard-coding values on where to find glib (or other libraries). pkgconfig 1.5.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development pkgconfig is a Python module to interface with the pkg-config command line tool pocl 1.4, 1.6 2019b, 2020b gpu iris Libraries Pocl is a portable open source (MIT-licensed) implementation of the OpenCL standard protobuf-python 3.10.0, 3.14.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development Python Protocol Buffers runtime library. protobuf 2.5.0, 3.10.0, 3.14.0 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Development Google Protocol Buffers pybind11 2.4.3, 2.6.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries pybind11 is a lightweight header-only library that exposes C++ types in Python and vice versa, mainly to create Python bindings of existing C++ code. re2c 1.2.1, 2.0.3 2019b, 2020b broadwell, skylake, epyc iris, aion Utilities re2c is a free and open-source lexer generator for C and C++. Its main goal is generating fast lexers: at least as fast as their reasonably optimized hand-coded counterparts. Instead of using traditional table-driven approach, re2c encodes the generated finite state automata directly in the form of conditional jumps and comparisons. scikit-build 0.11.1 2020b broadwell, epyc, skylake, gpu aion, iris Libraries Scikit-Build, or skbuild, is an improved build system generator for CPython C/C++/Fortran/Cython extensions. scikit-image 0.18.1 2020b broadwell, epyc, skylake, gpu aion, iris Visualisation scikit-image is a collection of algorithms for image processing. scikit-learn 0.23.2 2020b broadwell, epyc, skylake, gpu aion, iris Data processing Scikit-learn integrates machine learning algorithms in the tightly-knit scientific Python world, building upon numpy, scipy, and matplotlib. As a machine-learning module, it provides versatile tools for data mining and analysis in any field of science and engineering. It strives to be simple and efficient, accessible to everybody, and reusable in various contexts. scipy 1.4.1 2019b broadwell, skylake, gpu iris Mathematics SciPy is a collection of mathematical algorithms and convenience functions built on the Numpy extension for Python. setuptools 41.0.1 2019b broadwell, skylake iris Development Easily download, build, install, upgrade, and uninstall Python packages snappy 1.1.7, 1.1.8 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries Snappy is a compression/decompression library. It does not aim for maximum compression, or compatibility with any other compression library; instead, it aims for very high speeds and reasonable compression. sparsehash 2.0.3, 2.0.4 2019b, 2020b broadwell, skylake, epyc iris, aion Development An extremely memory-efficient hash_map implementation. 2 bits/entry overhead! The SparseHash library contains several hash-map implementations, including implementations that optimize for space or speed. spglib-python 1.16.0 2020b broadwell, epyc, skylake, gpu aion, iris Chemistry Spglib for Python. Spglib is a library for finding and handling crystal symmetries written in C. tbb 2019_U9, 2020.2, 2020.3 2019b, 2020b broadwell, skylake, epyc iris, aion Libraries Intel(R) Threading Building Blocks (Intel(R) TBB) lets you easily write parallel C++ programs that take full advantage of multicore performance, that are portable, composable and have future-proof scalability. texinfo 6.7 2019b broadwell, skylake iris Development Texinfo is the official documentation format of the GNU project. tqdm 4.56.2 2020b broadwell, epyc, skylake, gpu aion, iris Libraries A fast, extensible progress bar for Python and CLI typing-extensions 3.7.4.3 2019b, 2020b gpu, broadwell, epyc, skylake iris, aion Development Typing Extensions \u2013 Backported and Experimental Type Hints for Python util-linux 2.34, 2.36 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities Set of Linux utilities x264 20190925, 20201026 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation x264 is a free software library and application for encoding video streams into the H.264/MPEG-4 AVC compression format, and is released under the terms of the GNU GPL. x265 3.2, 3.3 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation x265 is a free software library and application for encoding video streams into the H.265 AVC compression format, and is released under the terms of the GNU GPL. xorg-macros 1.19.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development X.org macros utilities. xprop 1.2.4, 1.2.5 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation The xprop utility is for displaying window and font properties in an X server. One window or font is selected using the command line arguments or possibly in the case of a window, by clicking on the desired window. A list of properties is then given, possibly with formatting information. yaff 1.6.0 2019b broadwell, skylake iris Chemistry Yaff stands for 'Yet another force field'. It is a pythonic force-field code. zlib 1.2.11 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries zlib is designed to be a free, general-purpose, legally unencumbered -- that is, not covered by any patents -- lossless data-compression library for use on virtually any computer hardware and operating system. zstd 1.4.5 2020b broadwell, epyc, skylake, gpu aion, iris Libraries Zstandard is a real-time compression algorithm, providing high compression ratios. It offers a very wide range of compression/speed trade-off, while being backed by a very fast decoder. It also offers a special mode for small data, called dictionary compression, and can create dictionaries from any sample set."},{"location":"software/build/","title":"Compiling/Building your own software","text":"<p>We try to provide within the ULHPC software sets the most used application among our users. It may however happen that you may find a given software you expect to use to be either missing among the available software sets, or provided in a version you considered not enough recent.</p> <p>In that case, the RECOMMENDED approach is to rely on Easybuild to EXTEND the available software set. Below are guidelines to support that case.</p> <p>Alternatively, you can of course follow the installation guidelines provided on the software website to compile it the way it should be. For that case, you MUST rely on the provided toolchains and compilers.</p> <p>In all cases, NEVER compile or build softawre from the ULHPC frontends! Always perform these actions from the expected compute node, either reserved within an interactive job or through a passive submission</p>"},{"location":"software/build/#missing-or-outdated-software","title":"Missing or Outdated Software","text":"<p>You should first search if an existing Easyconfig exists for the software:</p> <pre><code># Typical check for user on ULHPC clusters\n$ si    # get an interactive job - use 'si-gpu' for GPU nodes on iris\n$ module load tools/EasyBuild\n$ eb -S &lt;name&gt;\n</code></pre> <p>It shoud match the available software set versions summarized below:</p> Name Type 2019b (<code>legacy</code>) 2020b (<code>release</code>) 2023b (<code>development</code>) (<code>testing</code>) GCCCore compiler 8.3.0 10.2.0 13.2.0 foss toolchain 2019b 2020b 2023b intel toolchain 2019b 2020b 2023b binutils 2.32 2.35 2.40 Python 3.7.4 (and 2.7.16) 3.8.6 3.11.5 Clang compiler 9.0.1 11.0.0 17.0.6 OpenMPI MPI 3.1.4 4.0.5 4.1.6 <p>You will then be confronted to the following cases.</p>"},{"location":"software/build/#an-existing-easyconfigs-exists-for-the-target-toolchain-version","title":"An existing easyconfigs exists for the target toolchain version","text":"<p>You're lucky but this is very likely to happen (and justify to rely on streamline Easyconfigs)</p> <ul> <li>Typical Example:<ul> <li><code>CMake-&lt;version&gt;-GCCcore-&lt;gccversion&gt;.eb</code>: depends on GCCcore, thus common to both <code>foss</code> and <code>intel</code>. The same happens with <code>GCC</code></li> <li><code>Go-&lt;version&gt;.eb</code> (no dependency on any toolchain)</li> <li><code>Boost-&lt;version&gt;-{gompi,iimpi}-&lt;toolchainversion&gt;.eb</code>, derived toolchains, compliant with <code>foss</code> (resp. <code>intel</code>) ones;</li> <li><code>GDAL-&lt;version&gt;-{foss,intel}-&lt;toolchainversion&gt;-Python-&lt;pythonversion&gt;.eb</code></li> </ul> </li> </ul> <p>In that case, you MUST test the build in your home or in a shared project using the <code>resif-load-{home,project}-swset-{prod,devel}</code> helpers to set a consistent environment for your builds compilant with the ULHPC software sets layout (in particular with regards the <code>$EASYBUILD_PREFIX</code> and <code>$MODULEPATH</code> environment variables). See below for building instructions.</p>"},{"location":"software/build/#an-outdated-easyconfig-exists","title":"An outdated easyconfig exists","text":"<p>Then the easiest way is to adapt the existing easyconfig file for the target softare version AND one of the available toolchain version. You may want also to ensure an ongoing Pull-Request is not dealing with the version you're looking for.</p> <p>Assuming you're looking for the software <code>&lt;name&gt;</code> (first letter <code>&lt;letter</code> (in lower case), for instance if <code>&lt;name&gt;=NWChem</code>, then <code>&lt;letter&gt;=n</code>), first copy the existing easyconfig file in a convenient place</p> <pre><code># Create host directory for your custom easyconfigs\n$ mkdir -p ~/easyconfigs/&lt;letter&gt;/&lt;name&gt;\n\n$ eb -S &lt;name&gt;` # find the complete path to the easyconfig file\nCFGS1=[...]/path/to/easyconfigs\n* $CFGS1/&lt;letter&gt;/&lt;name&gt;/&lt;name&gt;-&lt;oldversion&gt;[...].eb\n* $CFGS1/&lt;letter&gt;/&lt;name&gt;/&lt;name&gt;-[...].patch     # Eventual Patch file\n\n# copy/paste the definition of the CFGS1 variable (top line)\nCFGS1=[...]/path/to/easyconfigs\n# copy the eb file\ncp $CFGS1/&lt;letter&gt;/&lt;name&gt;/&lt;name&gt;-&lt;oldversion&gt;[...].eb ~/easyconfigs/&lt;letter&gt;/&lt;name&gt;\n</code></pre> <p>Now (eventually) check on the software website for the most up-to-date version <code>&lt;version&gt;</code> of the software released. Adapt the filename of the copied easyconfig to match the target version / toolchain</p> <pre><code>cd ~/easyconfigs/&lt;letter&gt;/&lt;name&gt;\nmv &lt;name&gt;-&lt;oldversion&gt;[...].eb &lt;name&gt;-&lt;version&gt;[...].eb\n</code></pre> <p>Example</p> <pre><code>cd ~/easyconfigs/n/NWCHem\nmv NWChem-7.0.0-intel-2019b-Python-3.7.4.eb NWChem-7.0.2-intel-2021b.eb  # Target 2021b intel toolchain, no more need for python suffix\n</code></pre> <p>Now you shall edit the content of the easyconfig -- you'll typically have to adapt the version of the dependencies and the checksum(s) to match the static versions set for the target toolchain, enforce https urls etc.</p> <p>Below is a past complex exemple illustrating the adaptation done for GDB</p> <pre><code>--- g/GDB/GDB-8.3-GCCcore-8.2.0-Python-3.7.2.eb 2020-03-31 12:17:03.000000000 +0200\n+++ g/GDB/GDB-9.1-GCCcore-8.3.0-Python-3.7.4.eb 2020-05-08 15:49:41.000000000 +0200\n@@ -1,31 +1,36 @@\n easyblock = 'ConfigureMake'\n\n name = 'GDB'\n-version = '8.3'\n+version = '9.1'\n versionsuffix = '-Python-%(pyver)s'\n\n-homepage = 'http://www.gnu.org/software/gdb/gdb.html'\n+homepage = 'https://www.gnu.org/software/gdb/gdb.html'\n description = \"The GNU Project Debugger\"\n\n-toolchain = {'name': 'GCCcore', 'version': '8.2.0'}\n+toolchain = {'name': 'GCCcore', 'version': '8.3.0'}\n\n source_urls = [GNU_SOURCE]\n sources = [SOURCELOWER_TAR_XZ]\n-checksums = ['802f7ee309dcc547d65a68d61ebd6526762d26c3051f52caebe2189ac1ffd72e']\n+checksums = ['699e0ec832fdd2f21c8266171ea5bf44024bd05164fdf064e4d10cc4cf0d1737']\n\n builddependencies = [\n-    ('binutils', '2.31.1'),\n-    ('texinfo', '6.6'),\n+    ('binutils', '2.32'),\n+    ('texinfo', '6.7'),\n ]\n\n dependencies = [\n     ('zlib', '1.2.11'),\n     ('libreadline', '8.0'),\n     ('ncurses', '6.1'),\n-    ('expat', '2.2.6'),\n-    ('Python', '3.7.2'),\n+    ('expat', '2.2.7'),\n+    ('Python', '3.7.4'),\n ]\n\n+preconfigopts = \"mkdir obj &amp;&amp; cd obj &amp;&amp; \"\n+configure_cmd_prefix = '../'\n+prebuildopts = \"cd obj &amp;&amp; \"\n+preinstallopts = prebuildopts\n+\n configopts = '--with-system-zlib --with-python=$EBROOTPYTHON/bin/python --with-expat=$EBROOTEXPAT '\n configopts += '--with-system-readline --enable-tui --enable-plugins --disable-install-libbfd '\n</code></pre> <p>Note on dependencies version: typically as in the above  example, the version to use for dependencies are not obvious to guess (Ex: <code>texinfo</code>, <code>expat</code> etc.) and you need to be aware of the matching toolchain/GCC/binutils versions for the available <code>prod</code> or <code>devel</code> software sets recalled before -- use <code>eb -S &lt;dependency&gt;</code> to find the appropriate versions.</p>"},{"location":"software/build/#none-or-only-very-oldobsolete-easyconfigs-are-suggested","title":"None (or only very old/obsolete) easyconfigs are suggested","text":"<p>Don't panic, it simply means that the  official repositories do not hold any recent reciPY for the considered software.  You may find a pending Pull-request addressing the software you're looking for.</p> <p>Otherwise, you can either try to create a new easyconfig file, or simply follow the installation guildes for the considered software to build it.</p>"},{"location":"software/build/#using-easybuild-to-build-software-in-your-home","title":"Using Easybuild to Build software in your Home","text":"<p>See also Technical documentation to better understand the Easybuild configuration.</p> <p>If upon Dry-run builds (<code>eb -Dr [...]</code>) you find most dependencies NOT satisfied, you've likely made an error and may be trying to build a software against a toolchain/software set not supported either as <code>prod</code> or <code>devel</code>.</p> <pre><code># BETTER work in a screen or tmux session ;)\n$ si[-gpu] [-c &lt;threads&gt;]   # get an interactive job\n$ module load tools/EasyBuild\n# /!\\ IMPORTANT: ensure EASYBUILD_PREFIX is correctly set to [basedir]/&lt;cluster&gt;/&lt;environment&gt;/&lt;arch&gt;\n#                and that MODULEPATH is prefixed accordingly\n$ resif-load-home-swset-{prod | devel}  # adapt environment\n$ eb -S &lt;softwarename&gt;   # confirm &lt;filename&gt;.eb == &lt;softwarename&gt;-&lt;version&gt;[-&lt;toolchain&gt;][-&lt;suffix&gt;].eb\n$ eb -Dr &lt;filename&gt;.eb   # check dependencies, normally most MUST be satisfied\n$ eb -r  &lt;filename&gt;.eb\n</code></pre> <p>From that point, the compiled software and associated module is available in your home and can be used as follows in launchers etc. -- see ULHPC launcher Examples</p> <pre><code>#!/bin/bash -l # &lt;--- DO NOT FORGET '-l' to facilitate further access to ULHPC modules\n#SBATCH -p &lt;partition&gt;\n#SBATCH -N 1\n#SBATCH --ntasks-per-node &lt;#sockets * s&gt;\n#SBATCH --ntasks-per-socket &lt;s&gt;\n#SBATCH -c &lt;thread&gt;\n\nprint_error_and_exit() { echo \"***ERROR*** $*\"; exit 1; }\n# Safeguard for NOT running this launcher on access/login nodes\nmodule purge || print_error_and_exit \"No 'module' command\"\n\nresif-load-home-swset-prod  # OR  resif-load-home-swset-devel\nmodule load &lt;softwarename&gt;[/&lt;version&gt;]\n[...]\n</code></pre>"},{"location":"software/build/#using-easybuild-to-build-software-in-the-project","title":"Using Easybuild to Build software in the  project <p>Similarly to the above home builds, you should repeat the procedure this time using the helper script <code>resif-load-project-swset-{prod | devel}</code>. Don't forget Project Data Management instructions: to avoid quotas issues, you have to use <code>sg</code></p> <pre><code># BETTER work in a screen or tmux session ;)\n$ si[-gpu] [-c &lt;threads&gt;]   # get an interactive job\n$ module load tools/EasyBuild\n# /!\\ IMPORTANT: ensure EASYBUILD_PREFIX is correctly set to [basedir]/&lt;cluster&gt;/&lt;environment&gt;/&lt;arch&gt;\n#                and that MODULEPATH is prefixed accordingly\n$ resif-load-project-swset-{prod | devel} $PROJECTHOME/&lt;project&gt; # /!\\ ADAPT environment and &lt;project&gt; accordingly\n$ sg &lt;project&gt; -c \"eb -S &lt;softwarename&gt;\"   # confirm &lt;filename&gt;.eb == &lt;softwarename&gt;-&lt;v&gt;-&lt;toolchain&gt;.eb\n$ sg &lt;project&gt; -c \"eb -Dr &lt;filename&gt;.eb\"   # check dependencies, normally most MUST be satisfied\n$ sg &lt;project&gt; -c \"eb -r  &lt;filename&gt;.eb\"\n</code></pre> <p>From that point, the compiled software and associated module is available in the project directoryand can be used by all project members as follows in launchers etc. -- see ULHPC launcher Examples</p> <pre><code>#!/bin/bash -l # &lt;--- DO NOT FORGET '-l' to facilitate further access to ULHPC modules\n#SBATCH -p &lt;partition&gt;\n#SBATCH -N 1\n#SBATCH --ntasks-per-node &lt;#sockets * s&gt;\n#SBATCH --ntasks-per-socket &lt;s&gt;\n#SBATCH -c &lt;thread&gt;\n\nprint_error_and_exit() { echo \"***ERROR*** $*\"; exit 1; }\n# Safeguard for NOT running this launcher on access/login nodes\nmodule purge || print_error_and_exit \"No 'module' command\"\n\nresif-load-project-swset-prod  $PROJECTHOME/&lt;project&gt; # OR resif-load-project-swset-devel $PROJECTHOME/&lt;project&gt;\nmodule load &lt;softwarename&gt;[/&lt;version&gt;]\n[...]\n</code></pre>","text":""},{"location":"software/build/#contribute-back-to-easybuild","title":"Contribute back to Easybuild <p>If you developped new easyconfig(s), you are expected to contribute them back to the Easybuilders community! Consider creating a Pull-Request. You can even do it by command-line assuming you have setup your Github integration.  On <code>iris</code> or <code>aion</code>, you will likely need to install the possibly-insecure, alternate keyrings <code>keyrings.alt</code> packages -- see https://pypi.org/project/keyring/</p> <pre><code># checking code style - see https://easybuild.readthedocs.io/en/latest/Code_style.html#code-style\neb --check-contrib &lt;ebfile&gt;\neb --new-pr &lt;ebfile&gt;\n</code></pre> <p>You can can also consider using the script <code>PR-create</code> provided as part of the RESIF 3 project.</p> <p>Once the pull request is merged, you can inform the ULHPC team to consider adding the submitted Easyconfig as part of the ULHPC bundles and see it deployed within the next ULHPC software set release.</p>","text":""},{"location":"software/eessi/","title":"EESSI - European Environment for Scientific Software Installations","text":"<p>The European Environment for Scientific Software Installations (EESSI, pronounced as \"easy\") is a collaboration between different European partners in HPC community. The goal of this project is to build a common stack of scientific software installations for HPC systems and beyond, including laptops, personal workstations and cloud infrastructure.</p> <p>The EESSI software stack is available on the ULHPC platform, and gives you access to software modules maintained by the EESSI project and optimized for the CPU architectures available on the ULHPC platform.</p> <p>On a compute node, to set up the EESSI environment, simply load the EESSI module:</p> <pre><code>module load EESSI\n</code></pre> <p>The first usage may be slow as the files are downloaded from an upstream Stratum 1 server, but the files are cached locally.</p> <p>You should see the following output:</p> <pre><code>$ module load EESSI\nEESSI/2023.06 loaded successfully\n</code></pre> <p>The last line is the shell output.</p> <p>Your environment is now set up, you are ready to start running software provided by EESSI! To see which modules (and extensions) are available, run:</p> <pre><code>module avail\n</code></pre> <p>Here is a short excerpt of the output produced by module avail:</p> <pre><code>----- /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen2/modules/all -----\n   ALL/0.9.2-foss-2023a           ESPResSo/4.2.1-foss-2023a        foss/2023a            h5py/3.9.0-foss-2023a\n   ParaView/5.11.2-foss-2023a     PyTorch/2.1.2-foss-2023a         QuantumESPRESSO/7.2-foss-2022b   VTK/9.3.0-foss-2023a\n   ELPA/2022.05.001-foss-2022b    foss/2022b                       foss/2023b (D)        OpenFOAM/11-foss-2023a\n...\n</code></pre> <p>For more precise information, please refer to the official documentation.</p>"},{"location":"software/swsets/","title":"ULHPC Software Sets","text":""},{"location":"software/swsets/#ul-hpc-toolchains-and-software-set-versioning","title":"UL HPC toolchains and software set versioning","text":"<p>Our centre offers a yearly release of the UL HPC software set based on corresponding release of EasyBuid toolchains.<sup>1</sup> Count at least 6 months of validation and testing after an EasyBuild release before a UL HPC release.</p> <p>Tool chains and software releases</p> <p>The idea behind toolchains is that a core set of modules is fixed per release and the rest of the software in the release is built around the core set. Only one version of the toolchain modules is present in the software set, where as multiple versions of other software can be present.</p> <p>For an exhaustive list of components version fixed per release have a look as the foss and intel toolchains.</p> <p>An overview of the currently available core toolchain component versions in the UL HPC releases is depicted below:</p> Name Type 2019b (<code>legacy</code>) 2020b (<code>release</code>) 2023b (<code>development</code>) (<code>testing</code>) GCCCore compiler 8.3.0 10.2.0 13.2.0 foss toolchain 2019b 2020b 2023b intel toolchain 2019b 2020b 2023b binutils 2.32 2.35 2.40 Python 3.7.4 (and 2.7.16) 3.8.6 3.11.5 Clang compiler 9.0.1 11.0.0 17.0.6 OpenMPI MPI 3.1.4 4.0.5 4.1.6 <p>In the natively optimized software sets loaded with the modules under <code>env</code>, you should always have a single core component of each type available. The EESSI software sets in contrast follows a more flat layout, and multiple core components will be available at once per type when the software set is loaded.</p>"},{"location":"software/swsets/#architecture-of-the-software-set","title":"Architecture of the software set","text":"<p>By default, the environment module system uses the contents of the <code>MODULEPATH</code> environment variable as the path where it looks for modules. In UL HPC the environment variable contains by default the following paths.</p> <ul> <li><code>/opt/apps/easybuild/environment/modules</code>: Location of sticky meta-modules under <code>env</code> that provide the native optimized software modules.</li> <li><code>/cvmfs/software.eessi.io/init/modules</code>: Location of modules under <code>EESSI</code>, a set of meta-modules that provide the EESSI software sets.</li> </ul> <p>The natively optimized modules under <code>env</code> prepend one of the following paths to <code>${MODULEPATH}</code> in the order described below.</p> <ul> <li>On all nodes except for the <code>gpu</code> partition of Iris:<ul> <li><code>/opt/apps/easybuild/systems/&lt;cluster name&gt;/&lt;build version&gt;/&lt;software set version&gt;/&lt;target architecture&gt;/modules/all</code>: Location of natively optimized modules.</li> <li><code>/opt/apps/easybuild/systems/binary/&lt;build version&gt;/&lt;software set version&gt;/generic/modules/all</code>: Location of software distributed as binaries that cannot be optimized for any target architecture.</li> </ul> </li> <li> <p>On nodes of the <code>gpu</code> partition of Iris:</p> <ul> <li><code>/opt/apps/easybuild/systems/iris/&lt;build version&gt;/&lt;software set version&gt;/gpu/modules/all</code>: Location of natively optimized modules that use the GPU.</li> <li><code>/opt/apps/easybuild/systems/iris/&lt;build version&gt;/&lt;software set version&gt;/skylake/modules/all</code>: Location of natively optimized modules.</li> <li><code>/opt/apps/easybuild/systems/binary/&lt;build version&gt;/&lt;software set version&gt;/generic/modules/all</code>: Location of software distributed as binaries that cannot be optimized for any target architecture.</li> </ul> <p>The GPU optimized modules still need the CPU modules to function, like for instance the MPI module. The GPU nodes use Skylake CPUs, so the modules optimized for Skylake are loaded.</p> </li> </ul> Parameters in the software set directory paths <p>The follow parameters are used in the paths to the software set directories:</p> <ul> <li><code>&lt;cluster name&gt;</code>: the name of the cluster (<code>iris</code> or <code>aion</code>), as set in the environment variable <code>${ULHPC_CLUSTER}</code>.</li> <li><code>&lt;build version&gt;</code>: the version of the software build, determined by the operating system (OS) and the date of the build as <code>&lt;OS version&gt;-&lt;ISO date squashed&gt;</code>, where for instance<ul> <li>RHEL 8.10 become <code>&lt;OS version&gt;</code>=<code>rhel810</code>, and</li> <li>2025-02-16 becomes <code>&lt;ISO date squashed&gt;</code>=<code>20252010</code>.</li> </ul> </li> <li><code>&lt;software set version&gt;</code>: the ULHPC Software set release, aligned with Easybuid toolchains release.</li> <li><code>&lt;target architecture&gt;</code>: the architecture for which the software set has been optimized, as set in the <code>${RESIF_ARCH}</code> environment variable.</li> </ul> <p>There are nodes with <code>broadwell</code> and nodes with <code>skylake</code> CPUs in the CPU partitions (<code>batch</code> and <code>interactive</code>) of Iris. To ensure that a compatible binary is used in all CPUs of the partition, modules loading software sets are configured to load binaries that are compatible for <code>broadwell</code>, the older architecture of the two (note that the binary is selected in the primary node of an allocation).</p> <p>The <code>RESIF_ARCH</code> environment variable is used to load the software set for the appropriate architecture in the natively optimized software sets under <code>env</code>. The <code>${RESIF_ARCH}</code> value used for all nodes in the CPU partitions of Iris is <code>broadwell</code>.</p> <p>Similarly to <code>RESIF_ARCH</code>, EESSI provides the <code>EESSI_ARCHDETECT_OPTIONS_OVERRIDE</code> environment variable to enforce an architecture; by default <code>EESSI_ARCHDETECT_OPTIONS_OVERRIDE</code> is unset, and the EESSI module selects an appropriate architecture for the software set (as the name suggests). The <code>EESSI_ARCHDETECT_OPTIONS_OVERRIDE</code> variable is set to <code>x86_64/intel/haswell</code> in the CPU partitions of iris by default and unset in every other partition. Note that architectural support in EESSI is relatively limited. The available CPU architectures in EESSI for Iris nodes are</p> <ul> <li><code>x86_64/intel/haswell</code> for <code>broadwell</code> CPUs, and</li> <li><code>x86_64/intel/skylake</code> for <code>skylake</code> CPUs.</li> </ul> <p>EESSI does not provide builds optimized for all architectures, so the older <code>haswell</code> was chosen as the best alternative for <code>broadwell</code> which is missing.</p> <p>Values for the <code>RESIF_ARCH</code> and <code>EESSI_ARCHDETECT_OPTIONS_OVERRIDE</code> environment variables in UL HPC systems</p> Cluster Partition (<code>--parition=</code>) Native architecture desciptor (<code>${RESIF_ARCH}</code>) EESSI Architecture descriptor (<code>${EESSI_ARCHDETECT_OPTIONS_OVERRIDE}</code>) Iris <code>batch</code> <code>broadwell</code> <code>x86_64/intel/haswell</code> Iris <code>interactive</code> <code>broadwell</code> <code>x86_64/intel/haswell</code> Iris <code>bigmem</code> <code>skylake</code> Iris <code>gpu</code> <code>gpu</code> Aion <code>batch</code> <code>epyc</code> Aion <code>interactive</code> <code>epyc</code> <p>Note that all <code>bigmen</code> and <code>skylake</code> nodes use Skylake CPUs.</p>"},{"location":"software/swsets/#manual-selection-of-the-software-set","title":"Manual selection of the software set","text":"<p>There are occasion where a user may want to set the software set manually. For instance, a job can be constrained to run on a single kind of CPU, using for instance the <code>--constraint=skylake</code> flag on <code>sbatch</code> or <code>salloc</code> to force the job to run only on Skylake nodes of the <code>batch</code> partition in Iris. In this case it makes sense to use a software set optimized for Skylake.</p> Selecting a natively optimized software set for Skylake CPUs in the Iris CPU partitions <p>The <code>${RESIF_ARCH}</code> value used for all nodes in the CPU partitions of Iris is <code>broadwell</code>. To use the more optimized <code>skylake</code> software set, first purge any loaded natively optimized software sets:</p> <pre><code>$ module --force purge\n</code></pre> <p>There there are 2 options to select the natively optimized software set:</p> <ul> <li>Set the <code>RESIF_ARCH</code> variable manually and load the software set you require with a <code>module</code>:   <pre><code>$ export RESIF_ARCH=skylake\n$ module load env/development/2023b\n</code></pre></li> <li>Edit the <code>MODULEPATH</code> variable with the <code>use</code> option of the <code>module</code> command:   <pre><code>$ module use /opt/apps/easybuild/systems/iris/rhel810-20250216/2023b/skylake/modules/all\n</code></pre></li> </ul> Use an optimal EESSI software sets for Skylake CPUs in the Iris CPU partitions <p>The EESSI module loading the software set is configured to load modules for a CPU architecture that is compatible with all CPUs in the CPU partitions of Iris. If you are sure that your program will run on a single type of CPU architecture simply unset the variable and load the EESSI software modules:</p> <pre><code>$ unset EESSI_ARCHDETECT_OPTIONS_OVERRIDE\n$ module load EESSI\n</code></pre> <p>Then the EESSI module automatically detects the architecture and load the appropriate modules.</p> <p>You can always add a software set manually to <code>MODULEPATH</code> using the <code>use</code> option of the <code>module</code> command. To facilitate the organization of the natively optimized software sets the values of the <code>RESIF_ARCH</code> are used to determine the storage path of each software set. These location are summarized in the following table.</p> <p>Location of natively optimized software set</p> Cluster Arch. <code>${RESIF_ARCH}</code> <code>${MODULEPATH}</code> Environment variable Iris <code>broadwell</code> <code>/opt/apps/easybuild/systems/iris/&lt;build version&gt;/&lt;software set version&gt;/broadwell/modules/all</code> Iris <code>skylake</code> <code>/opt/apps/easybuild/systems/iris/&lt;build version&gt;/&lt;software set version&gt;/skylake/modules/all</code> Iris <code>gpu</code> <code>/opt/apps/easybuild/systems/iris/&lt;build version&gt;/&lt;software set version&gt;/gpu/modules/all</code> Aion <code>epyc</code> <code>/opt/apps/easybuild/systems/aion/&lt;build version&gt;/&lt;software set version&gt;/epyc/modules/all</code>"},{"location":"software/swsets/#default-native-module-set","title":"Default native module set","text":"<p>By default a native module set is loaded when you login into a node. This software set is <pre><code>env/release/default\n</code></pre> and its simply a symbolic link pointing to a some of the software sets in <code>end/release</code>. You can change the default software set by setting the environment variable <code>LMOD_SYSTEM_DEFAULT_MODULES</code> to a colon separated list of the modules you want to be loaded by default when you link into a node. For instance, <pre><code>export LMOD_SYSTEM_DEFAULT_MODULES=/env/testing/2023b\n</code></pre> loads the <code>/env/testing/2023b</code> software set, and <pre><code>export LMOD_SYSTEM_DEFAULT_MODULES=/env/testing/2023b:EESSI/2023.06\n</code></pre> loads both <code>/env/testing/2023b</code> and <code>EESSI/2023.06</code> software sets. You can force a clean default environment by setting <code>LMOD_SYSTEM_DEFAULT_MODULES</code> to the empty string.</p> Modify the default software set <p>You can define the <code>LMOD_SYSTEM_DEFAULT_MODULES</code> environment variable in your <code>${HOME}/.bashrc</code> file to permanently modify the default software set loaded when logging into a compute node.</p> Inner workings of default modules <p>During login to a compute node, the command <pre><code>module --initial_load restore\n</code></pre> is executed (in a <code>profile.d</code> script). The <code>LMOD_SYSTEM_DEFAULT_MODULES</code> environment variable is used by the <code>restore</code> argument of the module command to purge the environment and load a default set of modules. The flag <code>--initial_load</code> is used to avoid the output of a report of the operations performed.</p>"},{"location":"software/swsets/#module-naming-schemes","title":"Module Naming Schemes","text":"<p>Module Naming Schemes on UL HPC system</p> <p>ULHPC modules are organised through the Categorized Naming Scheme.</p> <p>Format: <code>&lt;category&gt;/&lt;name&gt;/&lt;version&gt;-&lt;toolchain&gt;&lt;versionsuffix&gt;</code></p> <p>This means that the typical module hierarchy has as prefix a category level, taken from one of the supported software category or module class: <pre><code>$ eb --show-default-moduleclasses\nDefault available module classes:\n\n        base:      Default module class\n        ai:        Artificial Intelligence (incl. Machine Learning)\n        astro:     Astronomy, Astrophysics and Cosmology\n        bio:       Bioinformatics, biology and biomedical\n        cae:       Computer Aided Engineering (incl. CFD)\n        chem:      Chemistry, Computational Chemistry and Quantum Chemistry\n        compiler:  Compilers\n        data:      Data management &amp; processing tools\n        debugger:  Debuggers\n        devel:     Development tools\n        geo:       Earth Sciences\n        ide:       Integrated Development Environments (e.g. editors)\n        lang:      Languages and programming aids\n        lib:       General purpose libraries\n        math:      High-level mathematical software\n        mpi:       MPI stacks\n        numlib:    Numerical Libraries\n        perf:      Performance tools\n        quantum:   Quantum Computing\n        phys:      Physics and physical systems simulations\n        system:    System utilities (e.g. highly depending on system OS and hardware)\n        toolchain: EasyBuild toolchains\n        tools:     General purpose tools\n        vis:       Visualization, plotting, documentation and typesetting\n</code></pre></p> <p>It follows that the UL HPC software modules are structured accordingly.</p> <p> List of available software</p>"},{"location":"software/swsets/#software-list-by-category","title":"Software List By category","text":"<ul> <li>Biology</li> <li>CFD/Finite element modelling</li> <li>Chemistry</li> <li>Compilers</li> <li>Data processing</li> <li>Debugging</li> <li>Development</li> <li>Weather modelling</li> <li>Programming Languages</li> <li>Libraries</li> <li>Mathematics</li> <li>MPI</li> <li>Numerical libraries</li> <li>Performance measurements</li> <li>Physics</li> <li>System-level software</li> <li>Toolchains (software stacks)</li> <li>Utilities</li> <li>Visualisation</li> </ul> <ol> <li> <p>See the basic info section for the terminology related to toolchains.\u00a0\u21a9</p> </li> </ol>"},{"location":"software/cae/abaqus/","title":"Abaqus","text":"<p>The Abaqus Unified FEA product suite offers powerful and complete solutions for both routine and sophisticated engineering problems covering a vast spectrum of industrial applications. In the automotive industry engineering work groups are able to consider full vehicle loads, dynamic vibration, multibody systems, impact/crash, nonlinear static, thermal coupling, and acoustic-structural coupling using a common model data structure and integrated solver technology. Best-in-class companies are taking advantage of Abaqus Unified FEA to consolidate their processes and tools, reduce costs and inefficiencies, and gain a competitive advantage</p>"},{"location":"software/cae/abaqus/#available-versions-of-abaqus-in-ulhpc","title":"Available versions of Abaqus in ULHPC","text":"<p>To check available versions of Abaqus at ULHPC, type <code>module spider abaqus</code>. It will list the available versions with the following format: <pre><code>cae/ABAQUS/&lt;version&gt;[-hotfix-&lt;hotfix&gt;]\n</code></pre></p> <p>Don't forget to unset <code>SLURM_GTIDS</code></p> <p>You MUST unset the SLURM environment variable <code>SLURM_GTIDS</code> for both interactive/GUI and batch jobs <pre><code>unset SLURM_GTIDS\n</code></pre> Failure to do so will cause Abaqus to get stuck due to the MPI that Abaqus ships witch is not supporting the SLURM scheduler.</p> <p>When using a general compute node for Abaqus 2021, please run:</p> <ul> <li><code>abaqus cae -mesa</code> to launch the GUI without support for hardware-accelerated graphics rendering.<ul> <li>the option <code>-mesa</code> disables hardware-accelerated graphics rendering within Abaqus\u2019s GUI.</li> </ul> </li> <li>For a Non-Graphical execution, use   <pre><code>abaqus job=&lt;my_job_name&gt; input=&lt;filename&gt;.inp mp_mode=&lt;mode&gt; cpus=&lt;cores&gt; [gpus=&lt;gpus&gt;] scratch=$SCRATCH memory=\"&lt;mem&gt;gb\"\n</code></pre></li> </ul>"},{"location":"software/cae/abaqus/#supported-parallel-mode","title":"Supported parallel mode","text":"<p>Abaqus has two parallelization options which are mutually exclusive:</p> <ul> <li> <p>MPI (<code>mp_mode=mpi</code>), which is generally preferred since this allows for scaling the job to multiple compute nodes. As for MPI jobs, use <code>-N &lt;nodes&gt; --ntasks-per-node &lt;cores&gt; -c1</code> upon submission to use:</p> <pre><code> abaqus mp_mode=mpi cpu=$SLURM_NTASKS [...]\n</code></pre> </li> <li> <p>Shared memory / Threads (<code>mp_mode=threads</code>) for single node / multi-threaded executions. Typically use <code>-N1 --ntasks-per-node 1 -c &lt;threads&gt;</code> upon submission to use:</p> <pre><code>abaqus mp_mode=threads cpus=${SLURM_CPUS_PER_TASK} [...]\n</code></pre> </li> <li> <p>Shared memory for single node with GPU(s) / multi-threaded executions (<code>mp_mode=threads</code>).  Typically use <code>-N1 -G 1 --ntasks-per-node 1 -c &lt;threads&gt;</code> upon submission on a GPU node to use:</p> <pre><code>abaqus mp_mode=threads cpus=${SLURM_CPUS_PER_TASK} gpus=${SLURM_GPUS} [...]\n</code></pre> </li> </ul>"},{"location":"software/cae/abaqus/#abaqus-example-problems","title":"Abaqus example problems","text":"<p>Abaqus contains a large number of example problems which can be used to become familiar with Abaqus on the system. These example problems are described in the Abaqus documentation and can be obtained using the <code>abaqus fetch jobs=&lt;name&gt;</code> command.</p> <p>For example, after loading the Abaqus module <code>cae/ABAQUS</code>, enter the following at the command line to extract the input file for test problem s4d: <pre><code>abaqus fetch job=s4d\n</code></pre> This will extract the input file <code>s4d.inp</code> See also Abaqus performance data.</p>"},{"location":"software/cae/abaqus/#interactive-mode","title":"Interactive mode","text":"<p>To open an Abaqus in the interactive mode, please follow the following steps:</p> <p>(eventually) connect to the ULHPC login node with the <code>-X</code> (or <code>-Y</code>) option:</p> IrisAion <pre><code>ssh -X iris-cluster   # OR on Mac OS: ssh -Y iris-cluster\n</code></pre> <pre><code>ssh -X aion-cluster   # OR on Mac OS: ssh -Y aion-cluster\n</code></pre> <p>Then you can reserve an interactive job, for instance with 8 MPI processes. Don't forget to use the <code>--x11</code> option if you intend to use the GUI.</p> <pre><code>$ si --x11 -c 8               # Abaqus mp_mode=threads test\n# OR\n$ si --x11 --ntask-per-node 8 # abaqus mp_mode=mpi test\n\n# Load the module ABAQUS and needed environment\n(node)$ module purge\n(node)$ module load cae/ABAQUS\n(node)$ unset SLURM_GTIDS   # MANDATORY\n\n# /!\\ IMPORTANT: You MUST ADAPT the LM_LICENSE_FILE variable to point to YOUR licence server!!!\n(node)$ export LM_LICENSE_FILE=xyz\n\n# Check License server token available\n(node)$ abaqus licensing lmstat -a\nabaqus licensing lmstat -a\nlmutil - Copyright (c) 1989-2019 Flexera. All Rights Reserved.\nFlexible License Manager status on Wed 4/13/2022 22:39\n[...]\n</code></pre>"},{"location":"software/cae/abaqus/#non-graphical-abaqus","title":"Non-graphical Abaqus","text":"<p>Then the general format to run your Non-Graphical multithreaded interactive execution:</p> Shared Memory (<code>-c &lt;threads&gt;</code>)Distributed Memory (MPI) <p>Assuming a job submitted with <code>{sbatch|srun|si...} -N1 -c &lt;threads&gt;</code>: <pre><code># /!\\ ADAPT $INPUTFILE accordingly\nabaqus job=\"${SLURM_JOB_NAME}\" verbose=2 interactive \\\n    input=${INPUTFILE} \\\n    cpus=${SLURM_CPUS_PER_TASK} mp_mode=threads\n</code></pre></p> <p>Assuming a job submitted with <code>{sbatch|srun|si...} -N &lt;N&gt; --ntasks-per-node &lt;npn&gt; -c 1</code>: <pre><code># /!\\ ADAPT $INPUTFILE accordingly\nabaqus job=\"${SLURM_JOB_NAME}\" verbose=2 interactive \\\n    input=${INPUTFILE} \\\n    cpus=${SLURM_NTASKS} mp_mode=mpi\n</code></pre></p>"},{"location":"software/cae/abaqus/#gui","title":"GUI","text":"<p>If you want to run the GUI, use: <code>abaqus cae -mesa</code></p> License information <p>Assuming you have set the variable <code>LM_LICENSE_FILE</code> to point to YOUR licence server, you can check the available license and group you belongs to with: <pre><code>abaqus licensing lmstat -a\n</code></pre> If  your server is hosted outside the ULHPC network, you will have to contact the HPC team to adapt the network firewalls to allow the connection towards your license server.</p> <p>Use the following options for simulation to stop and resume it: <pre><code># /!\\ ADAPT &lt;jobname&gt; accordingly:\nabaqus job=&lt;jobname&gt; suspend\nabaqus job=&lt;jobname&gt; resume\n</code></pre></p>"},{"location":"software/cae/abaqus/#batch-mode","title":"Batch mode","text":"Shared memory (mp_mode=threads)Distributed memory (mp_mode=mpi)Shared memory with GPU <pre><code>#!/bin/bash -l                # &lt;--- DO NOT FORGET '-l'\n#SBATCH -J &lt;jobname&gt;\n#SBATCH -N 1\n#SBATCH --ntasks-per-node=1\n#SBATCH -c 4                  # /!\\ ADAPT accordingly\n#SBATCH --time=0-03:00:00\n#SBATCH -p batch\n\nprint_error_and_exit() { echo \"***ERROR*** $*\"; exit 1; }\nmodule purge || print_error_and_exit \"No 'module' command\"\nmodule load cae/ABAQUS\n# export LM_LICENSE_FILE=[...]\nunset SLURM_GTIDS\n\nINPUTFILE=s4d.inp\n[ ! -f \"${INPUTFILE}\" ] &amp;&amp; print_error_and_exit \"Unable to find input file ${INPUTFILE}\"\n\nabaqus job=\"${SLURM_JOB_NAME}\" verbose=2 interactive \\\n    input=${INPUTFILE} cpus=${SLURM_CPUS_PER_TASK} mp_mode=threads\n</code></pre> <pre><code>#!/bin/bash -l                # &lt;--- DO NOT FORGET '-l'\n#SBATCH -J &lt;jobname&gt;\n#SBATCH -N 2\n#SBATCH --ntasks-per-node=8  # /!\\ ADAPT accordingly\n#SBATCH -c 1\n#SBATCH --time=0-03:00:00\n#SBATCH -p batch\n\nprint_error_and_exit() { echo \"***ERROR*** $*\"; exit 1; }\nmodule purge || print_error_and_exit \"No 'module' command\"\nmodule load cae/ABAQUS\n# export LM_LICENSE_FILE=[...]\nunset SLURM_GTIDS\n\nINPUTFILE=s4d.inp\n[ ! -f \"${INPUTFILE}\" ] &amp;&amp; print_error_and_exit \"Unable to find input file ${INPUTFILE}\"\n\nabaqus job=\"${SLURM_JOB_NAME}\" verbose=2 interactive \\\n    input=${INPUTFILE} cpus=${SLURM_NTASKS} mp_mode=mpi\n</code></pre> <p>May not be supported depending on the software set <pre><code>#!/bin/bash -l                # &lt;--- DO NOT FORGET '-l'\n#SBATCH -J &lt;jobname&gt;\n#SBATCH -N 1\n#SBATCH --ntasks-per-node=1\n#SBATCH -c 7\n#SBATCH -G 1\n#SBATCH --time=0-03:00:00\n#SBATCH -p gpu\n\nprint_error_and_exit() { echo \"***ERROR*** $*\"; exit 1; }\nmodule purge || print_error_and_exit \"No 'module' command\"\nmodule load cae/ABAQUS\n# export LM_LICENSE_FILE=[...]\nunset SLURM_GTIDS\n\nINPUTFILE=s4d.inp\n[ ! -f \"${INPUTFILE}\" ] &amp;&amp; print_error_and_exit \"Unable to find input file ${INPUTFILE}\"\n\nabaqus job=\"${SLURM_JOB_NAME}\" verbose=2 interactive \\\n    input=${INPUTFILE} cpus=${SLURM_CPUS_PER_TASK} gpus=${SLURM_GPUS} mp_mode=threads\n</code></pre></p>"},{"location":"software/cae/abaqus/#additional-information","title":"Additional information","text":"<p>To know more about Abaqus documentation and tutorial, please refer Abaqus CAE</p> Tutorial <ul> <li>http://www.franc3d.com/wp-content/uploads/2012/05/FRANC3D_V7_ABAQUS_Tutorial.pdf</li> <li>https://sig.ias.edu/files/Abaqus%20tutorial.pdf</li> <li>https://sites.engineering.ucsb.edu/~tshugar/GET_STARTED.pdf?fbclid=IwAR2MQTzCTISqdPuM4D3PiDwXk9oVTBqZWXJUvMccVPYsd1kKPwPOZcnq078</li> </ul> <p>Tip</p> <p>If you find some issues with the instructions above, please file a support ticket.</p>"},{"location":"software/cae/ansys/","title":"ANSYS","text":"<p> ANSYS offers a comprehensive software suite that spans the entire range of physics, providing access to virtually any field of engineering simulation that a design process requires.</p>"},{"location":"software/cae/ansys/#available-versions-of-ansys-at-ulhpc","title":"Available versions of ANSYS at ULHPC","text":"<p>You can check the available versions of ANSYS at ULHPC with the command <code>module spider ansys</code>.</p> <pre><code>$ module spider ansys\n\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  tools/ANSYS:\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    Description:\n      ANSYS simulation software enables organizations to confidently predict how their products will operate in the real world. We believe that every product is a promise of something greater. \n\n     Versions:\n        tools/ANSYS/21.1\n        tools/ANSYS/2022R2\n        tools/ANSYS/2024R2\n\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  For detailed information about a specific \"tools/ANSYS\" package (including how to load the modules) use the module's full name.\n  Note that names that have a trailing (E) are extensions provided by other modules.\n  For example:\n\n     $ module spider tools/ANSYS/2024R2\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n</code></pre> <p>According to the output of the module command, there are 3 versions of ANSYS available, and <code>2024R2</code> is the latest.</p>"},{"location":"software/cae/ansys/#working-in-interactive-mode","title":"Working in interactive mode","text":"<p>To open an ANSYS in interactive mode, connect to the cluster with X11 forwarding enabled,</p> IrisAion <pre><code># From your local computer\n$ ssh -X iris-cluster\n</code></pre> <pre><code># From your local computer\n$ ssh -X aion-cluster\n</code></pre> <p>and then create and allocation with X11 forwarding enabled:</p> IrisAion <pre><code># Reserve the node for interactive computation\n$ salloc --partition=interactive --qos=debug --time=02:00:00 --ntasks=1 --cpus-per-task=14 --x11\n\n# Load the required version of ANSYS and needed environment\n$ module purge\n$ module load tools/ANSYS/2024R2\n\n# To launch ANSYS workbench\n$ runwb2\n</code></pre> <pre><code># Reserve the node for interactive computation\n$ salloc --partition=interactive --qos=debug --time=02:00:00 --ntasks=1 --cpus-per-task=16 --x11\n\n# Load the required version of ANSYS and needed environment\n$ module purge\n$ module load tools/ANSYS/2024R2\n\n# To launch ANSYS workbench\n$ runwb2\n</code></pre> <p>In this example the number of cpus is selected so that a single full socket is reserved in each cluster.</p>"},{"location":"software/cae/ansys/#submitting-batch-scripts","title":"Submitting batch scripts","text":"<p>When you run ANSYS packages such as Fluent in batch mode apart from the usual input, such as journal files, you may provide options to execute the computation across multiple compute nodes.</p> <p>According to the output of help function of Fluent, <code>fluent -h</code>,</p> <ul> <li><code>-gu</code> runs the software without GUI,</li> <li><code>-cnf</code> specifies the host file with the list of hosts available to the program,</li> <li><code>-t</code> specifies the number of host from the host file that will be used (only <code>-t&lt;number&gt;</code> notation is supported for this flag),</li> <li><code>-mpi</code> selects the MPI backend, and</li> <li><code>-p</code> selects the interconnect (only <code>-p&lt;interconnect&gt;</code> notation is supported for this flag).</li> </ul> IrisAion <pre><code>#!/usr/bin/bash --login\n#SBATCH --job-name=VOF\n#SBATCH --mail-type=end,fail\n#SBATCH --mail-user=name.surname@uni.lu\n#SBATCH --partition=batch\n#SBATCH --qos=normal\n#SBATCH --nodes=4\n#SBATCH --ntasks-per-node=28\n#SBATCH --cpus-per-task=1\n#SBATCH --exclusive\n#SBATCH --time=1-00:00:00\n#SBATCH --output=%x-%j.out\n#SBATCH --error=%x-%j.err\n\ndeclare HOSTSFILE=/tmp/hostlist-${SLURM_JOB_ID}\nscontrol show hostnames &gt; ${HOSTSFILE}\n\n# To get basic info. about the job\necho \"== Job ID: ${SLURM_JOBID}\"\necho \"== Node list: ${SLURM_NODELIST}\"\necho \"== Working directory: ${SLURM_SUBMIT_DIR}\"\necho \"Starting run at $(date)\"\necho \"\"\n\nmodule purge\nmodule load tools/ANSYS/2024R2\n\nfluent 3ddp -gu -t${SLURM_NTASKS} -cnf=${HOSTSFILE} -mpi=openmpi -pib -i journal.txt \n</code></pre> <pre><code>#!/usr/bin/bash --login\n#SBATCH --job-name=VOF\n#SBATCH --mail-type=end,fail\n#SBATCH --mail-user=name.surname@uni.lu\n#SBATCH --partition=batch\n#SBATCH --qos=normal\n#SBATCH --nodes=4\n#SBATCH --ntasks-per-node=128\n#SBATCH --cpus-per-task=1\n#SBATCH --exclusive\n#SBATCH --time=1-00:00:00\n#SBATCH --output=%x-%j.out\n#SBATCH --error=%x-%j.err\n\ndeclare HOSTSFILE=/tmp/hostlist-${SLURM_JOB_ID}\nscontrol show hostnames &gt; ${HOSTSFILE}\n\n# To get basic info. about the job\necho \"== Job ID: ${SLURM_JOBID}\"\necho \"== Node list: ${SLURM_NODELIST}\"\necho \"== Working directory: ${SLURM_SUBMIT_DIR}\"\necho \"Starting run at $(date)\"\necho \"\"\n\nmodule purge\nmodule load tools/ANSYS/2024R2\n\nfluent 3ddp -gu -t${SLURM_NTASKS} -cnf=${HOSTSFILE} -mpi=openmpi -pib -i journal.txt \n</code></pre> <p>The ANSYS binaries come bundled with their own version of MPI that operates independent of the system job launcher and so they need a host list and a communication backend to start the job. This job</p> <ul> <li>runs <code>SLURM_NTASKS</code> tasks (<code>-t${SLURM_NTASKS}</code>), where <code>SLURM_NTASKS = SLURM_NTASKS_PER_NODE * SLURM_JOB_NUM_NODES</code> are assigned in a round-robin fashion along the entries of <code>HOSTFILE</code>, and</li> <li>uses the <code>openmpi</code> backend (<code>-mpi=openmpi</code>)</li> <li>with the <code>ib</code> (Infiniband) interconnect (<code>-pid</code>).</li> </ul> <p>Similar options are used for other packages of ANSYS.</p>"},{"location":"software/cae/ansys/#additional-information","title":"Additional information","text":"<p>ANSYS provides the customer support, if you have a license key, you should be able to get access the support, manuals, and other useful documents.</p> <p>You can find an old but relevant version of the user manual section on parallel jobs in the documentation of the ENEAGRID/CRESCO hyper cluster. The user manual provides detailed explanation for the various parallel job options.</p> <p>Tip</p> <p>If you find some issues with the instructions above, please file a support ticket.</p>"},{"location":"software/cae/fds/","title":"FDS","text":"<p>Fire Dynamics Simulator (FDS) is a large-eddy simulation (LES) code for low-speed flows, with an emphasis on smoke and heat transport from fires.</p>"},{"location":"software/cae/fds/#available-versions-of-fds-in-ulhpc","title":"Available versions of FDS in ULHPC","text":"<p>To check available versions of FDS at ULHPC type <code>module spider abaqus</code>. The following versions of FDS are available in ULHPC:  <pre><code># Available versions\nphys/FDS/6.7.1-intel-2018a\nphys/FDS/6.7.1-intel-2019a\nphys/FDS/6.7.3-intel-2019a\n</code></pre></p>"},{"location":"software/cae/fds/#interactive-mode","title":"Interactive mode","text":"<p>To try FDS in the interactive mode, please follow the following steps: <pre><code># From your local computer\n$ ssh -X iris-cluster\n\n# Reserve the node for interactive computation\n$ salloc -p interactive --time=00:30:00 --ntasks 1 -c 4 --x11\n\n# Load the required version of FDS and needed environment\n$ module purge\n$ module load swenv/default-env/devel\n$ module load phys/FDS/6.7.3-intel-2019a\n\n# Example in fds \n$ fds example.fds\n</code></pre></p>"},{"location":"software/cae/fds/#batch-mode","title":"Batch mode","text":""},{"location":"software/cae/fds/#mpi-only","title":"MPI only:","text":"<pre><code>#!/bin/bash -l\n#SBATCH -J FDS-mpi\n#SBATCH -N 2\n#SBATCH --ntasks-per-node=28\n#SBATCH --ntasks-per-socket=14\n#SBATCH -c 1\n#SBATCH --time=00:30:00\n#SBATCH -p batch\n\n# Write out the stdout+stderr in a file\n#SBATCH -o output.txt\n\n# Mail me on job start &amp; end\n#SBATCH --mail-user=myemailaddress@universityname.domain\n#SBATCH --mail-type=BEGIN,END\n\n# To get basic info. about the job\necho \"== Starting run at $(date)\"\necho \"== Job ID: ${SLURM_JOBID}\"\necho \"== Node list: ${SLURM_NODELIST}\"\necho \"== Submit dir. : ${SLURM_SUBMIT_DIR}\"\n\n# Load the required version of FDS and needed environment\nmodule purge\nmodule load swenv/default-env/devel\nmodule load phys/FDS/6.7.3-intel-2019a\n\nsrun fds example.fds\n</code></pre>"},{"location":"software/cae/fds/#mpiopenmp-hybrid","title":"MPI+OpenMP (hybrid):","text":"<pre><code>#!/bin/bash -l\n#SBATCH -J FDS-hybrid\n#SBATCH -N 2\n#SBATCH --ntasks-per-node=56\n#SBATCH --cpus-per-task=2\n#SBATCH --time=00:30:00\n#SBATCH -p batch\n\n# Write out the stdout+stderr in a file\n#SBATCH -o output.txt\n\n# Mail me on job start &amp; end\n#SBATCH --mail-user=myemailaddress@universityname.domain\n#SBATCH --mail-type=BEGIN,END\n\n# To get basic info. about the job\necho \"== Starting run at $(date)\"\necho \"== Job ID: ${SLURM_JOBID}\"\necho \"== Node list: ${SLURM_NODELIST}\"\necho \"== Submit dir. : ${SLURM_SUBMIT_DIR}\"\n\n# Load the required version of FDS and needed environment\nmodule purge\nmodule load swenv/default-env/devel\nmodule load phys/FDS/6.7.3-intel-2019a\n\nsrun --cpus-per-task=2 fds_hyb example.fds\n</code></pre>"},{"location":"software/cae/fds/#additional-information","title":"Additional information","text":"<p>To know more about FDS documentation and tutorial, please refer https://pages.nist.gov/fds-smv/manuals.html</p> <p>Tip</p> <p>If you find some issues with the instructions above, please file a support ticket.</p>"},{"location":"software/cae/fenics/","title":"FEniCS","text":"<p> FEniCS is a popular open-source (LGPLv3) computing platform for solving partial differential equations (PDEs). FEniCS enables users to quickly translate scientific models into efficient finite element code. With the high-level Python and C++ interfaces to FEniCS, it is easy to get started, but FEniCS offers also powerful capabilities for more experienced programmers. FEniCS runs on a multitude of platforms ranging from laptops to high-performance clusters.</p>"},{"location":"software/cae/fenics/#how-to-access-the-fenics-through-anaconda","title":"How to access the FEniCS through Anaconda","text":"<p>The following steps provides information about how to installed on your local path.  <pre><code># From your local computer\n$ ssh -X iris-cluster    # OR ssh -Y iris-cluster on Mac\n\n# Reserve the node for interactive computation with grahics view (plots)\n$ si --x11 --ntasks-per-node 1 -c 4\n# salloc -p interactive --qos debug -C batch --x11 --ntasks-per-node 1 -c 4\n\n# Go to scratch directory \n$ cds\n\n/scratch/users/&lt;login&gt; $ Anaconda3-2020.07-Linux-x86_64.sh\n/scratch/users/&lt;login&gt; $ chmod +x Anaconda3-2020.07-Linux-x86_64.sh\n/scratch/users/&lt;login&gt; $ ./Anaconda3-2020.07-Linux-x86_64.sh\n\nDo you accept the license terms? [yes|no]\nyes\nAnaconda3 will now be installed into this location:\n/home/users/&lt;login&gt;/anaconda3\n\n  - Press ENTER to confirm the location\n  - Press CTRL-C to abort the installation\n  - Or specify a different location below\n\n# You can choose your path where you want to install it\n[/home/users/&lt;login&gt;/anaconda3] &gt;&gt;&gt; /scratch/users/&lt;login&gt;/Anaconda3\n\n# To activate the anaconda \n/scratch/users/&lt;login&gt; $ source /scratch/users/&lt;login&gt;/Anaconda3/bin/activate\n\n# Install the fenics in anaconda environment \n/scratch/users/&lt;login&gt; $ conda create -n fenicsproject -c conda-forge fenics\n\n# Install matplotlib for the visualization \n/scratch/users/&lt;login&gt; $ conda install -c conda-forge matplotlib \n</code></pre> Once you have installed the anaconda, you can always activate it by calling the <code>source activate</code> path where <code>anaconda</code> has been installed. </p>"},{"location":"software/cae/fenics/#working-example","title":"Working example","text":""},{"location":"software/cae/fenics/#interactive-mode","title":"Interactive mode","text":"<pre><code># From your local computer\n$ ssh -X iris-cluster      # or ssh -Y iris-cluster on Mac\n\n# Reserve the node for interactive computation with grahics view (plots)\n$ si --ntasks-per-node 1 -c 4 --x11\n# salloc -p interactive --qos debug -C batch --x11 --ntasks-per-node 1 -c 4\n\n# Activate anaconda  \n$ source /${SCRATCH}/Anaconda3/bin/activate\n\n# activate the fenicsproject\n$ conda activate fenicsproject\n\n# execute the Poisson.py example (you can uncomment the plot lines in Poission.py example)\n$ python3 Poisson.py\n</code></pre>"},{"location":"software/cae/fenics/#batch-script","title":"Batch script","text":"<pre><code>#!/bin/bash -l                                                                                                 \n#SBATCH -J FEniCS                                                                                        \n#SBATCH -N 1\n###SBATCH -A &lt;project name&gt;\n###SBATCH --ntasks-per-node=1\n#SBATCH -c 1\n#SBATCH --time=00:05:00                                                                      \n#SBATCH -p batch\n\necho \"== Starting run at $(date)\"                                                                                             \necho \"== Job ID: ${SLURM_JOBID}\"                                                                                            \necho \"== Node list: ${SLURM_NODELIST}\"                                                                                       \necho \"== Submit dir. : ${SLURM_SUBMIT_DIR}\"\n\n# activate the anaconda source \nsource ${SCRATCH}/Anaconda3/bin/activate\n\n# activate the fenicsproject from anaconda \nconda activate fenicsproject\n\n# execute the poisson.py through python\nsrun python3 Poisson.py  \n</code></pre>"},{"location":"software/cae/fenics/#example-poissonpy","title":"Example (Poisson.py)","text":"<pre><code># FEniCS tutorial demo program: Poisson equation with Dirichlet conditions.\n# Test problem is chosen to give an exact solution at all nodes of the mesh.\n#  -Laplace(u) = f    in the unit square\n#            u = u_D  on the boundary\n#  u_D = 1 + x^2 + 2y^2\n#    f = -6\n\nfrom __future__ import print_function\nfrom fenics import *\nimport matplotlib.pyplot as plt\n\n# Create mesh and define function space\nmesh = UnitSquareMesh(8, 8)\nV = FunctionSpace(mesh, 'P', 1)\n\n# Define boundary condition\nu_D = Expression('1 + x[0]*x[0] + 2*x[1]*x[1]', degree=2)\n\ndef boundary(x, on_boundary):\n    return on_boundary\n\nbc = DirichletBC(V, u_D, boundary)\n\n# Define variational problem\nu = TrialFunction(V)\nv = TestFunction(V)\nf = Constant(-6.0)\na = dot(grad(u), grad(v))*dx\nL = f*v*dx\n\n# Compute solution\nu = Function(V)\nsolve(a == L, u, bc)\n\n# Plot solution and mesh\n#plot(u)\n#plot(mesh)\n\n# Save solution to file in VTK format\nvtkfile = File('poisson/solution.pvd')\nvtkfile &lt;&lt; u\n\n# Compute error in L2 norm\nerror_L2 = errornorm(u_D, u, 'L2')\n\n# Compute maximum error at vertices\nvertex_values_u_D = u_D.compute_vertex_values(mesh)\nvertex_values_u = u.compute_vertex_values(mesh)\nimport numpy as np\nerror_max = np.max(np.abs(vertex_values_u_D - vertex_values_u))\n\n# Print errors\nprint('error_L2  =', error_L2)\nprint('error_max =', error_max)\n\n# Hold plot\n#plt.show()\n</code></pre>"},{"location":"software/cae/fenics/#additional-information","title":"Additional information","text":"<p>FEniCS provides the technical documentation, and also it provides lots of communication channel for support and development.</p> <p>Tip</p> <p>If you find some issues with the instructions above, please file a support ticket.</p>"},{"location":"software/cae/meshing-tools/","title":"Meshing Tools","text":""},{"location":"software/cae/meshing-tools/#gmsh","title":"Gmsh","text":"<p> Gmsh is an open source 3D finite element mesh generator with a built-in CAD engine and post-processor. Its design goal is to provide a fast, light and user-friendly meshing tool with parametric input and advanced visualization capabilities. Gmsh is built around four modules: geometry, mesh, solver and post-processing. The specification of any input to these modules is done either interactively using the graphical user interface, in ASCII text files using Gmsh's own scripting language (.geo files), or using the C++, C, Python or Julia Application Programming Interface (API).</p> <p>See this general presentation for a high-level overview of Gmsh and recent developments, the screencasts for a quick tour of Gmsh's graphical user interface, and the reference manual for a more thorough overview of Gmsh's capabilities, some frequently asked questions and the documentation of the C++, C, Python and Julia API.</p> <p>The source code repository contains many examples written using both the built-in script language and the API (see e.g. the tutorials and the and demos).</p>"},{"location":"software/cae/meshing-tools/#available-versions-of-gmsh-in-ulhpc","title":"Available versions of Gmsh in ULHPC","text":"<p>To check available versions of Gmsh at ULHPC type <code>module spider gmsh</code>. Below it shows list of available versions of Gmsh in ULHPC.  <pre><code>cae/gmsh/4.3.0-intel-2018a\ncae/gmsh/4.4.0-intel-2019a\n</code></pre></p>"},{"location":"software/cae/meshing-tools/#to-work-with-gmsh-interactively-on-ulhpc","title":"To work with Gmsh interactively on ULHPC:","text":"<pre><code># From your local computer\n$ ssh -X iris-cluster\n\n# Reserve the node for interactive computation\n$ salloc -p interactive --time=00:30:00 --ntasks 1 -c 4 --x11\n\n# Load the module for Gmesh and neeed environment\n$ module purge\n$ module load swenv/default-env/v1.2-20191021-production\n$ module load cae/gmsh/4.4.0-intel-2019a\n\n$ gmsh example.geo\n</code></pre>"},{"location":"software/cae/meshing-tools/#salome","title":"Salome","text":"<p>SALOME is an open-source software that provides a generic Pre- and Post-Processing platform for numerical simulation. It is based on an open and flexible architecture made of reusable components.</p> <p>SALOME is a cross-platform solution. It is distributed under the terms of the GNU LGPL license. You can download both the source code and the executables from this site.</p> <p>To know more about salome documentation please refer https://www.salome-platform.org/user-section/salome-tutorials</p>"},{"location":"software/cae/meshing-tools/#available-versions-of-salome-in-ulhpc","title":"Available versions of SALOME in ULHPC","text":"<p>To check available versions of SALOME at ULHPC type <code>module spider salome</code>. Below it shows list of available versions of SALOME in ULHPC.</p> <pre><code>cae/Salome/8.5.0-intel-2018a\ncae/Salome/8.5.0-intel-2019a\n</code></pre>"},{"location":"software/cae/meshing-tools/#to-work-with-salome-interactively-on-ulhpc","title":"To work with SALOME interactively on ULHPC:","text":"<pre><code># From your local computer\n$ ssh -X iris-cluster\n\n# Reserve the node for interactive computation\n$ srun -p batch --time=00:30:00 --ntasks 1 -c 4 --x11 --pty bash -i\n\n# Load the module Salome and needed environment\n$ module purge\n$ module load swenv/default-env/v1.2-20191021-production\n$ module load cae/Salome/8.5.0-intel-2019a\n\n$ salome start\n</code></pre> <p>Tip</p> <p>If you find some issues with the instructions above, please file a support ticket.</p>"},{"location":"software/cae/openfoam/","title":"OpenFOAM","text":"<p>OpenFOAM is the free, open source CFD software developed primarily by OpenCFD Ltd since 2004. It has a large user base across most areas of engineering and science, from both commercial and academic organisations. OpenFOAM has an extensive range of features to solve anything from complex fluid flows involving chemical reactions, turbulence and heat transfer, to acoustics, solid mechanics and electromagnetics</p>"},{"location":"software/cae/openfoam/#available-versions-of-openfoam-in-ulhpc","title":"Available versions of OpenFOAM in ULHPC","text":"<p>To check available versions of OpenFOAM at ULHPC type <code>module spider openfoam</code>. The following versions of OpenFOAM are available in ULHPC: <pre><code># Available versions\ncae/OpenFOAM/v1712-intel-2018a\ncae/OpenFOAM/v1812-foss-2019a   \n</code></pre></p>"},{"location":"software/cae/openfoam/#interactive-mode","title":"Interactive mode","text":"<p>To run an OpenFOAM in the interactive mode, please follow the following steps: <pre><code># From your local computer\n$ ssh -X iris-cluster\n\n# Reserve the node for interactive computation\n$ salloc -p batch --time=00:30:00 --ntasks 1 -c 4 --x11\n\n# Load the required version of OpenFOAM and Intel environment\n$ module load swenv/default-env/v1.1-20180716-production\n$ module load cae/OpenFOAM/v1712-intel-2018a\n\n# Load the OpenFOAM environment\n$ source $FOAM_BASH\n\n$ mkdir OpenFOAM\n$ cd OpenFOAM\n\n# Copy the example to your local folder (cavity example)\n$ cp -r /opt/apps/resif/data/production/v1.1-20180716/default/software/cae/OpenFOAM/v1712-intel-2018a/OpenFOAM-v1712/tutorials/incompressible/icoFoam/cavity/cavity .\n$ cd cavity\n\n# To initialize the mesh\n$ blockMesh\n\n# Run the simulation\n$ icoFoam\n\n# Visualize the solution\n$ paraFoam\n</code></pre></p>"},{"location":"software/cae/openfoam/#batch-mode","title":"Batch mode","text":"<p>Example of computational domain preparation (Dambreak example). <pre><code>$ mkdir OpenFOAM\n$ cd OpenFOAM\n$ cp -r /opt/apps/resif/data/production/v1.1-20180716/default/software/cae/OpenFOAM/v1712-intel-2018a/OpenFOAM-v1712/tutorials/multiphase/interFoam/laminar/damBreak/damBreak .\n$ blockMesh\n$ cd damBreak/system\n</code></pre> Open a <code>decomposeParDict</code> and set <code>numberOfSubdomains 16</code> where <code>n</code> is number of MPI processor. And do <code>blockMesh</code> to prepare the computational domain (mesh) and finally do the <code>decomposePar</code> to repartition the mesh domain. </p> <pre><code>#!/bin/bash -l\n#SBATCH -J OpenFOAM\n#SBATCH -N 1\n#SBATCH --ntasks-per-node=28\n#SBATCH --ntasks-per-socket=14\n#SBATCH -c 1\n#SBATCH --time=00:30:00\n#SBATCH -p batch\n\n# Write out the stdout+stderr in a file\n#SBATCH -o output.txt\n\n# Mail me on job start &amp; end\n#SBATCH --mail-user=myemailaddress@universityname.domain\n#SBATCH --mail-type=BEGIN,END\n\n# To get basic info. about the job\necho \"== Starting run at $(date)\"\necho \"== Job ID: ${SLURM_JOBID}\"\necho \"== Node list: ${SLURM_NODELIST}\"\necho \"== Submit dir. : ${SLURM_SUBMIT_DIR}\"\n\n# Load the required version of OpenFOAM and needed environment\nmodule purge\nmodule load swenv/default-env/v1.1-20180716-production\nmodule load cae/OpenFOAM/v1712-intel-2018a\n\n# Load the OpenFOAM environment\nsource $FOAM_BASH\n\nsrun interFoam -parallel\n</code></pre>"},{"location":"software/cae/openfoam/#additional-information","title":"Additional information","text":"<p>To know more information about OpenFOAM tutorial/documentation, please refer https://www.openfoam.com/documentation/tutorial-guide/</p> <p>Tip</p> <p>If you find some issues with the instructions above, please file a support ticket.</p>"},{"location":"software/computational-chemistry/electronics/abinit/","title":"ABINIT","text":"<p> ABINIT is a software suite to calculate the optical, mechanical, vibrational, and other observable properties of materials. Starting from the quantum equations of density functional theory, you can build up to advanced applications with perturbation theories based on DFT, and many-body Green's functions (GW and DMFT) . ABINIT can calculate molecules, nanostructures and solids with any chemical composition, and comes with several complete and robust tables of atomic potentials. On-line tutorials are available for the main features of the code, and several schools and workshops are organized each year.</p>"},{"location":"software/computational-chemistry/electronics/abinit/#available-versions-of-abinit-in-ulhpc","title":"Available versions of ABINIT in ULHPC","text":"<p>To check available versions of ABINIT at ULHPC type <code>module spider abinit</code>. The following list shows the available versions of ABINIT in ULHPC.  <pre><code>chem/ABINIT/8.2.3-intel-2017a\nchem/ABINIT/8.6.3-intel-2018a-trio-nc\nchem/ABINIT/8.6.3-intel-2018a\nchem/ABINIT/8.10.2-intel-2019a\n</code></pre></p>"},{"location":"software/computational-chemistry/electronics/abinit/#interactive-mode","title":"Interactive mode","text":"<p>To open an ABINIT in the interactive mode, please follow the following steps:</p> <pre><code># From your local computer\n$ ssh -X iris-cluster\n\n# Reserve the node for interactive computation\n$ salloc -p interactive --time=00:30:00 --ntasks 1 -c 4 --x11 # OR si --x11 [...]\n\n# Load the module abinit and needed environment\n$ module purge\n$ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) \n$ module load chem/ABINIT/8.10.2-intel-2019a\n\n$ export SRUN_CPUS_PER_TASK=$SLURM_CPUS_PER_TASK\n\n$ abinit &lt; example.in \n</code></pre>"},{"location":"software/computational-chemistry/electronics/abinit/#batch-mode","title":"Batch mode","text":"<pre><code>#!/bin/bash -l\n#SBATCH -J ABINIT\n#SBATCH -A &lt;project name&gt;\n#SBATCH -M --cluster iris \n#SBATCH -N 2\n#SBATCH --ntasks-per-node=28\n#SBATCH --time=00:30:00\n#SBATCH -p batch\n\n# Load the module abinit and needed environment\nmodule purge\nmodule load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) \nmodule load chem/ABINIT/8.10.2-intel-2019a\n\nsrun -n ${SLURM_NTASKS} abinit &lt; input.files &amp;&gt; out\n</code></pre>"},{"location":"software/computational-chemistry/electronics/abinit/#additional-information","title":"Additional information","text":"<p>To know more information about ABINIT tutorial and documentation, please refer to ABINIT tutorial.</p> <p>Tip</p> <p>If you find some issues with the instructions above, please report it to us using support ticket.</p>"},{"location":"software/computational-chemistry/electronics/ase/","title":"ASE","text":"<p>The Atomic Simulation Environment (ASE) is a set of tools and Python modules for setting up, manipulating, running, visualizing and analyzing atomistic simulations. The code is freely available under the GNU LGPL license. ASE provides interfaces to different codes through <code>Calculators</code> which are used together with the central <code>Atoms</code> object and the many available algorithms in ASE.</p>"},{"location":"software/computational-chemistry/electronics/ase/#available-versions-of-ase-in-ulhpc","title":"Available versions of ASE in ULHPC","text":"<p>To check available versions of ASE at ULHPC type <code>module spider ase</code>. The following list shows the available versions of ASE in ULHPC. <pre><code>chem/ASE/3.13.0-intel-2017a-Python-2.7.13\nchem/ASE/3.16.0-foss-2018a-Python-2.7.14\nchem/ASE/3.16.0-intel-2018a-Python-2.7.14\nchem/ASE/3.17.0-foss-2019a-Python-3.7.2\nchem/ASE/3.17.0-intel-2019a-Python-2.7.15\nchem/ASE/3.17.0-intel-2019a-Python-3.7.2\n</code></pre></p>"},{"location":"software/computational-chemistry/electronics/ase/#interactive-mode","title":"Interactive mode","text":"<p>To open an ASE in the interactive mode, please follow the following steps:</p> <pre><code># From your local computer\n$ ssh -X iris-cluster\n\n# Reserve the node for interactive computation\n$ salloc -p interactive --time=00:30:00 --ntasks 1 -c 4 --x11 # OR si --x11 [...]\n\n# Load the module ase and needed environment\n$ module purge\n$ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) \n$ module load chem/ASE/3.17.0-intel-2019a-Python-3.7.2\n\n$ python3 example.py\n</code></pre>"},{"location":"software/computational-chemistry/electronics/ase/#batch-mode","title":"Batch mode","text":"<pre><code>#!/bin/bash -l\n#SBATCH -J ASE\n#SBATCH -N 1\n#SBATCH -A &lt;project name&gt;\n#SBATCH -M --cluster iris \n#SBATCH --ntasks-per-node=1\n#SBATCH --time=00:30:00\n#SBATCH -p batch\n\n# Load the module ase and needed environment\n$ module purge\n$ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) \n$ module load chem/ASE/3.17.0-intel-2019a-Python-3.7.2\n\npython3 example.py\n</code></pre>"},{"location":"software/computational-chemistry/electronics/ase/#additional-information","title":"Additional information","text":"<p>To know more information about ASE tutorial and documentation, please refer to ASE tutorials.</p> <p>Tip</p> <p>If you find some issues with the instructions above, please report it to us using support ticket.</p>"},{"location":"software/computational-chemistry/electronics/crystal/","title":"Crystal","text":"<p> The CRYSTAL package performs <code>ab  initio</code> calculations  of  the  ground state  energy,  energy gradient, electronic wave function and properties of periodic systems.  Hartree-Fock or Kohn-Sham Hamiltonians (that adopt an Exchange-Correlation potential following the postulates of Density-Functional Theory) can be used.  Systems periodic in 0 (molecules, 0D), 1 (polymers,1D), 2 (slabs, 2D), and 3 dimensions (crystals, 3D) are treated on an equal footing.  In eachcase the fundamental approximation made is the expansion of the single particle wave functions(\u2019Crystalline Orbital\u2019, CO) as a linear combination of Bloch functions (BF) defined in terms of local functions (hereafter indicated as \u2019Atomic Orbitals\u2019, AOs). </p>"},{"location":"software/computational-chemistry/electronics/crystal/#available-versions-of-crystal-in-ulhpc","title":"Available versions of CRYSTAL in ULHPC","text":"<p>To check available versions of CRYSTAL at UL-HPC type <code>module spider crystal</code>. The following list shows the available versions of CRYSTAL in ULHPC.  <pre><code>chem/CRYSTAL/17-intel-2017a-1.0.1\nchem/CRYSTAL/17-intel-2018a-1.0.1\nchem/CRYSTAL/17-intel-2019a-1.0.2\n</code></pre></p>"},{"location":"software/computational-chemistry/electronics/crystal/#interactive-mode","title":"Interactive mode","text":"<p>To test CRYTAL in the interactive mode, please follow the following steps:</p> <pre><code># From your local computer\n$ ssh -X iris-cluster\n\n# Reserve the node for interactive computation\n$ salloc -p interactive --time=00:30:00 --ntasks 1 -c 4 --x11 # OR si --x11 [...]\n\n# Load the module crytal and needed environment\n$ module purge\n$ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) \n$ module load chem/CRYSTAL/17-intel-2019a-1.0.2\n\n$ Pcrystal &gt;&amp; log.out\n</code></pre> <p>Warning</p> <p>Please note your input file should be named just as <code>INPUT</code>. Pcrytal automatically will recognize the INPUT file from the folder where you are currently in.</p>"},{"location":"software/computational-chemistry/electronics/crystal/#batch-mode","title":"Batch mode","text":"<pre><code>#!/bin/bash -l\n#SBATCH -J CRYSTAL\n#SBATCH -N 2\n#SBATCH -A &lt;project name&gt;\n#SBATCH -M --cluster iris \n#SBATCH --ntasks-per-node=28\n#SBATCH --time=00:30:00\n#SBATCH -p batch\n\n# Load the module crytal and needed environment\n$ module purge\n$ module load swenv/default-env/devel    # Eventually (only relevant on 2019a software environment) \n$ module load chem/CRYSTAL/17-intel-2019a-1.0.2\n\nsrun -n ${SLURM_NTASKS} Pcrystal &gt;&amp; log.out\n</code></pre>"},{"location":"software/computational-chemistry/electronics/crystal/#additional-information","title":"Additional information","text":"<p>To know more information about CRYSTAL tutorial and documentation, please refer to CRYSTAL solutions tutorials.</p> <p>Tip</p> <p>If you find some issues with the instructions above, please report it to us using support ticket.</p>"},{"location":"software/computational-chemistry/electronics/meep/","title":"MEEP","text":"<p> Meep is a free and open-source software package for electromagnetics simulation via the finite-difference time-domain (FDTD) method spanning a broad range of applications.</p>"},{"location":"software/computational-chemistry/electronics/meep/#available-versions-of-meep-in-ulhpc","title":"Available versions of Meep in ULHPC","text":"<p>To check available versions of Meep at ULHPC type <code>module spider meep</code>. The following list shows the available versions of Meep in ULHPC. <pre><code>phys/Meep/1.3-intel-2017a\nphys/Meep/1.4.3-intel-2018a\nphys/Meep/1.4.3-intel-2019a\n</code></pre></p>"},{"location":"software/computational-chemistry/electronics/meep/#interactive-mode","title":"Interactive mode","text":"<p>To try Meep in the interactive mode, please follow the following steps:</p> <pre><code># From your local computer\n$ ssh -X iris-cluster\n\n# Reserve the node for interactive computation\n$ salloc -p interactive --time=00:30:00 --ntasks 1 -c 4 --x11 # OR si --x11 [...]\n\n# Load the module meep and needed environment \n$ module purge\n$ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) \n$ module load toolchain/intel/2019a\n$ module load phys/Meep/1.4.3-intel-2019a\n\n$ meep example.ctl &gt; result_output\n</code></pre>"},{"location":"software/computational-chemistry/electronics/meep/#batch-mode","title":"Batch mode","text":"<pre><code>#!/bin/bash -l\n#SBATCH -J Meep\n#SBATCH -N 2\n#SBATCH -A &lt;project name&gt;\n#SBATCH -M --cluster iris \n#SBATCH --ntasks-per-node=28\n#SBATCH --time=00:30:00\n#SBATCH -p batch\n\n# Load the module meep and needed environment \nmodule purge\nmodule load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) \nmodule load toolchain/intel/2019a\nmodule load phys/Meep/1.4.3-intel-2019a\n\nsrun -n ${SLURM_NTASKS} meep example.ctl &gt; result_output\n</code></pre>"},{"location":"software/computational-chemistry/electronics/meep/#additional-information","title":"Additional information","text":"<p>To know more information about Meep tutorial and documentation, please refer to Meep tutorial.</p> <p>Tip</p> <p>If you find some issues with the instructions above, please report it to us using support ticket.</p>"},{"location":"software/computational-chemistry/electronics/quantum-espresso/","title":"Quantum Espresso","text":"<p> Quantum ESPRESSO is an integrated suite of Open-Source computer codes for electronic-structure calculations and materials modeling at the nanoscale. It is based on density-functional theory, plane waves, and pseudopotentials.</p> <p>Quantum ESPRESSO has evolved into a distribution of independent and inter-operable codes in the spirit of an open-source project. The Quantum ESPRESSO distribution consists of a \u201chistorical\u201d core set of components, and a set of plug-ins that perform more advanced tasks, plus a number of third-party packages designed to be inter-operable with the core components. Researchers active in the field of electronic-structure calculations are encouraged to participate in the project by contributing their own codes or by implementing their own ideas into existing codes.</p>"},{"location":"software/computational-chemistry/electronics/quantum-espresso/#available-versions-of-quantum-espresso-in-ulhpc","title":"Available versions of Quantum ESPRESSO in ULHPC","text":"<p>To check available versions of Quantum ESPRESSO at ULHPC type <code>module spider quantum espresso</code>. The following list shows the available versions of Quantum ESPRESSO in ULHPC.  <pre><code>chem/QuantumESPRESSO/6.1-intel-2017a\nchem/QuantumESPRESSO/6.1-intel-2018a-maxter500\nchem/QuantumESPRESSO/6.1-intel-2018a\nchem/QuantumESPRESSO/6.2.1-intel-2018a\nchem/QuantumESPRESSO/6.4.1-intel-2019a\n</code></pre></p>"},{"location":"software/computational-chemistry/electronics/quantum-espresso/#interactive-mode","title":"Interactive mode","text":"<p>To open an Quantum ESPRESSO in the interactive mode, please follow the following steps:</p> <pre><code># From your local computer\n$ ssh -X iris-cluster\n\n# Reserve the node for interactive computation\n$ salloc -p interactive --time=00:30:00 --ntasks 1 -c 4 --x11  # OR si --x11 [...]\n\n# Load the module quantumespresso and needed environment \n$ module purge\n$ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) \n$ module load chem/QuantumESPRESSO/6.4.1-intel-2019a\n\n$ pw.x -input example.in\n</code></pre>"},{"location":"software/computational-chemistry/electronics/quantum-espresso/#batch-mode","title":"Batch mode","text":"<pre><code>#!/bin/bash -l\n#SBATCH -J QuantumESPRESSO\n#SBATCH -N 2\n#SBATCH -A &lt;project name&gt;\n#SBATCH -M --cluster iris \n#SBATCH --ntasks-per-node=28\n#SBATCH --time=00:30:00\n#SBATCH -p batch\n\n# Load the module quantumespresso and needed environment \nmodule purge\nmodule load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) \nmodule load chem/QuantumESPRESSO/6.4.1-intel-2019a\n\nsrun -n ${SLURM_NTASKS} pw.x -input example.inp\n</code></pre>"},{"location":"software/computational-chemistry/electronics/quantum-espresso/#additional-information","title":"Additional information","text":"<p>To know more information about Quantum ESPRESSO tutorial and documentation, please refer to Quantum ESPRESSO user manual.</p> <p>Tip</p> <p>If you find some issues with the instructions above, please report it to us using support ticket.</p>"},{"location":"software/computational-chemistry/electronics/vasp/","title":"VASP","text":"<p>VASP is a package for performing ab initio quantum-mechanical molecular dynamics (MD) using pseudopotentials and a plane wave basis set. The approach implemented in VASP is based on a finite-temperature local-density approximation (with the free energy as variational quantity) and an exact evaluation of the instantaneous electronic ground state at each MD step using efficient matrix diagonalization schemes and an efficient Pulay mixing.</p>"},{"location":"software/computational-chemistry/electronics/vasp/#accessing-vasp-in-ul-hpc-clusters","title":"Accessing VASP in UL HPC clusters","text":"<p>VASP is a proprietary software. If your group has access to VASP, then the UL HPC team can compile the source code and provide your group with a module that is optimally configured for use in our clusters.</p> <p>All software is installed in the group project directory which is backed up. <pre><code>${PROJECTHOME}/&lt;project name&gt;/backup/easubuild\n</code></pre> Add the following to your <code>bascrc</code> to access the project modules. <pre><code>if command -v module &gt;/dev/null 2&gt;&amp;1 ; then\n    module use \"${PROJECTHOME}/&lt;project name&gt;/backup/easubuild/${ULHPC_CLUSTER}/2023b/${RESIF_ARCH}/modules/all\"\nfi\n</code></pre></p> <p>After refreshing your environment you can check for the available versions of VASP by typing <code>module spider vasp</code>: <pre><code>$ module spider vasp\n...\nphys/VASP/6.4.3-foss-2023b\n...\n</code></pre></p>"},{"location":"software/computational-chemistry/electronics/vasp/#working-with-vasp","title":"Working with VASP","text":"<p>VASP support both interactive session with a graphical GUi and batch jobs submitted through a job scheduler.</p>"},{"location":"software/computational-chemistry/electronics/vasp/#interactive-mode","title":"Interactive mode","text":"<p>To open VASP in the interactive mode, please follow these steps.</p> IrisAion <pre><code># From your local computer\n$ ssh -X iris-cluster\n\n# Reserve the node for interactive computation\n$ salloc -p interactive -A &lt;project name&gt; -t 1:00:00 -n 1 -c 14 --x11\n# OR si -A &lt;project name&gt; -t 1:00:00 -n 1 -c 14 --x11 [...]\n\n# Load the vasp module\n$ module purge\n$ module load phys/VASP/6.4.3-foss-2023b\n\n$ vasp_[std/gam/ncl]\n</code></pre> <pre><code># From your local computer\n$ ssh -X iris-cluster\n\n# Reserve the node for interactive computation\n$ salloc -p interactive -A &lt;project name&gt; -t 1:00:00 -n 1 -c 16 --x11\n# OR si -A &lt;project name&gt; -t 1:00:00 -n 1 -c 16 --x11 [...]\n\n# Load the vasp module\n$ module purge\n$ module load phys/VASP/6.4.3-foss-2023b\n\n$ vasp_[std/gam/ncl]\n</code></pre>"},{"location":"software/computational-chemistry/electronics/vasp/#batch-mode","title":"Batch mode","text":"<p>VASP jobs can also be executed as batch scripts with the job scheduler. Please execute all large jobs in batch scripts.</p> <p>When executing very large VASP jobs you should take advantage of OpenMP to reduce the limitations imposed by cache size and memory bandwidth when using multiple MPI processes. According to the VASP documentation, you should</p> <ul> <li>setup a single process per L3 level cache (<code>OMP_NUM_THREADS</code>),</li> <li>assign cores to threads so that all threads run on cores that share the same L3 cache (<code>OMP_PLACES</code> and <code>OMP_PROC_BIND</code>), and</li> <li>increase the stack size per OpenMP process to at least <code>512 Mbytes</code> to avoid segmentation faults, as VASP is called a significant number of functions per OpenMP process (<code>OMP_STACKSIZE</code>).</li> </ul> AionIris <p>The processors of Aion are composed by Core Complexes (CCX), with each CCX having 4 core and an L3 cache where all cores have uniform access. Thus,</p> <ul> <li>assign a process per CCX with 4 threads per process by setting the number of <code>OMP_NUM_THREADS=4</code>, and</li> <li>bind all threads to the cores where they where assigned with <code>OMP_PLACES=cores</code> and <code>OMP_PROC_BIND=close</code>.</li> </ul> <pre><code>#!/usr/bin/bash --login\n\n#SBATCH --account=&lt;project name&gt;\n#SBATCH --job-name=VASP-casename\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=32\n#SBATCH --cpus-per-task=4\n#SBATCH --time=0-08:00:00\n#SBATCH --partition=batch\n#SBATCH --qos=normal\n\n# Set output and error files\n#SBATCH --error=%x-%j.err\n#SBATCH --output=%x-%j.out\n\nprint_error_and_exit() {\n  echo \"***ERROR*** $*\"\n  exit 1\n}\n\n# Load the vasp module\nmodule purge || print_error_and_exit \"No 'module' command found\"\nmodule load module load phys/VASP/6.4.3-foss-2023b\n\nexport OMP_NUM_THREADS=4\n\nexport OMP_PLACES=cores\nexport OMP_PROC_BIND=close\n\nexport OMP_STACKSIZE=512m\n\nsrun vasp_[std/gam/ncl]\n</code></pre> <p>The L3 cache structure of Iris is considerably simpler, with each socket having a single L3 cache where all cores have uniform access. Thus,</p> <ul> <li>assign a process per socket with 14 threads per process by setting the number of <code>OMP_NUM_THREADS=14</code>, and</li> <li>bind all threads to the cores where they where assigned with <code>OMP_PLACES=cores</code> and <code>OMP_PROC_BIND=close</code>.</li> </ul> <pre><code>#!/usr/bin/bash --login\n\n#SBATCH --account=&lt;project name&gt;\n#SBATCH --job-name=VASP-casename\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=2\n#SBATCH --cpus-per-task=14\n#SBATCH --time=0-08:00:00\n#SBATCH --partition=batch\n#SBATCH --qos=normal\n\n# Set output and error files\n#SBATCH --error=%x-%j.err\n#SBATCH --output=%x-%j.out\n\nprint_error_and_exit() {\n  echo \"***ERROR*** $*\"\n  exit 1\n}\n\n# Load the vasp module\nmodule purge || print_error_and_exit \"No 'module' command found\"\nmodule load module load phys/VASP/6.4.3-foss-2023b\n\nexport OMP_NUM_THREADS=14\n\nexport OMP_PLACES=cores\nexport OMP_PROC_BIND=close\n\nexport OMP_STACKSIZE=512m\n\nsrun vasp_[std/gam/ncl]\n</code></pre> <p>Sources</p> <ol> <li>The VASP Manual: Combining MPI and OpenMP</li> </ol>"},{"location":"software/computational-chemistry/electronics/vasp/#additional-information","title":"Additional information","text":"<p>To know more information about VASP tutorial and documentation, please refer to VASP manual.</p> <p>Tip</p> <p>If you find some issues with the instructions above, please report it to us using support ticket.</p>"},{"location":"software/computational-chemistry/molecular-dynamics/cp2k/","title":"CP2K","text":"<p> CP2K is a quantum chemistry and solid state physics software package that can perform atomistic simulations of solid state, liquid, molecular, periodic, material, crystal, and biological systems. CP2K provides a general framework for different modeling methods such as DFT using the mixed Gaussian and plane waves approaches GPW and GAPW. Supported theory levels include DFTB, LDA, GGA, MP2, RPA, semi-empirical methods (AM1, PM3, PM6, RM1, MNDO, \u2026), and classical force fields (AMBER, CHARMM, \u2026). CP2K can do simulations of molecular dynamics, metadynamics, Monte Carlo, Ehrenfest dynamics, vibrational analysis, core level spectroscopy, energy minimization, and transition state optimization using NEB or dimer method. CP2K is written in Fortran 2008 and can be run efficiently in parallel using a combination of multi-threading, MPI, and CUDA. It is freely available under the GPL license. It is therefore easy to give the code a try, and to make modifications as needed.</p>"},{"location":"software/computational-chemistry/molecular-dynamics/cp2k/#available-versions-of-cp2k-in-ulhpc","title":"Available versions of CP2K in ULHPC","text":"<p>To check available versions of CP2K at ULHPC type <code>module spider cp2k</code>. The following list shows the available versions of CP2K in ULHPC.  <pre><code>chem/CP2K/6.1-foss-2019a\nchem/CP2K/6.1-intel-2018a\n</code></pre></p>"},{"location":"software/computational-chemistry/molecular-dynamics/cp2k/#interactive-mode","title":"Interactive mode","text":"<p>To open CP2K in the interactive mode, please follow the following steps:</p> <pre><code># From your local computer\n$ ssh -X iris-cluster\n\n# Reserve the node for interactive computation\n$ salloc -p interactive --time=00:30:00 --ntasks 1 -c 4 --x11 # OR si --x11 [...]\n\n# Load the module cp2k and needed environment \n$ module purge\n$ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) \n$ module load chem/CP2K/6.1-intel-2018a\n\n$ cp2k.popt -i example.inp \n</code></pre>"},{"location":"software/computational-chemistry/molecular-dynamics/cp2k/#batch-mode","title":"Batch mode","text":"<pre><code>#!/bin/bash -l\n#SBATCH -J CP2K\n#SBATCH -N 2\n#SBATCH -A &lt;project name&gt;\n#SBATCH -M --cluster iris \n#SBATCH --ntasks-per-node=28\n#SBATCH --time=00:30:00\n#SBATCH -p batch\n\n# Load the module cp2k and needed environment \nmodule purge\nmodule load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) \nmodule load chem/CP2K/6.1-intel-2018a \n\nsrun -n ${SLURM_NTASKS} cp2k.popt -i example.inp &gt; outputfile.out\n</code></pre>"},{"location":"software/computational-chemistry/molecular-dynamics/cp2k/#additional-information","title":"Additional information","text":"<p>To know more information about CP2K tutorial and documentation, please refer to CP2K HOWTOs.</p> <p>Tip</p> <p>If you find some issues with the instructions above, please report it to us using support ticket.</p>"},{"location":"software/computational-chemistry/molecular-dynamics/gromacs/","title":"GROMACS","text":"<p> GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. It is primarily designed for biochemical molecules like proteins, lipids and nucleic acids that have a lot of complicated bonded interactions, but since GROMACS is extremely fast at calculating the nonbonded interactions (that usually dominate simulations) many groups are also using it for research on non-biological systems, e.g. polymers.</p>"},{"location":"software/computational-chemistry/molecular-dynamics/gromacs/#available-versions-of-gromacs-in-ulhpc","title":"Available versions of GROMACS in ULHPC","text":"<p>To check available versions of GROMACS at ULHPC type <code>module spider gromacs</code>. The following list shows the available versions  of GROMACS in ULHPC.  <pre><code>bio/GROMACS/2016.3-intel-2017a-hybrid\nbio/GROMACS/2016.5-intel-2018a-hybrid\nbio/GROMACS/2019.2-foss-2019a\nbio/GROMACS/2019.2-fosscuda-2019a\nbio/GROMACS/2019.2-intel-2019a\nbio/GROMACS/2019.2-intelcuda-2019a\n</code></pre></p>"},{"location":"software/computational-chemistry/molecular-dynamics/gromacs/#interactive-mode","title":"Interactive mode","text":"<p>To try GROMACS in the interactive mode, please follow the following steps:</p> <pre><code># From your local computer\n$ ssh -X iris-cluster\n\n# Reserve the node for interactive computation\n$ salloc -p interactive --time=00:30:00 --ntasks 1 -c 4 --x11 # OR si --x11 [...]\n\n# Load the module gromacs and needed environment \n$ module purge\n$ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) \n$ module load bio/GROMACS/2019.2-intel-2019a\n\n$ gmx_mpi mdrun &lt;all your GMX job specification options in here&gt;\n</code></pre>"},{"location":"software/computational-chemistry/molecular-dynamics/gromacs/#batch-mode","title":"Batch mode","text":"<pre><code>#!/bin/bash -l\n#SBATCH -J GROMAC\n#SBATCH -N 2\n#SBATCH -A &lt;project name&gt;\n#SBATCH -M --cluster iris \n#SBATCH --ntasks-per-node=28\n#SBATCH --time=00:30:00\n#SBATCH -p batch\n\n# Load the module gromacs and needed environment \nmodule purge\nmodule load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) \nmodule load bio/GROMACS/2019.2-intel-2019a\n\nsrun -n ${SLURM_NTASKS} gmx_mpi mdrun &lt;all your GMX job specification options in here&gt;\n</code></pre>"},{"location":"software/computational-chemistry/molecular-dynamics/gromacs/#additional-information","title":"Additional information","text":"<p>To know more information about GROMACS tutorial and documentation, please refer to GROMACS documentation.</p> <p>Tip</p> <p>If you find some issues with the instructions above, please report it to us using support ticket.</p>"},{"location":"software/computational-chemistry/molecular-dynamics/helping-libraries/","title":"Helping Libraries","text":""},{"location":"software/computational-chemistry/molecular-dynamics/helping-libraries/#libctl","title":"libctl","text":"<p>This is libctl, a Guile-based library for supporting flexible control files in scientific simulations. For more information about libctl, please refer to libctl Documentation.</p>"},{"location":"software/computational-chemistry/molecular-dynamics/helping-libraries/#available-versions-of-libctl-in-ulhpc","title":"Available versions of libctl in ULHPC:","text":"<pre><code>chem/libctl/3.2.2-intel-2017a\nchem/libctl/4.0.0-intel-2018a\nchem/libctl/4.0.0-intel-2019a\n</code></pre>"},{"location":"software/computational-chemistry/molecular-dynamics/helping-libraries/#libint","title":"Libint","text":"<p>Libint library is used to evaluate the traditional (electron repulsion) and certain novel two-body matrix elements (integrals) over Cartesian Gaussian functions used in modern atomic and molecular theory.</p>"},{"location":"software/computational-chemistry/molecular-dynamics/helping-libraries/#available-versions-of-libint-in-ulhpc","title":"Available versions of Libint in ULHPC:","text":"<pre><code>chem/Libint/1.1.6-GCC-8.2.0-2.31.1\nchem/Libint/1.2.1-intel-2018a\n</code></pre>"},{"location":"software/computational-chemistry/molecular-dynamics/helping-libraries/#libxc","title":"Libxc","text":"<p>Libxc is a library of exchange-correlation functionals for density-functional theory. The aim is to provide a portable, well tested and reliable set of exchange and correlation functionals that can be used by all the ETSF codes and also other codes.</p>"},{"location":"software/computational-chemistry/molecular-dynamics/helping-libraries/#available-versions-of-libxc-in-ulhpc","title":"Available versions of Libxc in ULHPC:","text":"<pre><code>chem/libxc/3.0.0-intel-2017a\nchem/libxc/3.0.1-intel-2018a\nchem/libxc/4.2.3-intel-2019a\nchem/libxc/4.3.4-GCC-8.2.0-2.31.1\nchem/libxc/4.3.4-iccifort-2019.1.144-GCC-8.2.0-2.31.1\n</code></pre>"},{"location":"software/computational-chemistry/molecular-dynamics/helping-libraries/#plumed","title":"PLUMED","text":"<p> PLUMED works together with some of the most popular MD engines, such as ACEMD, Amber, DL_POLY, GROMACS, LAMMPS, NAMD, OpenMM, DFTB+, ABIN, CP2K, i-PI, PINY-MD, and Quantum Espresso. In addition, PLUMED can be used to augment the capabilities of analysis tools such as VMD, HTMD, OpenPathSampling, and as a standalone utility to analyze pre-calculated MD trajectories.</p> <p>PLUMED can be interfaced with the host code using a single well-documented API that enables the PLUMED functionalities to be imported. The API is accessible from multiple languages (C, C++, FORTRAN, and Python), and is thus compatible with the majority of the codes used in the community. The PLUMED license (L-GPL) also allows it to be interfaced with proprietary software.</p>"},{"location":"software/computational-chemistry/molecular-dynamics/helping-libraries/#available-versions-of-plumed-in-ulhpc","title":"Available versions of PLUMED in ULHPC:","text":"<p><pre><code>chem/PLUMED/2.4.2-intel-2018a\nchem/PLUMED/2.5.1-foss-2019a\nchem/PLUMED/2.5.1-intel-2019a\n</code></pre> To know more information about PLUMMED tutorial and documentation, please refer to PLUMMED Cambridge tutorial.</p>"},{"location":"software/computational-chemistry/molecular-dynamics/helping-libraries/#espresso","title":"ESPResSo","text":"<p> ESPResSo is a highly versatile software package for performing and analyzing scientific Molecular Dynamics many-particle simulations of coarse-grained atomistic or bead-spring models as they are used in soft matter research in physics, chemistry and molecular biology. It can be used to simulate systems such as polymers, liquid crystals, colloids, polyelectrolytes, ferrofluids and biological systems, for example DNA and lipid membranes. It also has a DPD and lattice Boltzmann solver for hydrodynamic interactions, and allows several particle couplings to the LB fluid.</p> <p>ESPResSo is free, open-source software published under the GNU General Public License (GPL3). It is parallelized and can be employed on desktop machines, convenience clusters as well as on supercomputers with hundreds of CPUs, and some modules have also support for GPU acceleration. The parallel code is controlled via the scripting language Python, which gives the software its great flexibility.</p>"},{"location":"software/computational-chemistry/molecular-dynamics/helping-libraries/#available-versions-of-espresso-in-ulhpc","title":"Available versions of ESPResSo in ULHPC:","text":"<p><pre><code>phys/ESPResSo/3.3.1-intel-2017a-parallel\nphys/ESPResSo/3.3.1-intel-2018a-parallel\nphys/ESPResSo/4.0.2-intel-2019a\nphys/ESPResSo/4.0.2-intelcuda-2019a\n</code></pre> To know more information about ESPResSo tutorial and documentation, please refer to ESPRessSo documentation.</p>"},{"location":"software/computational-chemistry/molecular-dynamics/helping-libraries/#udunits","title":"UDUNITS","text":"<p> The UDUNITS package supports units of physical quantities. Its C library provides for arithmetic manipulation of units and for conversion of numeric values between compatible units. The package contains an extensive unit database, which is in XML format and user-extendable. The package also contains a command-line utility for investigating units and converting values.</p>"},{"location":"software/computational-chemistry/molecular-dynamics/helping-libraries/#available-version-of-udunits-in-ulhpc","title":"Available version of UDUNITS in ULHPC:","text":"<p><pre><code>phys/UDUNITS/2.2.26-GCCcore-8.2.0\n</code></pre> To know more information about UDUNITS tutorial and documentation, please refer to UDUNITS 2.2.26 Manual.</p> <p>Tip</p> <p>If you find some issues with the instructions above, please report it to us using support ticket.</p>"},{"location":"software/computational-chemistry/molecular-dynamics/namd/","title":"NAMD","text":"<p> NAMD, recipient of a 2002 Gordon Bell Award and a 2012 Sidney Fernbach Award, is a parallel molecular dynamics code designed for high-performance simulation of large biomolecular systems. Based on Charm++ parallel objects, NAMD scales to hundreds of cores for typical simulations and beyond 500,000 cores for the largest simulations. NAMD uses the popular molecular graphics program VMD for simulation setup and trajectory analysis, but is also file-compatible with AMBER, CHARMM, and X-PLOR. NAMD is distributed free of charge with source code. You can build NAMD yourself or download binaries for a wide variety of platforms. Our tutorials show you how to use NAMD and VMD for biomolecular modeling. </p>"},{"location":"software/computational-chemistry/molecular-dynamics/namd/#available-versions-of-namd-in-ulhpc","title":"Available versions of NAMD in ULHPC","text":"<p>To check available versions of NAMD at ULHPC type <code>module spider namd</code>. The following list shows the available versions of NAMD in ULHPC. <pre><code>chem/NAMD/2.12-intel-2017a-mpi\nchem/NAMD/2.12-intel-2018a-mpi\nchem/NAMD/2.13-foss-2019a-mpi\n</code></pre></p>"},{"location":"software/computational-chemistry/molecular-dynamics/namd/#interactive-mode","title":"Interactive mode","text":"<p>To open NAMD in the interactive mode, please follow the following steps:</p> <pre><code># From your local computer\n$ ssh -X iris-cluster\n\n# Reserve the node for interactive computation\n$ salloc -p interactive --time=00:30:00 --ntasks 1 -c 4 --x11 # OR si --x11 [...]\n\n# Load the module namd and needed environment \n$ module purge\n$ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) \n$ module load chem/NAMD/2.12-intel-2018a-mpi\n\n$ namd2 +setcpuaffinity +p4 config_file &gt; output_file\n</code></pre>"},{"location":"software/computational-chemistry/molecular-dynamics/namd/#batch-mode","title":"Batch mode","text":"<pre><code>#!/bin/bash -l\n#SBATCH -J NAMD\n#SBATCH -N 2\n#SBATCH -A &lt;project name&gt;\n#SBATCH -M --cluster iris \n#SBATCH --ntasks-per-node=28\n#SBATCH --time=00:30:00\n#SBATCH -p batch\n\n# Load the module namd and needed environment \nmodule purge\nmodule load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) \nmodule load chem/NAMD/2.12-intel-2018a-mpi\n\nsrun -n ${SLURM_NTASKS} namd2 +setcpuaffinity +p56 config_file.namd &gt; output_file\n</code></pre>"},{"location":"software/computational-chemistry/molecular-dynamics/namd/#additional-information","title":"Additional information","text":"<p>To know more information about NAMD tutorial and documentation, please refer to NAMD User's Guide.</p> <p>Tip</p> <p>If you find some issues with the instructions above, please report it to us using support ticket.</p>"},{"location":"software/computational-chemistry/molecular-dynamics/nwchem/","title":"NWCHEM","text":"<p> NWChem aims to provide its users with computational chemistry tools that are scalable both in their ability to efficiently treat large scientific problems, and in their use of available computing resources from high-performance parallel supercomputers to conventional workstation clusters.</p>"},{"location":"software/computational-chemistry/molecular-dynamics/nwchem/#available-versions-of-nwchem-in-ulhpc","title":"Available versions of NWChem in ULHPC","text":"<p>To check available versions of NWChem at ULHPC type <code>module spider nwchem</code>. The following list shows the available versions of NWChem in ULHPC.</p> <pre><code>chem/NWChem/6.6.revision27746-intel-2017a-2015-10-20-patches-20170814-Python-2.7.13\nchem/NWChem/6.8.revision47-intel-2018a-Python-2.7.14\nchem/NWChem/6.8.revision47-intel-2019a-Python-2.7.15\n</code></pre>"},{"location":"software/computational-chemistry/molecular-dynamics/nwchem/#interactive-mode","title":"Interactive mode","text":"<p>To try NWChem in the interactive mode, please follow the following steps:</p> <pre><code># From your local computer\n$ ssh -X iris-cluster\n\n# Reserve the node for interactive computation\n$ salloc -p interactive --time=00:30:00 --ntasks 1 -c 4 --x11 # OR si --x11 [...]\n\n# Load the module nwchem and needed environment \n$ module purge\n$ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) \n$ module load chem/NWChem/6.8.revision47-intel-2019a-Python-2.7.15\n\n$ nwchem example\n</code></pre> <p>naming input file</p> <p>Please note example file should be named with extension like <code>example.nw</code>.</p>"},{"location":"software/computational-chemistry/molecular-dynamics/nwchem/#batch-mode","title":"Batch mode","text":"<pre><code>#!/bin/bash -l\n#SBATCH -J NWChem\n#SBATCH -N 2\n#SBATCH -A &lt;project name&gt;\n#SBATCH -M --cluster iris \n#SBATCH --ntasks-per-node=28\n#SBATCH --time=00:30:00\n#SBATCH -p batch\n\n# Load the module nwchem and needed environment \nmodule purge \nmodule load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) \nmodule load chem/NWChem/6.8.revision47-intel-2019a-Python-2.7.15\n\nsrun -n ${SLURM_NTASKS} nwchem example \n</code></pre>"},{"location":"software/computational-chemistry/molecular-dynamics/nwchem/#additional-information","title":"Additional information","text":"<p>To know more information about NWChem tutorial and documentation, please refer to NWChem User Documentation.</p> <p>Tip</p> <p>If you find some issues with the instructions above, please report it to us using support ticket.</p>"},{"location":"software/maths/julia/","title":"Julia","text":"<p> Scientific computing has traditionally required the highest performance, yet domain experts have largely moved to slower dynamic languages for daily work. We believe there are many good reasons to prefer dynamic languages for these applications, and we do not expect their use to diminish. Fortunately, modern language design and compiler techniques make it possible to mostly eliminate the performance trade-off and provide a single environment productive enough for prototyping and efficient enough for deploying performance-intensive applications. The Julia programming language fills this role: it is a flexible dynamic language, appropriate for scientific and numerical computing, with performance comparable to traditional statically-typed languages.</p>"},{"location":"software/maths/julia/#available-versions-of-julia-in-ulhpc","title":"Available versions of Julia in ULHPC","text":"<p>To check available versions of Julia at ULHPC type <code>module spider julia</code>. The following list shows the available versions of Julia in ULHPC.  <pre><code>lang/Julia/1.1.1\nlang/Julia/1.3.0\n</code></pre></p>"},{"location":"software/maths/julia/#interactive-mode","title":"Interactive mode","text":"<p>To open an MATLAB in the interactive mode, please follow the following steps:</p> <pre><code># From your local computer\n$ ssh -X iris-cluster\n\n# Reserve the node for interactive computation\n$ salloc -p interactive --time=00:30:00 --ntasks 1 -c 4 # OR si [...]\n\n# Load the module Julia and needed environment\n$ module purge\n$ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) \n$ module load lang/Julia/1.3.0\n\n$ julia\n</code></pre>"},{"location":"software/maths/julia/#batch-mode","title":"Batch mode","text":""},{"location":"software/maths/julia/#an-example-for-serial-code","title":"An example for serial code","text":"<pre><code>#!/bin/bash -l\n#SBATCH -J Julia\n###SBATCH -A &lt;project name&gt;\n#SBATCH --ntasks-per-node 1\n#SBATCH -c 1\n#SBATCH --time=00:15:00\n#SBATCH -p batch\n\n# Load the module Julia and needed environment\nmodule purge\nmodule load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) \nmodule load lang/Julia/1.3.0\n\njulia {example}.jl\n</code></pre>"},{"location":"software/maths/julia/#an-example-for-parallel-code","title":"An example for parallel code","text":"<pre><code>#!/bin/bash -l\n#SBATCH -J Julia\n###SBATCH -A &lt;project name&gt;\n#SBATCH -N 1\n#SBATCH --ntasks-per-node 28\n#SBATCH --time=00:10:00\n#SBATCH -p batch\n\n# Load the module Julia and needed environment\nmodule purge\nmodule load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) \nmodule load lang/Julia/1.3.0\n\nsrun -n ${SLURM_NTASKS} julia {example}.jl\n</code></pre> <p>Example</p> <pre><code>using Distributed\n\n# launch worker processes\nnum_cores = parse(Int, ENV[\"SLURM_CPUS_PER_TASK\"])\naddprocs(num_cores)\n\nprintln(\"Number of cores: \", nprocs())\nprintln(\"Number of workers: \", nworkers())\n\n# each worker gets its id, process id and hostname\nfor i in workers()\nid, pid, host = fetch(@spawnat i (myid(), getpid(), gethostname()))\nprintln(id, \" \" , pid, \" \", host)\nend\n\n# remove the workers\nfor i in workers()\nrmprocs(i)\nend\n</code></pre>"},{"location":"software/maths/julia/#additional-information","title":"Additional information","text":"<p>To know more information about Julia tutorial and documentation, please refer to Julia tutorial.</p> <p>Tip</p> <p>If you find some issues with the instructions above, please file a support ticket.</p>"},{"location":"software/maths/mathematica/","title":"MATHEMATICA","text":"<p> For three decades, MATHEMATICA has defined the state of the art in technical computing-and provided the principal computation environment for millions of innovators, educators, students, and others around the world. Widely admired for both its technical prowess and elegant ease of use, Mathematica provides a single integrated, continually expanding system that covers the breadth and depth of technical computing-and seamlessly available in the cloud through any web browser, as well as natively on all modern desktop systems.</p>"},{"location":"software/maths/mathematica/#available-versions-of-mathematica-in-ulhpc","title":"Available versions of MATHEMATICA in ULHPC","text":"<p>To check available versions of MATHEMATICA at ULHPC type <code>module spider mathematica</code>. The following list shows the available versions of MATHEMATICA in ULHPC.  <pre><code>math/Mathematica/11.0.0\nmath/Mathematica/11.3.0\nmath/Mathematica/12.0.0\n</code></pre></p>"},{"location":"software/maths/mathematica/#interactive-mode","title":"Interactive mode","text":"<p>To open an MATHEMATICA in the interactive mode, please follow the following steps:</p> <pre><code># From your local computer\n$ ssh -X iris-cluster\n\n# Reserve the node for interactive computation\n$ salloc -p interactive --time=00:30:00 --ntasks 1 -c 4 # OR si [...]\n\n# Load the module MATHEMATICA and needed environment\n$ module purge\n$ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) \n$ module load math/Mathematica/12.0.0\n\n$ math\n</code></pre>"},{"location":"software/maths/mathematica/#batch-mode","title":"Batch mode","text":""},{"location":"software/maths/mathematica/#an-example-for-serial-case","title":"An example for serial case","text":"<pre><code>#!/bin/bash -l\n#SBATCH -J MATHEMATICA\n#SBATCH --ntasks-per-node 1\n#SBATCH -c 1\n#SBATCH --time=00:15:00\n#SBATCH -p batch\n### SBATCH -A &lt;project_name&gt;\n\n# Load the module MATHEMATICA and needed environment\n$ module purge\n$ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) \n$ module load math/Mathematica/12.0.0\n\n$ srun -n ${SLURM_NTASKS} math -run &lt; {mathematica-script-file}.m\n</code></pre>"},{"location":"software/maths/mathematica/#an-example-for-parallel-case","title":"An example for parallel case","text":"<pre><code>#!/bin/bash -l\n#SBATCH -J MATHEMATICA\n#SBATCH -N 1\n#SBATCH -c 28\n#SBATCH --time=00:10:00\n#SBATCH -p batch\n### SBATCH -A &lt;project_name&gt;\n\n# Load the module MATHEMATICA and needed environment\n$ module purge\n$ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) \n$ module load math/Mathematica/12.0.0\n\n$ srun -n ${SLURM_NTASKS} math -run &lt; {mathematica-script-file}.m\n</code></pre> <p>Exmaple</p> <pre><code># example for MATHEMATICA prallel (mathematica_script_file.m)\n//Limits Mathematica to requested resources\nUnprotect[$ProcessorCount];$ProcessorCount = 28;\n\n//Prints the machine name that each kernel is running on\nPrint[ParallelEvaluate[$MachineName]];\n\n//Prints all Prime numbers less than 3000\nPrint[Parallelize[Select[Range[3000],PrimeQ[2^#-1]&amp;]]];\n</code></pre>"},{"location":"software/maths/mathematica/#additional-information","title":"Additional information","text":"<p>To know more information about MATHEMATICA tutorial and documentation, please refer to MATHEMATICA tutorial.</p> <p>Tip</p> <p>If you find some issues with the instructions above, please report it to us using support ticket.</p>"},{"location":"software/maths/matlab/","title":"MATLAB","text":"<p> MATLAB\u00ae combines a desktop environment tuned for iterative analysis and design processes with a programming language that expresses matrix and array mathematics directly. It includes the Live Editor for creating scripts that combine code, output, and formatted text in an executable notebook.</p>"},{"location":"software/maths/matlab/#available-versions-of-matlab-in-ulhpc","title":"Available versions of MATLAB in ULHPC","text":"<p>To check available versions of MATLAB at ULHPC type <code>module spider matlab</code>. It will list the available versions: <pre><code>math/MATLAB/&lt;version&gt;\n</code></pre></p>"},{"location":"software/maths/matlab/#interactive-mode","title":"Interactive mode","text":"<p>To open MATLAB in the interactive mode, please follow the following steps:</p> <p>(eventually) connect to the ULHPC login node with the <code>-X</code> (or <code>-Y</code>) option:</p> IrisAion <pre><code>ssh -X iris-cluster   # OR on Mac OS: ssh -Y iris-cluster\n</code></pre> <pre><code>ssh -X aion-cluster   # OR on Mac OS: ssh -Y aion-cluster\n</code></pre> <p>Then you can reserve an interactive job, for instance with 4 cores. Don't forget to use the <code>--x11</code> option if you intend to use the GUI.</p> <pre><code>$ si --x11 -c4\n\n# Load the module MATLAB and needed environment\n(node)$ module purge\n(node)$ module load math/MATLAB\n\n# Non-Graphical version (CLI)\n(node)$ matlab -nodisplay -nosplash\n                  &lt; M A T L A B (R) &gt;\n        Copyright 1984-2021 The MathWorks, Inc.\n        R2021a (9.10.0.1602886) 64-bit (glnxa64)\n                   February 17, 2021\nTo get started, type doc.\nFor product information, visit www.mathworks.com.\n&gt;&gt; version()\nans =\n    '9.10.0.1602886 (R2021a)'\n# List of installed add-ons\n&gt;&gt;  matlab.addons.installedAddons\nans =\n  96x4 table\n            Name           Version     Enabled    Identifier\n    ___________________    ________    _______    __________\n\n    \"Mapping Toolbox\"      \"5.1\"        true         \"MG\"\n    \"Simulink Test\"        \"3.4\"        true         \"SZ\"\n    [...]\n&gt;&gt; quit()\n\n# To run the GUI version, over X11\n(node)$ matlab &amp;\n</code></pre>"},{"location":"software/maths/matlab/#batch-mode","title":"Batch mode","text":"<p>For non-interactive or long executions, MATLAB can be ran in passive or batch mode, reading all commands from an input file (with <code>.m</code> extension) you provide (Ex: <code>inputfile.m</code>) and saving the results into an output file, for instance <code>outputfile.out</code>). You have two ways to proceed:</p> using redirection operatorsUse command-line options <code>-r</code> and <code>-logfile</code> <pre><code>matlab -nodisplay -nosplash &lt; inputfile.m &gt; outputfile.out\n</code></pre> <pre><code># /!\\ IMPORTANT: notice the **missing** '.m' extension on -r !!!\nmatlab -nodisplay -nosplash -r inputfile -logfile outputfile.out\n</code></pre> <p>The following script uses one full socket in a compute node of Aion.</p> <pre><code>#!/usr/bin/bash --login\n#SBATCH --job-name=MATLAB_job\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=16\n#SBATCH --partition=batch\n#SBATCH --qos=normal\n#SBATCH --time=00:30:00\n\n# Uncomment if you use a non-default account to run your jobs\n##SBATCH --account=&lt;project_name&gt;\n\n# Load the module MATLAB\nmodule purge\nmodule load math/MATLAB\n\n# second form with CLI options '-r &lt;input&gt;' and '-logfile &lt;output&gt;.out'\nsrun matlab -nodisplay -r my_matlab_script -logfile output.out\n\n# example for if you need to have a input parameters for the computations\n# matlab_script_serial_file(x,y,z)\nsrun matlab -nodisplay -r my_matlab_script(2,2,1)' -logfile output.out\n\n# Cleaunp MATLAB autogenerated files from your home directory\nrm -rf ${HOME}/.matlab\nrm -rf ${HOME}/java*\n</code></pre> <p>Most matlab scripts cannot take advantage of more that one core (<code>--cpus-per-task=1</code>). If you want to use more than one core in your computations, use a parallel pool of threads.</p> <p>A parallel pool of threads allows the allocation of threads form the local computing node to independent workers that then use the thread to evaluate functions in parallel. Create a parallel pool on using the parpool function. After you create the pool, parallel pool features, such as <code>parfor</code> or <code>parfeval</code>, run computations on independent workers in parallel. With the ThreadPool object, you can interact with the parallel pool.</p> <p>Example</p> <pre><code># example for MATLAB ParFor (matlab_script_parallel_file.m)\np = parpool('local', str2num(getenv('SLURM_CPUS_PER_TASK'))) % set the default cores\n%as number of threads\ntic\nn = 50;\nA = 50;\na = zeros(1,n);\nparfor i = 1:n\n  a(i) = max(abs(eig(rand(A))));\nend\ntoc\ndelete(gcp); % you have to delete the parallel region after the work is done\nexit;\n</code></pre>"},{"location":"software/maths/matlab/#additional-information","title":"Additional information","text":"<p>To know more information about MATLAB tutorial and documentation, please refer to MATLAB tutorial.</p> <p>Tip</p> <p>If you find some issues with the instructions above, please report it to us using support ticket.</p>"},{"location":"software/maths/stata/","title":"Stata","text":"<p>Stata is a commercial statistical package, which provides a complete solution for data analysis, data management, and graphics.</p> <p>The University of Luxembourg contributes to a campus-wide license -- see SIU / Service Now Knowledge Base ticket on Stata MP2</p>"},{"location":"software/maths/stata/#available-versions-of-stata-on-ulhpc-platforms","title":"Available versions of Stata on ULHPC platforms","text":"<p>To check available versions of Stata at ULHPC, type <code>module spider stata</code>.</p> <pre><code>math/Stata/&lt;version&gt;\n</code></pre> <p>Once loaded, the modules brings to you the following binaries:</p> Binary Description <code>stata</code> Non-graphical standard Stata/IC. For better performance and support for larger databases, <code>stata-se</code> should be used. <code>stata-se</code> Non-graphical Stata/SE designed for large databases. Can be used to run tasks automatically with the batch flag <code>-b</code> and a Stata '<code>*.do</code> file <code>xstata</code> Graphical standard Stata/IC. For better performance and support for larger databases, xstata-se should be used. <code>xstata-se</code> Graphical Stata/SE designed for large databases. Can be used interactively in a similar working environment to Windows and Mac versions."},{"location":"software/maths/stata/#interactive-mode","title":"Interactive Mode","text":"<p>To open a Stata session in interactive mode, please follow the following steps:</p> <p>(eventually) connect to the ULHPC login node with the <code>-X</code> (or <code>-Y</code>) option:</p> IrisAion <pre><code>ssh -X iris-cluster   # OR on Mac OS: ssh -Y iris-cluster\n</code></pre> <pre><code>ssh -X aion-cluster   # OR on Mac OS: ssh -Y aion-cluster\n</code></pre> <p>Then you can reserve an interactive job, for instance with 2 cores. Don't forget to use the <code>--x11</code> option if you intend to use the GUI.</p> <pre><code>$ si --x11 -c2      # You CANNOT use more than 2 cores\n\n# Load the module Stata and needed environment\n(node)$ module purge\n(node)$ module load math/Stata\n\n# Non-Graphical version (CLI)\n(node)$ stata\n  ___  ____  ____  ____  ____ \u00ae\n /__    /   ____/   /   ____/      17.0\n___/   /   /___/   /   /___/       BE\u2014Basic Edition\n\n Statistics and Data Science       Copyright 1985-2021 StataCorp LLC\n                                   StataCorp\n                                   4905 Lakeway Drive\n                                   College Station, Texas 77845 USA\n                                   800-STATA-PC        https://www.stata.com\n                                   979-696-4600        stata@stata.com\n\nStata license: Unlimited-user network, expiring 31 Dec 2022\nSerial number: &lt;serial&gt;\n  Licensed to: University of Luxembourg\n               Campus License - see KB0010885 (Service Now)\n\n.\n# To quit Stata\n. exit, clear\n\n# To run the GUI version, over X11\n(node)$ stata &amp;\n</code></pre>"},{"location":"software/maths/stata/#location-of-your-ado-files","title":"Location of your ado files","text":"<p>Run the <code>sysdir</code> command to see the search path for ado files:</p> <pre><code>. sysdir\n   STATA:  /opt/apps/resif/&lt;cluster&gt;/&lt;version&gt;/&lt;arch&gt;/software/Stata/&lt;stataversion&gt;/\n    BASE:  /opt/apps/resif/&lt;cluster&gt;/&lt;version&gt;/&lt;arch&gt;/software/Stata/&lt;stataversion&gt;/ado/base/\n    SITE:  /opt/apps/resif/&lt;cluster&gt;/&lt;version&gt;/&lt;arch&gt;/software/Stata/&lt;stataversion&gt;/software/Stata/ado/\n    PLUS:  ~/ado/plus/\nPERSONAL:  ~/ado/personal/\n</code></pre> <p>You should thus store ado files in `$HOME/ado/personal. For more see this document.</p>"},{"location":"software/maths/stata/#batch-mode","title":"Batch mode","text":"<p>To run Stata in batch mode, you need to create do-files which contain the series of commands you would like to run. With a do file (<code>filename.do</code>) in hand, you can run it from the shell in the command line with:</p> <pre><code>stata -b do filename.do\n</code></pre> <p>With the <code>-b</code> flag, outputs will be automatically saved to the outputfile <code>filename.log</code>.</p> Serial StataParallel Stata (Stata/MP) <pre><code>#!/bin/bash -l\n#SBATCH -J Stata\n###SBATCH -A &lt;project_name&gt;\n#SBATCH --ntasks-per-node 1\n#SBATCH -c 1\n#SBATCH --time=00:30:00\n#SBATCH -p batch\n\n# Load the module Stata\nmodule purge\nmodule load math/Stata\n\nsrun stata -b do INPUTFILE.do\n</code></pre> <pre><code>#!/bin/bash -l\n#SBATCH -J Stata\n###SBATCH -A &lt;project_name&gt;\n#SBATCH --ntasks-per-node 1\n#SBATCH -c 2\n#SBATCH --time=00:30:00\n#SBATCH -p batch\n\n# Load the module Stata\nmodule purge\nmodule load math/Stata\n\n# Use stata-mp to run across multiple cores\nsrun -c $SLURM_CPUS_PER_TASK stata-mp -b do INPUTFILE.do\n</code></pre>"},{"location":"software/maths/stata/#running-stata-in-parallel","title":"Running Stata in Parallel","text":""},{"location":"software/maths/stata/#statamp","title":"Stata/MP","text":"<p>You can use Stata/MP to advantage of the advanced multiprocessing capabilities of Stata/MP. Stata/MP provides the most extensive multicore support of any statistics and data management package.</p> <p>Note however that the current license limits the maximum number of cores (to 2 !). Example of interactive usage:</p> <pre><code>$ si --x11 -c2      # You CANNOT use more than 2 cores\n\n# Load the module Stata and needed environment\n(node)$ module purge\n(node)$ module load math/Stata\n\n# Non-Graphical version (CLI)\n(node)$ stata-mp\n\n  ___  ____  ____  ____  ____ \u00ae\n /__    /   ____/   /   ____/      17.0\n___/   /   /___/   /   /___/       MP\u2014Parallel Edition\n\n Statistics and Data Science       Copyright 1985-2021 StataCorp LLC\n                                   StataCorp\n                                   4905 Lakeway Drive\n                                   College Station, Texas 77845 USA\n                                   800-STATA-PC        https://www.stata.com\n                                   979-696-4600        stata@stata.com\n\nStata license: Unlimited-user 2-core network, expiring 31 Dec 2022\nSerial number: &lt;serial&gt;\n  Licensed to: University of Luxembourg\n               Campus License - see KB0010885 (Service Now)\n. set processors 2     # or use env SLURM_CPU_PER_TASKS\n. [...]\n. exit, clear\n</code></pre> <p>Note that using the <code>stata-mp</code> executable, Stata will automatically use the requested number of cores from Slurm's <code>--cpus-per-task</code> option. This implicit parallelism does not require any changes to your code.</p>"},{"location":"software/maths/stata/#user-packages-parallel-and-gtools","title":"User-packages parallel and gtools","text":"<p>User-developed Stata packages can be installed from a login node using one of the Stata commands</p> <pre><code>  net install &lt;package&gt;\n</code></pre> <p>These packages will be installed in your home directory by default.</p> <p>Among others, the <code>parallel</code> package implements parallel for loops. Also, the <code>gtools</code> provides faster alternatives to some Stata commands when working with big data.</p> <pre><code>(node)$ stata\n# installation\n. net install parallel, from(https://raw.github.com/gvegayon/parallel/stable/) replace\nchecking parallel consistency and verifying not already installed...\ninstalling into /home/users/svarrette/ado/plus/...\ninstallation complete.\n\n# update index of the installed packages\n. mata mata mlib index\n.mlib libraries to be searched are now\n    lmatabase;lmatasvy;lmatabma;lmatapath;lmatatab;lmatanumlib;lmatacollect;lmatafc;lmatapss;lmat\n&gt; asem;lmatamixlog;lmatamcmc;lmatasp;lmatameta;lmataopt;lmataado;lmatagsem;lmatami;lmatapostest;l\n&gt; matalasso;lmataerm;lparallel\n\n# initial - ADAPT with SLURM_CPU_PER_TASKS\n. parallel initialize 4, f   # Or (better) find a way to use env SLURM_CPU_PER_TASKS\nN Child processes: 4\nStata dir:  /mnt/irisgpfs/apps/resif/iris/2020b/broadwell/software/Stata/17/stata\n\n. sysuse auto\n(1978 automobile data)\n\n. parallel, by(foreign): egen maxp = max(price)\nSmall workload/num groups. Temporarily setting number of child processes to 2\n--------------------------------------------------------------------------------\nParallel Computing with Stata\nChild processes: 2\npll_id         : bcrpvqtoi1\nRunning at     : /mnt/irisgpfs/users/svarrette\nRandtype       : datetime\n\nWaiting for the child processes to finish...\nchild process 0002 has exited without error...\nchild process 0001 has exited without error...\n--------------------------------------------------------------------------------\nEnter -parallel printlog #- to checkout logfiles.\n--------------------------------------------------------------------------------\n\n. tab maxp\n\n       maxp |      Freq.     Percent        Cum.\n------------+-----------------------------------\n      12990 |         22       29.73       29.73\n      15906 |         52       70.27      100.00\n------------+-----------------------------------\n      Total |         74      100.00\n\n. exit, clear\n</code></pre>"},{"location":"software/optim/","title":"Optimizers","text":"<p>Mathematical optimization (alternatively spelled optimisation) or mathematical programming is the selection of a best element (with regard to some criterion) from some set of available alternatives. Optimization problems of sorts arise in all quantitative disciplines from computer science and engineering to operations research and economics, and the development of solution methods has been of interest in mathematics for centuries</p>"},{"location":"software/optim/#mathematical-programming-with-cplex-and-gurobi","title":"Mathematical programming with Cplex and Gurobi","text":"<p>Cplex is an optimization software for mathematical programming. The Cplex optimizer can solve:</p> <ul> <li>Mixed-Integer programming problems (MIP)</li> <li>Very large linear programming problems (LP)</li> <li>Non-convex quadratic programming problems (QP)</li> <li>Convex quadratically constrained problems (QCP)</li> </ul> <p>Gurobi is a powerful optimization software and an alternative to Cplex for solving. Gurobi has some additionnal features compared to Cplex. For example, it can perform Mixed-Integer Quadratic Programming (MIQP) and Mixed-Integer Quadratic Constrained Programming (MIQCP).</p>"},{"location":"software/optim/#loading-cplex-or-gurobi","title":"Loading Cplex or Gurobi","text":"<p>To use these optimization sfotwares, you need to load the corresponding Lmod module.</p> <p>For Cplex</p> <pre><code>&gt;$ module load maths/Cplex\n</code></pre> <p>or for Gurobi</p> <pre><code>&gt;$ module load math/Gurobi\n</code></pre> <p>Warning</p> <p>Modules are not allowed on the access servers. To test interactively Singularity, rememerber to ask for an interactive job first. <pre><code>salloc -p interactive     # OR, use the helper script: si\n</code></pre></p>"},{"location":"software/optim/#using-cplex","title":"Using Cplex","text":"<p>In order to test cplex and gurobi, we need an optimization instance. Hereafter, we are going to rely on instances from the miplib. For example, let us the following instance ex10.mps.gz described in details here for the interested readers.</p>"},{"location":"software/optim/#multi-threaded-optimization-with-cplex","title":"Multi-threaded optimization with Cplex","text":"<p>In order to solve mathematical programs, cplex allows users to define a command line script that can be passed to the executable. On the Iris cluster, the following launcher can be used to perform multi-threaded MIP optimzation. A good practice is to request as many threads as available cores on the node. If you need more computing power, you have to consider a distributed version.  </p> <pre><code>#!/bin/bash -l\n#SBATCH -J Multi-threaded_cplex\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=28\n#SBATCH --time=0-01:00:00\n#SBATCH -p batch\n#SBATCH --qos=normal\n\n# Load cplex \nmodule load math/CPLEX\n\n# Some variable\nMPS_FILE=$1\nRES_FILE=$2\nCPLEX_COMMAND_SCRIPT=\"command_job${SLURM_JOBID}.lst\"\n\n\n\n# Create cplex command script\ncat &lt;&lt; EOF &gt; ${CPLEX_COMMAND_SCRIPT}\nset threads ${SLURM_CPUS_PER_TASK}\nread ${MPS_FILE} \nmipopt\nwrite \"${RES_FILE}.sol\" \nquit\nEOF\nchmod +x ${CPLEX_COMMAND_SCRIPT}\n\n# Cplex will use the required number of thread\ncplex -f ${CPLEX_COMMAND_SCRIPT}\nrm ${CPLEX_COMMAND_SCRIPT}\n</code></pre> <p>Using the script <code>cplex_mtt.slurm</code>, you can launch a batch job with the <code>sbatch</code> command as follows <code>sbatch cplex_mtt.slurm ex10.mps.gz cplex_mtt</code>.</p>"},{"location":"software/optim/#distributed-optimization-with-cplex","title":"Distributed optimization with Cplex","text":"<p>When you require more computing power (e.g. more cores), distributed computations is the way to go. The cplex optimization software embeds a feature that allows you to perform distributed MIP. Using the Message Passing Interface (MPI), cplex will distribute the exploration of the tree search to multiple workers. The below launcher is an example showing how to reserve ressources on multiple nodes through the Slurm scheduler. In this example, 31 tasks will be distributed over 2 nodes. </p> <pre><code>#!/bin/bash -l\n#SBATCH -J Distrbuted\\_cplex\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=14\n#SBATCH -c 2    # multithreading -- #threads (slurm cpu) per task \n#SBATCH --time=0-01:00:00\n#SBATCH -p batch\n#SBATCH --qos=normal\nmodule load math/CPLEX\n\n# Some variables\nMPS_FILE=$1\nRES_FILE=$2\nCPLEX_COMMAND_SCRIPT=\"command_job${SLURM_JOBID}.lst\"\n\n\n\n# Create cplex command script\ncat &lt;&lt; EOF &gt; ${CPLEX_COMMAND_SCRIPT}\nset distmip config mpi\nset threads ${SLURM_CPUS_PER_TASK}\nread ${MPS_FILE} \nmipopt\nwrite \"${RES_FILE}.sol\" \nquit\nEOF\nchmod +x ${CPLEX_COMMAND_SCRIPT}\n\n# Start Cplex with MPI\n# On first host, the master is running \nmpirun -np 1 cplex -f ${CPLEX_COMMAND_SCRIPT} -mpi : -np $((SLURM_NTASKS - 1)) cplex -mpi\nrm ${CPLEX_COMMAND_SCRIPT}\n</code></pre> <p>Using the script <code>cplex_dist.slurm</code>, you can launch a batch job with the <code>sbatch</code> command as follows <code>sbatch cplex_dist.slurm ex10.mps.gz cplex_dist</code>.</p>"},{"location":"software/optim/#gurobi","title":"Gurobi","text":""},{"location":"software/optim/#multi-threaded-optimization-with-gurobi","title":"Multi-threaded optimization with Gurobi","text":"<p>The script below allows you to start multi-threaded MIP optimization with Gurobi. </p> <pre><code>#!/bin/bash -l\n#SBATCH -J Multi-threaded_gurobi\n#SBATCH --ntasks-per-node=1\n#SBATCH -c 28     # multithreading -- #threads (slurm cpu) per task \n#SBATCH --time=0-01:00:00\n#SBATCH -p batch\n#SBATCH --qos=normal\n\n# Load Gurobi \nmodule load math/Gurobi\n\n# Some variable\nMPS_FILE=$1\nRES_FILE=$2\n\n# Gurobi will access use the required number of thread\ngurobi_cl Threads=${SLURM_CPUS_PER_TASK} ResultFile=\"${RES_FILE}.sol\" ${MPS_FILE}\n</code></pre> <p>Using the script <code>gurobi_mtt.slurm</code>, you can launch a batch job with the <code>sbatch</code> command as follows <code>sbatch gurobi_mtt.slurm ex10.mps.gz gurobi_mtt</code>.</p>"},{"location":"software/optim/#distributed-optimization-with-gurobi","title":"Distributed optimization with Gurobi","text":"<pre><code>#!/bin/bash -l\n#SBATCH -J Distrbuted_gurobi\n#SBATCH -N 3       # Number of nodes\n#SBATCH --ntasks-per-node=1\n#SBATCH -c 5   # multithreading -- #threads (slurm cpu) per task \n#SBATCH --time=00:15:00\n#SBATCH -p batch\n#SBATCH --qos normal\n#SBATCH -o %x-%j.log\n\n# Load personal modules\nmu\n# Load gurobi\nmodule load math/Gurobi\n\nexport MASTER_PORT=61000\nexport SLAVE_PORT=61000\nexport MPS_FILE=$1\nexport RES_FILE=$2\nexport GUROBI_INNER_LAUNCHER=\"inner_job${SLURM_JOBID}.sh\"\n\nif [[ -f \"grb_rs.cnf\" ]];then\n    sed -i \"s/^THREADLIMIT.*$/THREADLIMIT=${SLURM_CPUS_PER_TASK}/g\" grb_rs.cnf\nelse\n    $GUROBI_REMOTE_BIN_PATH/grb_rs init\n    echo \"THREADLIMIT=${SLURM_CPUS_PER_TASK}\" &gt;&gt; grb_rs.cnf\nfi\n\n\ncat &lt;&lt; 'EOF' &gt; ${GUROBI_INNER_LAUNCHER}\n#!/bin/bash\nMASTER_NODE=$(scontrol show hostname ${SLURM_NODELIST} | head -n 1)\n    ## Load configuration and environment\n    if [[ ${SLURM_PROCID} -eq 0 ]]; then\n        ## Start Gurobi master worker in background\n         $GUROBI_REMOTE_BIN_PATH/grb_rs --worker --port ${MASTER_PORT} &amp;\n         wait\n    elif [[ ${SLURM_PROCID} -eq 1 ]]; then\n        sleep 5\n        grbcluster nodes --server ${MASTER_NODE}:${MASTER_PORT} \n        gurobi_cl Threads=${SLURM_CPUS_PER_TASK} ResultFile=\"${RES_FILE}.sol\" Workerpool=${MASTER_NODE}:${MASTER_PORT} DistributedMIPJobs=$((SLURM_NNODES -1)) ${MPS_FILE}\n    else\n        sleep 2\n        ## Start Gurobi slave worker in background\n        $GUROBI_REMOTE_BIN_PATH/grb_rs --worker --port ${MASTER_PORT} --join ${MASTER_NODE}:${MASTER_PORT} &amp;\n        wait\nfi\nEOF\nchmod +x ${GUROBI_INNER_LAUNCHER}\n\n## Launch Gurobi and wait for it to start\nsrun ${GUROBI_INNER_LAUNCHER} &amp;\nwhile [[ ! -e \"${RES_FILE}.sol\" ]]; do\n    sleep 5\ndone\nrm ${GUROBI_INNER_LAUNCHER}\n</code></pre> <p>Using the script <code>gurobi_dist.slurm</code>, you can launch a batch job with the <code>sbatch</code> command as follows <code>sbatch gurobi_dist.slurm ex10.mps.gz gurobi_dist</code>.</p>"},{"location":"software/physics/wrf/","title":"Weather Research and Forecasting","text":"<p>Weather Research and Forecasting Model (WRF) is a state-of-the-art atmospheric modeling system designed for both meteorological research and numerical weather prediction. The official source code, models, usage instruction, and most importantly the user license, are found in the official repository.</p>"},{"location":"software/physics/wrf/#the-university-of-manchester-distribution","title":"The University of Manchester distribution","text":"<p>In our systems we provide repackaged containers developed by the Central Research IT Service of the University of Manchester. The repository for <code>wrf-docker</code> provides individual Docker containers for the following packages:</p> <ul> <li>WRF-WPS (<code>wrf-wps</code>): provides the main WRF and WPS applications</li> <li>WRF-Chem (<code>wrf-chem</code>): provides the WRF-Chem application</li> <li>WRF-4DVar (<code>wrf-4dvar</code>): provides the WRFPLUS and WRF-4DVar extensions</li> </ul>"},{"location":"software/physics/wrf/#available-versions-in-the-ul-hpc-systems","title":"Available versions in the UL HPC systems","text":"<p>In the UL HPC system we support Apptainer containers. The University of Manchester containers have been repackaged as Singularity containers for use in our systems. The Singularity containers are:</p> <ul> <li>WRF-WPS version 4.3.3: <code>/work/projects/singularity/ulhpc/wrf-wps-4.3.3.sif</code></li> <li>WRF-Chem version 4.3.3: <code>/work/projects/singularity/ulhpc/wrf-chem-4.3.3.sif</code></li> <li>WRF-4DVar version 4.3.3: <code>/work/projects/singularity/ulhpc/wrf-4dvar-4.3.3.sif</code></li> </ul> <p>There should be one-to-one correspondence when running the Singularity containers in UL HPC systems and when running the Docker containers in a local machine.</p> <p>Tip</p> <p>If you find any issues with the information above, please file a support ticket.</p>"},{"location":"software/swsets/","title":"Supported Software Sets","text":"<p>You can find here the list of the supported software modules that you can use on the ULHPC facility.</p> <ul> <li>Full list of software (in alphabetical order)</li> <li>Software list by ULHPC software set release:<ul> <li><code>2019b</code> (legacy)</li> <li><code>2020b</code> (prod)</li> </ul> </li> <li>Software list by category*:<ul> <li>Biology</li> <li>CFD/Finite element modelling</li> <li>Chemistry</li> <li>Compilers</li> <li>Data processing</li> <li>Debugging</li> <li>Development</li> <li>Weather modelling</li> <li>Programming Languages</li> <li>Libraries</li> <li>Mathematics</li> <li>MPI</li> <li>Numerical libraries</li> <li>Performance measurements</li> <li>Physics</li> <li>System-level software</li> <li>Toolchains (software stacks)</li> <li>Utilities</li> <li>Visualisation</li> </ul> </li> </ul>"},{"location":"software/swsets/2019b/","title":"2019b","text":"<p>Alphabetical list of available ULHPC software belonging to the '2019b' software set. To load a software of this set, use: <pre><code># Eventually: resif-load-swset-[...]\nmodule load &lt;category&gt;/&lt;software&gt;[/&lt;version&gt;]\n</code></pre></p> Software Architectures Clusters Category Description ABAQUS 2018 broadwell, skylake iris CFD/Finite element modelling Finite Element Analysis software for modeling, visualization and best-in-class implicit and explicit dynamics FEA. ACTC 1.1 broadwell, skylake iris Libraries ACTC converts independent triangles into triangle strips or fans. ANSYS 19.4 broadwell, skylake iris Utilities ANSYS simulation software enables organizations to confidently predict how their products will operate in the real world. We believe that every product is a promise of something greater. ANSYS 21.1 broadwell, skylake iris Utilities ANSYS simulation software enables organizations to confidently predict how their products will operate in the real world. We believe that every product is a promise of something greater. ASE 3.19.0 broadwell, skylake iris Chemistry ASE is a python package providing an open source Atomic Simulation Environment in the Python scripting language. From version 3.20.1 we also include the ase-ext package, it contains optional reimplementations in C of functions in ASE.  ASE uses it automatically when installed. ATK 2.34.1 broadwell, skylake iris Visualisation ATK provides the set of accessibility interfaces that are implemented by other toolkits and applications. Using the ATK interfaces, accessibility tools have full access to view and control running applications. Advisor 2019_update5 broadwell, skylake iris Performance measurements Vectorization Optimization and Thread Prototyping - Vectorize &amp; thread code or performance \u201cdies\u201d - Easy workflow + data + tips = faster code faster - Prioritize, Prototype &amp; Predict performance gain Anaconda3 2020.02 broadwell, skylake iris Programming Languages Built to complement the rich, open source Python community, the Anaconda platform provides an enterprise-ready data analytics platform that empowers companies to adopt a modern open data science analytics architecture. ArmForge 20.0.3 broadwell, skylake iris Utilities The industry standard development package for C, C++ and Fortran high performance code on Linux. Forge is designed to handle the complex software projects - including parallel, multiprocess and multithreaded code. Arm Forge combines an industry-leading debugger, Arm DDT, and an out-of-the-box-ready profiler, Arm MAP. ArmReports 20.0.3 broadwell, skylake iris Utilities Arm Performance Reports - a low-overhead tool that produces one-page text and HTML reports summarizing and characterizing both scalar and MPI application performance. Arm Performance Reports runs transparently on optimized production-ready codes by adding a single command to your scripts, and provides the most effective way to characterize and understand the performance of HPC application runs. Armadillo 9.900.1 broadwell, skylake iris Numerical libraries Armadillo is an open-source C++ linear algebra library (matrix maths) aiming towards a good balance between speed and ease of use. Integer, floating point and complex numbers are supported, as well as a subset of trigonometric and statistics functions. Arrow 0.16.0 broadwell, skylake iris Data processing Apache Arrow (incl. PyArrow Python bindings)), a cross-language development platform for in-memory data. Aspera-CLI 3.9.1 broadwell, skylake iris Utilities IBM Aspera Command-Line Interface (the Aspera CLI) is a collection of Aspera tools for performing high-speed, secure data transfers from the command line. The Aspera CLI is for users and organizations who want to automate their transfer workflows. Autoconf 2.69 broadwell, skylake, gpu iris Development Autoconf is an extensible package of M4 macros that produce shell scripts to automatically configure software source code packages. These scripts can adapt the packages to many kinds of UNIX-like systems without manual user intervention. Autoconf creates a configuration script for a package from a template file that lists the operating system features that the package can use, in the form of M4 macro calls. Automake 1.16.1 broadwell, skylake, gpu iris Development Automake: GNU Standards-compliant Makefile generator Autotools 20180311 broadwell, skylake, gpu iris Development This bundle collect the standard GNU build tools: Autoconf, Automake and libtool BEDTools 2.29.2 broadwell, skylake iris Biology BEDTools: a powerful toolset for genome arithmetic. The BEDTools utilities allow one to address common genomics tasks such as finding feature overlaps and computing coverage. The utilities are largely based on four widely-used file formats: BED, GFF/GTF, VCF, and SAM/BAM. BLAST+ 2.9.0 broadwell, skylake iris Biology Basic Local Alignment Search Tool, or BLAST, is an algorithm for comparing primary biological sequence information, such as the amino-acid sequences of different proteins or the nucleotides of DNA sequences. BWA 0.7.17 broadwell, skylake iris Biology Burrows-Wheeler Aligner (BWA) is an efficient program that aligns relatively short nucleotide sequences against a long reference sequence such as the human genome. BamTools 2.5.1 broadwell, skylake iris Biology BamTools provides both a programmer's API and an end-user's toolkit for handling BAM files. Bazel 0.26.1 gpu iris Development Bazel is a build tool that builds code quickly and reliably. It is used to build the majority of Google's software. Bazel 0.29.1 gpu iris Development Bazel is a build tool that builds code quickly and reliably. It is used to build the majority of Google's software. BioPerl 1.7.2 broadwell, skylake iris Biology Bioperl is the product of a community effort to produce Perl code which is useful in biology. Examples include Sequence objects, Alignment objects and database searching objects. Bison 3.3.2 broadwell, skylake, gpu iris Programming Languages Bison is a general-purpose parser generator that converts an annotated context-free grammar into a deterministic LR or generalized LR (GLR) parser employing LALR(1) parser tables. Boost 1.71.0 broadwell, skylake iris Development Boost provides free peer-reviewed portable C++ source libraries. Bowtie2 2.3.5.1 broadwell, skylake iris Biology Bowtie 2 is an ultrafast and memory-efficient tool for aligning sequencing reads to long reference sequences. It is particularly good at aligning reads of about 50 up to 100s or 1,000s of characters, and particularly good at aligning to relatively long (e.g. mammalian) genomes. Bowtie 2 indexes the genome with an FM Index to keep its memory footprint small: for the human genome, its memory footprint is typically around 3.2 GB. Bowtie 2 supports gapped, local, and paired-end alignment modes. CGAL 4.14.1 broadwell, skylake iris Numerical libraries The goal of the CGAL Open Source Project is to provide easy access to efficient and reliable geometric algorithms in the form of a C++ library. CMake 3.15.3 broadwell, skylake, gpu iris Development CMake, the cross-platform, open-source build system.  CMake is a family of tools designed to build, test and package software. CPLEX 12.10 broadwell, skylake iris Mathematics IBM ILOG CPLEX Optimizer's mathematical programming technology enables analytical decision support for improving efficiency, reducing costs, and increasing profitability. CRYSTAL 17 broadwell, skylake iris Chemistry The CRYSTAL package performs ab initio calculations of the ground state energy, energy gradient, electronic wave function and properties of periodic systems. Hartree-Fock or Kohn- Sham Hamiltonians (that adopt an Exchange-Correlation potential following the postulates of Density-Functional Theory) can be used. CUDA 10.1.243 gpu iris System-level software CUDA (formerly Compute Unified Device Architecture) is a parallel computing platform and programming model created by NVIDIA and implemented by the graphics processing units (GPUs) that they produce. CUDA gives developers access to the virtual instruction set and memory of the parallel computational elements in CUDA GPUs. Clang 9.0.1 broadwell, skylake, gpu iris Compilers C, C++, Objective-C compiler, based on LLVM.  Does not include C++ standard library -- use libstdc++ from GCC. CubeGUI 4.4.4 broadwell, skylake iris Performance measurements Cube, which is used as performance report explorer for Scalasca and Score-P, is a generic tool for displaying a multi-dimensional performance space consisting of the dimensions (i) performance metric, (ii) call path, and (iii) system resource. Each dimension can be represented as a tree, where non-leaf nodes of the tree can be collapsed or expanded to achieve the desired level of granularity. This module provides the Cube graphical report explorer. CubeLib 4.4.4 broadwell, skylake iris Performance measurements Cube, which is used as performance report explorer for Scalasca and Score-P, is a generic tool for displaying a multi-dimensional performance space consisting of the dimensions (i) performance metric, (ii) call path, and (iii) system resource. Each dimension can be represented as a tree, where non-leaf nodes of the tree can be collapsed or expanded to achieve the desired level of granularity. This module provides the Cube general purpose C++ library component and command-line tools. CubeWriter 4.4.3 broadwell, skylake iris Performance measurements Cube, which is used as performance report explorer for Scalasca and Score-P, is a generic tool for displaying a multi-dimensional performance space consisting of the dimensions (i) performance metric, (ii) call path, and (iii) system resource. Each dimension can be represented as a tree, where non-leaf nodes of the tree can be collapsed or expanded to achieve the desired level of granularity. This module provides the Cube high-performance C writer library component. DB 18.1.32 broadwell, skylake iris Utilities Berkeley DB enables the development of custom data management solutions, without the overhead traditionally associated with such custom projects. DBus 1.13.12 broadwell, skylake iris Development D-Bus is a message bus system, a simple way for applications to talk to one another.  In addition to interprocess communication, D-Bus helps coordinate process lifecycle; it makes it simple and reliable to code a \"single instance\" application or daemon, and to launch applications and daemons on demand when their services are needed. DMTCP 2.5.2 broadwell, skylake iris Utilities DMTCP is a tool to transparently checkpoint the state of multiple simultaneous applications, including multi-threaded and distributed applications. It operates directly on the user binary executable, without any Linux kernel modules or other kernel modifications. Dakota 6.11.0 broadwell, skylake iris Mathematics The Dakota project delivers both state-of-the-art research and robust, usable software for optimization and UQ. Broadly, the Dakota software's advanced parametric analyses enable design exploration, model calibration, risk analysis, and quantification of margins and uncertainty with computational models.\" Doxygen 1.8.16 broadwell, skylake, gpu iris Development Doxygen is a documentation system for C++, C, Java, Objective-C, Python, IDL (Corba and Microsoft flavors), Fortran, VHDL, PHP, C#, and to some extent D. ELPA 2019.11.001 broadwell iris Mathematics Eigenvalue SoLvers for Petaflop-Applications . EasyBuild 4.3.0 broadwell, skylake, gpu iris Utilities EasyBuild is a software build and installation framework written in Python that allows you to install software in a structured, repeatable and robust way. EasyBuild 4.3.3 broadwell, skylake, gpu iris Utilities EasyBuild is a software build and installation framework written in Python that allows you to install software in a structured, repeatable and robust way. Eigen 3.3.7 broadwell, skylake, gpu iris Mathematics Eigen is a C++ template library for linear algebra: matrices, vectors, numerical solvers, and related algorithms. Elk 6.3.2 broadwell, skylake iris Physics An all-electron full-potential linearised augmented-plane wave (FP-LAPW) code with many advanced features. Written originally at Karl-Franzens-Universit\u00e4t Graz as a milestone of the EXCITING EU Research and Training Network, the code is designed to be as simple as possible so that new developments in the field of density functional theory (DFT) can be added quickly and reliably. FDS 6.7.1 broadwell, skylake iris Physics Fire Dynamics Simulator (FDS) is a large-eddy simulation (LES) code for low-speed flows, with an emphasis on smoke and heat transport from fires. FFTW 3.3.8 broadwell, skylake, gpu iris Numerical libraries FFTW is a C subroutine library for computing the discrete Fourier transform (DFT) in one or more dimensions, of arbitrary input size, and of both real and complex data. FFmpeg 4.2.1 broadwell, skylake, gpu iris Visualisation A complete, cross-platform solution to record, convert and stream audio and video. FLTK 1.3.5 broadwell, skylake iris Visualisation FLTK is a cross-platform C++ GUI toolkit for UNIX/Linux (X11), Microsoft Windows, and MacOS X. FLTK provides modern GUI functionality without the bloat and supports 3D graphics via OpenGL and its built-in GLUT emulation. FastQC 0.11.9 broadwell, skylake iris Biology FastQC is a quality control application for high throughput sequence data. It reads in sequence data in a variety of formats and can either provide an interactive application to review the results of several different QC checks, or create an HTML based report which can be integrated into a pipeline. FriBidi 1.0.5 broadwell, skylake, gpu iris Programming Languages The Free Implementation of the Unicode Bidirectional Algorithm. GCC 8.3.0 broadwell, skylake, gpu iris Compilers The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Java, and Ada, as well as libraries for these languages (libstdc++, libgcj,...). GCCcore 8.3.0 broadwell, skylake, gpu iris Compilers The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Java, and Ada, as well as libraries for these languages (libstdc++, libgcj,...). GDAL 3.0.2 broadwell, skylake, gpu iris Data processing GDAL is a translator library for raster geospatial data formats that is released under an X/MIT style Open Source license by the Open Source Geospatial Foundation. As a library, it presents a single abstract data model to the calling application for all supported formats. It also comes with a variety of useful commandline utilities for data translation and processing. GDB 9.1 broadwell, skylake iris Debugging The GNU Project Debugger GEOS 3.8.0 broadwell, skylake, gpu iris Mathematics GEOS (Geometry Engine - Open Source) is a C++ port of the Java Topology Suite (JTS) GLPK 4.65 broadwell, skylake iris Utilities The GLPK (GNU Linear Programming Kit) package is intended for solving large-scale linear programming (LP), mixed integer programming (MIP), and other related problems. It is a set of routines written in ANSI C and organized in the form of a callable library. GLib 2.62.0 broadwell, skylake, gpu iris Visualisation GLib is one of the base libraries of the GTK+ project GMP 6.1.2 broadwell, skylake, gpu iris Mathematics GMP is a free library for arbitrary precision arithmetic, operating on signed integers, rational numbers, and floating point numbers. GObject-Introspection 1.63.1 broadwell, skylake iris Development GObject introspection is a middleware layer between C libraries (using GObject) and language bindings. The C library can be scanned at compile time and generate a metadata file, in addition to the actual native C library. Then at runtime, language bindings can read this metadata and automatically provide bindings to call into the C library. GPAW-setups 0.9.20000 broadwell, skylake iris Chemistry PAW setup for the GPAW Density Functional Theory package. Users can install setups manually using 'gpaw install-data' or use setups from this package. The versions of GPAW and GPAW-setups can be intermixed. GPAW 20.1.0 broadwell, skylake iris Chemistry GPAW is a density-functional theory (DFT) Python code based on the projector-augmented wave (PAW) method and the atomic simulation environment (ASE). It uses real-space uniform grids and multigrid methods or atom-centered basis-functions. GROMACS 2019.4 broadwell, skylake iris Biology GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. This is a CPU only build, containing both MPI and threadMPI builds for both single and double precision. It also contains the gmxapi extension for the single precision MPI build. GROMACS 2019.6 broadwell, skylake iris Biology GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. This is a CPU only build, containing both MPI and threadMPI builds for both single and double precision. It also contains the gmxapi extension for the single precision MPI build. GROMACS 2020 broadwell, skylake iris Biology GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. This is a CPU only build, containing both MPI and threadMPI builds for both single and double precision. It also contains the gmxapi extension for the single precision MPI build. GSL 2.6 broadwell, skylake, gpu iris Numerical libraries The GNU Scientific Library (GSL) is a numerical library for C and C++ programmers. The library provides a wide range of mathematical routines such as random number generators, special functions and least-squares fitting. GTK+ 3.24.13 broadwell, skylake iris Visualisation GTK+ is the primary library used to construct user interfaces in GNOME. It provides all the user interface controls, or widgets, used in a common graphical application. Its object-oriented API allows you to construct user interfaces without dealing with the low-level details of drawing and device interaction. Gdk-Pixbuf 2.38.2 broadwell, skylake iris Visualisation The Gdk Pixbuf is a toolkit for image loading and pixel buffer manipulation. It is used by GTK+ 2 and GTK+ 3 to load and manipulate images. In the past it was distributed as part of GTK+ 2 but it was split off into a separate package in preparation for the change to GTK+ 3. Ghostscript 9.50 broadwell, skylake, gpu iris Utilities Ghostscript is a versatile processor for PostScript data with the ability to render PostScript to different targets. It used to be part of the cups printing stack, but is no longer used for that. Go 1.14.1 broadwell, skylake iris Compilers Go is an open source programming language that makes it easy to build simple, reliable, and efficient software. Guile 1.8.8 broadwell, skylake iris Programming Languages Guile is a programming language, designed to help programmers create flexible applications that can be extended by users or other programmers with plug-ins, modules, or scripts. Guile 2.2.4 broadwell, skylake iris Programming Languages Guile is a programming language, designed to help programmers create flexible applications that can be extended by users or other programmers with plug-ins, modules, or scripts. Gurobi 9.0.0 broadwell, skylake iris Mathematics The Gurobi Optimizer is a state-of-the-art solver for mathematical programming. The solvers in the Gurobi Optimizer were designed from the ground up to exploit modern architectures and multi-core processors, using the most advanced implementations of the latest algorithms. HDF5 1.10.5 broadwell, skylake, gpu iris Data processing HDF5 is a data model, library, and file format for storing and managing data. It supports an unlimited variety of datatypes, and is designed for flexible and efficient I/O and for high volume and complex data. HTSlib 1.10.2 broadwell, skylake iris Biology A C library for reading/writing high-throughput sequencing data. This package includes the utilities bgzip and tabix HarfBuzz 2.6.4 broadwell, skylake iris Visualisation HarfBuzz is an OpenType text shaping engine. Harminv 1.4.1 broadwell, skylake iris Mathematics Harminv is a free program (and accompanying library) to solve the problem of harmonic inversion - given a discrete-time, finite-length signal that consists of a sum of finitely-many sinusoids (possibly exponentially decaying) in a given bandwidth, it determines the frequencies, decay constants, amplitudes, and phases of those sinusoids. Horovod 0.19.1 broadwell, skylake, gpu iris Utilities Horovod is a distributed training framework for TensorFlow. ICU 64.2 broadwell, skylake, gpu iris Libraries ICU is a mature, widely used set of C/C++ and Java libraries providing Unicode and Globalization support for software applications. ImageMagick 7.0.9-5 broadwell, skylake, gpu iris Visualisation ImageMagick is a software suite to create, edit, compose, or convert bitmap images Inspector 2019_update5 broadwell, skylake iris Utilities Intel Inspector XE is an easy to use memory error checker and thread checker for serial and parallel applications JasPer 2.0.14 broadwell, skylake, gpu iris Visualisation The JasPer Project is an open-source initiative to provide a free software-based reference implementation of the codec specified in the JPEG-2000 Part-1 standard. Java 1.8.0_241 broadwell, skylake, gpu iris Programming Languages Java Platform, Standard Edition (Java SE) lets you develop and deploy Java applications on desktops and servers. Java 11.0.2 broadwell, skylake, gpu iris Programming Languages Java Platform, Standard Edition (Java SE) lets you develop and deploy Java applications on desktops and servers. Java 13.0.2 broadwell, skylake, gpu iris Programming Languages Java Platform, Standard Edition (Java SE) lets you develop and deploy Java applications on desktops and servers. Jellyfish 2.3.0 broadwell, skylake iris Biology Jellyfish is a tool for fast, memory-efficient counting of k-mers in DNA. JsonCpp 1.9.3 broadwell, skylake, gpu iris Libraries JsonCpp is a C++ library that allows manipulating JSON values, including serialization and deserialization to and from strings. It can also preserve existing comment in unserialization/serialization steps, making it a convenient format to store user input files. Julia 1.4.1 broadwell, skylake iris Programming Languages Julia is a high-level, high-performance dynamic programming language for numerical computing Keras 2.3.1 gpu iris Mathematics Keras is a deep learning API written in Python, running on top of the machine learning platform TensorFlow. LAME 3.100 broadwell, skylake, gpu iris Data processing LAME is a high quality MPEG Audio Layer III (MP3) encoder licensed under the LGPL. LLVM 9.0.0 broadwell, skylake, gpu iris Compilers The LLVM Core libraries provide a modern source- and target-independent optimizer, along with code generation support for many popular CPUs (as well as some less common ones!) These libraries are built around a well specified code representation known as the LLVM intermediate representation (\"LLVM IR\"). The LLVM Core libraries are well documented, and it is particularly easy to invent your own language (or port an existing compiler) to use LLVM as an optimizer and code generator. LLVM 9.0.1 broadwell, skylake, gpu iris Compilers The LLVM Core libraries provide a modern source- and target-independent optimizer, along with code generation support for many popular CPUs (as well as some less common ones!) These libraries are built around a well specified code representation known as the LLVM intermediate representation (\"LLVM IR\"). The LLVM Core libraries are well documented, and it is particularly easy to invent your own language (or port an existing compiler) to use LLVM as an optimizer and code generator. LMDB 0.9.24 broadwell, skylake, gpu iris Libraries LMDB is a fast, memory-efficient database. With memory-mapped files, it has the read performance of a pure in-memory database while retaining the persistence of standard disk-based databases. LibTIFF 4.0.10 broadwell, skylake, gpu iris Libraries tiff: Library and tools for reading and writing TIFF data files LittleCMS 2.9 broadwell, skylake, gpu iris Visualisation Little CMS intends to be an OPEN SOURCE small-footprint color management engine, with special focus on accuracy and performance. Lua 5.1.5 broadwell, skylake iris Programming Languages Lua is a powerful, fast, lightweight, embeddable scripting language. Lua combines simple procedural syntax with powerful data description constructs based on associative arrays and extensible semantics. Lua is dynamically typed, runs by interpreting bytecode for a register-based virtual machine, and has automatic memory management with incremental garbage collection, making it ideal for configuration, scripting, and rapid prototyping. M4 1.4.18 broadwell, skylake, gpu iris Development GNU M4 is an implementation of the traditional Unix macro processor. It is mostly SVR4 compatible although it has some extensions (for example, handling more than 9 positional parameters to macros). GNU M4 also has built-in functions for including files, running shell commands, doing arithmetic, etc. MATLAB 2019b broadwell, skylake iris Mathematics MATLAB is a high-level language and interactive environment that enables you to perform computationally intensive tasks faster than with traditional programming languages such as C, C++, and Fortran. MATLAB 2020a broadwell, skylake iris Mathematics MATLAB is a high-level language and interactive environment that enables you to perform computationally intensive tasks faster than with traditional programming languages such as C, C++, and Fortran. METIS 5.1.0 broadwell, skylake iris Mathematics METIS is a set of serial programs for partitioning graphs, partitioning finite element meshes, and producing fill reducing orderings for sparse matrices. The algorithms implemented in METIS are based on the multilevel recursive-bisection, multilevel k-way, and multi-constraint partitioning schemes. MPFR 4.0.2 broadwell, skylake, gpu iris Mathematics The MPFR library is a C library for multiple-precision floating-point computations with correct rounding. Mako 1.1.0 broadwell, skylake, gpu iris Development A super-fast templating language that borrows the best ideas from the existing templating languages Mathematica 12.0.0 broadwell, skylake iris Mathematics Mathematica is a computational software program used in many scientific, engineering, mathematical and computing fields. Maven 3.6.3 broadwell, skylake iris Development Binary maven install, Apache Maven is a software project management and comprehension tool. Based on the concept of a project object model (POM), Maven can manage a project's build, reporting and documentation from a central piece of information. Meep 1.4.3 broadwell, skylake iris Physics Meep (or MEEP) is a free finite-difference time-domain (FDTD) simulation software package developed at MIT to model electromagnetic systems. Mesa 19.1.7 broadwell, skylake, gpu iris Visualisation Mesa is an open-source implementation of the OpenGL specification - a system for rendering interactive 3D graphics. Mesa 19.2.1 broadwell, skylake, gpu iris Visualisation Mesa is an open-source implementation of the OpenGL specification - a system for rendering interactive 3D graphics. Meson 0.51.2 broadwell, skylake, gpu iris Utilities Meson is a cross-platform build system designed to be both as fast and as user friendly as possible. Mesquite 2.3.0 broadwell, skylake iris Mathematics Mesh-Quality Improvement Library NAMD 2.13 broadwell, skylake iris Chemistry NAMD is a parallel molecular dynamics code designed for high-performance simulation of large biomolecular systems. NASM 2.14.02 broadwell, skylake, gpu iris Programming Languages NASM: General-purpose x86 assembler NCCL 2.4.8 gpu iris Libraries The NVIDIA Collective Communications Library (NCCL) implements multi-GPU and multi-node collective communication primitives that are performance optimized for NVIDIA GPUs. NLopt 2.6.1 broadwell, skylake, gpu iris Numerical libraries NLopt is a free/open-source library for nonlinear optimization, providing a common interface for a number of different free optimization routines available online as well as original implementations of various other algorithms. NSPR 4.21 broadwell, skylake iris Libraries Netscape Portable Runtime (NSPR) provides a platform-neutral API for system level and libc-like functions. NSS 3.45 broadwell, skylake iris Libraries Network Security Services (NSS) is a set of libraries designed to support cross-platform development of security-enabled client and server applications. Ninja 1.9.0 broadwell, skylake, gpu iris Utilities Ninja is a small build system with a focus on speed. OPARI2 2.0.5 broadwell, skylake iris Performance measurements OPARI2, the successor of Forschungszentrum Juelich's OPARI, is a source-to-source instrumentation tool for OpenMP and hybrid codes. It surrounds OpenMP directives and runtime library calls with calls to the POMP2 measurement interface. OTF2 2.2 broadwell, skylake iris Performance measurements The Open Trace Format 2 is a highly scalable, memory efficient event trace data format plus support library. It is the new standard trace format for Scalasca, Vampir, and TAU and is open for other tools. OpenBLAS 0.3.7 broadwell, skylake, gpu iris Numerical libraries OpenBLAS is an optimized BLAS library based on GotoBLAS2 1.13 BSD version. OpenCV 4.2.0 broadwell, skylake iris Visualisation OpenCV (Open Source Computer Vision Library) is an open source computer vision and machine learning software library. OpenCV was built to provide a common infrastructure for computer vision applications and to accelerate the use of machine perception in the commercial products. Includes extra modules for OpenCV from the contrib repository. OpenFOAM-Extend 4.1-20200408 broadwell, skylake iris CFD/Finite element modelling OpenFOAM is a free, open source CFD software package. OpenFOAM has an extensive range of features to solve anything from complex fluid flows involving chemical reactions, turbulence and heat transfer, to solid dynamics and electromagnetics. OpenFOAM v1912 broadwell, skylake iris CFD/Finite element modelling OpenFOAM is a free, open source CFD software package. OpenFOAM has an extensive range of features to solve anything from complex fluid flows involving chemical reactions, turbulence and heat transfer, to solid dynamics and electromagnetics. OpenMPI 3.1.4 broadwell, skylake, gpu iris MPI The Open MPI Project is an open source MPI-3 implementation. PAPI 6.0.0 broadwell, skylake iris Performance measurements PAPI provides the tool designer and application engineer with a consistent interface and methodology for use of the performance counter hardware found in most major microprocessors. PAPI enables software engineers to see, in near real time, the relation between software performance and processor events. In addition Component PAPI provides access to a collection of components that expose performance measurement opportunites across the hardware and software stack. PCRE2 10.33 broadwell, skylake iris Development The PCRE library is a set of functions that implement regular expression pattern matching using the same syntax and semantics as Perl 5. PCRE 8.43 broadwell, skylake, gpu iris Development The PCRE library is a set of functions that implement regular expression pattern matching using the same syntax and semantics as Perl 5. PDT 3.25 broadwell, skylake iris Performance measurements Program Database Toolkit (PDT) is a framework for analyzing source code written in several programming languages and for making rich program knowledge accessible to developers of static and dynamic analysis tools. PDT implements a standard program representation, the program database (PDB), that can be accessed in a uniform way through a class library supporting common PDB operations. PGI 19.10 broadwell, skylake iris Compilers C, C++ and Fortran compilers from The Portland Group - PGI PLUMED 2.5.3 broadwell, skylake iris Chemistry PLUMED is an open source library for free energy calculations in molecular systems which works together with some of the most popular molecular dynamics engines. Free energy calculations can be performed as a function of many order parameters with a particular  focus on biological problems, using state of the art methods such as metadynamics, umbrella sampling and Jarzynski-equation based steered MD. The software, written in C++, can be easily interfaced with both fortran and C/C++ codes. PROJ 6.2.1 broadwell, skylake, gpu iris Libraries Program proj is a standard Unix filter function which converts geographic longitude and latitude coordinates into cartesian coordinates Pango 1.44.7 broadwell, skylake iris Visualisation Pango is a library for laying out and rendering of text, with an emphasis on internationalization. Pango can be used anywhere that text layout is needed, though most of the work on Pango so far has been done in the context of the GTK+ widget toolkit. Pango forms the core of text and font handling for GTK+-2.x. ParMETIS 4.0.3 broadwell, skylake iris Mathematics ParMETIS is an MPI-based parallel library that implements a variety of algorithms for partitioning unstructured graphs, meshes, and for computing fill-reducing orderings of sparse matrices. ParMETIS extends the functionality provided by METIS and includes routines that are especially suited for parallel AMR computations and large scale numerical simulations. The algorithms implemented in ParMETIS are based on the parallel multilevel k-way graph-partitioning, adaptive repartitioning, and parallel multi-constrained partitioning schemes. ParMGridGen 1.0 broadwell, skylake iris Mathematics ParMGridGen is an MPI-based parallel library that is based on the serial package MGridGen, that implements (serial) algorithms for obtaining a sequence of successive coarse grids that are well-suited for geometric multigrid methods. ParaView 5.6.2 broadwell, skylake iris Visualisation ParaView is a scientific parallel visualizer. Perl 5.30.0 broadwell, skylake, gpu iris Programming Languages Larry Wall's Practical Extraction and Report Language This is a minimal build without any modules. Should only be used for build dependencies. Pillow 6.2.1 broadwell, skylake, gpu iris Visualisation Pillow is the 'friendly PIL fork' by Alex Clark and Contributors. PIL is the Python Imaging Library by Fredrik Lundh and Contributors. PyTorch 1.4.0 broadwell, skylake iris Development Tensors and Dynamic neural networks in Python with strong GPU acceleration. PyTorch is a deep learning framework that puts Python first. PyTorch 1.7.1 broadwell, skylake iris Development Tensors and Dynamic neural networks in Python with strong GPU acceleration. PyTorch is a deep learning framework that puts Python first. PyYAML 5.1.2 broadwell, skylake, gpu iris Libraries PyYAML is a YAML parser and emitter for the Python programming language. Python 2.7.16 broadwell, skylake, gpu iris Programming Languages Python is a programming language that lets you work more quickly and integrate your systems more effectively. Python 3.7.4 broadwell, skylake, gpu iris Programming Languages Python is a programming language that lets you work more quickly and integrate your systems more effectively. Qt5 5.13.1 broadwell, skylake iris Development Qt is a comprehensive cross-platform C++ application framework. QuantumESPRESSO 6.7 broadwell iris Chemistry Quantum ESPRESSO  is an integrated suite of computer codes for electronic-structure calculations and materials modeling at the nanoscale. It is based on density-functional theory, plane waves, and pseudopotentials (both norm-conserving and ultrasoft). R 3.6.2 broadwell, skylake, gpu iris Programming Languages R is a free software environment for statistical computing and graphics. ReFrame 2.21 broadwell, skylake iris Development ReFrame is a framework for writing regression tests for HPC systems. Ruby 2.7.1 broadwell, skylake iris Programming Languages Ruby is a dynamic, open source programming language with a focus on simplicity and productivity. It has an elegant syntax that is natural to read and easy to write. Rust 1.37.0 broadwell, skylake iris Programming Languages Rust is a systems programming language that runs blazingly fast, prevents segfaults, and guarantees thread safety. SAMtools 1.10 broadwell, skylake iris Biology SAM Tools provide various utilities for manipulating alignments in the SAM format, including sorting, merging, indexing and generating alignments in a per-position format. SCOTCH 6.0.9 broadwell, skylake iris Mathematics Software package and libraries for sequential and parallel graph partitioning, static mapping, and sparse matrix block ordering, and sequential mesh and hypergraph partitioning. SIONlib 1.7.6 broadwell, skylake iris Libraries SIONlib is a scalable I/O library for parallel access to task-local files. The library not only supports writing and reading binary data to or from several thousands of processors into a single or a small number of physical files, but also provides global open and close functions to access SIONlib files in parallel. This package provides a stripped-down installation of SIONlib for use with performance tools (e.g., Score-P), with renamed symbols to avoid conflicts when an application using SIONlib itself is linked against a tool requiring a different SIONlib version. SQLite 3.29.0 broadwell, skylake, gpu iris Development SQLite: SQL Database Engine in a C Library SWIG 4.0.1 broadwell, skylake, gpu iris Development SWIG is a software development tool that connects programs written in C and C++ with a variety of high-level programming languages. Salmon 1.1.0 broadwell, skylake iris Biology Salmon is a wicked-fast program to produce a highly-accurate, transcript-level quantification estimates from RNA-seq data. Salome 8.5.0 broadwell, skylake iris CFD/Finite element modelling The SALOME platform is an open source software framework for pre- and post-processing and integration of numerical solvers from various scientific fields. CEA and EDF use SALOME to perform a large number of simulations, typically related to power plant equipment and alternative energy. To address these challenges, SALOME includes a CAD/CAE modelling tool, mesh generators, an advanced 3D visualization tool, etc. ScaLAPACK 2.0.2 broadwell, skylake, gpu iris Numerical libraries The ScaLAPACK (or Scalable LAPACK) library includes a subset of LAPACK routines redesigned for distributed memory MIMD parallel computers. Scalasca 2.5 broadwell, skylake iris Performance measurements Scalasca is a software tool that supports the performance optimization of parallel programs by measuring and analyzing their runtime behavior. The analysis identifies potential performance bottlenecks -- in particular those concerning communication and synchronization -- and offers guidance in exploring their causes. SciPy-bundle 2019.10 broadwell, skylake, gpu iris Programming Languages Bundle of Python packages for scientific software Score-P 6.0 broadwell, skylake iris Performance measurements The Score-P measurement infrastructure is a highly scalable and easy-to-use tool suite for profiling, event tracing, and online analysis of HPC applications. Singularity 3.6.0 broadwell, skylake iris Utilities SingularityCE is an open source container platform designed to be simple, fast, and secure.  Singularity is optimized for EPC and HPC workloads, allowing untrusted users to run untrusted containers in a trusted way. Spack 0.12.1 broadwell, skylake iris Development Spack is a package manager for supercomputers, Linux, and macOS. It makes installing scientific software easy. With Spack, you can build a package with multiple versions, configurations, platforms, and compilers, and all of these builds can coexist on the same machine. Spark 2.4.3 broadwell, skylake iris Development Spark is Hadoop MapReduce done in memory Sumo 1.3.1 broadwell, skylake iris Utilities Sumo is an open source, highly portable, microscopic and continuous traffic simulation package designed to handle large road networks. Szip 2.1.1 broadwell, skylake, gpu iris Utilities Szip compression software, providing lossless compression of scientific data Tcl 8.6.9 broadwell, skylake, gpu iris Programming Languages Tcl (Tool Command Language) is a very powerful but easy to learn dynamic programming language, suitable for a very wide range of uses, including web and desktop applications, networking, administration, testing and many more. TensorFlow 1.15.5 gpu iris Libraries An open-source software library for Machine Intelligence TensorFlow 2.1.0 gpu iris Libraries An open-source software library for Machine Intelligence Theano 1.0.4 gpu iris Mathematics Theano is a Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. Tk 8.6.9 broadwell, skylake, gpu iris Visualisation Tk is an open source, cross-platform widget toolchain that provides a library of basic elements for building a graphical user interface (GUI) in many different programming languages. Tkinter 3.7.4 broadwell, skylake iris Programming Languages Tkinter module, built with the Python buildsystem TopHat 2.1.2 broadwell, skylake iris Biology TopHat is a fast splice junction mapper for RNA-Seq reads. Trinity 2.10.0 broadwell, skylake iris Biology Trinity represents a novel method for the efficient and robust de novo reconstruction of transcriptomes from RNA-Seq data. Trinity combines three independent software modules: Inchworm, Chrysalis, and Butterfly, applied sequentially to process large volumes of RNA-Seq reads. UDUNITS 2.2.26 broadwell, skylake, gpu iris Physics UDUNITS supports conversion of unit specifications between formatted and binary forms, arithmetic manipulation of units, and conversion of values between compatible scales of measurement. ULHPC-bio 2019b broadwell, skylake iris System-level software Generic Module bundle for Bioinformatics, biology and biomedical software in use on the UL HPC Facility, especially at LCSB ULHPC-cs 2019b broadwell, skylake iris System-level software Generic Module bundle for Computational science software in use on the UL HPC Facility, including: - Computer Aided Engineering, incl. CFD - Chemistry, Computational Chemistry and Quantum Chemistry - Data management &amp; processing tools - Earth Sciences - Quantum Computing - Physics and physical systems simulations ULHPC-dl 2019b broadwell, skylake iris System-level software Generic Module bundle for (CPU-version) of AI / Deep Learning / Machine Learning software in use on the UL HPC Facility ULHPC-gpu 2019b gpu iris System-level software Generic Module bundle for GPU accelerated User Software in use on the UL HPC Facility ULHPC-math 2019b broadwell, skylake iris System-level software Generic Module bundle for  High-level mathematical software and Linear Algrebra libraries in use on the UL HPC Facility ULHPC-toolchains 2019b broadwell, skylake iris System-level software Generic Module bundle that contains all the dependencies required to enable toolchains and building tools/programming language in use on the UL HPC Facility ULHPC-tools 2019b broadwell, skylake iris System-level software Misc tools, incl. - perf:      Performance tools - tools:     General purpose tools VASP 5.4.4 broadwell, skylake iris Physics The Vienna Ab initio Simulation Package (VASP) is a computer program for atomic scale materials modelling, e.g. electronic structure calculations and quantum-mechanical molecular dynamics, from first principles. VTK 8.2.0 broadwell, skylake iris Visualisation The Visualization Toolkit (VTK) is an open-source, freely available software system for 3D computer graphics, image processing and visualization. VTK consists of a C++ class library and several interpreted interface layers including Tcl/Tk, Java, and Python. VTK supports a wide variety of visualization algorithms including: scalar, vector, tensor, texture, and volumetric methods; and advanced modeling techniques such as: implicit modeling, polygon reduction, mesh smoothing, cutting, contouring, and Delaunay triangulation. VTune 2019_update8 broadwell, skylake iris Utilities Intel VTune Amplifier XE is the premier performance profiler for C, C++, C#, Fortran, Assembly and Java. Valgrind 3.15.0 broadwell, skylake iris Debugging Valgrind: Debugging and profiling tools VirtualGL 2.6.2 broadwell, skylake iris Visualisation VirtualGL is an open source toolkit that gives any Linux or Unix remote display software the ability to run OpenGL applications with full hardware acceleration. Voro++ 0.4.6 broadwell, skylake iris Mathematics Voro++ is a software library for carrying out three-dimensional computations of the Voronoi tessellation. A distinguishing feature of the Voro++ library is that it carries out cell-based calculations, computing the Voronoi cell for each particle individually. It is particularly well-suited for applications that rely on cell-based statistics, where features of Voronoi cells (eg. volume, centroid, number of faces) can be used to analyze a system of particles. X11 20190717 broadwell, skylake, gpu iris Visualisation The X Window System (X11) is a windowing system for bitmap displays XML-LibXML 2.0201 broadwell, skylake iris Data processing Perl binding for libxml2 XZ 5.2.4 broadwell, skylake, gpu iris Utilities xz: XZ utilities Xerces-C++ 3.2.2 broadwell, skylake iris Libraries Xerces-C++ is a validating XML parser written in a portable subset of C++. Xerces-C++ makes it easy to give your application the ability to read and write XML data. A shared library is provided for parsing, generating, manipulating, and validating XML documents using the DOM, SAX, and SAX2 APIs. Yasm 1.3.0 broadwell, skylake, gpu iris Programming Languages Yasm: Complete rewrite of the NASM assembler with BSD license Zip 3.0 broadwell, skylake, gpu iris Utilities Zip is a compression and file packaging/archive utility. Although highly compatible both with PKWARE's PKZIP and PKUNZIP utilities for MS-DOS and with Info-ZIP's own UnZip, our primary objectives have been portability and other-than-MSDOS functionality ant 1.10.6 broadwell, skylake iris Development Apache Ant is a Java library and command-line tool whose mission is to drive processes described in build files as targets and extension points dependent upon each other. The main known usage of Ant is the build of Java applications. ant 1.10.7 broadwell, skylake iris Development Apache Ant is a Java library and command-line tool whose mission is to drive processes described in build files as targets and extension points dependent upon each other. The main known usage of Ant is the build of Java applications. archspec 0.1.0 broadwell, skylake iris Utilities A library for detecting, labeling, and reasoning about microarchitectures arpack-ng 3.7.0 broadwell, skylake iris Numerical libraries ARPACK is a collection of Fortran77 subroutines designed to solve large scale eigenvalue problems. at-spi2-atk 2.34.1 broadwell, skylake iris Visualisation AT-SPI 2 toolkit bridge at-spi2-core 2.34.0 broadwell, skylake iris Visualisation Assistive Technology Service Provider Interface. binutils 2.32 broadwell, skylake, gpu iris Utilities binutils: GNU binary utilities bzip2 1.0.8 broadwell, skylake, gpu iris Utilities bzip2 is a freely available, patent free, high-quality data compressor. It typically compresses files to within 10% to 15% of the best available techniques (the PPM family of statistical compressors), whilst being around twice as fast at compression and six times faster at decompression. cURL 7.66.0 broadwell, skylake, gpu iris Utilities libcurl is a free and easy-to-use client-side URL transfer library, supporting DICT, FILE, FTP, FTPS, Gopher, HTTP, HTTPS, IMAP, IMAPS, LDAP, LDAPS, POP3, POP3S, RTMP, RTSP, SCP, SFTP, SMTP, SMTPS, Telnet and TFTP. libcurl supports SSL certificates, HTTP POST, HTTP PUT, FTP uploading, HTTP form based upload, proxies, cookies, user+password authentication (Basic, Digest, NTLM, Negotiate, Kerberos), file transfer resume, http proxy tunneling and more. cairo 1.16.0 broadwell, skylake, gpu iris Visualisation Cairo is a 2D graphics library with support for multiple output devices. Currently supported output targets include the X Window System (via both Xlib and XCB), Quartz, Win32, image buffers, PostScript, PDF, and SVG file output. Experimental backends include OpenGL, BeOS, OS/2, and DirectFB cuDNN 7.6.4.38 gpu iris Numerical libraries The NVIDIA CUDA Deep Neural Network library (cuDNN) is a GPU-accelerated library of primitives for deep neural networks. double-conversion 3.1.4 broadwell, skylake, gpu iris Libraries Efficient binary-decimal and decimal-binary conversion routines for IEEE doubles. expat 2.2.7 broadwell, skylake, gpu iris Utilities Expat is an XML parser library written in C. It is a stream-oriented parser in which an application registers handlers for things the parser might find in the XML document (like start tags) flatbuffers 1.12.0 broadwell, skylake, gpu iris Development FlatBuffers: Memory Efficient Serialization Library flex 2.6.4 broadwell, skylake, gpu iris Programming Languages Flex (Fast Lexical Analyzer) is a tool for generating scanners. A scanner, sometimes called a tokenizer, is a program which recognizes lexical patterns in text. fontconfig 2.13.1 broadwell, skylake, gpu iris Visualisation Fontconfig is a library designed to provide system-wide font configuration, customization and application access. foss 2019b broadwell, skylake iris Toolchains (software stacks) GNU Compiler Collection (GCC) based compiler toolchain, including OpenMPI for MPI support, OpenBLAS (BLAS and LAPACK support), FFTW and ScaLAPACK. fosscuda 2019b gpu iris Toolchains (software stacks) GCC based compiler toolchain with CUDA support, and including OpenMPI for MPI support, OpenBLAS (BLAS and LAPACK support), FFTW and ScaLAPACK. freetype 2.10.1 broadwell, skylake, gpu iris Visualisation FreeType 2 is a software font engine that is designed to be small, efficient, highly customizable, and portable while capable of producing high-quality output (glyph images). It can be used in graphics libraries, display servers, font conversion tools, text image generation tools, and many other products as well. gc 7.6.12 broadwell, skylake iris Libraries The Boehm-Demers-Weiser conservative garbage collector can be used as a garbage collecting replacement for C malloc or C++ new. gcccuda 2019b gpu iris Toolchains (software stacks) GNU Compiler Collection (GCC) based compiler toolchain, along with CUDA toolkit. gettext 0.19.8.1 broadwell, skylake, gpu iris Utilities GNU 'gettext' is an important step for the GNU Translation Project, as it is an asset on which we may build many other steps. This package offers to programmers, translators, and even users, a well integrated set of tools and documentation gettext 0.20.1 broadwell, skylake, gpu iris Utilities GNU 'gettext' is an important step for the GNU Translation Project, as it is an asset on which we may build many other steps. This package offers to programmers, translators, and even users, a well integrated set of tools and documentation gflags 2.2.2 broadwell, skylake iris Development The gflags package contains a C++ library that implements commandline flags processing.  It includes built-in support for standard types such as string and the ability to define flags in the source file in which they are used. giflib 5.2.1 broadwell, skylake, gpu iris Libraries giflib is a library for reading and writing gif images. It is API and ABI compatible with libungif which was in wide use while the LZW compression algorithm was patented. git 2.23.0 broadwell, skylake, gpu iris Utilities Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. glog 0.4.0 broadwell, skylake iris Development A C++ implementation of the Google logging module. gmsh 4.4.0 broadwell, skylake iris CFD/Finite element modelling Salome is an open-source software that provides a generic Pre- and Post-Processing platform for numerical simulation. It is based on an open and flexible architecture made of reusable components. gnuplot 5.2.8 broadwell, skylake iris Visualisation Portable interactive, function plotting utility gocryptfs 1.7.1 broadwell, skylake iris Utilities Encrypted overlay filesystem written in Go. gocryptfs uses file-based encryption that is implemented as a mountable FUSE filesystem. Each file in gocryptfs is stored as one corresponding encrypted file on the hard disk. gompi 2019b broadwell, skylake iris Toolchains (software stacks) GNU Compiler Collection (GCC) based compiler toolchain, including OpenMPI for MPI support. gompic 2019b gpu iris Toolchains (software stacks) GNU Compiler Collection (GCC) based compiler toolchain along with CUDA toolkit, including OpenMPI for MPI support with CUDA features enabled. googletest 1.10.0 broadwell, skylake iris Development Google's framework for writing C++ tests on a variety of platforms gperf 3.1 broadwell, skylake, gpu iris Development GNU gperf is a perfect hash function generator. For a given list of strings, it produces a hash function and hash table, in form of C or C++ code, for looking up a value depending on the input string. The hash function is perfect, which means that the hash table has no collisions, and the hash table lookup needs a single string comparison only. gzip 1.10 broadwell, skylake iris Utilities gzip (GNU zip) is a popular data compression program as a replacement for compress h5py 2.10.0 broadwell, skylake, gpu iris Data processing HDF5 for Python (h5py) is a general-purpose Python interface to the Hierarchical Data Format library, version 5. HDF5 is a versatile, mature scientific software library designed for the fast, flexible storage of enormous amounts of data. help2man 1.47.4 broadwell, skylake, gpu iris Utilities help2man produces simple manual pages from the '--help' and '--version' output of other commands. help2man 1.47.8 broadwell, skylake, gpu iris Utilities help2man produces simple manual pages from the '--help' and '--version' output of other commands. hwloc 1.11.12 broadwell, skylake, gpu iris System-level software The Portable Hardware Locality (hwloc) software package provides a portable abstraction (across OS, versions, architectures, ...) of the hierarchical topology of modern architectures, including NUMA memory nodes, sockets, shared caches, cores and simultaneous multithreading. It also gathers various system attributes such as cache and memory information as well as the locality of I/O devices such as network interfaces, InfiniBand HCAs or GPUs. It primarily aims at helping applications with gathering information about modern computing hardware so as to exploit it accordingly and efficiently. hypothesis 4.44.2 broadwell, skylake, gpu iris Utilities Hypothesis is an advanced testing library for Python. It lets you write tests which are parametrized by a source of examples, and then generates simple and comprehensible examples that make your tests fail. This lets you find more bugs in your code with less work. iccifort 2019.5.281 broadwell, skylake, gpu iris Compilers Intel C, C++ &amp; Fortran compilers iccifortcuda 2019b gpu iris Toolchains (software stacks) Intel C, C++ &amp; Fortran compilers with CUDA toolkit iimpi 2019b broadwell, skylake iris Toolchains (software stacks) Intel C/C++ and Fortran compilers, alongside Intel MPI. iimpic 2019b gpu iris Toolchains (software stacks) Intel C/C++ and Fortran compilers, alongside Intel MPI and CUDA. imkl 2019.5.281 broadwell, skylake, gpu iris Numerical libraries Intel Math Kernel Library is a library of highly optimized, extensively threaded math routines for science, engineering, and financial applications that require maximum performance. Core math functions include BLAS, LAPACK, ScaLAPACK, Sparse Solvers, Fast Fourier Transforms, Vector Math, and more. impi 2018.5.288 broadwell, skylake, gpu iris MPI Intel MPI Library, compatible with MPICH ABI intel 2019b broadwell, skylake iris Toolchains (software stacks) Compiler toolchain including Intel compilers, Intel MPI and Intel Math Kernel Library (MKL). intelcuda 2019b gpu iris Toolchains (software stacks) Intel Cluster Toolkit Compiler Edition provides Intel C/C++ and Fortran compilers, Intel MPI &amp; Intel MKL, with CUDA toolkit intltool 0.51.0 broadwell, skylake, gpu iris Development intltool is a set of tools to centralize translation of many different file formats using GNU gettext-compatible PO files. itac 2019.4.036 broadwell, skylake iris Utilities The Intel Trace Collector is a low-overhead tracing library that performs event-based tracing in applications. The Intel Trace Analyzer provides a convenient way to monitor application activities gathered by the Intel Trace Collector through graphical displays. jemalloc 5.2.1 broadwell, skylake iris Libraries jemalloc is a general purpose malloc(3) implementation that emphasizes fragmentation avoidance and scalable concurrency support. kallisto 0.46.1 broadwell, skylake iris Biology kallisto is a program for quantifying abundances of transcripts from RNA-Seq data, or more generally of target sequences using high-throughput sequencing reads. kim-api 2.1.3 broadwell, skylake iris Chemistry Open Knowledgebase of Interatomic Models. KIM is an API and OpenKIM is a collection of interatomic models (potentials) for atomistic simulations.  This is a library that can be used by simulation programs to get access to the models in the OpenKIM database. This EasyBuild only installs the API, the models can be installed with the package openkim-models, or the user can install them manually by running kim-api-collections-management install user MODELNAME or kim-api-collections-management install user OpenKIM to install them all. libGLU 9.0.1 broadwell, skylake, gpu iris Visualisation The OpenGL Utility Library (GLU) is a computer graphics library for OpenGL. libcerf 1.13 broadwell, skylake iris Mathematics libcerf is a self-contained numeric library that provides an efficient and accurate implementation of complex error functions, along with Dawson, Faddeeva, and Voigt functions. libctl 4.0.0 broadwell, skylake iris Chemistry libctl is a free Guile-based library implementing flexible control files for scientific simulations. libdrm 2.4.99 broadwell, skylake, gpu iris Libraries Direct Rendering Manager runtime library. libepoxy 1.5.4 broadwell, skylake iris Libraries Epoxy is a library for handling OpenGL function pointer management for you libevent 2.1.11 broadwell, skylake iris Libraries The libevent API provides a mechanism to execute a callback function when a specific event occurs on a file descriptor or after a timeout has been reached.  Furthermore, libevent also support callbacks due to signals or regular timeouts. libffi 3.2.1 broadwell, skylake, gpu iris Libraries The libffi library provides a portable, high level programming interface to various calling conventions. This allows a programmer to call any function specified by a call interface description at run-time. libgd 2.2.5 broadwell, skylake iris Libraries GD is an open source code library for the dynamic creation of images by programmers. libgeotiff 1.5.1 broadwell, skylake, gpu iris Libraries Library for reading and writing coordinate system information from/to GeoTIFF files libglvnd 1.2.0 broadwell, skylake iris Libraries libglvnd is a vendor-neutral dispatch layer for arbitrating OpenGL API calls between multiple vendors. libgpuarray 0.7.6 gpu iris Libraries Library to manipulate tensors on the GPU. libiconv 1.16 broadwell, skylake, gpu iris Libraries Libiconv converts from one character encoding to another through Unicode conversion libjpeg-turbo 2.0.3 broadwell, skylake, gpu iris Libraries libjpeg-turbo is a fork of the original IJG libjpeg which uses SIMD to accelerate baseline JPEG compression and decompression. libjpeg is a library that implements JPEG image encoding, decoding and transcoding. libmatheval 1.1.11 broadwell, skylake iris Libraries GNU libmatheval is a library (callable from C and Fortran) to parse and evaluate symbolic expressions input as text. libpciaccess 0.14 broadwell, skylake, gpu iris System-level software Generic PCI access library. libpng 1.6.37 broadwell, skylake, gpu iris Libraries libpng is the official PNG reference library libreadline 8.0 broadwell, skylake, gpu iris Libraries The GNU Readline library provides a set of functions for use by applications that allow users to edit command lines as they are typed in. Both Emacs and vi editing modes are available. The Readline library includes additional functions to maintain a list of previously-entered command lines, to recall and perhaps reedit those lines, and perform csh-like history expansion on previous commands. libsndfile 1.0.28 broadwell, skylake, gpu iris Libraries Libsndfile is a C library for reading and writing files containing sampled sound (such as MS Windows WAV and the Apple/SGI AIFF format) through one standard library interface. libtool 2.4.6 broadwell, skylake, gpu iris Libraries GNU libtool is a generic library support script. Libtool hides the complexity of using shared libraries behind a consistent, portable interface. libunistring 0.9.10 broadwell, skylake iris Libraries This library provides functions for manipulating Unicode strings and for manipulating C strings according to the Unicode standard. libunwind 1.3.1 broadwell, skylake, gpu iris Libraries The primary goal of libunwind is to define a portable and efficient C programming interface (API) to determine the call-chain of a program. The API additionally provides the means to manipulate the preserved (callee-saved) state of each call-frame and to resume execution at any point in the call-chain (non-local goto). The API supports both local (same-process) and remote (across-process) operation. As such, the API is useful in a number of applications libxc 4.3.4 broadwell, skylake iris Chemistry Libxc is a library of exchange-correlation functionals for density-functional theory. The aim is to provide a portable, well tested and reliable set of exchange and correlation functionals. libxml2 2.9.9 broadwell, skylake, gpu iris Libraries Libxml2 is the XML C parser and toolchain developed for the Gnome project (but usable outside of the Gnome platform). libxslt 1.1.34 broadwell, skylake iris Libraries Libxslt is the XSLT C library developed for the GNOME project (but usable outside of the Gnome platform). libyaml 0.2.2 broadwell, skylake, gpu iris Libraries LibYAML is a YAML parser and emitter written in C. lxml 4.4.2 broadwell, skylake iris Libraries The lxml XML toolkit is a Pythonic binding for the C libraries libxml2 and libxslt. magma 2.5.1 gpu iris Mathematics The MAGMA project aims to develop a dense linear algebra library similar to LAPACK but for heterogeneous/hybrid architectures, starting with current Multicore+GPU systems. magma 2.5.4 gpu iris Mathematics The MAGMA project aims to develop a dense linear algebra library similar to LAPACK but for heterogeneous/hybrid architectures, starting with current Multicore+GPU systems. matplotlib 3.1.1 broadwell, skylake iris Visualisation matplotlib is a python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms. matplotlib can be used in python scripts, the python and ipython shell, web application servers, and six graphical user interface toolkits. molmod 1.4.5 broadwell, skylake iris Mathematics MolMod is a Python library with many compoments that are useful to write molecular modeling programs. ncurses 6.0 broadwell, skylake, gpu iris Development The Ncurses (new curses) library is a free software emulation of curses in System V Release 4.0, and more. It uses Terminfo format, supports pads and color and multiple highlights and forms characters and function-key mapping, and has all the other SYSV-curses enhancements over BSD Curses. ncurses 6.1 broadwell, skylake, gpu iris Development The Ncurses (new curses) library is a free software emulation of curses in System V Release 4.0, and more. It uses Terminfo format, supports pads and color and multiple highlights and forms characters and function-key mapping, and has all the other SYSV-curses enhancements over BSD Curses. netCDF-Fortran 4.5.2 broadwell, skylake iris Data processing NetCDF (network Common Data Form) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. netCDF 4.7.1 broadwell, skylake, gpu iris Data processing NetCDF (network Common Data Form) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. nettle 3.5.1 broadwell, skylake, gpu iris Libraries Nettle is a cryptographic library that is designed to fit easily in more or less any context: In crypto toolkits for object-oriented languages (C++, Python, Pike, ...), in applications like LSH or GNUPG, or even in kernel space. nsync 1.24.0 broadwell, skylake, gpu iris Development nsync is a C library that exports various synchronization primitives, such as mutexes numactl 2.0.12 broadwell, skylake, gpu iris Utilities The numactl program allows you to run your application program on specific cpu's and memory nodes. It does this by supplying a NUMA memory policy to the operating system before running your program. The libnuma library provides convenient ways for you to add NUMA memory policies into your own program. phonopy 2.2.0 broadwell, skylake iris Libraries Phonopy is an open source package of phonon calculations based on the supercell approach. pixman 0.38.4 broadwell, skylake, gpu iris Visualisation Pixman is a low-level software library for pixel manipulation, providing features such as image compositing and trapezoid rasterization. Important users of pixman are the cairo graphics library and the X server. pkg-config 0.29.2 broadwell, skylake, gpu iris Development pkg-config is a helper tool used when compiling applications and libraries. It helps you insert the correct compiler options on the command line so an application can use gcc -o test test.c <code>pkg-config --libs --cflags glib-2.0</code> for instance, rather than hard-coding values on where to find glib (or other libraries). pkgconfig 1.5.1 broadwell, skylake, gpu iris Development pkgconfig is a Python module to interface with the pkg-config command line tool pocl 1.4 gpu iris Libraries Pocl is a portable open source (MIT-licensed) implementation of the OpenCL standard protobuf-python 3.10.0 broadwell, skylake, gpu iris Development Python Protocol Buffers runtime library. protobuf 2.5.0 broadwell, skylake iris Development Google Protocol Buffers protobuf 3.10.0 broadwell, skylake iris Development Google Protocol Buffers pybind11 2.4.3 broadwell, skylake, gpu iris Libraries pybind11 is a lightweight header-only library that exposes C++ types in Python and vice versa, mainly to create Python bindings of existing C++ code. re2c 1.2.1 broadwell, skylake iris Utilities re2c is a free and open-source lexer generator for C and C++. Its main goal is generating fast lexers: at least as fast as their reasonably optimized hand-coded counterparts. Instead of using traditional table-driven approach, re2c encodes the generated finite state automata directly in the form of conditional jumps and comparisons. scipy 1.4.1 broadwell, skylake, gpu iris Mathematics SciPy is a collection of mathematical algorithms and convenience functions built on the Numpy extension for Python. setuptools 41.0.1 broadwell, skylake iris Development Easily download, build, install, upgrade, and uninstall Python packages snappy 1.1.7 broadwell, skylake, gpu iris Libraries Snappy is a compression/decompression library. It does not aim for maximum compression, or compatibility with any other compression library; instead, it aims for very high speeds and reasonable compression. sparsehash 2.0.3 broadwell, skylake iris Development An extremely memory-efficient hash_map implementation. 2 bits/entry overhead! The SparseHash library contains several hash-map implementations, including implementations that optimize for space or speed. tbb 2019_U9 broadwell, skylake iris Libraries Intel(R) Threading Building Blocks (Intel(R) TBB) lets you easily write parallel C++ programs that take full advantage of multicore performance, that are portable, composable and have future-proof scalability. tbb 2020.2 broadwell, skylake iris Libraries Intel(R) Threading Building Blocks (Intel(R) TBB) lets you easily write parallel C++ programs that take full advantage of multicore performance, that are portable, composable and have future-proof scalability. texinfo 6.7 broadwell, skylake iris Development Texinfo is the official documentation format of the GNU project. typing-extensions 3.7.4.3 gpu iris Development Typing Extensions \u2013 Backported and Experimental Type Hints for Python util-linux 2.34 broadwell, skylake, gpu iris Utilities Set of Linux utilities x264 20190925 broadwell, skylake, gpu iris Visualisation x264 is a free software library and application for encoding video streams into the H.264/MPEG-4 AVC compression format, and is released under the terms of the GNU GPL. x265 3.2 broadwell, skylake, gpu iris Visualisation x265 is a free software library and application for encoding video streams into the H.265 AVC compression format, and is released under the terms of the GNU GPL. xorg-macros 1.19.2 broadwell, skylake, gpu iris Development X.org macros utilities. xprop 1.2.4 broadwell, skylake iris Visualisation The xprop utility is for displaying window and font properties in an X server. One window or font is selected using the command line arguments or possibly in the case of a window, by clicking on the desired window. A list of properties is then given, possibly with formatting information. yaff 1.6.0 broadwell, skylake iris Chemistry Yaff stands for 'Yet another force field'. It is a pythonic force-field code. zlib 1.2.11 broadwell, skylake, gpu iris Libraries zlib is designed to be a free, general-purpose, legally unencumbered -- that is, not covered by any patents -- lossless data-compression library for use on virtually any computer hardware and operating system."},{"location":"software/swsets/2020b/","title":"2020a","text":"<p>Alphabetical list of available ULHPC software belonging to the '2020b' software set. To load a software of this set, use: <pre><code># Eventually: resif-load-swset-[...]\nmodule load &lt;category&gt;/&lt;software&gt;[/&lt;version&gt;]\n</code></pre></p> Software Architectures Clusters Category Description ABAQUS 2021 broadwell, epyc, skylake aion, iris CFD/Finite element modelling Finite Element Analysis software for modeling, visualization and best-in-class implicit and explicit dynamics FEA. ABINIT 9.4.1 epyc aion Chemistry ABINIT is a package whose main program allows one to find the total energy, charge density and electronic structure of systems made of electrons and nuclei (molecules and periodic solids) within Density Functional Theory (DFT), using pseudopotentials and a planewave or wavelet basis. ABySS 2.2.5 broadwell, epyc, skylake aion, iris Biology Assembly By Short Sequences - a de novo, parallel, paired-end sequence assembler ACTC 1.1 broadwell, skylake iris Libraries ACTC converts independent triangles into triangle strips or fans. ANSYS 21.1 broadwell, skylake iris Utilities ANSYS simulation software enables organizations to confidently predict how their products will operate in the real world. We believe that every product is a promise of something greater. AOCC 3.1.0 epyc aion Compilers AMD Optimized C/C++ &amp; Fortran compilers (AOCC) based on LLVM 12.0 ASE 3.20.1 broadwell, epyc, skylake, gpu aion, iris Chemistry ASE is a python package providing an open source Atomic Simulation Environment in the Python scripting language. From version 3.20.1 we also include the ase-ext package, it contains optional reimplementations in C of functions in ASE.  ASE uses it automatically when installed. ASE 3.21.1 broadwell, epyc, skylake, gpu aion, iris Chemistry ASE is a python package providing an open source Atomic Simulation Environment in the Python scripting language. From version 3.20.1 we also include the ase-ext package, it contains optional reimplementations in C of functions in ASE.  ASE uses it automatically when installed. ATK 2.36.0 broadwell, epyc, skylake aion, iris Visualisation ATK provides the set of accessibility interfaces that are implemented by other toolkits and applications. Using the ATK interfaces, accessibility tools have full access to view and control running applications. Anaconda3 2020.11 broadwell, epyc, skylake aion, iris Programming Languages Built to complement the rich, open source Python community, the Anaconda platform provides an enterprise-ready data analytics platform that empowers companies to adopt a modern open data science analytics architecture. ArmForge 20.0.3 broadwell, skylake iris Utilities The industry standard development package for C, C++ and Fortran high performance code on Linux. Forge is designed to handle the complex software projects - including parallel, multiprocess and multithreaded code. Arm Forge combines an industry-leading debugger, Arm DDT, and an out-of-the-box-ready profiler, Arm MAP. ArmReports 20.0.3 broadwell, skylake iris Utilities Arm Performance Reports - a low-overhead tool that produces one-page text and HTML reports summarizing and characterizing both scalar and MPI application performance. Arm Performance Reports runs transparently on optimized production-ready codes by adding a single command to your scripts, and provides the most effective way to characterize and understand the performance of HPC application runs. Armadillo 10.5.3 broadwell, epyc, skylake aion, iris Numerical libraries Armadillo is an open-source C++ linear algebra library (matrix maths) aiming towards a good balance between speed and ease of use. Integer, floating point and complex numbers are supported, as well as a subset of trigonometric and statistics functions. Aspera-CLI 3.9.6 broadwell, epyc, skylake aion, iris Utilities IBM Aspera Command-Line Interface (the Aspera CLI) is a collection of Aspera tools for performing high-speed, secure data transfers from the command line. The Aspera CLI is for users and organizations who want to automate their transfer workflows. Autoconf 2.69 broadwell, skylake, gpu iris Development Autoconf is an extensible package of M4 macros that produce shell scripts to automatically configure software source code packages. These scripts can adapt the packages to many kinds of UNIX-like systems without manual user intervention. Autoconf creates a configuration script for a package from a template file that lists the operating system features that the package can use, in the form of M4 macro calls. Automake 1.16.2 broadwell, epyc, skylake, gpu aion, iris Development Automake: GNU Standards-compliant Makefile generator Autotools 20200321 broadwell, epyc, skylake, gpu aion, iris Development This bundle collect the standard GNU build tools: Autoconf, Automake and libtool BEDTools 2.30.0 broadwell, epyc, skylake aion, iris Biology BEDTools: a powerful toolset for genome arithmetic. The BEDTools utilities allow one to address common genomics tasks such as finding feature overlaps and computing coverage. The utilities are largely based on four widely-used file formats: BED, GFF/GTF, VCF, and SAM/BAM. BLAST+ 2.11.0 broadwell, epyc, skylake aion, iris Biology Basic Local Alignment Search Tool, or BLAST, is an algorithm for comparing primary biological sequence information, such as the amino-acid sequences of different proteins or the nucleotides of DNA sequences. BWA 0.7.17 broadwell, skylake iris Biology Burrows-Wheeler Aligner (BWA) is an efficient program that aligns relatively short nucleotide sequences against a long reference sequence such as the human genome. BamTools 2.5.1 broadwell, skylake iris Biology BamTools provides both a programmer's API and an end-user's toolkit for handling BAM files. Bazel 3.7.2 broadwell, epyc, skylake, gpu aion, iris Development Bazel is a build tool that builds code quickly and reliably. It is used to build the majority of Google's software. BioPerl 1.7.8 broadwell, epyc, skylake aion, iris Biology Bioperl is the product of a community effort to produce Perl code which is useful in biology. Examples include Sequence objects, Alignment objects and database searching objects. Bison 3.5.3 broadwell, epyc, skylake, gpu aion, iris Programming Languages Bison is a general-purpose parser generator that converts an annotated context-free grammar into a deterministic LR or generalized LR (GLR) parser employing LALR(1) parser tables. Bison 3.7.1 broadwell, epyc, skylake, gpu aion, iris Programming Languages Bison is a general-purpose parser generator that converts an annotated context-free grammar into a deterministic LR or generalized LR (GLR) parser employing LALR(1) parser tables. Boost.Python 1.74.0 broadwell, epyc, skylake aion, iris Libraries Boost.Python is a C++ library which enables seamless interoperability between C++ and the Python programming language. Boost 1.74.0 broadwell, epyc, skylake aion, iris Development Boost provides free peer-reviewed portable C++ source libraries. Bowtie2 2.4.2 broadwell, epyc, skylake aion, iris Biology Bowtie 2 is an ultrafast and memory-efficient tool for aligning sequencing reads to long reference sequences. It is particularly good at aligning reads of about 50 up to 100s or 1,000s of characters, and particularly good at aligning to relatively long (e.g. mammalian) genomes. Bowtie 2 indexes the genome with an FM Index to keep its memory footprint small: for the human genome, its memory footprint is typically around 3.2 GB. Bowtie 2 supports gapped, local, and paired-end alignment modes. CGAL 5.2 broadwell, epyc, skylake aion, iris Numerical libraries The goal of the CGAL Open Source Project is to provide easy access to efficient and reliable geometric algorithms in the form of a C++ library. CMake 3.18.4 broadwell, epyc, skylake, gpu aion, iris Development CMake, the cross-platform, open-source build system.  CMake is a family of tools designed to build, test and package software. CMake 3.20.1 broadwell, epyc, skylake, gpu aion, iris Development CMake, the cross-platform, open-source build system.  CMake is a family of tools designed to build, test and package software. CUDA 11.1.1 gpu iris System-level software CUDA (formerly Compute Unified Device Architecture) is a parallel computing platform and programming model created by NVIDIA and implemented by the graphics processing units (GPUs) that they produce. CUDA gives developers access to the virtual instruction set and memory of the parallel computational elements in CUDA GPUs. CUDAcore 11.1.1 gpu iris System-level software CUDA (formerly Compute Unified Device Architecture) is a parallel computing platform and programming model created by NVIDIA and implemented by the graphics processing units (GPUs) that they produce. CUDA gives developers access to the virtual instruction set and memory of the parallel computational elements in CUDA GPUs. Check 0.15.2 gpu iris Libraries Check is a unit testing framework for C. It features a simple interface for defining unit tests, putting little in the way of the developer. Tests are run in a separate address space, so both assertion failures and code errors that cause segmentation faults or other signals can be caught. Test results are reportable in the following: Subunit, TAP, XML, and a generic logging format. Clang 11.0.1 broadwell, epyc, skylake, gpu aion, iris Compilers C, C++, Objective-C compiler, based on LLVM.  Does not include C++ standard library -- use libstdc++ from GCC. DB 18.1.40 broadwell, epyc, skylake, gpu aion, iris Utilities Berkeley DB enables the development of custom data management solutions, without the overhead traditionally associated with such custom projects. DB_File 1.855 broadwell, epyc, skylake aion, iris Data processing Perl5 access to Berkeley DB version 1.x. DBus 1.13.18 broadwell, epyc, skylake aion, iris Development D-Bus is a message bus system, a simple way for applications to talk to one another.  In addition to interprocess communication, D-Bus helps coordinate process lifecycle; it makes it simple and reliable to code a \"single instance\" application or daemon, and to launch applications and daemons on demand when their services are needed. Dakota 6.15.0 broadwell, skylake iris Mathematics The Dakota project delivers both state-of-the-art research and robust, usable software for optimization and UQ. Broadly, the Dakota software's advanced parametric analyses enable design exploration, model calibration, risk analysis, and quantification of margins and uncertainty with computational models.\" Doxygen 1.8.20 broadwell, epyc, skylake, gpu aion, iris Development Doxygen is a documentation system for C++, C, Java, Objective-C, Python, IDL (Corba and Microsoft flavors), Fortran, VHDL, PHP, C#, and to some extent D. ELPA 2020.11.001 broadwell, epyc, skylake aion, iris Mathematics Eigenvalue SoLvers for Petaflop-Applications . EasyBuild 4.4.1 broadwell, epyc, skylake aion, iris Utilities EasyBuild is a software build and installation framework written in Python that allows you to install software in a structured, repeatable and robust way. EasyBuild 4.4.2 broadwell, epyc, skylake aion, iris Utilities EasyBuild is a software build and installation framework written in Python that allows you to install software in a structured, repeatable and robust way. EasyBuild 4.5.4 broadwell, epyc, skylake aion, iris Utilities EasyBuild is a software build and installation framework written in Python that allows you to install software in a structured, repeatable and robust way. Eigen 3.3.8 broadwell, epyc, skylake, gpu aion, iris Mathematics Eigen is a C++ template library for linear algebra: matrices, vectors, numerical solvers, and related algorithms. Eigen 3.4.0 broadwell, epyc, skylake, gpu aion, iris Mathematics Eigen is a C++ template library for linear algebra: matrices, vectors, numerical solvers, and related algorithms. Elk 7.0.12 broadwell, epyc, skylake aion, iris Physics An all-electron full-potential linearised augmented-plane wave (FP-LAPW) code with many advanced features. Written originally at Karl-Franzens-Universit\u00e4t Graz as a milestone of the EXCITING EU Research and Training Network, the code is designed to be as simple as possible so that new developments in the field of density functional theory (DFT) can be added quickly and reliably. FDS 6.7.6 broadwell, epyc, skylake aion, iris Physics Fire Dynamics Simulator (FDS) is a large-eddy simulation (LES) code for low-speed flows, with an emphasis on smoke and heat transport from fires. FFTW 3.3.8 broadwell, skylake, gpu iris Numerical libraries FFTW is a C subroutine library for computing the discrete Fourier transform (DFT) in one or more dimensions, of arbitrary input size, and of both real and complex data. FFmpeg 4.3.1 broadwell, epyc, skylake, gpu aion, iris Visualisation A complete, cross-platform solution to record, convert and stream audio and video. FLAC 1.3.3 broadwell, epyc, skylake, gpu aion, iris Libraries FLAC stands for Free Lossless Audio Codec, an audio format similar to MP3, but lossless, meaning that audio is compressed in FLAC without any loss in quality. FLTK 1.3.5 broadwell, skylake iris Visualisation FLTK is a cross-platform C++ GUI toolkit for UNIX/Linux (X11), Microsoft Windows, and MacOS X. FLTK provides modern GUI functionality without the bloat and supports 3D graphics via OpenGL and its built-in GLUT emulation. FastQC 0.11.9 broadwell, skylake iris Biology FastQC is a quality control application for high throughput sequence data. It reads in sequence data in a variety of formats and can either provide an interactive application to review the results of several different QC checks, or create an HTML based report which can be integrated into a pipeline. Flask 1.1.2 broadwell, epyc, skylake, gpu aion, iris Libraries Flask is a lightweight WSGI web application framework. It is designed to make getting started quick and easy, with the ability to scale up to complex applications. This module includes the Flask extensions: Flask-Cors Flink 1.11.2 broadwell, epyc, skylake aion, iris Development Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale. FreeImage 3.18.0 broadwell, epyc, skylake aion, iris Visualisation FreeImage is an Open Source library project for developers who would like to support popular graphics image formats like PNG, BMP, JPEG, TIFF and others as needed by today's multimedia applications. FreeImage is easy to use, fast, multithreading safe. FriBidi 1.0.10 broadwell, epyc, skylake, gpu aion, iris Programming Languages The Free Implementation of the Unicode Bidirectional Algorithm. GCC 10.2.0 broadwell, epyc, skylake, gpu aion, iris Compilers The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Java, and Ada, as well as libraries for these languages (libstdc++, libgcj,...). GCCcore 10.2.0 broadwell, epyc, skylake, gpu aion, iris Compilers The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Java, and Ada, as well as libraries for these languages (libstdc++, libgcj,...). GDAL 3.2.1 broadwell, epyc, skylake, gpu aion, iris Data processing GDAL is a translator library for raster geospatial data formats that is released under an X/MIT style Open Source license by the Open Source Geospatial Foundation. As a library, it presents a single abstract data model to the calling application for all supported formats. It also comes with a variety of useful commandline utilities for data translation and processing. GDB 10.1 broadwell, epyc, skylake aion, iris Debugging The GNU Project Debugger GDRCopy 2.1 gpu iris Libraries A low-latency GPU memory copy library based on NVIDIA GPUDirect RDMA technology. GEOS 3.9.1 broadwell, epyc, skylake, gpu aion, iris Mathematics GEOS (Geometry Engine - Open Source) is a C++ port of the Java Topology Suite (JTS) GLPK 4.65 broadwell, skylake iris Utilities The GLPK (GNU Linear Programming Kit) package is intended for solving large-scale linear programming (LP), mixed integer programming (MIP), and other related problems. It is a set of routines written in ANSI C and organized in the form of a callable library. GLib 2.66.1 broadwell, epyc, skylake, gpu aion, iris Visualisation GLib is one of the base libraries of the GTK+ project GMP 6.2.0 broadwell, epyc, skylake, gpu aion, iris Mathematics GMP is a free library for arbitrary precision arithmetic, operating on signed integers, rational numbers, and floating point numbers. GObject-Introspection 1.66.1 broadwell, epyc, skylake aion, iris Development GObject introspection is a middleware layer between C libraries (using GObject) and language bindings. The C library can be scanned at compile time and generate a metadata file, in addition to the actual native C library. Then at runtime, language bindings can read this metadata and automatically provide bindings to call into the C library. GROMACS 2021 broadwell, epyc, skylake aion, iris Biology GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. This is a CPU only build, containing both MPI and threadMPI builds for both single and double precision. It also contains the gmxapi extension for the single precision MPI build. GROMACS 2021.2 broadwell, epyc, skylake aion, iris Biology GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. This is a CPU only build, containing both MPI and threadMPI builds for both single and double precision. It also contains the gmxapi extension for the single precision MPI build. GSL 2.6 broadwell, skylake, gpu iris Numerical libraries The GNU Scientific Library (GSL) is a numerical library for C and C++ programmers. The library provides a wide range of mathematical routines such as random number generators, special functions and least-squares fitting. GTK+ 3.24.23 broadwell, epyc, skylake aion, iris Visualisation GTK+ is the primary library used to construct user interfaces in GNOME. It provides all the user interface controls, or widgets, used in a common graphical application. Its object-oriented API allows you to construct user interfaces without dealing with the low-level details of drawing and device interaction. Gdk-Pixbuf 2.40.0 broadwell, epyc, skylake aion, iris Visualisation The Gdk Pixbuf is a toolkit for image loading and pixel buffer manipulation. It is used by GTK+ 2 and GTK+ 3 to load and manipulate images. In the past it was distributed as part of GTK+ 2 but it was split off into a separate package in preparation for the change to GTK+ 3. Ghostscript 9.53.3 broadwell, epyc, skylake, gpu aion, iris Utilities Ghostscript is a versatile processor for PostScript data with the ability to render PostScript to different targets. It used to be part of the cups printing stack, but is no longer used for that. Go 1.14.1 broadwell, skylake iris Compilers Go is an open source programming language that makes it easy to build simple, reliable, and efficient software. Go 1.16.6 broadwell, epyc, skylake aion, iris Compilers Go is an open source programming language that makes it easy to build simple, reliable, and efficient software. Gurobi 9.1.2 broadwell, epyc, skylake aion, iris Mathematics The Gurobi Optimizer is a state-of-the-art solver for mathematical programming. The solvers in the Gurobi Optimizer were designed from the ground up to exploit modern architectures and multi-core processors, using the most advanced implementations of the latest algorithms. HDF5 1.10.7 broadwell, epyc, skylake, gpu aion, iris Data processing HDF5 is a data model, library, and file format for storing and managing data. It supports an unlimited variety of datatypes, and is designed for flexible and efficient I/O and for high volume and complex data. HDF 4.2.15 broadwell, epyc, skylake, gpu aion, iris Data processing HDF (also known as HDF4) is a library and multi-object file format for storing and managing data between machines. HTSlib 1.12 broadwell, epyc, skylake aion, iris Biology A C library for reading/writing high-throughput sequencing data. This package includes the utilities bgzip and tabix Hadoop 2.10.0 broadwell, epyc, skylake aion, iris Utilities Hadoop MapReduce by Cloudera HarfBuzz 2.6.7 broadwell, epyc, skylake aion, iris Visualisation HarfBuzz is an OpenType text shaping engine. Horovod 0.22.0 gpu iris Utilities Horovod is a distributed training framework for TensorFlow. Hypre 2.20.0 broadwell, epyc, skylake aion, iris Numerical libraries Hypre is a library for solving large, sparse linear systems of equations on massively parallel computers. The problems of interest arise in the simulation codes being developed at LLNL and elsewhere to study physical phenomena in the defense, environmental, energy, and biological sciences. ICU 67.1 broadwell, epyc, skylake, gpu aion, iris Libraries ICU is a mature, widely used set of C/C++ and Java libraries providing Unicode and Globalization support for software applications. ISL 0.23 broadwell, epyc, skylake aion, iris Mathematics isl is a library for manipulating sets and relations of integer points bounded by linear constraints. ImageMagick 7.0.10-35 broadwell, epyc, skylake, gpu aion, iris Visualisation ImageMagick is a software suite to create, edit, compose, or convert bitmap images JasPer 2.0.24 broadwell, epyc, skylake, gpu aion, iris Visualisation The JasPer Project is an open-source initiative to provide a free software-based reference implementation of the codec specified in the JPEG-2000 Part-1 standard. Java 1.8.0_241 broadwell, skylake, gpu iris Programming Languages Java Platform, Standard Edition (Java SE) lets you develop and deploy Java applications on desktops and servers. Java 11.0.2 broadwell, epyc, skylake aion, iris Programming Languages Java Platform, Standard Edition (Java SE) lets you develop and deploy Java applications on desktops and servers. Java 13.0.2 broadwell, epyc, skylake aion, iris Programming Languages Java Platform, Standard Edition (Java SE) lets you develop and deploy Java applications on desktops and servers. Java 16.0.1 broadwell, epyc, skylake aion, iris Programming Languages Java Platform, Standard Edition (Java SE) lets you develop and deploy Java applications on desktops and servers. JsonCpp 1.9.4 broadwell, epyc, skylake, gpu aion, iris Libraries JsonCpp is a C++ library that allows manipulating JSON values, including serialization and deserialization to and from strings. It can also preserve existing comment in unserialization/serialization steps, making it a convenient format to store user input files. Julia 1.6.2 broadwell, epyc, skylake aion, iris Programming Languages Julia is a high-level, high-performance dynamic programming language for numerical computing Keras 2.4.3 broadwell, epyc, skylake, gpu aion, iris Mathematics Keras is a deep learning API written in Python, running on top of the machine learning platform TensorFlow. LAME 3.100 broadwell, skylake, gpu iris Data processing LAME is a high quality MPEG Audio Layer III (MP3) encoder licensed under the LGPL. LLVM 10.0.1 broadwell, epyc, skylake, gpu aion, iris Compilers The LLVM Core libraries provide a modern source- and target-independent optimizer, along with code generation support for many popular CPUs (as well as some less common ones!) These libraries are built around a well specified code representation known as the LLVM intermediate representation (\"LLVM IR\"). The LLVM Core libraries are well documented, and it is particularly easy to invent your own language (or port an existing compiler) to use LLVM as an optimizer and code generator. LLVM 11.0.0 broadwell, epyc, skylake, gpu aion, iris Compilers The LLVM Core libraries provide a modern source- and target-independent optimizer, along with code generation support for many popular CPUs (as well as some less common ones!) These libraries are built around a well specified code representation known as the LLVM intermediate representation (\"LLVM IR\"). The LLVM Core libraries are well documented, and it is particularly easy to invent your own language (or port an existing compiler) to use LLVM as an optimizer and code generator. LMDB 0.9.24 broadwell, skylake, gpu iris Libraries LMDB is a fast, memory-efficient database. With memory-mapped files, it has the read performance of a pure in-memory database while retaining the persistence of standard disk-based databases. LibTIFF 4.1.0 broadwell, epyc, skylake, gpu aion, iris Libraries tiff: Library and tools for reading and writing TIFF data files LittleCMS 2.11 broadwell, epyc, skylake, gpu aion, iris Visualisation Little CMS intends to be an OPEN SOURCE small-footprint color management engine, with special focus on accuracy and performance. Lua 5.4.2 broadwell, epyc, skylake aion, iris Programming Languages Lua is a powerful, fast, lightweight, embeddable scripting language. Lua combines simple procedural syntax with powerful data description constructs based on associative arrays and extensible semantics. Lua is dynamically typed, runs by interpreting bytecode for a register-based virtual machine, and has automatic memory management with incremental garbage collection, making it ideal for configuration, scripting, and rapid prototyping. M4 1.4.18 broadwell, skylake, gpu iris Development GNU M4 is an implementation of the traditional Unix macro processor. It is mostly SVR4 compatible although it has some extensions (for example, handling more than 9 positional parameters to macros). GNU M4 also has built-in functions for including files, running shell commands, doing arithmetic, etc. MATLAB 2021a broadwell, epyc, skylake aion, iris Mathematics MATLAB is a high-level language and interactive environment that enables you to perform computationally intensive tasks faster than with traditional programming languages such as C, C++, and Fortran. METIS 5.1.0 broadwell, skylake iris Mathematics METIS is a set of serial programs for partitioning graphs, partitioning finite element meshes, and producing fill reducing orderings for sparse matrices. The algorithms implemented in METIS are based on the multilevel recursive-bisection, multilevel k-way, and multi-constraint partitioning schemes. MPC 1.2.1 broadwell, epyc, skylake aion, iris Mathematics Gnu Mpc is a C library for the arithmetic of complex numbers with arbitrarily high precision and correct rounding of the result. It extends the principles of the IEEE-754 standard for fixed precision real floating point numbers to complex numbers, providing well-defined semantics for every operation. At the same time, speed of operation at high precision is a major design goal. MPFR 4.1.0 broadwell, epyc, skylake, gpu aion, iris Mathematics The MPFR library is a C library for multiple-precision floating-point computations with correct rounding. MUMPS 5.3.5 broadwell, epyc, skylake aion, iris Mathematics A parallel sparse direct solver Mako 1.1.3 broadwell, epyc, skylake, gpu aion, iris Development A super-fast templating language that borrows the best ideas from the existing templating languages Mathematica 12.1.0 broadwell, epyc, skylake aion, iris Mathematics Mathematica is a computational software program used in many scientific, engineering, mathematical and computing fields. Maven 3.6.3 broadwell, skylake iris Development Binary maven install, Apache Maven is a software project management and comprehension tool. Based on the concept of a project object model (POM), Maven can manage a project's build, reporting and documentation from a central piece of information. Mesa 20.2.1 broadwell, epyc, skylake, gpu aion, iris Visualisation Mesa is an open-source implementation of the OpenGL specification - a system for rendering interactive 3D graphics. Meson 0.55.3 broadwell, epyc, skylake, gpu aion, iris Utilities Meson is a cross-platform build system designed to be both as fast and as user friendly as possible. NASM 2.15.05 broadwell, epyc, skylake, gpu aion, iris Programming Languages NASM: General-purpose x86 assembler NCCL 2.8.3 gpu iris Libraries The NVIDIA Collective Communications Library (NCCL) implements multi-GPU and multi-node collective communication primitives that are performance optimized for NVIDIA GPUs. NLopt 2.6.2 broadwell, epyc, skylake, gpu aion, iris Numerical libraries NLopt is a free/open-source library for nonlinear optimization, providing a common interface for a number of different free optimization routines available online as well as original implementations of various other algorithms. NSPR 4.29 broadwell, epyc, skylake aion, iris Libraries Netscape Portable Runtime (NSPR) provides a platform-neutral API for system level and libc-like functions. NSS 3.57 broadwell, epyc, skylake aion, iris Libraries Network Security Services (NSS) is a set of libraries designed to support cross-platform development of security-enabled client and server applications. Ninja 1.10.1 broadwell, epyc, skylake, gpu aion, iris Utilities Ninja is a small build system with a focus on speed. OpenBLAS 0.3.12 broadwell, epyc, skylake, gpu aion, iris Numerical libraries OpenBLAS is an optimized BLAS library based on GotoBLAS2 1.13 BSD version. OpenCV 4.5.1 broadwell, epyc, skylake aion, iris Visualisation OpenCV (Open Source Computer Vision Library) is an open source computer vision and machine learning software library. OpenCV was built to provide a common infrastructure for computer vision applications and to accelerate the use of machine perception in the commercial products. Includes extra modules for OpenCV from the contrib repository. OpenEXR 2.5.5 broadwell, epyc, skylake aion, iris Visualisation OpenEXR is a high dynamic-range (HDR) image file format developed by Industrial Light &amp; Magic for use in computer imaging applications OpenFOAM 8 epyc aion CFD/Finite element modelling OpenFOAM is a free, open source CFD software package. OpenFOAM has an extensive range of features to solve anything from complex fluid flows involving chemical reactions, turbulence and heat transfer, to solid dynamics and electromagnetics. OpenMPI 4.0.5 broadwell, epyc, skylake, gpu aion, iris MPI The Open MPI Project is an open source MPI-3 implementation. PAPI 6.0.0 broadwell, skylake iris Performance measurements PAPI provides the tool designer and application engineer with a consistent interface and methodology for use of the performance counter hardware found in most major microprocessors. PAPI enables software engineers to see, in near real time, the relation between software performance and processor events. In addition Component PAPI provides access to a collection of components that expose performance measurement opportunites across the hardware and software stack. PCRE2 10.35 broadwell, epyc, skylake, gpu aion, iris Development The PCRE library is a set of functions that implement regular expression pattern matching using the same syntax and semantics as Perl 5. PCRE 8.44 broadwell, epyc, skylake, gpu aion, iris Development The PCRE library is a set of functions that implement regular expression pattern matching using the same syntax and semantics as Perl 5. PETSc 3.14.4 broadwell, epyc, skylake aion, iris Numerical libraries PETSc, pronounced PET-see (the S is silent), is a suite of data structures and routines for the scalable (parallel) solution of scientific applications modeled by partial differential equations. PLUMED 2.7.0 broadwell, epyc, skylake aion, iris Chemistry PLUMED is an open source library for free energy calculations in molecular systems which works together with some of the most popular molecular dynamics engines. Free energy calculations can be performed as a function of many order parameters with a particular  focus on biological problems, using state of the art methods such as metadynamics, umbrella sampling and Jarzynski-equation based steered MD. The software, written in C++, can be easily interfaced with both fortran and C/C++ codes. POV-Ray 3.7.0.8 broadwell, epyc, skylake aion, iris Visualisation The Persistence of Vision Raytracer, or POV-Ray, is a ray tracing program which generates images from a text-based scene description, and is available for a variety of computer platforms. POV-Ray is a high-quality, Free Software tool for creating stunning three-dimensional graphics. The source code is available for those wanting to do their own ports. PROJ 7.2.1 broadwell, epyc, skylake, gpu aion, iris Libraries Program proj is a standard Unix filter function which converts geographic longitude and latitude coordinates into cartesian coordinates Pango 1.47.0 broadwell, epyc, skylake aion, iris Visualisation Pango is a library for laying out and rendering of text, with an emphasis on internationalization. Pango can be used anywhere that text layout is needed, though most of the work on Pango so far has been done in the context of the GTK+ widget toolkit. Pango forms the core of text and font handling for GTK+-2.x. ParaView 5.8.1 broadwell, epyc, skylake aion, iris Visualisation ParaView is a scientific parallel visualizer. Perl 5.32.0 broadwell, epyc, skylake, gpu aion, iris Programming Languages Larry Wall's Practical Extraction and Report Language This is a minimal build without any modules. Should only be used for build dependencies. Pillow 8.0.1 broadwell, epyc, skylake, gpu aion, iris Visualisation Pillow is the 'friendly PIL fork' by Alex Clark and Contributors. PIL is the Python Imaging Library by Fredrik Lundh and Contributors. PyOpenGL 3.1.5 broadwell, epyc, skylake aion, iris Visualisation PyOpenGL is the most common cross platform Python binding to OpenGL and related APIs. PyQt5 5.15.1 broadwell, epyc, skylake aion, iris Visualisation PyQt5 is a set of Python bindings for v5 of the Qt application framework from The Qt Company. This bundle includes PyQtWebEngine, a set of Python bindings for The Qt Company\u2019s Qt WebEngine framework. PyQtGraph 0.11.1 broadwell, epyc, skylake aion, iris Visualisation PyQtGraph is a pure-python graphics and GUI library built on PyQt5/PySide2 and numpy. PyTorch-Geometric 1.6.3 broadwell, epyc, skylake, gpu aion, iris Libraries PyTorch Geometric (PyG) is a geometric deep learning extension library for PyTorch. PyTorch 1.7.1 gpu iris Development Tensors and Dynamic neural networks in Python with strong GPU acceleration. PyTorch is a deep learning framework that puts Python first. PyTorch 1.8.1 broadwell, epyc, skylake, gpu aion, iris Development Tensors and Dynamic neural networks in Python with strong GPU acceleration. PyTorch is a deep learning framework that puts Python first. PyTorch 1.9.0 broadwell, epyc, skylake, gpu aion, iris Development Tensors and Dynamic neural networks in Python with strong GPU acceleration. PyTorch is a deep learning framework that puts Python first. PyYAML 5.3.1 broadwell, epyc, skylake, gpu aion, iris Libraries PyYAML is a YAML parser and emitter for the Python programming language. Python 2.7.18 broadwell, epyc, skylake, gpu aion, iris Programming Languages Python is a programming language that lets you work more quickly and integrate your systems more effectively. Python 3.8.6 broadwell, epyc, skylake, gpu aion, iris Programming Languages Python is a programming language that lets you work more quickly and integrate your systems more effectively. Qt5 5.14.2 broadwell, epyc, skylake aion, iris Development Qt is a comprehensive cross-platform C++ application framework. QuantumESPRESSO 6.7 broadwell iris Chemistry Quantum ESPRESSO  is an integrated suite of computer codes for electronic-structure calculations and materials modeling at the nanoscale. It is based on density-functional theory, plane waves, and pseudopotentials (both norm-conserving and ultrasoft). RDFlib 5.0.0 broadwell, epyc, skylake, gpu aion, iris Libraries RDFLib is a Python library for working with RDF, a simple yet powerful language for representing information. R 4.0.5 broadwell, epyc, skylake, gpu aion, iris Programming Languages R is a free software environment for statistical computing and graphics. ReFrame 3.6.3 broadwell, epyc, skylake aion, iris Development ReFrame is a framework for writing regression tests for HPC systems. Ruby 2.7.2 broadwell, epyc, skylake aion, iris Programming Languages Ruby is a dynamic, open source programming language with a focus on simplicity and productivity. It has an elegant syntax that is natural to read and easy to write. SAMtools 1.12 broadwell, epyc, skylake aion, iris Biology SAM Tools provide various utilities for manipulating alignments in the SAM format, including sorting, merging, indexing and generating alignments in a per-position format. SCOTCH 6.1.0 broadwell, epyc, skylake aion, iris Mathematics Software package and libraries for sequential and parallel graph partitioning, static mapping, and sparse matrix block ordering, and sequential mesh and hypergraph partitioning. SDL2 2.0.14 broadwell, epyc, skylake aion, iris Libraries SDL: Simple DirectMedia Layer, a cross-platform multimedia library SLEPc 3.14.2 broadwell, epyc, skylake aion, iris Numerical libraries SLEPc (Scalable Library for Eigenvalue Problem Computations) is a software library for the solution of large scale sparse eigenvalue problems on parallel computers. It is an extension of PETSc and can be used for either standard or generalized eigenproblems, with real or complex arithmetic. It can also be used for computing a partial SVD of a large, sparse, rectangular matrix, and to solve quadratic eigenvalue problems. SQLite 3.33.0 broadwell, epyc, skylake, gpu aion, iris Development SQLite: SQL Database Engine in a C Library SWIG 4.0.2 broadwell, epyc, skylake aion, iris Development SWIG is a software development tool that connects programs written in C and C++ with a variety of high-level programming languages. Salome 9.8.0 broadwell, epyc, skylake aion, iris CFD/Finite element modelling The SALOME platform is an open source software framework for pre- and post-processing and integration of numerical solvers from various scientific fields. CEA and EDF use SALOME to perform a large number of simulations, typically related to power plant equipment and alternative energy. To address these challenges, SALOME includes a CAD/CAE modelling tool, mesh generators, an advanced 3D visualization tool, etc. ScaLAPACK 2.1.0 broadwell, epyc, skylake, gpu aion, iris Numerical libraries The ScaLAPACK (or Scalable LAPACK) library includes a subset of LAPACK routines redesigned for distributed memory MIMD parallel computers. SciPy-bundle 2020.11 broadwell, epyc, skylake, gpu aion, iris Programming Languages Bundle of Python packages for scientific software Singularity 3.8.1 broadwell, epyc, skylake aion, iris Utilities SingularityCE is an open source container platform designed to be simple, fast, and secure.  Singularity is optimized for EPC and HPC workloads, allowing untrusted users to run untrusted containers in a trusted way. Spack 0.12.1 broadwell, skylake iris Development Spack is a package manager for supercomputers, Linux, and macOS. It makes installing scientific software easy. With Spack, you can build a package with multiple versions, configurations, platforms, and compilers, and all of these builds can coexist on the same machine. Stata 17 broadwell, epyc, skylake aion, iris Mathematics Stata is a complete, integrated statistical software package that provides everything you need for data analysis, data management, and graphics. SuiteSparse 5.8.1 broadwell, epyc, skylake aion, iris Numerical libraries SuiteSparse is a collection of libraries manipulate sparse matrices. Szip 2.1.1 broadwell, skylake, gpu iris Utilities Szip compression software, providing lossless compression of scientific data Tcl 8.6.10 broadwell, epyc, skylake, gpu aion, iris Programming Languages Tcl (Tool Command Language) is a very powerful but easy to learn dynamic programming language, suitable for a very wide range of uses, including web and desktop applications, networking, administration, testing and many more. TensorFlow 2.4.1 broadwell, epyc, skylake, gpu aion, iris Libraries An open-source software library for Machine Intelligence TensorFlow 2.5.0 broadwell, epyc, skylake, gpu aion, iris Libraries An open-source software library for Machine Intelligence Theano 1.1.2 broadwell, epyc, skylake, gpu aion, iris Mathematics Theano is a Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. Tk 8.6.10 broadwell, epyc, skylake, gpu aion, iris Visualisation Tk is an open source, cross-platform widget toolchain that provides a library of basic elements for building a graphical user interface (GUI) in many different programming languages. Tkinter 3.8.6 broadwell, epyc, skylake, gpu aion, iris Programming Languages Tkinter module, built with the Python buildsystem TopHat 2.1.2 broadwell, skylake iris Biology TopHat is a fast splice junction mapper for RNA-Seq reads. UCX 1.9.0 broadwell, epyc, skylake, gpu aion, iris Libraries Unified Communication X An open-source production grade communication framework for data centric and high-performance applications UDUNITS 2.2.26 broadwell, skylake, gpu iris Physics UDUNITS supports conversion of unit specifications between formatted and binary forms, arithmetic manipulation of units, and conversion of values between compatible scales of measurement. ULHPC-bd 2020b broadwell, epyc, skylake aion, iris System-level software Generic Module bundle for BigData Analytics software in use on the UL HPC Facility ULHPC-bio 2020b broadwell, epyc, skylake aion, iris System-level software Generic Module bundle for Bioinformatics, biology and biomedical software in use on the UL HPC Facility, especially at LCSB ULHPC-cs 2020b epyc aion System-level software Generic Module bundle for Computational science software in use on the UL HPC Facility, including: - Computer Aided Engineering, incl. CFD - Chemistry, Computational Chemistry and Quantum Chemistry - Data management &amp; processing tools - Earth Sciences - Quantum Computing - Physics and physical systems simulations ULHPC-dl 2020b broadwell, epyc, skylake aion, iris System-level software Generic Module bundle for (CPU-version) of AI / Deep Learning / Machine Learning software in use on the UL HPC Facility ULHPC-gpu 2020b gpu iris System-level software Generic Module bundle for GPU accelerated User Software in use on the UL HPC Facility ULHPC-math 2020b broadwell, epyc, skylake aion, iris System-level software Generic Module bundle for  High-level mathematical software and Linear Algrebra libraries in use on the UL HPC Facility ULHPC-toolchains 2020b broadwell, epyc, skylake aion, iris System-level software Generic Module bundle that contains all the dependencies required to enable toolchains and building tools/programming language in use on the UL HPC Facility ULHPC-tools 2020b broadwell, epyc, skylake aion, iris System-level software Misc tools, incl. - perf:      Performance tools - tools:     General purpose tools UnZip 6.0 broadwell, epyc, skylake, gpu aion, iris Utilities UnZip is an extraction utility for archives compressed in .zip format (also called \"zipfiles\"). Although highly compatible both with PKWARE's PKZIP and PKUNZIP utilities for MS-DOS and with Info-ZIP's own Zip program, our primary objectives have been portability and non-MSDOS functionality. VASP 5.4.4 broadwell, skylake iris Physics The Vienna Ab initio Simulation Package (VASP) is a computer program for atomic scale materials modelling, e.g. electronic structure calculations and quantum-mechanical molecular dynamics, from first principles. VASP 6.2.1 broadwell, epyc, skylake aion, iris Physics The Vienna Ab initio Simulation Package (VASP) is a computer program for atomic scale materials modelling, e.g. electronic structure calculations and quantum-mechanical molecular dynamics, from first principles. VMD 1.9.4a51 broadwell, epyc, skylake aion, iris Visualisation VMD is a molecular visualization program for displaying, animating, and analyzing large biomolecular systems using 3-D graphics and built-in scripting. VTK 9.0.1 broadwell, epyc, skylake aion, iris Visualisation The Visualization Toolkit (VTK) is an open-source, freely available software system for 3D computer graphics, image processing and visualization. VTK consists of a C++ class library and several interpreted interface layers including Tcl/Tk, Java, and Python. VTK supports a wide variety of visualization algorithms including: scalar, vector, tensor, texture, and volumetric methods; and advanced modeling techniques such as: implicit modeling, polygon reduction, mesh smoothing, cutting, contouring, and Delaunay triangulation. VTune 2020_update3 broadwell, epyc, skylake aion, iris Utilities Intel VTune Amplifier XE is the premier performance profiler for C, C++, C#, Fortran, Assembly and Java. Valgrind 3.16.1 broadwell, epyc, skylake aion, iris Debugging Valgrind: Debugging and profiling tools Wannier90 3.1.0 broadwell, epyc, skylake aion, iris Chemistry A tool for obtaining maximally-localised Wannier functions X11 20201008 broadwell, epyc, skylake, gpu aion, iris Visualisation The X Window System (X11) is a windowing system for bitmap displays XML-LibXML 2.0206 broadwell, epyc, skylake aion, iris Data processing Perl binding for libxml2 XZ 5.2.5 broadwell, epyc, skylake, gpu aion, iris Utilities xz: XZ utilities Xvfb 1.20.9 broadwell, epyc, skylake, gpu aion, iris Visualisation Xvfb is an X server that can run on machines with no display hardware and no physical input devices. It emulates a dumb framebuffer using virtual memory. YACS 0.1.8 broadwell, epyc, skylake aion, iris Libraries YACS was created as a lightweight library to define and manage system configurations, such as those commonly found in software designed for scientific experimentation. These \"configurations\" typically cover concepts like hyperparameters used in training a machine learning model or configurable model hyperparameters, such as the depth of a convolutional neural network. Yasm 1.3.0 broadwell, skylake, gpu iris Programming Languages Yasm: Complete rewrite of the NASM assembler with BSD license Z3 4.8.10 broadwell, epyc, skylake, gpu aion, iris Utilities Z3 is a theorem prover from Microsoft Research. Zip 3.0 broadwell, skylake, gpu iris Utilities Zip is a compression and file packaging/archive utility. Although highly compatible both with PKWARE's PKZIP and PKUNZIP utilities for MS-DOS and with Info-ZIP's own UnZip, our primary objectives have been portability and other-than-MSDOS functionality ant 1.10.9 broadwell, epyc, skylake aion, iris Development Apache Ant is a Java library and command-line tool whose mission is to drive processes described in build files as targets and extension points dependent upon each other. The main known usage of Ant is the build of Java applications. arpack-ng 3.8.0 broadwell, epyc, skylake aion, iris Numerical libraries ARPACK is a collection of Fortran77 subroutines designed to solve large scale eigenvalue problems. at-spi2-atk 2.38.0 broadwell, epyc, skylake aion, iris Visualisation AT-SPI 2 toolkit bridge at-spi2-core 2.38.0 broadwell, epyc, skylake aion, iris Visualisation Assistive Technology Service Provider Interface. binutils 2.35 broadwell, epyc, skylake, gpu aion, iris Utilities binutils: GNU binary utilities bokeh 2.2.3 broadwell, epyc, skylake, gpu aion, iris Utilities Statistical and novel interactive HTML plots for Python bzip2 1.0.8 broadwell, skylake, gpu iris Utilities bzip2 is a freely available, patent free, high-quality data compressor. It typically compresses files to within 10% to 15% of the best available techniques (the PPM family of statistical compressors), whilst being around twice as fast at compression and six times faster at decompression. cURL 7.72.0 broadwell, epyc, skylake, gpu aion, iris Utilities libcurl is a free and easy-to-use client-side URL transfer library, supporting DICT, FILE, FTP, FTPS, Gopher, HTTP, HTTPS, IMAP, IMAPS, LDAP, LDAPS, POP3, POP3S, RTMP, RTSP, SCP, SFTP, SMTP, SMTPS, Telnet and TFTP. libcurl supports SSL certificates, HTTP POST, HTTP PUT, FTP uploading, HTTP form based upload, proxies, cookies, user+password authentication (Basic, Digest, NTLM, Negotiate, Kerberos), file transfer resume, http proxy tunneling and more. cairo 1.16.0 broadwell, skylake, gpu iris Visualisation Cairo is a 2D graphics library with support for multiple output devices. Currently supported output targets include the X Window System (via both Xlib and XCB), Quartz, Win32, image buffers, PostScript, PDF, and SVG file output. Experimental backends include OpenGL, BeOS, OS/2, and DirectFB cuDNN 8.0.4.30 gpu iris Numerical libraries The NVIDIA CUDA Deep Neural Network library (cuDNN) is a GPU-accelerated library of primitives for deep neural networks. cuDNN 8.0.5.39 gpu iris Numerical libraries The NVIDIA CUDA Deep Neural Network library (cuDNN) is a GPU-accelerated library of primitives for deep neural networks. dask 2021.2.0 broadwell, epyc, skylake, gpu aion, iris Data processing Dask natively scales Python. Dask provides advanced parallelism for analytics, enabling performance at scale for the tools you love. double-conversion 3.1.5 broadwell, epyc, skylake, gpu aion, iris Libraries Efficient binary-decimal and decimal-binary conversion routines for IEEE doubles. elfutils 0.183 gpu iris Libraries The elfutils project provides libraries and tools for ELF files and DWARF data. expat 2.2.9 broadwell, epyc, skylake, gpu aion, iris Utilities Expat is an XML parser library written in C. It is a stream-oriented parser in which an application registers handlers for things the parser might find in the XML document (like start tags) flatbuffers-python 1.12 broadwell, epyc, skylake, gpu aion, iris Development Python Flatbuffers runtime library. flatbuffers 1.12.0 broadwell, skylake, gpu iris Development FlatBuffers: Memory Efficient Serialization Library flex 2.6.4 broadwell, skylake, gpu iris Programming Languages Flex (Fast Lexical Analyzer) is a tool for generating scanners. A scanner, sometimes called a tokenizer, is a program which recognizes lexical patterns in text. fontconfig 2.13.92 broadwell, epyc, skylake, gpu aion, iris Visualisation Fontconfig is a library designed to provide system-wide font configuration, customization and application access. foss 2020b broadwell, epyc, skylake aion, iris Toolchains (software stacks) GNU Compiler Collection (GCC) based compiler toolchain, including OpenMPI for MPI support, OpenBLAS (BLAS and LAPACK support), FFTW and ScaLAPACK. fosscuda 2020b gpu iris Toolchains (software stacks) GCC based compiler toolchain with CUDA support, and including OpenMPI for MPI support, OpenBLAS (BLAS and LAPACK support), FFTW and ScaLAPACK. freetype 2.10.3 broadwell, epyc, skylake, gpu aion, iris Visualisation FreeType 2 is a software font engine that is designed to be small, efficient, highly customizable, and portable while capable of producing high-quality output (glyph images). It can be used in graphics libraries, display servers, font conversion tools, text image generation tools, and many other products as well. gcccuda 2020b gpu iris Toolchains (software stacks) GNU Compiler Collection (GCC) based compiler toolchain, along with CUDA toolkit. gettext 0.21 broadwell, epyc, skylake, gpu aion, iris Utilities GNU 'gettext' is an important step for the GNU Translation Project, as it is an asset on which we may build many other steps. This package offers to programmers, translators, and even users, a well integrated set of tools and documentation giflib 5.2.1 broadwell, skylake, gpu iris Libraries giflib is a library for reading and writing gif images. It is API and ABI compatible with libungif which was in wide use while the LZW compression algorithm was patented. git 2.28.0 broadwell, epyc, skylake, gpu aion, iris Utilities Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. gmsh 4.8.4 broadwell, epyc, skylake aion, iris Mathematics Gmsh is a 3D finite element grid generator with a build-in CAD engine and post-processor. gnuplot 5.4.1 broadwell, epyc, skylake aion, iris Visualisation Portable interactive, function plotting utility gocryptfs 2.0.1 broadwell, epyc, skylake aion, iris Utilities Encrypted overlay filesystem written in Go. gocryptfs uses file-based encryption that is implemented as a mountable FUSE filesystem. Each file in gocryptfs is stored as one corresponding encrypted file on the hard disk. gompi 2020b broadwell, epyc, skylake aion, iris Toolchains (software stacks) GNU Compiler Collection (GCC) based compiler toolchain, including OpenMPI for MPI support. gompic 2020b gpu iris Toolchains (software stacks) GNU Compiler Collection (GCC) based compiler toolchain along with CUDA toolkit, including OpenMPI for MPI support with CUDA features enabled. gperf 3.1 broadwell, skylake, gpu iris Development GNU gperf is a perfect hash function generator. For a given list of strings, it produces a hash function and hash table, in form of C or C++ code, for looking up a value depending on the input string. The hash function is perfect, which means that the hash table has no collisions, and the hash table lookup needs a single string comparison only. groff 1.22.4 broadwell, epyc, skylake, gpu aion, iris Utilities Groff (GNU troff) is a typesetting system that reads plain text mixed with formatting commands and produces formatted output. gzip 1.10 broadwell, skylake iris Utilities gzip (GNU zip) is a popular data compression program as a replacement for compress h5py 3.1.0 broadwell, epyc, skylake, gpu aion, iris Data processing HDF5 for Python (h5py) is a general-purpose Python interface to the Hierarchical Data Format library, version 5. HDF5 is a versatile, mature scientific software library designed for the fast, flexible storage of enormous amounts of data. help2man 1.47.16 broadwell, epyc, skylake, gpu aion, iris Utilities help2man produces simple manual pages from the '--help' and '--version' output of other commands. help2man 1.47.4 broadwell, epyc, skylake, gpu aion, iris Utilities help2man produces simple manual pages from the '--help' and '--version' output of other commands. hwloc 2.2.0 broadwell, epyc, skylake, gpu aion, iris System-level software The Portable Hardware Locality (hwloc) software package provides a portable abstraction (across OS, versions, architectures, ...) of the hierarchical topology of modern architectures, including NUMA memory nodes, sockets, shared caches, cores and simultaneous multithreading. It also gathers various system attributes such as cache and memory information as well as the locality of I/O devices such as network interfaces, InfiniBand HCAs or GPUs. It primarily aims at helping applications with gathering information about modern computing hardware so as to exploit it accordingly and efficiently. hypothesis 5.41.2 broadwell, epyc, skylake, gpu aion, iris Utilities Hypothesis is an advanced testing library for Python. It lets you write tests which are parametrized by a source of examples, and then generates simple and comprehensible examples that make your tests fail. This lets you find more bugs in your code with less work. hypothesis 5.41.5 broadwell, epyc, skylake, gpu aion, iris Utilities Hypothesis is an advanced testing library for Python. It lets you write tests which are parametrized by a source of examples, and then generates simple and comprehensible examples that make your tests fail. This lets you find more bugs in your code with less work. iccifort 2020.4.304 broadwell, epyc, skylake, gpu aion, iris Compilers Intel C, C++ &amp; Fortran compilers iccifortcuda 2020b gpu iris Toolchains (software stacks) Intel C, C++ &amp; Fortran compilers with CUDA toolkit iimpi 2020b broadwell, epyc, skylake, gpu aion, iris Toolchains (software stacks) Intel C/C++ and Fortran compilers, alongside Intel MPI. iimpic 2020b gpu iris Toolchains (software stacks) Intel C/C++ and Fortran compilers, alongside Intel MPI and CUDA. imkl 2020.4.304 broadwell, epyc, skylake, gpu aion, iris Numerical libraries Intel Math Kernel Library is a library of highly optimized, extensively threaded math routines for science, engineering, and financial applications that require maximum performance. Core math functions include BLAS, LAPACK, ScaLAPACK, Sparse Solvers, Fast Fourier Transforms, Vector Math, and more. impi 2019.9.304 broadwell, epyc, skylake, gpu aion, iris MPI Intel MPI Library, compatible with MPICH ABI intel 2020b broadwell, epyc, skylake, gpu aion, iris Toolchains (software stacks) Compiler toolchain including Intel compilers, Intel MPI and Intel Math Kernel Library (MKL). intelcuda 2020b gpu iris Toolchains (software stacks) Intel Cluster Toolkit Compiler Edition provides Intel C/C++ and Fortran compilers, Intel MPI &amp; Intel MKL, with CUDA toolkit intltool 0.51.0 broadwell, skylake, gpu iris Development intltool is a set of tools to centralize translation of many different file formats using GNU gettext-compatible PO files. libGLU 9.0.1 broadwell, skylake, gpu iris Visualisation The OpenGL Utility Library (GLU) is a computer graphics library for OpenGL. libarchive 3.4.3 broadwell, epyc, skylake, gpu aion, iris Utilities Multi-format archive and compression library libcerf 1.14 broadwell, epyc, skylake aion, iris Mathematics libcerf is a self-contained numeric library that provides an efficient and accurate implementation of complex error functions, along with Dawson, Faddeeva, and Voigt functions. libdrm 2.4.102 broadwell, epyc, skylake, gpu aion, iris Libraries Direct Rendering Manager runtime library. libepoxy 1.5.4 broadwell, skylake iris Libraries Epoxy is a library for handling OpenGL function pointer management for you libevent 2.1.12 broadwell, epyc, skylake aion, iris Libraries The libevent API provides a mechanism to execute a callback function when a specific event occurs on a file descriptor or after a timeout has been reached.  Furthermore, libevent also support callbacks due to signals or regular timeouts. libffi 3.3 broadwell, epyc, skylake, gpu aion, iris Libraries The libffi library provides a portable, high level programming interface to various calling conventions. This allows a programmer to call any function specified by a call interface description at run-time. libgd 2.3.0 broadwell, epyc, skylake aion, iris Libraries GD is an open source code library for the dynamic creation of images by programmers. libgeotiff 1.6.0 broadwell, epyc, skylake, gpu aion, iris Libraries Library for reading and writing coordinate system information from/to GeoTIFF files libglvnd 1.3.2 broadwell, epyc, skylake, gpu aion, iris Libraries libglvnd is a vendor-neutral dispatch layer for arbitrating OpenGL API calls between multiple vendors. libgpuarray 0.7.6 gpu iris Libraries Library to manipulate tensors on the GPU. libiconv 1.16 broadwell, skylake, gpu iris Libraries Libiconv converts from one character encoding to another through Unicode conversion libjpeg-turbo 2.0.5 broadwell, epyc, skylake, gpu aion, iris Libraries libjpeg-turbo is a fork of the original IJG libjpeg which uses SIMD to accelerate baseline JPEG compression and decompression. libjpeg is a library that implements JPEG image encoding, decoding and transcoding. libogg 1.3.4 broadwell, epyc, skylake, gpu aion, iris Libraries Ogg is a multimedia container format, and the native file and stream format for the Xiph.org multimedia codecs. libpciaccess 0.16 broadwell, epyc, skylake, gpu aion, iris System-level software Generic PCI access library. libpng 1.6.37 broadwell, skylake, gpu iris Libraries libpng is the official PNG reference library libreadline 8.0 broadwell, skylake, gpu iris Libraries The GNU Readline library provides a set of functions for use by applications that allow users to edit command lines as they are typed in. Both Emacs and vi editing modes are available. The Readline library includes additional functions to maintain a list of previously-entered command lines, to recall and perhaps reedit those lines, and perform csh-like history expansion on previous commands. libsndfile 1.0.28 broadwell, skylake, gpu iris Libraries Libsndfile is a C library for reading and writing files containing sampled sound (such as MS Windows WAV and the Apple/SGI AIFF format) through one standard library interface. libtirpc 1.3.1 broadwell, epyc, skylake, gpu aion, iris Libraries Libtirpc is a port of Suns Transport-Independent RPC library to Linux. libtool 2.4.6 broadwell, skylake, gpu iris Libraries GNU libtool is a generic library support script. Libtool hides the complexity of using shared libraries behind a consistent, portable interface. libunwind 1.4.0 broadwell, epyc, skylake, gpu aion, iris Libraries The primary goal of libunwind is to define a portable and efficient C programming interface (API) to determine the call-chain of a program. The API additionally provides the means to manipulate the preserved (callee-saved) state of each call-frame and to resume execution at any point in the call-chain (non-local goto). The API supports both local (same-process) and remote (across-process) operation. As such, the API is useful in a number of applications libvorbis 1.3.7 broadwell, epyc, skylake, gpu aion, iris Libraries Ogg Vorbis is a fully open, non-proprietary, patent-and-royalty-free, general-purpose compressed audio format libwebp 1.1.0 broadwell, epyc, skylake aion, iris Libraries WebP is a modern image format that provides superior lossless and lossy compression for images on the web. Using WebP, webmasters and web developers can create smaller, richer images that make the web faster. libxc 4.3.4 broadwell, skylake iris Chemistry Libxc is a library of exchange-correlation functionals for density-functional theory. The aim is to provide a portable, well tested and reliable set of exchange and correlation functionals. libxc 5.1.2 broadwell, epyc, skylake aion, iris Chemistry Libxc is a library of exchange-correlation functionals for density-functional theory. The aim is to provide a portable, well tested and reliable set of exchange and correlation functionals. libxml2 2.9.10 broadwell, epyc, skylake, gpu aion, iris Libraries Libxml2 is the XML C parser and toolchain developed for the Gnome project (but usable outside of the Gnome platform). libyaml 0.2.5 broadwell, epyc, skylake, gpu aion, iris Libraries LibYAML is a YAML parser and emitter written in C. lz4 1.9.2 broadwell, epyc, skylake, gpu aion, iris Libraries LZ4 is lossless compression algorithm, providing compression speed at 400 MB/s per core. It features an extremely fast decoder, with speed in multiple GB/s per core. magma 2.5.4 gpu iris Mathematics The MAGMA project aims to develop a dense linear algebra library similar to LAPACK but for heterogeneous/hybrid architectures, starting with current Multicore+GPU systems. makeinfo 6.7 broadwell, epyc, skylake, gpu aion, iris Development makeinfo is part of the Texinfo project, the official documentation format of the GNU project. matplotlib 3.3.3 broadwell, epyc, skylake, gpu aion, iris Visualisation matplotlib is a python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms. matplotlib can be used in python scripts, the python and ipython shell, web application servers, and six graphical user interface toolkits. ncurses 6.2 broadwell, epyc, skylake, gpu aion, iris Development The Ncurses (new curses) library is a free software emulation of curses in System V Release 4.0, and more. It uses Terminfo format, supports pads and color and multiple highlights and forms characters and function-key mapping, and has all the other SYSV-curses enhancements over BSD Curses. netCDF-Fortran 4.5.3 epyc aion Data processing NetCDF (network Common Data Form) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. netCDF 4.7.4 broadwell, epyc, skylake, gpu aion, iris Data processing NetCDF (network Common Data Form) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. nettle 3.6 broadwell, epyc, skylake, gpu aion, iris Libraries Nettle is a cryptographic library that is designed to fit easily in more or less any context: In crypto toolkits for object-oriented languages (C++, Python, Pike, ...), in applications like LSH or GNUPG, or even in kernel space. networkx 2.5 broadwell, epyc, skylake, gpu aion, iris Utilities NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks. nodejs 12.19.0 broadwell, epyc, skylake, gpu aion, iris Programming Languages Node.js is a platform built on Chrome's JavaScript runtime for easily building fast, scalable network applications. Node.js uses an event-driven, non-blocking I/O model that makes it lightweight and efficient, perfect for data-intensive real-time applications that run across distributed devices. nsync 1.24.0 broadwell, skylake, gpu iris Development nsync is a C library that exports various synchronization primitives, such as mutexes numactl 2.0.13 broadwell, epyc, skylake, gpu aion, iris Utilities The numactl program allows you to run your application program on specific cpu's and memory nodes. It does this by supplying a NUMA memory policy to the operating system before running your program. The libnuma library provides convenient ways for you to add NUMA memory policies into your own program. numba 0.52.0 broadwell, epyc, skylake, gpu aion, iris Programming Languages Numba is an Open Source NumPy-aware optimizing compiler for Python sponsored by Continuum Analytics, Inc. It uses the remarkable LLVM compiler infrastructure to compile Python syntax to machine code. pixman 0.40.0 broadwell, epyc, skylake, gpu aion, iris Visualisation Pixman is a low-level software library for pixel manipulation, providing features such as image compositing and trapezoid rasterization. Important users of pixman are the cairo graphics library and the X server. pkg-config 0.29.2 broadwell, skylake, gpu iris Development pkg-config is a helper tool used when compiling applications and libraries. It helps you insert the correct compiler options on the command line so an application can use gcc -o test test.c <code>pkg-config --libs --cflags glib-2.0</code> for instance, rather than hard-coding values on where to find glib (or other libraries). pkgconfig 1.5.1 broadwell, skylake, gpu iris Development pkgconfig is a Python module to interface with the pkg-config command line tool pocl 1.6 gpu iris Libraries Pocl is a portable open source (MIT-licensed) implementation of the OpenCL standard protobuf-python 3.14.0 broadwell, epyc, skylake, gpu aion, iris Development Python Protocol Buffers runtime library. protobuf 2.5.0 broadwell, skylake iris Development Google Protocol Buffers protobuf 3.14.0 broadwell, epyc, skylake aion, iris Development Google Protocol Buffers pybind11 2.6.0 broadwell, epyc, skylake, gpu aion, iris Libraries pybind11 is a lightweight header-only library that exposes C++ types in Python and vice versa, mainly to create Python bindings of existing C++ code. re2c 2.0.3 broadwell, epyc, skylake aion, iris Utilities re2c is a free and open-source lexer generator for C and C++. Its main goal is generating fast lexers: at least as fast as their reasonably optimized hand-coded counterparts. Instead of using traditional table-driven approach, re2c encodes the generated finite state automata directly in the form of conditional jumps and comparisons. scikit-build 0.11.1 broadwell, epyc, skylake, gpu aion, iris Libraries Scikit-Build, or skbuild, is an improved build system generator for CPython C/C++/Fortran/Cython extensions. scikit-image 0.18.1 broadwell, epyc, skylake, gpu aion, iris Visualisation scikit-image is a collection of algorithms for image processing. scikit-learn 0.23.2 broadwell, epyc, skylake, gpu aion, iris Data processing Scikit-learn integrates machine learning algorithms in the tightly-knit scientific Python world, building upon numpy, scipy, and matplotlib. As a machine-learning module, it provides versatile tools for data mining and analysis in any field of science and engineering. It strives to be simple and efficient, accessible to everybody, and reusable in various contexts. snappy 1.1.8 broadwell, epyc, skylake, gpu aion, iris Libraries Snappy is a compression/decompression library. It does not aim for maximum compression, or compatibility with any other compression library; instead, it aims for very high speeds and reasonable compression. sparsehash 2.0.4 broadwell, epyc, skylake aion, iris Development An extremely memory-efficient hash_map implementation. 2 bits/entry overhead! The SparseHash library contains several hash-map implementations, including implementations that optimize for space or speed. spglib-python 1.16.0 broadwell, epyc, skylake, gpu aion, iris Chemistry Spglib for Python. Spglib is a library for finding and handling crystal symmetries written in C. tbb 2020.3 broadwell, epyc, skylake aion, iris Libraries Intel(R) Threading Building Blocks (Intel(R) TBB) lets you easily write parallel C++ programs that take full advantage of multicore performance, that are portable, composable and have future-proof scalability. tqdm 4.56.2 broadwell, epyc, skylake, gpu aion, iris Libraries A fast, extensible progress bar for Python and CLI typing-extensions 3.7.4.3 gpu iris Development Typing Extensions \u2013 Backported and Experimental Type Hints for Python util-linux 2.36 broadwell, epyc, skylake, gpu aion, iris Utilities Set of Linux utilities x264 20201026 broadwell, epyc, skylake, gpu aion, iris Visualisation x264 is a free software library and application for encoding video streams into the H.264/MPEG-4 AVC compression format, and is released under the terms of the GNU GPL. x265 3.3 broadwell, epyc, skylake, gpu aion, iris Visualisation x265 is a free software library and application for encoding video streams into the H.265 AVC compression format, and is released under the terms of the GNU GPL. xorg-macros 1.19.2 broadwell, skylake, gpu iris Development X.org macros utilities. xprop 1.2.5 broadwell, epyc, skylake aion, iris Visualisation The xprop utility is for displaying window and font properties in an X server. One window or font is selected using the command line arguments or possibly in the case of a window, by clicking on the desired window. A list of properties is then given, possibly with formatting information. zlib 1.2.11 broadwell, skylake, gpu iris Libraries zlib is designed to be a free, general-purpose, legally unencumbered -- that is, not covered by any patents -- lossless data-compression library for use on virtually any computer hardware and operating system. zstd 1.4.5 broadwell, epyc, skylake, gpu aion, iris Libraries Zstandard is a real-time compression algorithm, providing high compression ratios. It offers a very wide range of compression/speed trade-off, while being backed by a very fast decoder. It also offers a special mode for small data, called dictionary compression, and can create dictionaries from any sample set."},{"location":"software/swsets/all_softwares/","title":"Full List (alphabetical order)","text":"Software Versions Swsets Architectures Clusters Category Description ABAQUS 2018, 2021 2019b, 2020b broadwell, skylake, epyc iris, aion CFD/Finite element modelling Finite Element Analysis software for modeling, visualization and best-in-class implicit and explicit dynamics FEA. ABINIT 9.4.1 2020b epyc aion Chemistry ABINIT is a package whose main program allows one to find the total energy, charge density and electronic structure of systems made of electrons and nuclei (molecules and periodic solids) within Density Functional Theory (DFT), using pseudopotentials and a planewave or wavelet basis. ABySS 2.2.5 2020b broadwell, epyc, skylake aion, iris Biology Assembly By Short Sequences - a de novo, parallel, paired-end sequence assembler ACTC 1.1 2019b, 2020b broadwell, skylake, epyc iris, aion Libraries ACTC converts independent triangles into triangle strips or fans. ANSYS 19.4, 21.1 2019b, 2020b broadwell, skylake, epyc iris, aion Utilities ANSYS simulation software enables organizations to confidently predict how their products will operate in the real world. We believe that every product is a promise of something greater. AOCC 3.1.0 2020b epyc aion Compilers AMD Optimized C/C++ &amp; Fortran compilers (AOCC) based on LLVM 12.0 ASE 3.19.0, 3.20.1, 3.21.1 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Chemistry ASE is a python package providing an open source Atomic Simulation Environment in the Python scripting language. From version 3.20.1 we also include the ase-ext package, it contains optional reimplementations in C of functions in ASE.  ASE uses it automatically when installed. ATK 2.34.1, 2.36.0 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation ATK provides the set of accessibility interfaces that are implemented by other toolkits and applications. Using the ATK interfaces, accessibility tools have full access to view and control running applications. Advisor 2019_update5 2019b broadwell, skylake iris Performance measurements Vectorization Optimization and Thread Prototyping - Vectorize &amp; thread code or performance \u201cdies\u201d - Easy workflow + data + tips = faster code faster - Prioritize, Prototype &amp; Predict performance gain Anaconda3 2020.02, 2020.11 2019b, 2020b broadwell, skylake, epyc iris, aion Programming Languages Built to complement the rich, open source Python community, the Anaconda platform provides an enterprise-ready data analytics platform that empowers companies to adopt a modern open data science analytics architecture. ArmForge 20.0.3 2019b, 2020b broadwell, skylake, epyc iris, aion Utilities The industry standard development package for C, C++ and Fortran high performance code on Linux. Forge is designed to handle the complex software projects - including parallel, multiprocess and multithreaded code. Arm Forge combines an industry-leading debugger, Arm DDT, and an out-of-the-box-ready profiler, Arm MAP. ArmReports 20.0.3 2019b, 2020b broadwell, skylake, epyc iris, aion Utilities Arm Performance Reports - a low-overhead tool that produces one-page text and HTML reports summarizing and characterizing both scalar and MPI application performance. Arm Performance Reports runs transparently on optimized production-ready codes by adding a single command to your scripts, and provides the most effective way to characterize and understand the performance of HPC application runs. Armadillo 10.5.3, 9.900.1 2020b, 2019b broadwell, epyc, skylake aion, iris Numerical libraries Armadillo is an open-source C++ linear algebra library (matrix maths) aiming towards a good balance between speed and ease of use. Integer, floating point and complex numbers are supported, as well as a subset of trigonometric and statistics functions. Arrow 0.16.0 2019b broadwell, skylake iris Data processing Apache Arrow (incl. PyArrow Python bindings)), a cross-language development platform for in-memory data. Aspera-CLI 3.9.1, 3.9.6 2019b, 2020b broadwell, skylake, epyc iris, aion Utilities IBM Aspera Command-Line Interface (the Aspera CLI) is a collection of Aspera tools for performing high-speed, secure data transfers from the command line. The Aspera CLI is for users and organizations who want to automate their transfer workflows. Autoconf 2.69 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development Autoconf is an extensible package of M4 macros that produce shell scripts to automatically configure software source code packages. These scripts can adapt the packages to many kinds of UNIX-like systems without manual user intervention. Autoconf creates a configuration script for a package from a template file that lists the operating system features that the package can use, in the form of M4 macro calls. Automake 1.16.1, 1.16.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development Automake: GNU Standards-compliant Makefile generator Autotools 20180311, 20200321 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development This bundle collect the standard GNU build tools: Autoconf, Automake and libtool BEDTools 2.29.2, 2.30.0 2019b, 2020b broadwell, skylake, epyc iris, aion Biology BEDTools: a powerful toolset for genome arithmetic. The BEDTools utilities allow one to address common genomics tasks such as finding feature overlaps and computing coverage. The utilities are largely based on four widely-used file formats: BED, GFF/GTF, VCF, and SAM/BAM. BLAST+ 2.11.0, 2.9.0 2020b, 2019b broadwell, epyc, skylake aion, iris Biology Basic Local Alignment Search Tool, or BLAST, is an algorithm for comparing primary biological sequence information, such as the amino-acid sequences of different proteins or the nucleotides of DNA sequences. BWA 0.7.17 2019b, 2020b broadwell, skylake, epyc iris, aion Biology Burrows-Wheeler Aligner (BWA) is an efficient program that aligns relatively short nucleotide sequences against a long reference sequence such as the human genome. BamTools 2.5.1 2019b, 2020b broadwell, skylake, epyc iris, aion Biology BamTools provides both a programmer's API and an end-user's toolkit for handling BAM files. Bazel 0.26.1, 0.29.1, 3.7.2 2019b, 2020b gpu, broadwell, skylake, epyc iris, aion Development Bazel is a build tool that builds code quickly and reliably. It is used to build the majority of Google's software. BioPerl 1.7.2, 1.7.8 2019b, 2020b broadwell, skylake, epyc iris, aion Biology Bioperl is the product of a community effort to produce Perl code which is useful in biology. Examples include Sequence objects, Alignment objects and database searching objects. Bison 3.3.2, 3.5.3, 3.7.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages Bison is a general-purpose parser generator that converts an annotated context-free grammar into a deterministic LR or generalized LR (GLR) parser employing LALR(1) parser tables. Boost.Python 1.74.0 2020b broadwell, epyc, skylake aion, iris Libraries Boost.Python is a C++ library which enables seamless interoperability between C++ and the Python programming language. Boost 1.71.0, 1.74.0 2019b, 2020b broadwell, skylake, epyc iris, aion Development Boost provides free peer-reviewed portable C++ source libraries. Bowtie2 2.3.5.1, 2.4.2 2019b, 2020b broadwell, skylake, epyc iris, aion Biology Bowtie 2 is an ultrafast and memory-efficient tool for aligning sequencing reads to long reference sequences. It is particularly good at aligning reads of about 50 up to 100s or 1,000s of characters, and particularly good at aligning to relatively long (e.g. mammalian) genomes. Bowtie 2 indexes the genome with an FM Index to keep its memory footprint small: for the human genome, its memory footprint is typically around 3.2 GB. Bowtie 2 supports gapped, local, and paired-end alignment modes. CGAL 4.14.1, 5.2 2019b, 2020b broadwell, skylake, epyc iris, aion Numerical libraries The goal of the CGAL Open Source Project is to provide easy access to efficient and reliable geometric algorithms in the form of a C++ library. CMake 3.15.3, 3.18.4, 3.20.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development CMake, the cross-platform, open-source build system.  CMake is a family of tools designed to build, test and package software. CPLEX 12.10 2019b broadwell, skylake iris Mathematics IBM ILOG CPLEX Optimizer's mathematical programming technology enables analytical decision support for improving efficiency, reducing costs, and increasing profitability. CRYSTAL 17 2019b broadwell, skylake iris Chemistry The CRYSTAL package performs ab initio calculations of the ground state energy, energy gradient, electronic wave function and properties of periodic systems. Hartree-Fock or Kohn- Sham Hamiltonians (that adopt an Exchange-Correlation potential following the postulates of Density-Functional Theory) can be used. CUDA 10.1.243, 11.1.1 2019b, 2020b gpu iris System-level software CUDA (formerly Compute Unified Device Architecture) is a parallel computing platform and programming model created by NVIDIA and implemented by the graphics processing units (GPUs) that they produce. CUDA gives developers access to the virtual instruction set and memory of the parallel computational elements in CUDA GPUs. CUDAcore 11.1.1 2020b gpu iris System-level software CUDA (formerly Compute Unified Device Architecture) is a parallel computing platform and programming model created by NVIDIA and implemented by the graphics processing units (GPUs) that they produce. CUDA gives developers access to the virtual instruction set and memory of the parallel computational elements in CUDA GPUs. Check 0.15.2 2020b gpu iris Libraries Check is a unit testing framework for C. It features a simple interface for defining unit tests, putting little in the way of the developer. Tests are run in a separate address space, so both assertion failures and code errors that cause segmentation faults or other signals can be caught. Test results are reportable in the following: Subunit, TAP, XML, and a generic logging format. Clang 11.0.1, 9.0.1 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Compilers C, C++, Objective-C compiler, based on LLVM.  Does not include C++ standard library -- use libstdc++ from GCC. CubeGUI 4.4.4 2019b broadwell, skylake iris Performance measurements Cube, which is used as performance report explorer for Scalasca and Score-P, is a generic tool for displaying a multi-dimensional performance space consisting of the dimensions (i) performance metric, (ii) call path, and (iii) system resource. Each dimension can be represented as a tree, where non-leaf nodes of the tree can be collapsed or expanded to achieve the desired level of granularity. This module provides the Cube graphical report explorer. CubeLib 4.4.4 2019b broadwell, skylake iris Performance measurements Cube, which is used as performance report explorer for Scalasca and Score-P, is a generic tool for displaying a multi-dimensional performance space consisting of the dimensions (i) performance metric, (ii) call path, and (iii) system resource. Each dimension can be represented as a tree, where non-leaf nodes of the tree can be collapsed or expanded to achieve the desired level of granularity. This module provides the Cube general purpose C++ library component and command-line tools. CubeWriter 4.4.3 2019b broadwell, skylake iris Performance measurements Cube, which is used as performance report explorer for Scalasca and Score-P, is a generic tool for displaying a multi-dimensional performance space consisting of the dimensions (i) performance metric, (ii) call path, and (iii) system resource. Each dimension can be represented as a tree, where non-leaf nodes of the tree can be collapsed or expanded to achieve the desired level of granularity. This module provides the Cube high-performance C writer library component. DB 18.1.32, 18.1.40 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Utilities Berkeley DB enables the development of custom data management solutions, without the overhead traditionally associated with such custom projects. DB_File 1.855 2020b broadwell, epyc, skylake aion, iris Data processing Perl5 access to Berkeley DB version 1.x. DBus 1.13.12, 1.13.18 2019b, 2020b broadwell, skylake, epyc iris, aion Development D-Bus is a message bus system, a simple way for applications to talk to one another.  In addition to interprocess communication, D-Bus helps coordinate process lifecycle; it makes it simple and reliable to code a \"single instance\" application or daemon, and to launch applications and daemons on demand when their services are needed. DMTCP 2.5.2 2019b broadwell, skylake iris Utilities DMTCP is a tool to transparently checkpoint the state of multiple simultaneous applications, including multi-threaded and distributed applications. It operates directly on the user binary executable, without any Linux kernel modules or other kernel modifications. Dakota 6.11.0, 6.15.0 2019b, 2020b broadwell, skylake iris Mathematics The Dakota project delivers both state-of-the-art research and robust, usable software for optimization and UQ. Broadly, the Dakota software's advanced parametric analyses enable design exploration, model calibration, risk analysis, and quantification of margins and uncertainty with computational models.\" Doxygen 1.8.16, 1.8.20 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development Doxygen is a documentation system for C++, C, Java, Objective-C, Python, IDL (Corba and Microsoft flavors), Fortran, VHDL, PHP, C#, and to some extent D. ELPA 2019.11.001, 2020.11.001 2019b, 2020b broadwell, epyc, skylake iris, aion Mathematics Eigenvalue SoLvers for Petaflop-Applications . EasyBuild 4.3.0, 4.3.3, 4.4.1, 4.4.2, 4.5.4 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities EasyBuild is a software build and installation framework written in Python that allows you to install software in a structured, repeatable and robust way. Eigen 3.3.7, 3.3.8, 3.4.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Mathematics Eigen is a C++ template library for linear algebra: matrices, vectors, numerical solvers, and related algorithms. Elk 6.3.2, 7.0.12 2019b, 2020b broadwell, skylake, epyc iris, aion Physics An all-electron full-potential linearised augmented-plane wave (FP-LAPW) code with many advanced features. Written originally at Karl-Franzens-Universit\u00e4t Graz as a milestone of the EXCITING EU Research and Training Network, the code is designed to be as simple as possible so that new developments in the field of density functional theory (DFT) can be added quickly and reliably. FDS 6.7.1, 6.7.6 2019b, 2020b broadwell, skylake, epyc iris, aion Physics Fire Dynamics Simulator (FDS) is a large-eddy simulation (LES) code for low-speed flows, with an emphasis on smoke and heat transport from fires. FFTW 3.3.8 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Numerical libraries FFTW is a C subroutine library for computing the discrete Fourier transform (DFT) in one or more dimensions, of arbitrary input size, and of both real and complex data. FFmpeg 4.2.1, 4.3.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation A complete, cross-platform solution to record, convert and stream audio and video. FLAC 1.3.3 2020b broadwell, epyc, skylake, gpu aion, iris Libraries FLAC stands for Free Lossless Audio Codec, an audio format similar to MP3, but lossless, meaning that audio is compressed in FLAC without any loss in quality. FLTK 1.3.5 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation FLTK is a cross-platform C++ GUI toolkit for UNIX/Linux (X11), Microsoft Windows, and MacOS X. FLTK provides modern GUI functionality without the bloat and supports 3D graphics via OpenGL and its built-in GLUT emulation. FastQC 0.11.9 2019b, 2020b broadwell, skylake, epyc iris, aion Biology FastQC is a quality control application for high throughput sequence data. It reads in sequence data in a variety of formats and can either provide an interactive application to review the results of several different QC checks, or create an HTML based report which can be integrated into a pipeline. Flask 1.1.2 2020b broadwell, epyc, skylake, gpu aion, iris Libraries Flask is a lightweight WSGI web application framework. It is designed to make getting started quick and easy, with the ability to scale up to complex applications. This module includes the Flask extensions: Flask-Cors Flink 1.11.2 2020b broadwell, epyc, skylake aion, iris Development Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale. FreeImage 3.18.0 2020b broadwell, epyc, skylake aion, iris Visualisation FreeImage is an Open Source library project for developers who would like to support popular graphics image formats like PNG, BMP, JPEG, TIFF and others as needed by today's multimedia applications. FreeImage is easy to use, fast, multithreading safe. FriBidi 1.0.10, 1.0.5 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Programming Languages The Free Implementation of the Unicode Bidirectional Algorithm. GCC 10.2.0, 8.3.0 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Compilers The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Java, and Ada, as well as libraries for these languages (libstdc++, libgcj,...). GCCcore 10.2.0, 8.3.0 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Compilers The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Java, and Ada, as well as libraries for these languages (libstdc++, libgcj,...). GDAL 3.0.2, 3.2.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Data processing GDAL is a translator library for raster geospatial data formats that is released under an X/MIT style Open Source license by the Open Source Geospatial Foundation. As a library, it presents a single abstract data model to the calling application for all supported formats. It also comes with a variety of useful commandline utilities for data translation and processing. GDB 10.1, 9.1 2020b, 2019b broadwell, epyc, skylake aion, iris Debugging The GNU Project Debugger GDRCopy 2.1 2020b gpu iris Libraries A low-latency GPU memory copy library based on NVIDIA GPUDirect RDMA technology. GEOS 3.8.0, 3.9.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Mathematics GEOS (Geometry Engine - Open Source) is a C++ port of the Java Topology Suite (JTS) GLPK 4.65 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Utilities The GLPK (GNU Linear Programming Kit) package is intended for solving large-scale linear programming (LP), mixed integer programming (MIP), and other related problems. It is a set of routines written in ANSI C and organized in the form of a callable library. GLib 2.62.0, 2.66.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation GLib is one of the base libraries of the GTK+ project GMP 6.1.2, 6.2.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Mathematics GMP is a free library for arbitrary precision arithmetic, operating on signed integers, rational numbers, and floating point numbers. GObject-Introspection 1.63.1, 1.66.1 2019b, 2020b broadwell, skylake, epyc iris, aion Development GObject introspection is a middleware layer between C libraries (using GObject) and language bindings. The C library can be scanned at compile time and generate a metadata file, in addition to the actual native C library. Then at runtime, language bindings can read this metadata and automatically provide bindings to call into the C library. GPAW-setups 0.9.20000 2019b broadwell, skylake iris Chemistry PAW setup for the GPAW Density Functional Theory package. Users can install setups manually using 'gpaw install-data' or use setups from this package. The versions of GPAW and GPAW-setups can be intermixed. GPAW 20.1.0 2019b broadwell, skylake iris Chemistry GPAW is a density-functional theory (DFT) Python code based on the projector-augmented wave (PAW) method and the atomic simulation environment (ASE). It uses real-space uniform grids and multigrid methods or atom-centered basis-functions. GROMACS 2019.4, 2019.6, 2020, 2021, 2021.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Biology GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. This is a CPU only build, containing both MPI and threadMPI builds for both single and double precision. It also contains the gmxapi extension for the single precision MPI build. GSL 2.6 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Numerical libraries The GNU Scientific Library (GSL) is a numerical library for C and C++ programmers. The library provides a wide range of mathematical routines such as random number generators, special functions and least-squares fitting. GTK+ 3.24.13, 3.24.23 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation GTK+ is the primary library used to construct user interfaces in GNOME. It provides all the user interface controls, or widgets, used in a common graphical application. Its object-oriented API allows you to construct user interfaces without dealing with the low-level details of drawing and device interaction. Gdk-Pixbuf 2.38.2, 2.40.0 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation The Gdk Pixbuf is a toolkit for image loading and pixel buffer manipulation. It is used by GTK+ 2 and GTK+ 3 to load and manipulate images. In the past it was distributed as part of GTK+ 2 but it was split off into a separate package in preparation for the change to GTK+ 3. Ghostscript 9.50, 9.53.3 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities Ghostscript is a versatile processor for PostScript data with the ability to render PostScript to different targets. It used to be part of the cups printing stack, but is no longer used for that. Go 1.14.1, 1.16.6 2019b, 2020b broadwell, skylake, epyc iris, aion Compilers Go is an open source programming language that makes it easy to build simple, reliable, and efficient software. Guile 1.8.8, 2.2.4 2019b broadwell, skylake iris Programming Languages Guile is a programming language, designed to help programmers create flexible applications that can be extended by users or other programmers with plug-ins, modules, or scripts. Gurobi 9.0.0, 9.1.2 2019b, 2020b broadwell, skylake, epyc iris, aion Mathematics The Gurobi Optimizer is a state-of-the-art solver for mathematical programming. The solvers in the Gurobi Optimizer were designed from the ground up to exploit modern architectures and multi-core processors, using the most advanced implementations of the latest algorithms. HDF5 1.10.5, 1.10.7 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Data processing HDF5 is a data model, library, and file format for storing and managing data. It supports an unlimited variety of datatypes, and is designed for flexible and efficient I/O and for high volume and complex data. HDF 4.2.15 2020b broadwell, epyc, skylake, gpu aion, iris Data processing HDF (also known as HDF4) is a library and multi-object file format for storing and managing data between machines. HTSlib 1.10.2, 1.12 2019b, 2020b broadwell, skylake, epyc iris, aion Biology A C library for reading/writing high-throughput sequencing data. This package includes the utilities bgzip and tabix Hadoop 2.10.0 2020b broadwell, epyc, skylake aion, iris Utilities Hadoop MapReduce by Cloudera HarfBuzz 2.6.4, 2.6.7 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation HarfBuzz is an OpenType text shaping engine. Harminv 1.4.1 2019b broadwell, skylake iris Mathematics Harminv is a free program (and accompanying library) to solve the problem of harmonic inversion - given a discrete-time, finite-length signal that consists of a sum of finitely-many sinusoids (possibly exponentially decaying) in a given bandwidth, it determines the frequencies, decay constants, amplitudes, and phases of those sinusoids. Horovod 0.19.1, 0.22.0 2019b, 2020b broadwell, skylake, gpu iris Utilities Horovod is a distributed training framework for TensorFlow. Hypre 2.20.0 2020b broadwell, epyc, skylake aion, iris Numerical libraries Hypre is a library for solving large, sparse linear systems of equations on massively parallel computers. The problems of interest arise in the simulation codes being developed at LLNL and elsewhere to study physical phenomena in the defense, environmental, energy, and biological sciences. ICU 64.2, 67.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries ICU is a mature, widely used set of C/C++ and Java libraries providing Unicode and Globalization support for software applications. ISL 0.23 2020b broadwell, epyc, skylake aion, iris Mathematics isl is a library for manipulating sets and relations of integer points bounded by linear constraints. ImageMagick 7.0.10-35, 7.0.9-5 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Visualisation ImageMagick is a software suite to create, edit, compose, or convert bitmap images Inspector 2019_update5 2019b broadwell, skylake iris Utilities Intel Inspector XE is an easy to use memory error checker and thread checker for serial and parallel applications JasPer 2.0.14, 2.0.24 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation The JasPer Project is an open-source initiative to provide a free software-based reference implementation of the codec specified in the JPEG-2000 Part-1 standard. Java 1.8.0_241, 11.0.2, 13.0.2, 16.0.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages Java Platform, Standard Edition (Java SE) lets you develop and deploy Java applications on desktops and servers. Jellyfish 2.3.0 2019b broadwell, skylake iris Biology Jellyfish is a tool for fast, memory-efficient counting of k-mers in DNA. JsonCpp 1.9.3, 1.9.4 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries JsonCpp is a C++ library that allows manipulating JSON values, including serialization and deserialization to and from strings. It can also preserve existing comment in unserialization/serialization steps, making it a convenient format to store user input files. Julia 1.4.1, 1.6.2 2019b, 2020b broadwell, skylake, epyc iris, aion Programming Languages Julia is a high-level, high-performance dynamic programming language for numerical computing Keras 2.3.1, 2.4.3 2019b, 2020b gpu, broadwell, epyc, skylake iris, aion Mathematics Keras is a deep learning API written in Python, running on top of the machine learning platform TensorFlow. LAME 3.100 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Data processing LAME is a high quality MPEG Audio Layer III (MP3) encoder licensed under the LGPL. LLVM 10.0.1, 11.0.0, 9.0.0, 9.0.1 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Compilers The LLVM Core libraries provide a modern source- and target-independent optimizer, along with code generation support for many popular CPUs (as well as some less common ones!) These libraries are built around a well specified code representation known as the LLVM intermediate representation (\"LLVM IR\"). The LLVM Core libraries are well documented, and it is particularly easy to invent your own language (or port an existing compiler) to use LLVM as an optimizer and code generator. LMDB 0.9.24 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries LMDB is a fast, memory-efficient database. With memory-mapped files, it has the read performance of a pure in-memory database while retaining the persistence of standard disk-based databases. LibTIFF 4.0.10, 4.1.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries tiff: Library and tools for reading and writing TIFF data files LittleCMS 2.11, 2.9 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Visualisation Little CMS intends to be an OPEN SOURCE small-footprint color management engine, with special focus on accuracy and performance. Lua 5.1.5, 5.4.2 2019b, 2020b broadwell, skylake, epyc iris, aion Programming Languages Lua is a powerful, fast, lightweight, embeddable scripting language. Lua combines simple procedural syntax with powerful data description constructs based on associative arrays and extensible semantics. Lua is dynamically typed, runs by interpreting bytecode for a register-based virtual machine, and has automatic memory management with incremental garbage collection, making it ideal for configuration, scripting, and rapid prototyping. M4 1.4.18 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development GNU M4 is an implementation of the traditional Unix macro processor. It is mostly SVR4 compatible although it has some extensions (for example, handling more than 9 positional parameters to macros). GNU M4 also has built-in functions for including files, running shell commands, doing arithmetic, etc. MATLAB 2019b, 2020a, 2021a 2019b, 2020b broadwell, skylake, epyc iris, aion Mathematics MATLAB is a high-level language and interactive environment that enables you to perform computationally intensive tasks faster than with traditional programming languages such as C, C++, and Fortran. METIS 5.1.0 2019b, 2020b broadwell, skylake, epyc iris, aion Mathematics METIS is a set of serial programs for partitioning graphs, partitioning finite element meshes, and producing fill reducing orderings for sparse matrices. The algorithms implemented in METIS are based on the multilevel recursive-bisection, multilevel k-way, and multi-constraint partitioning schemes. MPC 1.2.1 2020b broadwell, epyc, skylake aion, iris Mathematics Gnu Mpc is a C library for the arithmetic of complex numbers with arbitrarily high precision and correct rounding of the result. It extends the principles of the IEEE-754 standard for fixed precision real floating point numbers to complex numbers, providing well-defined semantics for every operation. At the same time, speed of operation at high precision is a major design goal. MPFR 4.0.2, 4.1.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Mathematics The MPFR library is a C library for multiple-precision floating-point computations with correct rounding. MUMPS 5.3.5 2020b broadwell, epyc, skylake aion, iris Mathematics A parallel sparse direct solver Mako 1.1.0, 1.1.3 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development A super-fast templating language that borrows the best ideas from the existing templating languages Mathematica 12.0.0, 12.1.0 2019b, 2020b broadwell, skylake, epyc iris, aion Mathematics Mathematica is a computational software program used in many scientific, engineering, mathematical and computing fields. Maven 3.6.3 2019b, 2020b broadwell, skylake, epyc iris, aion Development Binary maven install, Apache Maven is a software project management and comprehension tool. Based on the concept of a project object model (POM), Maven can manage a project's build, reporting and documentation from a central piece of information. Meep 1.4.3 2019b broadwell, skylake iris Physics Meep (or MEEP) is a free finite-difference time-domain (FDTD) simulation software package developed at MIT to model electromagnetic systems. Mesa 19.1.7, 19.2.1, 20.2.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation Mesa is an open-source implementation of the OpenGL specification - a system for rendering interactive 3D graphics. Meson 0.51.2, 0.55.3 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities Meson is a cross-platform build system designed to be both as fast and as user friendly as possible. Mesquite 2.3.0 2019b broadwell, skylake iris Mathematics Mesh-Quality Improvement Library NAMD 2.13 2019b broadwell, skylake iris Chemistry NAMD is a parallel molecular dynamics code designed for high-performance simulation of large biomolecular systems. NASM 2.14.02, 2.15.05 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages NASM: General-purpose x86 assembler NCCL 2.4.8, 2.8.3 2019b, 2020b gpu iris Libraries The NVIDIA Collective Communications Library (NCCL) implements multi-GPU and multi-node collective communication primitives that are performance optimized for NVIDIA GPUs. NLopt 2.6.1, 2.6.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Numerical libraries NLopt is a free/open-source library for nonlinear optimization, providing a common interface for a number of different free optimization routines available online as well as original implementations of various other algorithms. NSPR 4.21, 4.29 2019b, 2020b broadwell, skylake, epyc iris, aion Libraries Netscape Portable Runtime (NSPR) provides a platform-neutral API for system level and libc-like functions. NSS 3.45, 3.57 2019b, 2020b broadwell, skylake, epyc iris, aion Libraries Network Security Services (NSS) is a set of libraries designed to support cross-platform development of security-enabled client and server applications. Ninja 1.10.1, 1.9.0 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Utilities Ninja is a small build system with a focus on speed. OPARI2 2.0.5 2019b broadwell, skylake iris Performance measurements OPARI2, the successor of Forschungszentrum Juelich's OPARI, is a source-to-source instrumentation tool for OpenMP and hybrid codes. It surrounds OpenMP directives and runtime library calls with calls to the POMP2 measurement interface. OTF2 2.2 2019b broadwell, skylake iris Performance measurements The Open Trace Format 2 is a highly scalable, memory efficient event trace data format plus support library. It is the new standard trace format for Scalasca, Vampir, and TAU and is open for other tools. OpenBLAS 0.3.12, 0.3.7 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Numerical libraries OpenBLAS is an optimized BLAS library based on GotoBLAS2 1.13 BSD version. OpenCV 4.2.0, 4.5.1 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation OpenCV (Open Source Computer Vision Library) is an open source computer vision and machine learning software library. OpenCV was built to provide a common infrastructure for computer vision applications and to accelerate the use of machine perception in the commercial products. Includes extra modules for OpenCV from the contrib repository. OpenEXR 2.5.5 2020b broadwell, epyc, skylake aion, iris Visualisation OpenEXR is a high dynamic-range (HDR) image file format developed by Industrial Light &amp; Magic for use in computer imaging applications OpenFOAM-Extend 4.1-20200408 2019b broadwell, skylake iris CFD/Finite element modelling OpenFOAM is a free, open source CFD software package. OpenFOAM has an extensive range of features to solve anything from complex fluid flows involving chemical reactions, turbulence and heat transfer, to solid dynamics and electromagnetics. OpenFOAM 8, v1912 2020b, 2019b epyc, broadwell, skylake aion, iris CFD/Finite element modelling OpenFOAM is a free, open source CFD software package. OpenFOAM has an extensive range of features to solve anything from complex fluid flows involving chemical reactions, turbulence and heat transfer, to solid dynamics and electromagnetics. OpenMPI 3.1.4, 4.0.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion MPI The Open MPI Project is an open source MPI-3 implementation. PAPI 6.0.0 2019b, 2020b broadwell, skylake, epyc iris, aion Performance measurements PAPI provides the tool designer and application engineer with a consistent interface and methodology for use of the performance counter hardware found in most major microprocessors. PAPI enables software engineers to see, in near real time, the relation between software performance and processor events. In addition Component PAPI provides access to a collection of components that expose performance measurement opportunites across the hardware and software stack. PCRE2 10.33, 10.35 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Development The PCRE library is a set of functions that implement regular expression pattern matching using the same syntax and semantics as Perl 5. PCRE 8.43, 8.44 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development The PCRE library is a set of functions that implement regular expression pattern matching using the same syntax and semantics as Perl 5. PDT 3.25 2019b broadwell, skylake iris Performance measurements Program Database Toolkit (PDT) is a framework for analyzing source code written in several programming languages and for making rich program knowledge accessible to developers of static and dynamic analysis tools. PDT implements a standard program representation, the program database (PDB), that can be accessed in a uniform way through a class library supporting common PDB operations. PETSc 3.14.4 2020b broadwell, epyc, skylake aion, iris Numerical libraries PETSc, pronounced PET-see (the S is silent), is a suite of data structures and routines for the scalable (parallel) solution of scientific applications modeled by partial differential equations. PGI 19.10 2019b broadwell, skylake iris Compilers C, C++ and Fortran compilers from The Portland Group - PGI PLUMED 2.5.3, 2.7.0 2019b, 2020b broadwell, skylake, epyc iris, aion Chemistry PLUMED is an open source library for free energy calculations in molecular systems which works together with some of the most popular molecular dynamics engines. Free energy calculations can be performed as a function of many order parameters with a particular  focus on biological problems, using state of the art methods such as metadynamics, umbrella sampling and Jarzynski-equation based steered MD. The software, written in C++, can be easily interfaced with both fortran and C/C++ codes. POV-Ray 3.7.0.8 2020b broadwell, epyc, skylake aion, iris Visualisation The Persistence of Vision Raytracer, or POV-Ray, is a ray tracing program which generates images from a text-based scene description, and is available for a variety of computer platforms. POV-Ray is a high-quality, Free Software tool for creating stunning three-dimensional graphics. The source code is available for those wanting to do their own ports. PROJ 6.2.1, 7.2.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries Program proj is a standard Unix filter function which converts geographic longitude and latitude coordinates into cartesian coordinates Pango 1.44.7, 1.47.0 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation Pango is a library for laying out and rendering of text, with an emphasis on internationalization. Pango can be used anywhere that text layout is needed, though most of the work on Pango so far has been done in the context of the GTK+ widget toolkit. Pango forms the core of text and font handling for GTK+-2.x. ParMETIS 4.0.3 2019b broadwell, skylake iris Mathematics ParMETIS is an MPI-based parallel library that implements a variety of algorithms for partitioning unstructured graphs, meshes, and for computing fill-reducing orderings of sparse matrices. ParMETIS extends the functionality provided by METIS and includes routines that are especially suited for parallel AMR computations and large scale numerical simulations. The algorithms implemented in ParMETIS are based on the parallel multilevel k-way graph-partitioning, adaptive repartitioning, and parallel multi-constrained partitioning schemes. ParMGridGen 1.0 2019b broadwell, skylake iris Mathematics ParMGridGen is an MPI-based parallel library that is based on the serial package MGridGen, that implements (serial) algorithms for obtaining a sequence of successive coarse grids that are well-suited for geometric multigrid methods. ParaView 5.6.2, 5.8.1 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation ParaView is a scientific parallel visualizer. Perl 5.30.0, 5.32.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages Larry Wall's Practical Extraction and Report Language This is a minimal build without any modules. Should only be used for build dependencies. Pillow 6.2.1, 8.0.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation Pillow is the 'friendly PIL fork' by Alex Clark and Contributors. PIL is the Python Imaging Library by Fredrik Lundh and Contributors. PyOpenGL 3.1.5 2020b broadwell, epyc, skylake aion, iris Visualisation PyOpenGL is the most common cross platform Python binding to OpenGL and related APIs. PyQt5 5.15.1 2020b broadwell, epyc, skylake aion, iris Visualisation PyQt5 is a set of Python bindings for v5 of the Qt application framework from The Qt Company. This bundle includes PyQtWebEngine, a set of Python bindings for The Qt Company\u2019s Qt WebEngine framework. PyQtGraph 0.11.1 2020b broadwell, epyc, skylake aion, iris Visualisation PyQtGraph is a pure-python graphics and GUI library built on PyQt5/PySide2 and numpy. PyTorch-Geometric 1.6.3 2020b broadwell, epyc, skylake, gpu aion, iris Libraries PyTorch Geometric (PyG) is a geometric deep learning extension library for PyTorch. PyTorch 1.4.0, 1.7.1, 1.8.1, 1.9.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development Tensors and Dynamic neural networks in Python with strong GPU acceleration. PyTorch is a deep learning framework that puts Python first. PyYAML 5.1.2, 5.3.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries PyYAML is a YAML parser and emitter for the Python programming language. Python 2.7.16, 2.7.18, 3.7.4, 3.8.6 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages Python is a programming language that lets you work more quickly and integrate your systems more effectively. Qt5 5.13.1, 5.14.2 2019b, 2020b broadwell, skylake, epyc iris, aion Development Qt is a comprehensive cross-platform C++ application framework. QuantumESPRESSO 6.7 2019b, 2020b broadwell, epyc, skylake iris, aion Chemistry Quantum ESPRESSO  is an integrated suite of computer codes for electronic-structure calculations and materials modeling at the nanoscale. It is based on density-functional theory, plane waves, and pseudopotentials (both norm-conserving and ultrasoft). RDFlib 5.0.0 2020b broadwell, epyc, skylake, gpu aion, iris Libraries RDFLib is a Python library for working with RDF, a simple yet powerful language for representing information. R 3.6.2, 4.0.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages R is a free software environment for statistical computing and graphics. ReFrame 2.21, 3.6.3 2019b, 2020b broadwell, skylake, epyc iris, aion Development ReFrame is a framework for writing regression tests for HPC systems. Ruby 2.7.1, 2.7.2 2019b, 2020b broadwell, skylake, epyc iris, aion Programming Languages Ruby is a dynamic, open source programming language with a focus on simplicity and productivity. It has an elegant syntax that is natural to read and easy to write. Rust 1.37.0 2019b broadwell, skylake iris Programming Languages Rust is a systems programming language that runs blazingly fast, prevents segfaults, and guarantees thread safety. SAMtools 1.10, 1.12 2019b, 2020b broadwell, skylake, epyc iris, aion Biology SAM Tools provide various utilities for manipulating alignments in the SAM format, including sorting, merging, indexing and generating alignments in a per-position format. SCOTCH 6.0.9, 6.1.0 2019b, 2020b broadwell, skylake, epyc iris, aion Mathematics Software package and libraries for sequential and parallel graph partitioning, static mapping, and sparse matrix block ordering, and sequential mesh and hypergraph partitioning. SDL2 2.0.14 2020b broadwell, epyc, skylake aion, iris Libraries SDL: Simple DirectMedia Layer, a cross-platform multimedia library SIONlib 1.7.6 2019b broadwell, skylake iris Libraries SIONlib is a scalable I/O library for parallel access to task-local files. The library not only supports writing and reading binary data to or from several thousands of processors into a single or a small number of physical files, but also provides global open and close functions to access SIONlib files in parallel. This package provides a stripped-down installation of SIONlib for use with performance tools (e.g., Score-P), with renamed symbols to avoid conflicts when an application using SIONlib itself is linked against a tool requiring a different SIONlib version. SLEPc 3.14.2 2020b broadwell, epyc, skylake aion, iris Numerical libraries SLEPc (Scalable Library for Eigenvalue Problem Computations) is a software library for the solution of large scale sparse eigenvalue problems on parallel computers. It is an extension of PETSc and can be used for either standard or generalized eigenproblems, with real or complex arithmetic. It can also be used for computing a partial SVD of a large, sparse, rectangular matrix, and to solve quadratic eigenvalue problems. SQLite 3.29.0, 3.33.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development SQLite: SQL Database Engine in a C Library SWIG 4.0.1, 4.0.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development SWIG is a software development tool that connects programs written in C and C++ with a variety of high-level programming languages. Salmon 1.1.0 2019b broadwell, skylake iris Biology Salmon is a wicked-fast program to produce a highly-accurate, transcript-level quantification estimates from RNA-seq data. Salome 8.5.0, 9.8.0 2019b, 2020b broadwell, skylake, epyc iris, aion CFD/Finite element modelling The SALOME platform is an open source software framework for pre- and post-processing and integration of numerical solvers from various scientific fields. CEA and EDF use SALOME to perform a large number of simulations, typically related to power plant equipment and alternative energy. To address these challenges, SALOME includes a CAD/CAE modelling tool, mesh generators, an advanced 3D visualization tool, etc. ScaLAPACK 2.0.2, 2.1.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Numerical libraries The ScaLAPACK (or Scalable LAPACK) library includes a subset of LAPACK routines redesigned for distributed memory MIMD parallel computers. Scalasca 2.5 2019b broadwell, skylake iris Performance measurements Scalasca is a software tool that supports the performance optimization of parallel programs by measuring and analyzing their runtime behavior. The analysis identifies potential performance bottlenecks -- in particular those concerning communication and synchronization -- and offers guidance in exploring their causes. SciPy-bundle 2019.10, 2020.11 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages Bundle of Python packages for scientific software Score-P 6.0 2019b broadwell, skylake iris Performance measurements The Score-P measurement infrastructure is a highly scalable and easy-to-use tool suite for profiling, event tracing, and online analysis of HPC applications. Singularity 3.6.0, 3.8.1 2019b, 2020b broadwell, skylake, epyc iris, aion Utilities SingularityCE is an open source container platform designed to be simple, fast, and secure.  Singularity is optimized for EPC and HPC workloads, allowing untrusted users to run untrusted containers in a trusted way. Spack 0.12.1 2019b, 2020b broadwell, skylake, epyc iris, aion Development Spack is a package manager for supercomputers, Linux, and macOS. It makes installing scientific software easy. With Spack, you can build a package with multiple versions, configurations, platforms, and compilers, and all of these builds can coexist on the same machine. Spark 2.4.3 2019b broadwell, skylake iris Development Spark is Hadoop MapReduce done in memory Stata 17 2020b broadwell, epyc, skylake aion, iris Mathematics Stata is a complete, integrated statistical software package that provides everything you need for data analysis, data management, and graphics. SuiteSparse 5.8.1 2020b broadwell, epyc, skylake aion, iris Numerical libraries SuiteSparse is a collection of libraries manipulate sparse matrices. Sumo 1.3.1 2019b broadwell, skylake iris Utilities Sumo is an open source, highly portable, microscopic and continuous traffic simulation package designed to handle large road networks. Szip 2.1.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities Szip compression software, providing lossless compression of scientific data Tcl 8.6.10, 8.6.9 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Programming Languages Tcl (Tool Command Language) is a very powerful but easy to learn dynamic programming language, suitable for a very wide range of uses, including web and desktop applications, networking, administration, testing and many more. TensorFlow 1.15.5, 2.1.0, 2.4.1, 2.5.0 2019b, 2020b gpu, broadwell, skylake, epyc iris, aion Libraries An open-source software library for Machine Intelligence Theano 1.0.4, 1.1.2 2019b, 2020b gpu, broadwell, epyc, skylake iris, aion Mathematics Theano is a Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. Tk 8.6.10, 8.6.9 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Visualisation Tk is an open source, cross-platform widget toolchain that provides a library of basic elements for building a graphical user interface (GUI) in many different programming languages. Tkinter 3.7.4, 3.8.6 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Programming Languages Tkinter module, built with the Python buildsystem TopHat 2.1.2 2019b, 2020b broadwell, skylake, epyc iris, aion Biology TopHat is a fast splice junction mapper for RNA-Seq reads. Trinity 2.10.0 2019b broadwell, skylake iris Biology Trinity represents a novel method for the efficient and robust de novo reconstruction of transcriptomes from RNA-Seq data. Trinity combines three independent software modules: Inchworm, Chrysalis, and Butterfly, applied sequentially to process large volumes of RNA-Seq reads. UCX 1.9.0 2020b broadwell, epyc, skylake, gpu aion, iris Libraries Unified Communication X An open-source production grade communication framework for data centric and high-performance applications UDUNITS 2.2.26 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Physics UDUNITS supports conversion of unit specifications between formatted and binary forms, arithmetic manipulation of units, and conversion of values between compatible scales of measurement. ULHPC-bd 2020b 2020b broadwell, epyc, skylake aion, iris System-level software Generic Module bundle for BigData Analytics software in use on the UL HPC Facility ULHPC-bio 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion System-level software Generic Module bundle for Bioinformatics, biology and biomedical software in use on the UL HPC Facility, especially at LCSB ULHPC-cs 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion System-level software Generic Module bundle for Computational science software in use on the UL HPC Facility, including: - Computer Aided Engineering, incl. CFD - Chemistry, Computational Chemistry and Quantum Chemistry - Data management &amp; processing tools - Earth Sciences - Quantum Computing - Physics and physical systems simulations ULHPC-dl 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion System-level software Generic Module bundle for (CPU-version) of AI / Deep Learning / Machine Learning software in use on the UL HPC Facility ULHPC-gpu 2019b, 2020b 2019b, 2020b gpu iris System-level software Generic Module bundle for GPU accelerated User Software in use on the UL HPC Facility ULHPC-math 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion System-level software Generic Module bundle for  High-level mathematical software and Linear Algrebra libraries in use on the UL HPC Facility ULHPC-toolchains 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion System-level software Generic Module bundle that contains all the dependencies required to enable toolchains and building tools/programming language in use on the UL HPC Facility ULHPC-tools 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion System-level software Misc tools, incl. - perf:      Performance tools - tools:     General purpose tools UnZip 6.0 2020b broadwell, epyc, skylake, gpu aion, iris Utilities UnZip is an extraction utility for archives compressed in .zip format (also called \"zipfiles\"). Although highly compatible both with PKWARE's PKZIP and PKUNZIP utilities for MS-DOS and with Info-ZIP's own Zip program, our primary objectives have been portability and non-MSDOS functionality. VASP 5.4.4, 6.2.1 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Physics The Vienna Ab initio Simulation Package (VASP) is a computer program for atomic scale materials modelling, e.g. electronic structure calculations and quantum-mechanical molecular dynamics, from first principles. VMD 1.9.4a51 2020b broadwell, epyc, skylake aion, iris Visualisation VMD is a molecular visualization program for displaying, animating, and analyzing large biomolecular systems using 3-D graphics and built-in scripting. VTK 8.2.0, 9.0.1 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation The Visualization Toolkit (VTK) is an open-source, freely available software system for 3D computer graphics, image processing and visualization. VTK consists of a C++ class library and several interpreted interface layers including Tcl/Tk, Java, and Python. VTK supports a wide variety of visualization algorithms including: scalar, vector, tensor, texture, and volumetric methods; and advanced modeling techniques such as: implicit modeling, polygon reduction, mesh smoothing, cutting, contouring, and Delaunay triangulation. VTune 2019_update8, 2020_update3 2019b, 2020b broadwell, skylake, epyc iris, aion Utilities Intel VTune Amplifier XE is the premier performance profiler for C, C++, C#, Fortran, Assembly and Java. Valgrind 3.15.0, 3.16.1 2019b, 2020b broadwell, skylake, epyc iris, aion Debugging Valgrind: Debugging and profiling tools VirtualGL 2.6.2 2019b broadwell, skylake iris Visualisation VirtualGL is an open source toolkit that gives any Linux or Unix remote display software the ability to run OpenGL applications with full hardware acceleration. Voro++ 0.4.6 2019b broadwell, skylake iris Mathematics Voro++ is a software library for carrying out three-dimensional computations of the Voronoi tessellation. A distinguishing feature of the Voro++ library is that it carries out cell-based calculations, computing the Voronoi cell for each particle individually. It is particularly well-suited for applications that rely on cell-based statistics, where features of Voronoi cells (eg. volume, centroid, number of faces) can be used to analyze a system of particles. Wannier90 3.1.0 2020b broadwell, epyc, skylake aion, iris Chemistry A tool for obtaining maximally-localised Wannier functions X11 20190717, 20201008 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation The X Window System (X11) is a windowing system for bitmap displays XML-LibXML 2.0201, 2.0206 2019b, 2020b broadwell, skylake, epyc iris, aion Data processing Perl binding for libxml2 XZ 5.2.4, 5.2.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities xz: XZ utilities Xerces-C++ 3.2.2 2019b broadwell, skylake iris Libraries Xerces-C++ is a validating XML parser written in a portable subset of C++. Xerces-C++ makes it easy to give your application the ability to read and write XML data. A shared library is provided for parsing, generating, manipulating, and validating XML documents using the DOM, SAX, and SAX2 APIs. Xvfb 1.20.9 2020b broadwell, epyc, skylake, gpu aion, iris Visualisation Xvfb is an X server that can run on machines with no display hardware and no physical input devices. It emulates a dumb framebuffer using virtual memory. YACS 0.1.8 2020b broadwell, epyc, skylake aion, iris Libraries YACS was created as a lightweight library to define and manage system configurations, such as those commonly found in software designed for scientific experimentation. These \"configurations\" typically cover concepts like hyperparameters used in training a machine learning model or configurable model hyperparameters, such as the depth of a convolutional neural network. Yasm 1.3.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages Yasm: Complete rewrite of the NASM assembler with BSD license Z3 4.8.10 2020b broadwell, epyc, skylake, gpu aion, iris Utilities Z3 is a theorem prover from Microsoft Research. Zip 3.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities Zip is a compression and file packaging/archive utility. Although highly compatible both with PKWARE's PKZIP and PKUNZIP utilities for MS-DOS and with Info-ZIP's own UnZip, our primary objectives have been portability and other-than-MSDOS functionality ant 1.10.6, 1.10.7, 1.10.9 2019b, 2020b broadwell, skylake, epyc iris, aion Development Apache Ant is a Java library and command-line tool whose mission is to drive processes described in build files as targets and extension points dependent upon each other. The main known usage of Ant is the build of Java applications. archspec 0.1.0 2019b broadwell, skylake iris Utilities A library for detecting, labeling, and reasoning about microarchitectures arpack-ng 3.7.0, 3.8.0 2019b, 2020b broadwell, skylake, epyc iris, aion Numerical libraries ARPACK is a collection of Fortran77 subroutines designed to solve large scale eigenvalue problems. at-spi2-atk 2.34.1, 2.38.0 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation AT-SPI 2 toolkit bridge at-spi2-core 2.34.0, 2.38.0 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation Assistive Technology Service Provider Interface. binutils 2.32, 2.35 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities binutils: GNU binary utilities bokeh 2.2.3 2020b broadwell, epyc, skylake, gpu aion, iris Utilities Statistical and novel interactive HTML plots for Python bzip2 1.0.8 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities bzip2 is a freely available, patent free, high-quality data compressor. It typically compresses files to within 10% to 15% of the best available techniques (the PPM family of statistical compressors), whilst being around twice as fast at compression and six times faster at decompression. cURL 7.66.0, 7.72.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities libcurl is a free and easy-to-use client-side URL transfer library, supporting DICT, FILE, FTP, FTPS, Gopher, HTTP, HTTPS, IMAP, IMAPS, LDAP, LDAPS, POP3, POP3S, RTMP, RTSP, SCP, SFTP, SMTP, SMTPS, Telnet and TFTP. libcurl supports SSL certificates, HTTP POST, HTTP PUT, FTP uploading, HTTP form based upload, proxies, cookies, user+password authentication (Basic, Digest, NTLM, Negotiate, Kerberos), file transfer resume, http proxy tunneling and more. cairo 1.16.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation Cairo is a 2D graphics library with support for multiple output devices. Currently supported output targets include the X Window System (via both Xlib and XCB), Quartz, Win32, image buffers, PostScript, PDF, and SVG file output. Experimental backends include OpenGL, BeOS, OS/2, and DirectFB cuDNN 7.6.4.38, 8.0.4.30, 8.0.5.39 2019b, 2020b gpu iris Numerical libraries The NVIDIA CUDA Deep Neural Network library (cuDNN) is a GPU-accelerated library of primitives for deep neural networks. dask 2021.2.0 2020b broadwell, epyc, skylake, gpu aion, iris Data processing Dask natively scales Python. Dask provides advanced parallelism for analytics, enabling performance at scale for the tools you love. double-conversion 3.1.4, 3.1.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries Efficient binary-decimal and decimal-binary conversion routines for IEEE doubles. elfutils 0.183 2020b gpu iris Libraries The elfutils project provides libraries and tools for ELF files and DWARF data. expat 2.2.7, 2.2.9 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities Expat is an XML parser library written in C. It is a stream-oriented parser in which an application registers handlers for things the parser might find in the XML document (like start tags) flatbuffers-python 1.12 2020b broadwell, epyc, skylake, gpu aion, iris Development Python Flatbuffers runtime library. flatbuffers 1.12.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development FlatBuffers: Memory Efficient Serialization Library flex 2.6.4 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages Flex (Fast Lexical Analyzer) is a tool for generating scanners. A scanner, sometimes called a tokenizer, is a program which recognizes lexical patterns in text. fontconfig 2.13.1, 2.13.92 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation Fontconfig is a library designed to provide system-wide font configuration, customization and application access. foss 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion Toolchains (software stacks) GNU Compiler Collection (GCC) based compiler toolchain, including OpenMPI for MPI support, OpenBLAS (BLAS and LAPACK support), FFTW and ScaLAPACK. fosscuda 2019b, 2020b 2019b, 2020b gpu iris Toolchains (software stacks) GCC based compiler toolchain with CUDA support, and including OpenMPI for MPI support, OpenBLAS (BLAS and LAPACK support), FFTW and ScaLAPACK. freetype 2.10.1, 2.10.3 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation FreeType 2 is a software font engine that is designed to be small, efficient, highly customizable, and portable while capable of producing high-quality output (glyph images). It can be used in graphics libraries, display servers, font conversion tools, text image generation tools, and many other products as well. gc 7.6.12 2019b broadwell, skylake iris Libraries The Boehm-Demers-Weiser conservative garbage collector can be used as a garbage collecting replacement for C malloc or C++ new. gcccuda 2019b, 2020b 2019b, 2020b gpu iris Toolchains (software stacks) GNU Compiler Collection (GCC) based compiler toolchain, along with CUDA toolkit. gettext 0.19.8.1, 0.20.1, 0.21 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities GNU 'gettext' is an important step for the GNU Translation Project, as it is an asset on which we may build many other steps. This package offers to programmers, translators, and even users, a well integrated set of tools and documentation gflags 2.2.2 2019b broadwell, skylake iris Development The gflags package contains a C++ library that implements commandline flags processing.  It includes built-in support for standard types such as string and the ability to define flags in the source file in which they are used. giflib 5.2.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries giflib is a library for reading and writing gif images. It is API and ABI compatible with libungif which was in wide use while the LZW compression algorithm was patented. git 2.23.0, 2.28.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. glog 0.4.0 2019b broadwell, skylake iris Development A C++ implementation of the Google logging module. gmsh 4.4.0 2019b broadwell, skylake iris CFD/Finite element modelling Salome is an open-source software that provides a generic Pre- and Post-Processing platform for numerical simulation. It is based on an open and flexible architecture made of reusable components. gmsh 4.8.4 2020b broadwell, epyc, skylake aion, iris Mathematics Gmsh is a 3D finite element grid generator with a build-in CAD engine and post-processor. gnuplot 5.2.8, 5.4.1 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation Portable interactive, function plotting utility gocryptfs 1.7.1, 2.0.1 2019b, 2020b broadwell, skylake, epyc iris, aion Utilities Encrypted overlay filesystem written in Go. gocryptfs uses file-based encryption that is implemented as a mountable FUSE filesystem. Each file in gocryptfs is stored as one corresponding encrypted file on the hard disk. gompi 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion Toolchains (software stacks) GNU Compiler Collection (GCC) based compiler toolchain, including OpenMPI for MPI support. gompic 2019b, 2020b 2019b, 2020b gpu iris Toolchains (software stacks) GNU Compiler Collection (GCC) based compiler toolchain along with CUDA toolkit, including OpenMPI for MPI support with CUDA features enabled. googletest 1.10.0 2019b broadwell, skylake iris Development Google's framework for writing C++ tests on a variety of platforms gperf 3.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development GNU gperf is a perfect hash function generator. For a given list of strings, it produces a hash function and hash table, in form of C or C++ code, for looking up a value depending on the input string. The hash function is perfect, which means that the hash table has no collisions, and the hash table lookup needs a single string comparison only. groff 1.22.4 2020b broadwell, epyc, skylake, gpu aion, iris Utilities Groff (GNU troff) is a typesetting system that reads plain text mixed with formatting commands and produces formatted output. gzip 1.10 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Utilities gzip (GNU zip) is a popular data compression program as a replacement for compress h5py 2.10.0, 3.1.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Data processing HDF5 for Python (h5py) is a general-purpose Python interface to the Hierarchical Data Format library, version 5. HDF5 is a versatile, mature scientific software library designed for the fast, flexible storage of enormous amounts of data. help2man 1.47.16, 1.47.4, 1.47.8 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Utilities help2man produces simple manual pages from the '--help' and '--version' output of other commands. hwloc 1.11.12, 2.2.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion System-level software The Portable Hardware Locality (hwloc) software package provides a portable abstraction (across OS, versions, architectures, ...) of the hierarchical topology of modern architectures, including NUMA memory nodes, sockets, shared caches, cores and simultaneous multithreading. It also gathers various system attributes such as cache and memory information as well as the locality of I/O devices such as network interfaces, InfiniBand HCAs or GPUs. It primarily aims at helping applications with gathering information about modern computing hardware so as to exploit it accordingly and efficiently. hypothesis 4.44.2, 5.41.2, 5.41.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities Hypothesis is an advanced testing library for Python. It lets you write tests which are parametrized by a source of examples, and then generates simple and comprehensible examples that make your tests fail. This lets you find more bugs in your code with less work. iccifort 2019.5.281, 2020.4.304 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Compilers Intel C, C++ &amp; Fortran compilers iccifortcuda 2019b, 2020b 2019b, 2020b gpu iris Toolchains (software stacks) Intel C, C++ &amp; Fortran compilers with CUDA toolkit iimpi 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Toolchains (software stacks) Intel C/C++ and Fortran compilers, alongside Intel MPI. iimpic 2019b, 2020b 2019b, 2020b gpu iris Toolchains (software stacks) Intel C/C++ and Fortran compilers, alongside Intel MPI and CUDA. imkl 2019.5.281, 2020.4.304 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Numerical libraries Intel Math Kernel Library is a library of highly optimized, extensively threaded math routines for science, engineering, and financial applications that require maximum performance. Core math functions include BLAS, LAPACK, ScaLAPACK, Sparse Solvers, Fast Fourier Transforms, Vector Math, and more. impi 2018.5.288, 2019.9.304 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion MPI Intel MPI Library, compatible with MPICH ABI intel 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Toolchains (software stacks) Compiler toolchain including Intel compilers, Intel MPI and Intel Math Kernel Library (MKL). intelcuda 2019b, 2020b 2019b, 2020b gpu iris Toolchains (software stacks) Intel Cluster Toolkit Compiler Edition provides Intel C/C++ and Fortran compilers, Intel MPI &amp; Intel MKL, with CUDA toolkit intltool 0.51.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development intltool is a set of tools to centralize translation of many different file formats using GNU gettext-compatible PO files. itac 2019.4.036 2019b broadwell, skylake iris Utilities The Intel Trace Collector is a low-overhead tracing library that performs event-based tracing in applications. The Intel Trace Analyzer provides a convenient way to monitor application activities gathered by the Intel Trace Collector through graphical displays. jemalloc 5.2.1 2019b broadwell, skylake iris Libraries jemalloc is a general purpose malloc(3) implementation that emphasizes fragmentation avoidance and scalable concurrency support. kallisto 0.46.1 2019b broadwell, skylake iris Biology kallisto is a program for quantifying abundances of transcripts from RNA-Seq data, or more generally of target sequences using high-throughput sequencing reads. kim-api 2.1.3 2019b broadwell, skylake iris Chemistry Open Knowledgebase of Interatomic Models. KIM is an API and OpenKIM is a collection of interatomic models (potentials) for atomistic simulations.  This is a library that can be used by simulation programs to get access to the models in the OpenKIM database. This EasyBuild only installs the API, the models can be installed with the package openkim-models, or the user can install them manually by running kim-api-collections-management install user MODELNAME or kim-api-collections-management install user OpenKIM to install them all. libGLU 9.0.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation The OpenGL Utility Library (GLU) is a computer graphics library for OpenGL. libarchive 3.4.3 2020b broadwell, epyc, skylake, gpu aion, iris Utilities Multi-format archive and compression library libcerf 1.13, 1.14 2019b, 2020b broadwell, skylake, epyc iris, aion Mathematics libcerf is a self-contained numeric library that provides an efficient and accurate implementation of complex error functions, along with Dawson, Faddeeva, and Voigt functions. libctl 4.0.0 2019b broadwell, skylake iris Chemistry libctl is a free Guile-based library implementing flexible control files for scientific simulations. libdrm 2.4.102, 2.4.99 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Libraries Direct Rendering Manager runtime library. libepoxy 1.5.4 2019b, 2020b broadwell, skylake, epyc iris, aion Libraries Epoxy is a library for handling OpenGL function pointer management for you libevent 2.1.11, 2.1.12 2019b, 2020b broadwell, skylake, epyc iris, aion Libraries The libevent API provides a mechanism to execute a callback function when a specific event occurs on a file descriptor or after a timeout has been reached.  Furthermore, libevent also support callbacks due to signals or regular timeouts. libffi 3.2.1, 3.3 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries The libffi library provides a portable, high level programming interface to various calling conventions. This allows a programmer to call any function specified by a call interface description at run-time. libgd 2.2.5, 2.3.0 2019b, 2020b broadwell, skylake, epyc iris, aion Libraries GD is an open source code library for the dynamic creation of images by programmers. libgeotiff 1.5.1, 1.6.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries Library for reading and writing coordinate system information from/to GeoTIFF files libglvnd 1.2.0, 1.3.2 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Libraries libglvnd is a vendor-neutral dispatch layer for arbitrating OpenGL API calls between multiple vendors. libgpuarray 0.7.6 2019b, 2020b gpu iris Libraries Library to manipulate tensors on the GPU. libiconv 1.16 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries Libiconv converts from one character encoding to another through Unicode conversion libjpeg-turbo 2.0.3, 2.0.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries libjpeg-turbo is a fork of the original IJG libjpeg which uses SIMD to accelerate baseline JPEG compression and decompression. libjpeg is a library that implements JPEG image encoding, decoding and transcoding. libmatheval 1.1.11 2019b broadwell, skylake iris Libraries GNU libmatheval is a library (callable from C and Fortran) to parse and evaluate symbolic expressions input as text. libogg 1.3.4 2020b broadwell, epyc, skylake, gpu aion, iris Libraries Ogg is a multimedia container format, and the native file and stream format for the Xiph.org multimedia codecs. libpciaccess 0.14, 0.16 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion System-level software Generic PCI access library. libpng 1.6.37 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries libpng is the official PNG reference library libreadline 8.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries The GNU Readline library provides a set of functions for use by applications that allow users to edit command lines as they are typed in. Both Emacs and vi editing modes are available. The Readline library includes additional functions to maintain a list of previously-entered command lines, to recall and perhaps reedit those lines, and perform csh-like history expansion on previous commands. libsndfile 1.0.28 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries Libsndfile is a C library for reading and writing files containing sampled sound (such as MS Windows WAV and the Apple/SGI AIFF format) through one standard library interface. libtirpc 1.3.1 2020b broadwell, epyc, skylake, gpu aion, iris Libraries Libtirpc is a port of Suns Transport-Independent RPC library to Linux. libtool 2.4.6 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries GNU libtool is a generic library support script. Libtool hides the complexity of using shared libraries behind a consistent, portable interface. libunistring 0.9.10 2019b broadwell, skylake iris Libraries This library provides functions for manipulating Unicode strings and for manipulating C strings according to the Unicode standard. libunwind 1.3.1, 1.4.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries The primary goal of libunwind is to define a portable and efficient C programming interface (API) to determine the call-chain of a program. The API additionally provides the means to manipulate the preserved (callee-saved) state of each call-frame and to resume execution at any point in the call-chain (non-local goto). The API supports both local (same-process) and remote (across-process) operation. As such, the API is useful in a number of applications libvorbis 1.3.7 2020b broadwell, epyc, skylake, gpu aion, iris Libraries Ogg Vorbis is a fully open, non-proprietary, patent-and-royalty-free, general-purpose compressed audio format libwebp 1.1.0 2020b broadwell, epyc, skylake aion, iris Libraries WebP is a modern image format that provides superior lossless and lossy compression for images on the web. Using WebP, webmasters and web developers can create smaller, richer images that make the web faster. libxc 4.3.4, 5.1.2 2019b, 2020b broadwell, skylake, epyc iris, aion Chemistry Libxc is a library of exchange-correlation functionals for density-functional theory. The aim is to provide a portable, well tested and reliable set of exchange and correlation functionals. libxml2 2.9.10, 2.9.9 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Libraries Libxml2 is the XML C parser and toolchain developed for the Gnome project (but usable outside of the Gnome platform). libxslt 1.1.34 2019b broadwell, skylake iris Libraries Libxslt is the XSLT C library developed for the GNOME project (but usable outside of the Gnome platform). libyaml 0.2.2, 0.2.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries LibYAML is a YAML parser and emitter written in C. lxml 4.4.2 2019b broadwell, skylake iris Libraries The lxml XML toolkit is a Pythonic binding for the C libraries libxml2 and libxslt. lz4 1.9.2 2020b broadwell, epyc, skylake, gpu aion, iris Libraries LZ4 is lossless compression algorithm, providing compression speed at 400 MB/s per core. It features an extremely fast decoder, with speed in multiple GB/s per core. magma 2.5.1, 2.5.4 2019b, 2020b gpu iris Mathematics The MAGMA project aims to develop a dense linear algebra library similar to LAPACK but for heterogeneous/hybrid architectures, starting with current Multicore+GPU systems. makeinfo 6.7 2020b broadwell, epyc, skylake, gpu aion, iris Development makeinfo is part of the Texinfo project, the official documentation format of the GNU project. matplotlib 3.1.1, 3.3.3 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Visualisation matplotlib is a python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms. matplotlib can be used in python scripts, the python and ipython shell, web application servers, and six graphical user interface toolkits. molmod 1.4.5 2019b broadwell, skylake iris Mathematics MolMod is a Python library with many compoments that are useful to write molecular modeling programs. ncurses 6.0, 6.1, 6.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development The Ncurses (new curses) library is a free software emulation of curses in System V Release 4.0, and more. It uses Terminfo format, supports pads and color and multiple highlights and forms characters and function-key mapping, and has all the other SYSV-curses enhancements over BSD Curses. netCDF-Fortran 4.5.2, 4.5.3 2019b, 2020b broadwell, skylake, epyc iris, aion Data processing NetCDF (network Common Data Form) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. netCDF 4.7.1, 4.7.4 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Data processing NetCDF (network Common Data Form) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. nettle 3.5.1, 3.6 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries Nettle is a cryptographic library that is designed to fit easily in more or less any context: In crypto toolkits for object-oriented languages (C++, Python, Pike, ...), in applications like LSH or GNUPG, or even in kernel space. networkx 2.5 2020b broadwell, epyc, skylake, gpu aion, iris Utilities NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks. nodejs 12.19.0 2020b broadwell, epyc, skylake, gpu aion, iris Programming Languages Node.js is a platform built on Chrome's JavaScript runtime for easily building fast, scalable network applications. Node.js uses an event-driven, non-blocking I/O model that makes it lightweight and efficient, perfect for data-intensive real-time applications that run across distributed devices. nsync 1.24.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development nsync is a C library that exports various synchronization primitives, such as mutexes numactl 2.0.12, 2.0.13 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities The numactl program allows you to run your application program on specific cpu's and memory nodes. It does this by supplying a NUMA memory policy to the operating system before running your program. The libnuma library provides convenient ways for you to add NUMA memory policies into your own program. numba 0.52.0 2020b broadwell, epyc, skylake, gpu aion, iris Programming Languages Numba is an Open Source NumPy-aware optimizing compiler for Python sponsored by Continuum Analytics, Inc. It uses the remarkable LLVM compiler infrastructure to compile Python syntax to machine code. phonopy 2.2.0 2019b broadwell, skylake iris Libraries Phonopy is an open source package of phonon calculations based on the supercell approach. pixman 0.38.4, 0.40.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation Pixman is a low-level software library for pixel manipulation, providing features such as image compositing and trapezoid rasterization. Important users of pixman are the cairo graphics library and the X server. pkg-config 0.29.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development pkg-config is a helper tool used when compiling applications and libraries. It helps you insert the correct compiler options on the command line so an application can use gcc -o test test.c <code>pkg-config --libs --cflags glib-2.0</code> for instance, rather than hard-coding values on where to find glib (or other libraries). pkgconfig 1.5.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development pkgconfig is a Python module to interface with the pkg-config command line tool pocl 1.4, 1.6 2019b, 2020b gpu iris Libraries Pocl is a portable open source (MIT-licensed) implementation of the OpenCL standard protobuf-python 3.10.0, 3.14.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development Python Protocol Buffers runtime library. protobuf 2.5.0, 3.10.0, 3.14.0 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Development Google Protocol Buffers pybind11 2.4.3, 2.6.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries pybind11 is a lightweight header-only library that exposes C++ types in Python and vice versa, mainly to create Python bindings of existing C++ code. re2c 1.2.1, 2.0.3 2019b, 2020b broadwell, skylake, epyc iris, aion Utilities re2c is a free and open-source lexer generator for C and C++. Its main goal is generating fast lexers: at least as fast as their reasonably optimized hand-coded counterparts. Instead of using traditional table-driven approach, re2c encodes the generated finite state automata directly in the form of conditional jumps and comparisons. scikit-build 0.11.1 2020b broadwell, epyc, skylake, gpu aion, iris Libraries Scikit-Build, or skbuild, is an improved build system generator for CPython C/C++/Fortran/Cython extensions. scikit-image 0.18.1 2020b broadwell, epyc, skylake, gpu aion, iris Visualisation scikit-image is a collection of algorithms for image processing. scikit-learn 0.23.2 2020b broadwell, epyc, skylake, gpu aion, iris Data processing Scikit-learn integrates machine learning algorithms in the tightly-knit scientific Python world, building upon numpy, scipy, and matplotlib. As a machine-learning module, it provides versatile tools for data mining and analysis in any field of science and engineering. It strives to be simple and efficient, accessible to everybody, and reusable in various contexts. scipy 1.4.1 2019b broadwell, skylake, gpu iris Mathematics SciPy is a collection of mathematical algorithms and convenience functions built on the Numpy extension for Python. setuptools 41.0.1 2019b broadwell, skylake iris Development Easily download, build, install, upgrade, and uninstall Python packages snappy 1.1.7, 1.1.8 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries Snappy is a compression/decompression library. It does not aim for maximum compression, or compatibility with any other compression library; instead, it aims for very high speeds and reasonable compression. sparsehash 2.0.3, 2.0.4 2019b, 2020b broadwell, skylake, epyc iris, aion Development An extremely memory-efficient hash_map implementation. 2 bits/entry overhead! The SparseHash library contains several hash-map implementations, including implementations that optimize for space or speed. spglib-python 1.16.0 2020b broadwell, epyc, skylake, gpu aion, iris Chemistry Spglib for Python. Spglib is a library for finding and handling crystal symmetries written in C. tbb 2019_U9, 2020.2, 2020.3 2019b, 2020b broadwell, skylake, epyc iris, aion Libraries Intel(R) Threading Building Blocks (Intel(R) TBB) lets you easily write parallel C++ programs that take full advantage of multicore performance, that are portable, composable and have future-proof scalability. texinfo 6.7 2019b broadwell, skylake iris Development Texinfo is the official documentation format of the GNU project. tqdm 4.56.2 2020b broadwell, epyc, skylake, gpu aion, iris Libraries A fast, extensible progress bar for Python and CLI typing-extensions 3.7.4.3 2019b, 2020b gpu, broadwell, epyc, skylake iris, aion Development Typing Extensions \u2013 Backported and Experimental Type Hints for Python util-linux 2.34, 2.36 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities Set of Linux utilities x264 20190925, 20201026 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation x264 is a free software library and application for encoding video streams into the H.264/MPEG-4 AVC compression format, and is released under the terms of the GNU GPL. x265 3.2, 3.3 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation x265 is a free software library and application for encoding video streams into the H.265 AVC compression format, and is released under the terms of the GNU GPL. xorg-macros 1.19.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development X.org macros utilities. xprop 1.2.4, 1.2.5 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation The xprop utility is for displaying window and font properties in an X server. One window or font is selected using the command line arguments or possibly in the case of a window, by clicking on the desired window. A list of properties is then given, possibly with formatting information. yaff 1.6.0 2019b broadwell, skylake iris Chemistry Yaff stands for 'Yet another force field'. It is a pythonic force-field code. zlib 1.2.11 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries zlib is designed to be a free, general-purpose, legally unencumbered -- that is, not covered by any patents -- lossless data-compression library for use on virtually any computer hardware and operating system. zstd 1.4.5 2020b broadwell, epyc, skylake, gpu aion, iris Libraries Zstandard is a real-time compression algorithm, providing high compression ratios. It offers a very wide range of compression/speed trade-off, while being backed by a very fast decoder. It also offers a special mode for small data, called dictionary compression, and can create dictionaries from any sample set."},{"location":"software/swsets/bio/","title":"Biology","text":"<p>Alphabetical list of available ULHPC software belonging to the 'bio' category. To load a software of this category, use: <code>module load bio/&lt;software&gt;[/&lt;version&gt;]</code></p> Software Versions Swsets Architectures Clusters Description ABySS 2.2.5 2020b broadwell, epyc, skylake aion, iris Assembly By Short Sequences - a de novo, parallel, paired-end sequence assembler BEDTools 2.29.2, 2.30.0 2019b, 2020b broadwell, skylake, epyc iris, aion BEDTools: a powerful toolset for genome arithmetic. The BEDTools utilities allow one to address common genomics tasks such as finding feature overlaps and computing coverage. The utilities are largely based on four widely-used file formats: BED, GFF/GTF, VCF, and SAM/BAM. BLAST+ 2.11.0, 2.9.0 2020b, 2019b broadwell, epyc, skylake aion, iris Basic Local Alignment Search Tool, or BLAST, is an algorithm for comparing primary biological sequence information, such as the amino-acid sequences of different proteins or the nucleotides of DNA sequences. BWA 0.7.17 2019b, 2020b broadwell, skylake, epyc iris, aion Burrows-Wheeler Aligner (BWA) is an efficient program that aligns relatively short nucleotide sequences against a long reference sequence such as the human genome. BamTools 2.5.1 2019b, 2020b broadwell, skylake, epyc iris, aion BamTools provides both a programmer's API and an end-user's toolkit for handling BAM files. BioPerl 1.7.2, 1.7.8 2019b, 2020b broadwell, skylake, epyc iris, aion Bioperl is the product of a community effort to produce Perl code which is useful in biology. Examples include Sequence objects, Alignment objects and database searching objects. Bowtie2 2.3.5.1, 2.4.2 2019b, 2020b broadwell, skylake, epyc iris, aion Bowtie 2 is an ultrafast and memory-efficient tool for aligning sequencing reads to long reference sequences. It is particularly good at aligning reads of about 50 up to 100s or 1,000s of characters, and particularly good at aligning to relatively long (e.g. mammalian) genomes. Bowtie 2 indexes the genome with an FM Index to keep its memory footprint small: for the human genome, its memory footprint is typically around 3.2 GB. Bowtie 2 supports gapped, local, and paired-end alignment modes. FastQC 0.11.9 2019b, 2020b broadwell, skylake, epyc iris, aion FastQC is a quality control application for high throughput sequence data. It reads in sequence data in a variety of formats and can either provide an interactive application to review the results of several different QC checks, or create an HTML based report which can be integrated into a pipeline. GROMACS 2019.4, 2019.6, 2020, 2021, 2021.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. This is a CPU only build, containing both MPI and threadMPI builds for both single and double precision. It also contains the gmxapi extension for the single precision MPI build. HTSlib 1.10.2, 1.12 2019b, 2020b broadwell, skylake, epyc iris, aion A C library for reading/writing high-throughput sequencing data. This package includes the utilities bgzip and tabix Jellyfish 2.3.0 2019b broadwell, skylake iris Jellyfish is a tool for fast, memory-efficient counting of k-mers in DNA. SAMtools 1.10, 1.12 2019b, 2020b broadwell, skylake, epyc iris, aion SAM Tools provide various utilities for manipulating alignments in the SAM format, including sorting, merging, indexing and generating alignments in a per-position format. Salmon 1.1.0 2019b broadwell, skylake iris Salmon is a wicked-fast program to produce a highly-accurate, transcript-level quantification estimates from RNA-seq data. TopHat 2.1.2 2019b, 2020b broadwell, skylake, epyc iris, aion TopHat is a fast splice junction mapper for RNA-Seq reads. Trinity 2.10.0 2019b broadwell, skylake iris Trinity represents a novel method for the efficient and robust de novo reconstruction of transcriptomes from RNA-Seq data. Trinity combines three independent software modules: Inchworm, Chrysalis, and Butterfly, applied sequentially to process large volumes of RNA-Seq reads. kallisto 0.46.1 2019b broadwell, skylake iris kallisto is a program for quantifying abundances of transcripts from RNA-Seq data, or more generally of target sequences using high-throughput sequencing reads."},{"location":"software/swsets/cae/","title":"CFD/Finite element modelling","text":"<p>Alphabetical list of available ULHPC software belonging to the 'cae' category. To load a software of this category, use: <code>module load cae/&lt;software&gt;[/&lt;version&gt;]</code></p> Software Versions Swsets Architectures Clusters Description ABAQUS 2018, 2021 2019b, 2020b broadwell, skylake, epyc iris, aion Finite Element Analysis software for modeling, visualization and best-in-class implicit and explicit dynamics FEA. OpenFOAM-Extend 4.1-20200408 2019b broadwell, skylake iris OpenFOAM is a free, open source CFD software package. OpenFOAM has an extensive range of features to solve anything from complex fluid flows involving chemical reactions, turbulence and heat transfer, to solid dynamics and electromagnetics. OpenFOAM 8, v1912 2020b, 2019b epyc, broadwell, skylake aion, iris OpenFOAM is a free, open source CFD software package. OpenFOAM has an extensive range of features to solve anything from complex fluid flows involving chemical reactions, turbulence and heat transfer, to solid dynamics and electromagnetics. Salome 8.5.0, 9.8.0 2019b, 2020b broadwell, skylake, epyc iris, aion The SALOME platform is an open source software framework for pre- and post-processing and integration of numerical solvers from various scientific fields. CEA and EDF use SALOME to perform a large number of simulations, typically related to power plant equipment and alternative energy. To address these challenges, SALOME includes a CAD/CAE modelling tool, mesh generators, an advanced 3D visualization tool, etc. gmsh 4.4.0 2019b broadwell, skylake iris Salome is an open-source software that provides a generic Pre- and Post-Processing platform for numerical simulation. It is based on an open and flexible architecture made of reusable components."},{"location":"software/swsets/chem/","title":"Chemistry","text":"<p>Alphabetical list of available ULHPC software belonging to the 'chem' category. To load a software of this category, use: <code>module load chem/&lt;software&gt;[/&lt;version&gt;]</code></p> Software Versions Swsets Architectures Clusters Description ABINIT 9.4.1 2020b epyc aion ABINIT is a package whose main program allows one to find the total energy, charge density and electronic structure of systems made of electrons and nuclei (molecules and periodic solids) within Density Functional Theory (DFT), using pseudopotentials and a planewave or wavelet basis. ASE 3.19.0, 3.20.1, 3.21.1 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion ASE is a python package providing an open source Atomic Simulation Environment in the Python scripting language. From version 3.20.1 we also include the ase-ext package, it contains optional reimplementations in C of functions in ASE.  ASE uses it automatically when installed. CRYSTAL 17 2019b broadwell, skylake iris The CRYSTAL package performs ab initio calculations of the ground state energy, energy gradient, electronic wave function and properties of periodic systems. Hartree-Fock or Kohn- Sham Hamiltonians (that adopt an Exchange-Correlation potential following the postulates of Density-Functional Theory) can be used. GPAW-setups 0.9.20000 2019b broadwell, skylake iris PAW setup for the GPAW Density Functional Theory package. Users can install setups manually using 'gpaw install-data' or use setups from this package. The versions of GPAW and GPAW-setups can be intermixed. GPAW 20.1.0 2019b broadwell, skylake iris GPAW is a density-functional theory (DFT) Python code based on the projector-augmented wave (PAW) method and the atomic simulation environment (ASE). It uses real-space uniform grids and multigrid methods or atom-centered basis-functions. NAMD 2.13 2019b broadwell, skylake iris NAMD is a parallel molecular dynamics code designed for high-performance simulation of large biomolecular systems. PLUMED 2.5.3, 2.7.0 2019b, 2020b broadwell, skylake, epyc iris, aion PLUMED is an open source library for free energy calculations in molecular systems which works together with some of the most popular molecular dynamics engines. Free energy calculations can be performed as a function of many order parameters with a particular  focus on biological problems, using state of the art methods such as metadynamics, umbrella sampling and Jarzynski-equation based steered MD. The software, written in C++, can be easily interfaced with both fortran and C/C++ codes. QuantumESPRESSO 6.7 2019b, 2020b broadwell, epyc, skylake iris, aion Quantum ESPRESSO  is an integrated suite of computer codes for electronic-structure calculations and materials modeling at the nanoscale. It is based on density-functional theory, plane waves, and pseudopotentials (both norm-conserving and ultrasoft). Wannier90 3.1.0 2020b broadwell, epyc, skylake aion, iris A tool for obtaining maximally-localised Wannier functions kim-api 2.1.3 2019b broadwell, skylake iris Open Knowledgebase of Interatomic Models. KIM is an API and OpenKIM is a collection of interatomic models (potentials) for atomistic simulations.  This is a library that can be used by simulation programs to get access to the models in the OpenKIM database. This EasyBuild only installs the API, the models can be installed with the package openkim-models, or the user can install them manually by running kim-api-collections-management install user MODELNAME or kim-api-collections-management install user OpenKIM to install them all. libctl 4.0.0 2019b broadwell, skylake iris libctl is a free Guile-based library implementing flexible control files for scientific simulations. libxc 4.3.4, 5.1.2 2019b, 2020b broadwell, skylake, epyc iris, aion Libxc is a library of exchange-correlation functionals for density-functional theory. The aim is to provide a portable, well tested and reliable set of exchange and correlation functionals. spglib-python 1.16.0 2020b broadwell, epyc, skylake, gpu aion, iris Spglib for Python. Spglib is a library for finding and handling crystal symmetries written in C. yaff 1.6.0 2019b broadwell, skylake iris Yaff stands for 'Yet another force field'. It is a pythonic force-field code."},{"location":"software/swsets/compiler/","title":"Compilers","text":"<p>Alphabetical list of available ULHPC software belonging to the 'compiler' category. To load a software of this category, use: <code>module load compiler/&lt;software&gt;[/&lt;version&gt;]</code></p> Software Versions Swsets Architectures Clusters Description AOCC 3.1.0 2020b epyc aion AMD Optimized C/C++ &amp; Fortran compilers (AOCC) based on LLVM 12.0 Clang 11.0.1, 9.0.1 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris C, C++, Objective-C compiler, based on LLVM.  Does not include C++ standard library -- use libstdc++ from GCC. GCC 10.2.0, 8.3.0 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Java, and Ada, as well as libraries for these languages (libstdc++, libgcj,...). GCCcore 10.2.0, 8.3.0 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Java, and Ada, as well as libraries for these languages (libstdc++, libgcj,...). Go 1.14.1, 1.16.6 2019b, 2020b broadwell, skylake, epyc iris, aion Go is an open source programming language that makes it easy to build simple, reliable, and efficient software. LLVM 10.0.1, 11.0.0, 9.0.0, 9.0.1 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris The LLVM Core libraries provide a modern source- and target-independent optimizer, along with code generation support for many popular CPUs (as well as some less common ones!) These libraries are built around a well specified code representation known as the LLVM intermediate representation (\"LLVM IR\"). The LLVM Core libraries are well documented, and it is particularly easy to invent your own language (or port an existing compiler) to use LLVM as an optimizer and code generator. PGI 19.10 2019b broadwell, skylake iris C, C++ and Fortran compilers from The Portland Group - PGI iccifort 2019.5.281, 2020.4.304 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Intel C, C++ &amp; Fortran compilers"},{"location":"software/swsets/data/","title":"Data processing","text":"<p>Alphabetical list of available ULHPC software belonging to the 'data' category. To load a software of this category, use: <code>module load data/&lt;software&gt;[/&lt;version&gt;]</code></p> Software Versions Swsets Architectures Clusters Description Arrow 0.16.0 2019b broadwell, skylake iris Apache Arrow (incl. PyArrow Python bindings)), a cross-language development platform for in-memory data. DB_File 1.855 2020b broadwell, epyc, skylake aion, iris Perl5 access to Berkeley DB version 1.x. GDAL 3.0.2, 3.2.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion GDAL is a translator library for raster geospatial data formats that is released under an X/MIT style Open Source license by the Open Source Geospatial Foundation. As a library, it presents a single abstract data model to the calling application for all supported formats. It also comes with a variety of useful commandline utilities for data translation and processing. HDF5 1.10.5, 1.10.7 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion HDF5 is a data model, library, and file format for storing and managing data. It supports an unlimited variety of datatypes, and is designed for flexible and efficient I/O and for high volume and complex data. HDF 4.2.15 2020b broadwell, epyc, skylake, gpu aion, iris HDF (also known as HDF4) is a library and multi-object file format for storing and managing data between machines. LAME 3.100 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion LAME is a high quality MPEG Audio Layer III (MP3) encoder licensed under the LGPL. XML-LibXML 2.0201, 2.0206 2019b, 2020b broadwell, skylake, epyc iris, aion Perl binding for libxml2 dask 2021.2.0 2020b broadwell, epyc, skylake, gpu aion, iris Dask natively scales Python. Dask provides advanced parallelism for analytics, enabling performance at scale for the tools you love. h5py 2.10.0, 3.1.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion HDF5 for Python (h5py) is a general-purpose Python interface to the Hierarchical Data Format library, version 5. HDF5 is a versatile, mature scientific software library designed for the fast, flexible storage of enormous amounts of data. netCDF-Fortran 4.5.2, 4.5.3 2019b, 2020b broadwell, skylake, epyc iris, aion NetCDF (network Common Data Form) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. netCDF 4.7.1, 4.7.4 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion NetCDF (network Common Data Form) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. scikit-learn 0.23.2 2020b broadwell, epyc, skylake, gpu aion, iris Scikit-learn integrates machine learning algorithms in the tightly-knit scientific Python world, building upon numpy, scipy, and matplotlib. As a machine-learning module, it provides versatile tools for data mining and analysis in any field of science and engineering. It strives to be simple and efficient, accessible to everybody, and reusable in various contexts."},{"location":"software/swsets/debugger/","title":"Debugging","text":"<p>Alphabetical list of available ULHPC software belonging to the 'debugger' category. To load a software of this category, use: <code>module load debugger/&lt;software&gt;[/&lt;version&gt;]</code></p> Software Versions Swsets Architectures Clusters Description GDB 10.1, 9.1 2020b, 2019b broadwell, epyc, skylake aion, iris The GNU Project Debugger Valgrind 3.15.0, 3.16.1 2019b, 2020b broadwell, skylake, epyc iris, aion Valgrind: Debugging and profiling tools"},{"location":"software/swsets/devel/","title":"Development","text":"<p>Alphabetical list of available ULHPC software belonging to the 'devel' category. To load a software of this category, use: <code>module load devel/&lt;software&gt;[/&lt;version&gt;]</code></p> Software Versions Swsets Architectures Clusters Description Autoconf 2.69 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Autoconf is an extensible package of M4 macros that produce shell scripts to automatically configure software source code packages. These scripts can adapt the packages to many kinds of UNIX-like systems without manual user intervention. Autoconf creates a configuration script for a package from a template file that lists the operating system features that the package can use, in the form of M4 macro calls. Automake 1.16.1, 1.16.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Automake: GNU Standards-compliant Makefile generator Autotools 20180311, 20200321 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion This bundle collect the standard GNU build tools: Autoconf, Automake and libtool Bazel 0.26.1, 0.29.1, 3.7.2 2019b, 2020b gpu, broadwell, skylake, epyc iris, aion Bazel is a build tool that builds code quickly and reliably. It is used to build the majority of Google's software. Boost 1.71.0, 1.74.0 2019b, 2020b broadwell, skylake, epyc iris, aion Boost provides free peer-reviewed portable C++ source libraries. CMake 3.15.3, 3.18.4, 3.20.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion CMake, the cross-platform, open-source build system.  CMake is a family of tools designed to build, test and package software. DBus 1.13.12, 1.13.18 2019b, 2020b broadwell, skylake, epyc iris, aion D-Bus is a message bus system, a simple way for applications to talk to one another.  In addition to interprocess communication, D-Bus helps coordinate process lifecycle; it makes it simple and reliable to code a \"single instance\" application or daemon, and to launch applications and daemons on demand when their services are needed. Doxygen 1.8.16, 1.8.20 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Doxygen is a documentation system for C++, C, Java, Objective-C, Python, IDL (Corba and Microsoft flavors), Fortran, VHDL, PHP, C#, and to some extent D. Flink 1.11.2 2020b broadwell, epyc, skylake aion, iris Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale. GObject-Introspection 1.63.1, 1.66.1 2019b, 2020b broadwell, skylake, epyc iris, aion GObject introspection is a middleware layer between C libraries (using GObject) and language bindings. The C library can be scanned at compile time and generate a metadata file, in addition to the actual native C library. Then at runtime, language bindings can read this metadata and automatically provide bindings to call into the C library. M4 1.4.18 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion GNU M4 is an implementation of the traditional Unix macro processor. It is mostly SVR4 compatible although it has some extensions (for example, handling more than 9 positional parameters to macros). GNU M4 also has built-in functions for including files, running shell commands, doing arithmetic, etc. Mako 1.1.0, 1.1.3 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion A super-fast templating language that borrows the best ideas from the existing templating languages Maven 3.6.3 2019b, 2020b broadwell, skylake, epyc iris, aion Binary maven install, Apache Maven is a software project management and comprehension tool. Based on the concept of a project object model (POM), Maven can manage a project's build, reporting and documentation from a central piece of information. PCRE2 10.33, 10.35 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion The PCRE library is a set of functions that implement regular expression pattern matching using the same syntax and semantics as Perl 5. PCRE 8.43, 8.44 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion The PCRE library is a set of functions that implement regular expression pattern matching using the same syntax and semantics as Perl 5. PyTorch 1.4.0, 1.7.1, 1.8.1, 1.9.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Tensors and Dynamic neural networks in Python with strong GPU acceleration. PyTorch is a deep learning framework that puts Python first. Qt5 5.13.1, 5.14.2 2019b, 2020b broadwell, skylake, epyc iris, aion Qt is a comprehensive cross-platform C++ application framework. ReFrame 2.21, 3.6.3 2019b, 2020b broadwell, skylake, epyc iris, aion ReFrame is a framework for writing regression tests for HPC systems. SQLite 3.29.0, 3.33.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion SQLite: SQL Database Engine in a C Library SWIG 4.0.1, 4.0.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion SWIG is a software development tool that connects programs written in C and C++ with a variety of high-level programming languages. Spack 0.12.1 2019b, 2020b broadwell, skylake, epyc iris, aion Spack is a package manager for supercomputers, Linux, and macOS. It makes installing scientific software easy. With Spack, you can build a package with multiple versions, configurations, platforms, and compilers, and all of these builds can coexist on the same machine. Spark 2.4.3 2019b broadwell, skylake iris Spark is Hadoop MapReduce done in memory ant 1.10.6, 1.10.7, 1.10.9 2019b, 2020b broadwell, skylake, epyc iris, aion Apache Ant is a Java library and command-line tool whose mission is to drive processes described in build files as targets and extension points dependent upon each other. The main known usage of Ant is the build of Java applications. flatbuffers-python 1.12 2020b broadwell, epyc, skylake, gpu aion, iris Python Flatbuffers runtime library. flatbuffers 1.12.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion FlatBuffers: Memory Efficient Serialization Library gflags 2.2.2 2019b broadwell, skylake iris The gflags package contains a C++ library that implements commandline flags processing.  It includes built-in support for standard types such as string and the ability to define flags in the source file in which they are used. glog 0.4.0 2019b broadwell, skylake iris A C++ implementation of the Google logging module. googletest 1.10.0 2019b broadwell, skylake iris Google's framework for writing C++ tests on a variety of platforms gperf 3.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion GNU gperf is a perfect hash function generator. For a given list of strings, it produces a hash function and hash table, in form of C or C++ code, for looking up a value depending on the input string. The hash function is perfect, which means that the hash table has no collisions, and the hash table lookup needs a single string comparison only. intltool 0.51.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion intltool is a set of tools to centralize translation of many different file formats using GNU gettext-compatible PO files. makeinfo 6.7 2020b broadwell, epyc, skylake, gpu aion, iris makeinfo is part of the Texinfo project, the official documentation format of the GNU project. ncurses 6.0, 6.1, 6.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion The Ncurses (new curses) library is a free software emulation of curses in System V Release 4.0, and more. It uses Terminfo format, supports pads and color and multiple highlights and forms characters and function-key mapping, and has all the other SYSV-curses enhancements over BSD Curses. nsync 1.24.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion nsync is a C library that exports various synchronization primitives, such as mutexes pkg-config 0.29.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion pkg-config is a helper tool used when compiling applications and libraries. It helps you insert the correct compiler options on the command line so an application can use gcc -o test test.c <code>pkg-config --libs --cflags glib-2.0</code> for instance, rather than hard-coding values on where to find glib (or other libraries). pkgconfig 1.5.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion pkgconfig is a Python module to interface with the pkg-config command line tool protobuf-python 3.10.0, 3.14.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Python Protocol Buffers runtime library. protobuf 2.5.0, 3.10.0, 3.14.0 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Google Protocol Buffers setuptools 41.0.1 2019b broadwell, skylake iris Easily download, build, install, upgrade, and uninstall Python packages sparsehash 2.0.3, 2.0.4 2019b, 2020b broadwell, skylake, epyc iris, aion An extremely memory-efficient hash_map implementation. 2 bits/entry overhead! The SparseHash library contains several hash-map implementations, including implementations that optimize for space or speed. texinfo 6.7 2019b broadwell, skylake iris Texinfo is the official documentation format of the GNU project. typing-extensions 3.7.4.3 2019b, 2020b gpu, broadwell, epyc, skylake iris, aion Typing Extensions \u2013 Backported and Experimental Type Hints for Python xorg-macros 1.19.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion X.org macros utilities."},{"location":"software/swsets/lang/","title":"Programming Languages","text":"<p>Alphabetical list of available ULHPC software belonging to the 'lang' category. To load a software of this category, use: <code>module load lang/&lt;software&gt;[/&lt;version&gt;]</code></p> Software Versions Swsets Architectures Clusters Description Anaconda3 2020.02, 2020.11 2019b, 2020b broadwell, skylake, epyc iris, aion Built to complement the rich, open source Python community, the Anaconda platform provides an enterprise-ready data analytics platform that empowers companies to adopt a modern open data science analytics architecture. Bison 3.3.2, 3.5.3, 3.7.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Bison is a general-purpose parser generator that converts an annotated context-free grammar into a deterministic LR or generalized LR (GLR) parser employing LALR(1) parser tables. FriBidi 1.0.10, 1.0.5 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris The Free Implementation of the Unicode Bidirectional Algorithm. Guile 1.8.8, 2.2.4 2019b broadwell, skylake iris Guile is a programming language, designed to help programmers create flexible applications that can be extended by users or other programmers with plug-ins, modules, or scripts. Java 1.8.0_241, 11.0.2, 13.0.2, 16.0.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Java Platform, Standard Edition (Java SE) lets you develop and deploy Java applications on desktops and servers. Julia 1.4.1, 1.6.2 2019b, 2020b broadwell, skylake, epyc iris, aion Julia is a high-level, high-performance dynamic programming language for numerical computing Lua 5.1.5, 5.4.2 2019b, 2020b broadwell, skylake, epyc iris, aion Lua is a powerful, fast, lightweight, embeddable scripting language. Lua combines simple procedural syntax with powerful data description constructs based on associative arrays and extensible semantics. Lua is dynamically typed, runs by interpreting bytecode for a register-based virtual machine, and has automatic memory management with incremental garbage collection, making it ideal for configuration, scripting, and rapid prototyping. NASM 2.14.02, 2.15.05 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion NASM: General-purpose x86 assembler Perl 5.30.0, 5.32.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Larry Wall's Practical Extraction and Report Language This is a minimal build without any modules. Should only be used for build dependencies. Python 2.7.16, 2.7.18, 3.7.4, 3.8.6 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Python is a programming language that lets you work more quickly and integrate your systems more effectively. R 3.6.2, 4.0.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion R is a free software environment for statistical computing and graphics. Ruby 2.7.1, 2.7.2 2019b, 2020b broadwell, skylake, epyc iris, aion Ruby is a dynamic, open source programming language with a focus on simplicity and productivity. It has an elegant syntax that is natural to read and easy to write. Rust 1.37.0 2019b broadwell, skylake iris Rust is a systems programming language that runs blazingly fast, prevents segfaults, and guarantees thread safety. SciPy-bundle 2019.10, 2020.11 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Bundle of Python packages for scientific software Tcl 8.6.10, 8.6.9 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Tcl (Tool Command Language) is a very powerful but easy to learn dynamic programming language, suitable for a very wide range of uses, including web and desktop applications, networking, administration, testing and many more. Tkinter 3.7.4, 3.8.6 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Tkinter module, built with the Python buildsystem Yasm 1.3.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Yasm: Complete rewrite of the NASM assembler with BSD license flex 2.6.4 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Flex (Fast Lexical Analyzer) is a tool for generating scanners. A scanner, sometimes called a tokenizer, is a program which recognizes lexical patterns in text. nodejs 12.19.0 2020b broadwell, epyc, skylake, gpu aion, iris Node.js is a platform built on Chrome's JavaScript runtime for easily building fast, scalable network applications. Node.js uses an event-driven, non-blocking I/O model that makes it lightweight and efficient, perfect for data-intensive real-time applications that run across distributed devices. numba 0.52.0 2020b broadwell, epyc, skylake, gpu aion, iris Numba is an Open Source NumPy-aware optimizing compiler for Python sponsored by Continuum Analytics, Inc. It uses the remarkable LLVM compiler infrastructure to compile Python syntax to machine code."},{"location":"software/swsets/lib/","title":"Libraries","text":"<p>Alphabetical list of available ULHPC software belonging to the 'lib' category. To load a software of this category, use: <code>module load lib/&lt;software&gt;[/&lt;version&gt;]</code></p> Software Versions Swsets Architectures Clusters Description ACTC 1.1 2019b, 2020b broadwell, skylake, epyc iris, aion ACTC converts independent triangles into triangle strips or fans. Boost.Python 1.74.0 2020b broadwell, epyc, skylake aion, iris Boost.Python is a C++ library which enables seamless interoperability between C++ and the Python programming language. Check 0.15.2 2020b gpu iris Check is a unit testing framework for C. It features a simple interface for defining unit tests, putting little in the way of the developer. Tests are run in a separate address space, so both assertion failures and code errors that cause segmentation faults or other signals can be caught. Test results are reportable in the following: Subunit, TAP, XML, and a generic logging format. FLAC 1.3.3 2020b broadwell, epyc, skylake, gpu aion, iris FLAC stands for Free Lossless Audio Codec, an audio format similar to MP3, but lossless, meaning that audio is compressed in FLAC without any loss in quality. Flask 1.1.2 2020b broadwell, epyc, skylake, gpu aion, iris Flask is a lightweight WSGI web application framework. It is designed to make getting started quick and easy, with the ability to scale up to complex applications. This module includes the Flask extensions: Flask-Cors GDRCopy 2.1 2020b gpu iris A low-latency GPU memory copy library based on NVIDIA GPUDirect RDMA technology. ICU 64.2, 67.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion ICU is a mature, widely used set of C/C++ and Java libraries providing Unicode and Globalization support for software applications. JsonCpp 1.9.3, 1.9.4 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion JsonCpp is a C++ library that allows manipulating JSON values, including serialization and deserialization to and from strings. It can also preserve existing comment in unserialization/serialization steps, making it a convenient format to store user input files. LMDB 0.9.24 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion LMDB is a fast, memory-efficient database. With memory-mapped files, it has the read performance of a pure in-memory database while retaining the persistence of standard disk-based databases. LibTIFF 4.0.10, 4.1.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion tiff: Library and tools for reading and writing TIFF data files NCCL 2.4.8, 2.8.3 2019b, 2020b gpu iris The NVIDIA Collective Communications Library (NCCL) implements multi-GPU and multi-node collective communication primitives that are performance optimized for NVIDIA GPUs. NSPR 4.21, 4.29 2019b, 2020b broadwell, skylake, epyc iris, aion Netscape Portable Runtime (NSPR) provides a platform-neutral API for system level and libc-like functions. NSS 3.45, 3.57 2019b, 2020b broadwell, skylake, epyc iris, aion Network Security Services (NSS) is a set of libraries designed to support cross-platform development of security-enabled client and server applications. PROJ 6.2.1, 7.2.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Program proj is a standard Unix filter function which converts geographic longitude and latitude coordinates into cartesian coordinates PyTorch-Geometric 1.6.3 2020b broadwell, epyc, skylake, gpu aion, iris PyTorch Geometric (PyG) is a geometric deep learning extension library for PyTorch. PyYAML 5.1.2, 5.3.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion PyYAML is a YAML parser and emitter for the Python programming language. RDFlib 5.0.0 2020b broadwell, epyc, skylake, gpu aion, iris RDFLib is a Python library for working with RDF, a simple yet powerful language for representing information. SDL2 2.0.14 2020b broadwell, epyc, skylake aion, iris SDL: Simple DirectMedia Layer, a cross-platform multimedia library SIONlib 1.7.6 2019b broadwell, skylake iris SIONlib is a scalable I/O library for parallel access to task-local files. The library not only supports writing and reading binary data to or from several thousands of processors into a single or a small number of physical files, but also provides global open and close functions to access SIONlib files in parallel. This package provides a stripped-down installation of SIONlib for use with performance tools (e.g., Score-P), with renamed symbols to avoid conflicts when an application using SIONlib itself is linked against a tool requiring a different SIONlib version. TensorFlow 1.15.5, 2.1.0, 2.4.1, 2.5.0 2019b, 2020b gpu, broadwell, skylake, epyc iris, aion An open-source software library for Machine Intelligence UCX 1.9.0 2020b broadwell, epyc, skylake, gpu aion, iris Unified Communication X An open-source production grade communication framework for data centric and high-performance applications Xerces-C++ 3.2.2 2019b broadwell, skylake iris Xerces-C++ is a validating XML parser written in a portable subset of C++. Xerces-C++ makes it easy to give your application the ability to read and write XML data. A shared library is provided for parsing, generating, manipulating, and validating XML documents using the DOM, SAX, and SAX2 APIs. YACS 0.1.8 2020b broadwell, epyc, skylake aion, iris YACS was created as a lightweight library to define and manage system configurations, such as those commonly found in software designed for scientific experimentation. These \"configurations\" typically cover concepts like hyperparameters used in training a machine learning model or configurable model hyperparameters, such as the depth of a convolutional neural network. double-conversion 3.1.4, 3.1.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Efficient binary-decimal and decimal-binary conversion routines for IEEE doubles. elfutils 0.183 2020b gpu iris The elfutils project provides libraries and tools for ELF files and DWARF data. gc 7.6.12 2019b broadwell, skylake iris The Boehm-Demers-Weiser conservative garbage collector can be used as a garbage collecting replacement for C malloc or C++ new. giflib 5.2.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion giflib is a library for reading and writing gif images. It is API and ABI compatible with libungif which was in wide use while the LZW compression algorithm was patented. jemalloc 5.2.1 2019b broadwell, skylake iris jemalloc is a general purpose malloc(3) implementation that emphasizes fragmentation avoidance and scalable concurrency support. libdrm 2.4.102, 2.4.99 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Direct Rendering Manager runtime library. libepoxy 1.5.4 2019b, 2020b broadwell, skylake, epyc iris, aion Epoxy is a library for handling OpenGL function pointer management for you libevent 2.1.11, 2.1.12 2019b, 2020b broadwell, skylake, epyc iris, aion The libevent API provides a mechanism to execute a callback function when a specific event occurs on a file descriptor or after a timeout has been reached.  Furthermore, libevent also support callbacks due to signals or regular timeouts. libffi 3.2.1, 3.3 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion The libffi library provides a portable, high level programming interface to various calling conventions. This allows a programmer to call any function specified by a call interface description at run-time. libgd 2.2.5, 2.3.0 2019b, 2020b broadwell, skylake, epyc iris, aion GD is an open source code library for the dynamic creation of images by programmers. libgeotiff 1.5.1, 1.6.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Library for reading and writing coordinate system information from/to GeoTIFF files libglvnd 1.2.0, 1.3.2 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion libglvnd is a vendor-neutral dispatch layer for arbitrating OpenGL API calls between multiple vendors. libgpuarray 0.7.6 2019b, 2020b gpu iris Library to manipulate tensors on the GPU. libiconv 1.16 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libiconv converts from one character encoding to another through Unicode conversion libjpeg-turbo 2.0.3, 2.0.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion libjpeg-turbo is a fork of the original IJG libjpeg which uses SIMD to accelerate baseline JPEG compression and decompression. libjpeg is a library that implements JPEG image encoding, decoding and transcoding. libmatheval 1.1.11 2019b broadwell, skylake iris GNU libmatheval is a library (callable from C and Fortran) to parse and evaluate symbolic expressions input as text. libogg 1.3.4 2020b broadwell, epyc, skylake, gpu aion, iris Ogg is a multimedia container format, and the native file and stream format for the Xiph.org multimedia codecs. libpng 1.6.37 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion libpng is the official PNG reference library libreadline 8.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion The GNU Readline library provides a set of functions for use by applications that allow users to edit command lines as they are typed in. Both Emacs and vi editing modes are available. The Readline library includes additional functions to maintain a list of previously-entered command lines, to recall and perhaps reedit those lines, and perform csh-like history expansion on previous commands. libsndfile 1.0.28 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libsndfile is a C library for reading and writing files containing sampled sound (such as MS Windows WAV and the Apple/SGI AIFF format) through one standard library interface. libtirpc 1.3.1 2020b broadwell, epyc, skylake, gpu aion, iris Libtirpc is a port of Suns Transport-Independent RPC library to Linux. libtool 2.4.6 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion GNU libtool is a generic library support script. Libtool hides the complexity of using shared libraries behind a consistent, portable interface. libunistring 0.9.10 2019b broadwell, skylake iris This library provides functions for manipulating Unicode strings and for manipulating C strings according to the Unicode standard. libunwind 1.3.1, 1.4.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion The primary goal of libunwind is to define a portable and efficient C programming interface (API) to determine the call-chain of a program. The API additionally provides the means to manipulate the preserved (callee-saved) state of each call-frame and to resume execution at any point in the call-chain (non-local goto). The API supports both local (same-process) and remote (across-process) operation. As such, the API is useful in a number of applications libvorbis 1.3.7 2020b broadwell, epyc, skylake, gpu aion, iris Ogg Vorbis is a fully open, non-proprietary, patent-and-royalty-free, general-purpose compressed audio format libwebp 1.1.0 2020b broadwell, epyc, skylake aion, iris WebP is a modern image format that provides superior lossless and lossy compression for images on the web. Using WebP, webmasters and web developers can create smaller, richer images that make the web faster. libxml2 2.9.10, 2.9.9 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Libxml2 is the XML C parser and toolchain developed for the Gnome project (but usable outside of the Gnome platform). libxslt 1.1.34 2019b broadwell, skylake iris Libxslt is the XSLT C library developed for the GNOME project (but usable outside of the Gnome platform). libyaml 0.2.2, 0.2.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion LibYAML is a YAML parser and emitter written in C. lxml 4.4.2 2019b broadwell, skylake iris The lxml XML toolkit is a Pythonic binding for the C libraries libxml2 and libxslt. lz4 1.9.2 2020b broadwell, epyc, skylake, gpu aion, iris LZ4 is lossless compression algorithm, providing compression speed at 400 MB/s per core. It features an extremely fast decoder, with speed in multiple GB/s per core. nettle 3.5.1, 3.6 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Nettle is a cryptographic library that is designed to fit easily in more or less any context: In crypto toolkits for object-oriented languages (C++, Python, Pike, ...), in applications like LSH or GNUPG, or even in kernel space. phonopy 2.2.0 2019b broadwell, skylake iris Phonopy is an open source package of phonon calculations based on the supercell approach. pocl 1.4, 1.6 2019b, 2020b gpu iris Pocl is a portable open source (MIT-licensed) implementation of the OpenCL standard pybind11 2.4.3, 2.6.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion pybind11 is a lightweight header-only library that exposes C++ types in Python and vice versa, mainly to create Python bindings of existing C++ code. scikit-build 0.11.1 2020b broadwell, epyc, skylake, gpu aion, iris Scikit-Build, or skbuild, is an improved build system generator for CPython C/C++/Fortran/Cython extensions. snappy 1.1.7, 1.1.8 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Snappy is a compression/decompression library. It does not aim for maximum compression, or compatibility with any other compression library; instead, it aims for very high speeds and reasonable compression. tbb 2019_U9, 2020.2, 2020.3 2019b, 2020b broadwell, skylake, epyc iris, aion Intel(R) Threading Building Blocks (Intel(R) TBB) lets you easily write parallel C++ programs that take full advantage of multicore performance, that are portable, composable and have future-proof scalability. tqdm 4.56.2 2020b broadwell, epyc, skylake, gpu aion, iris A fast, extensible progress bar for Python and CLI zlib 1.2.11 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion zlib is designed to be a free, general-purpose, legally unencumbered -- that is, not covered by any patents -- lossless data-compression library for use on virtually any computer hardware and operating system. zstd 1.4.5 2020b broadwell, epyc, skylake, gpu aion, iris Zstandard is a real-time compression algorithm, providing high compression ratios. It offers a very wide range of compression/speed trade-off, while being backed by a very fast decoder. It also offers a special mode for small data, called dictionary compression, and can create dictionaries from any sample set."},{"location":"software/swsets/math/","title":"Mathematics","text":"<p>Alphabetical list of available ULHPC software belonging to the 'math' category. To load a software of this category, use: <code>module load math/&lt;software&gt;[/&lt;version&gt;]</code></p> Software Versions Swsets Architectures Clusters Description CPLEX 12.10 2019b broadwell, skylake iris IBM ILOG CPLEX Optimizer's mathematical programming technology enables analytical decision support for improving efficiency, reducing costs, and increasing profitability. Dakota 6.11.0, 6.15.0 2019b, 2020b broadwell, skylake iris The Dakota project delivers both state-of-the-art research and robust, usable software for optimization and UQ. Broadly, the Dakota software's advanced parametric analyses enable design exploration, model calibration, risk analysis, and quantification of margins and uncertainty with computational models.\" ELPA 2019.11.001, 2020.11.001 2019b, 2020b broadwell, epyc, skylake iris, aion Eigenvalue SoLvers for Petaflop-Applications . Eigen 3.3.7, 3.3.8, 3.4.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Eigen is a C++ template library for linear algebra: matrices, vectors, numerical solvers, and related algorithms. GEOS 3.8.0, 3.9.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion GEOS (Geometry Engine - Open Source) is a C++ port of the Java Topology Suite (JTS) GMP 6.1.2, 6.2.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion GMP is a free library for arbitrary precision arithmetic, operating on signed integers, rational numbers, and floating point numbers. Gurobi 9.0.0, 9.1.2 2019b, 2020b broadwell, skylake, epyc iris, aion The Gurobi Optimizer is a state-of-the-art solver for mathematical programming. The solvers in the Gurobi Optimizer were designed from the ground up to exploit modern architectures and multi-core processors, using the most advanced implementations of the latest algorithms. Harminv 1.4.1 2019b broadwell, skylake iris Harminv is a free program (and accompanying library) to solve the problem of harmonic inversion - given a discrete-time, finite-length signal that consists of a sum of finitely-many sinusoids (possibly exponentially decaying) in a given bandwidth, it determines the frequencies, decay constants, amplitudes, and phases of those sinusoids. ISL 0.23 2020b broadwell, epyc, skylake aion, iris isl is a library for manipulating sets and relations of integer points bounded by linear constraints. Keras 2.3.1, 2.4.3 2019b, 2020b gpu, broadwell, epyc, skylake iris, aion Keras is a deep learning API written in Python, running on top of the machine learning platform TensorFlow. MATLAB 2019b, 2020a, 2021a 2019b, 2020b broadwell, skylake, epyc iris, aion MATLAB is a high-level language and interactive environment that enables you to perform computationally intensive tasks faster than with traditional programming languages such as C, C++, and Fortran. METIS 5.1.0 2019b, 2020b broadwell, skylake, epyc iris, aion METIS is a set of serial programs for partitioning graphs, partitioning finite element meshes, and producing fill reducing orderings for sparse matrices. The algorithms implemented in METIS are based on the multilevel recursive-bisection, multilevel k-way, and multi-constraint partitioning schemes. MPC 1.2.1 2020b broadwell, epyc, skylake aion, iris Gnu Mpc is a C library for the arithmetic of complex numbers with arbitrarily high precision and correct rounding of the result. It extends the principles of the IEEE-754 standard for fixed precision real floating point numbers to complex numbers, providing well-defined semantics for every operation. At the same time, speed of operation at high precision is a major design goal. MPFR 4.0.2, 4.1.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion The MPFR library is a C library for multiple-precision floating-point computations with correct rounding. MUMPS 5.3.5 2020b broadwell, epyc, skylake aion, iris A parallel sparse direct solver Mathematica 12.0.0, 12.1.0 2019b, 2020b broadwell, skylake, epyc iris, aion Mathematica is a computational software program used in many scientific, engineering, mathematical and computing fields. Mesquite 2.3.0 2019b broadwell, skylake iris Mesh-Quality Improvement Library ParMETIS 4.0.3 2019b broadwell, skylake iris ParMETIS is an MPI-based parallel library that implements a variety of algorithms for partitioning unstructured graphs, meshes, and for computing fill-reducing orderings of sparse matrices. ParMETIS extends the functionality provided by METIS and includes routines that are especially suited for parallel AMR computations and large scale numerical simulations. The algorithms implemented in ParMETIS are based on the parallel multilevel k-way graph-partitioning, adaptive repartitioning, and parallel multi-constrained partitioning schemes. ParMGridGen 1.0 2019b broadwell, skylake iris ParMGridGen is an MPI-based parallel library that is based on the serial package MGridGen, that implements (serial) algorithms for obtaining a sequence of successive coarse grids that are well-suited for geometric multigrid methods. SCOTCH 6.0.9, 6.1.0 2019b, 2020b broadwell, skylake, epyc iris, aion Software package and libraries for sequential and parallel graph partitioning, static mapping, and sparse matrix block ordering, and sequential mesh and hypergraph partitioning. Stata 17 2020b broadwell, epyc, skylake aion, iris Stata is a complete, integrated statistical software package that provides everything you need for data analysis, data management, and graphics. Theano 1.0.4, 1.1.2 2019b, 2020b gpu, broadwell, epyc, skylake iris, aion Theano is a Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. Voro++ 0.4.6 2019b broadwell, skylake iris Voro++ is a software library for carrying out three-dimensional computations of the Voronoi tessellation. A distinguishing feature of the Voro++ library is that it carries out cell-based calculations, computing the Voronoi cell for each particle individually. It is particularly well-suited for applications that rely on cell-based statistics, where features of Voronoi cells (eg. volume, centroid, number of faces) can be used to analyze a system of particles. gmsh 4.8.4 2020b broadwell, epyc, skylake aion, iris Gmsh is a 3D finite element grid generator with a build-in CAD engine and post-processor. libcerf 1.13, 1.14 2019b, 2020b broadwell, skylake, epyc iris, aion libcerf is a self-contained numeric library that provides an efficient and accurate implementation of complex error functions, along with Dawson, Faddeeva, and Voigt functions. magma 2.5.1, 2.5.4 2019b, 2020b gpu iris The MAGMA project aims to develop a dense linear algebra library similar to LAPACK but for heterogeneous/hybrid architectures, starting with current Multicore+GPU systems. molmod 1.4.5 2019b broadwell, skylake iris MolMod is a Python library with many compoments that are useful to write molecular modeling programs. scipy 1.4.1 2019b broadwell, skylake, gpu iris SciPy is a collection of mathematical algorithms and convenience functions built on the Numpy extension for Python."},{"location":"software/swsets/mpi/","title":"MPI","text":"<p>Alphabetical list of available ULHPC software belonging to the 'mpi' category. To load a software of this category, use: <code>module load mpi/&lt;software&gt;[/&lt;version&gt;]</code></p> Software Versions Swsets Architectures Clusters Description OpenMPI 3.1.4, 4.0.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion The Open MPI Project is an open source MPI-3 implementation. impi 2018.5.288, 2019.9.304 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Intel MPI Library, compatible with MPICH ABI"},{"location":"software/swsets/numlib/","title":"Numerical libraries","text":"<p>Alphabetical list of available ULHPC software belonging to the 'numlib' category. To load a software of this category, use: <code>module load numlib/&lt;software&gt;[/&lt;version&gt;]</code></p> Software Versions Swsets Architectures Clusters Description Armadillo 10.5.3, 9.900.1 2020b, 2019b broadwell, epyc, skylake aion, iris Armadillo is an open-source C++ linear algebra library (matrix maths) aiming towards a good balance between speed and ease of use. Integer, floating point and complex numbers are supported, as well as a subset of trigonometric and statistics functions. CGAL 4.14.1, 5.2 2019b, 2020b broadwell, skylake, epyc iris, aion The goal of the CGAL Open Source Project is to provide easy access to efficient and reliable geometric algorithms in the form of a C++ library. FFTW 3.3.8 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion FFTW is a C subroutine library for computing the discrete Fourier transform (DFT) in one or more dimensions, of arbitrary input size, and of both real and complex data. GSL 2.6 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion The GNU Scientific Library (GSL) is a numerical library for C and C++ programmers. The library provides a wide range of mathematical routines such as random number generators, special functions and least-squares fitting. Hypre 2.20.0 2020b broadwell, epyc, skylake aion, iris Hypre is a library for solving large, sparse linear systems of equations on massively parallel computers. The problems of interest arise in the simulation codes being developed at LLNL and elsewhere to study physical phenomena in the defense, environmental, energy, and biological sciences. NLopt 2.6.1, 2.6.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion NLopt is a free/open-source library for nonlinear optimization, providing a common interface for a number of different free optimization routines available online as well as original implementations of various other algorithms. OpenBLAS 0.3.12, 0.3.7 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris OpenBLAS is an optimized BLAS library based on GotoBLAS2 1.13 BSD version. PETSc 3.14.4 2020b broadwell, epyc, skylake aion, iris PETSc, pronounced PET-see (the S is silent), is a suite of data structures and routines for the scalable (parallel) solution of scientific applications modeled by partial differential equations. SLEPc 3.14.2 2020b broadwell, epyc, skylake aion, iris SLEPc (Scalable Library for Eigenvalue Problem Computations) is a software library for the solution of large scale sparse eigenvalue problems on parallel computers. It is an extension of PETSc and can be used for either standard or generalized eigenproblems, with real or complex arithmetic. It can also be used for computing a partial SVD of a large, sparse, rectangular matrix, and to solve quadratic eigenvalue problems. ScaLAPACK 2.0.2, 2.1.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion The ScaLAPACK (or Scalable LAPACK) library includes a subset of LAPACK routines redesigned for distributed memory MIMD parallel computers. SuiteSparse 5.8.1 2020b broadwell, epyc, skylake aion, iris SuiteSparse is a collection of libraries manipulate sparse matrices. arpack-ng 3.7.0, 3.8.0 2019b, 2020b broadwell, skylake, epyc iris, aion ARPACK is a collection of Fortran77 subroutines designed to solve large scale eigenvalue problems. cuDNN 7.6.4.38, 8.0.4.30, 8.0.5.39 2019b, 2020b gpu iris The NVIDIA CUDA Deep Neural Network library (cuDNN) is a GPU-accelerated library of primitives for deep neural networks. imkl 2019.5.281, 2020.4.304 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Intel Math Kernel Library is a library of highly optimized, extensively threaded math routines for science, engineering, and financial applications that require maximum performance. Core math functions include BLAS, LAPACK, ScaLAPACK, Sparse Solvers, Fast Fourier Transforms, Vector Math, and more."},{"location":"software/swsets/perf/","title":"Performance measurements","text":"<p>Alphabetical list of available ULHPC software belonging to the 'perf' category. To load a software of this category, use: <code>module load perf/&lt;software&gt;[/&lt;version&gt;]</code></p> Software Versions Swsets Architectures Clusters Description Advisor 2019_update5 2019b broadwell, skylake iris Vectorization Optimization and Thread Prototyping - Vectorize &amp; thread code or performance \u201cdies\u201d - Easy workflow + data + tips = faster code faster - Prioritize, Prototype &amp; Predict performance gain CubeGUI 4.4.4 2019b broadwell, skylake iris Cube, which is used as performance report explorer for Scalasca and Score-P, is a generic tool for displaying a multi-dimensional performance space consisting of the dimensions (i) performance metric, (ii) call path, and (iii) system resource. Each dimension can be represented as a tree, where non-leaf nodes of the tree can be collapsed or expanded to achieve the desired level of granularity. This module provides the Cube graphical report explorer. CubeLib 4.4.4 2019b broadwell, skylake iris Cube, which is used as performance report explorer for Scalasca and Score-P, is a generic tool for displaying a multi-dimensional performance space consisting of the dimensions (i) performance metric, (ii) call path, and (iii) system resource. Each dimension can be represented as a tree, where non-leaf nodes of the tree can be collapsed or expanded to achieve the desired level of granularity. This module provides the Cube general purpose C++ library component and command-line tools. CubeWriter 4.4.3 2019b broadwell, skylake iris Cube, which is used as performance report explorer for Scalasca and Score-P, is a generic tool for displaying a multi-dimensional performance space consisting of the dimensions (i) performance metric, (ii) call path, and (iii) system resource. Each dimension can be represented as a tree, where non-leaf nodes of the tree can be collapsed or expanded to achieve the desired level of granularity. This module provides the Cube high-performance C writer library component. OPARI2 2.0.5 2019b broadwell, skylake iris OPARI2, the successor of Forschungszentrum Juelich's OPARI, is a source-to-source instrumentation tool for OpenMP and hybrid codes. It surrounds OpenMP directives and runtime library calls with calls to the POMP2 measurement interface. OTF2 2.2 2019b broadwell, skylake iris The Open Trace Format 2 is a highly scalable, memory efficient event trace data format plus support library. It is the new standard trace format for Scalasca, Vampir, and TAU and is open for other tools. PAPI 6.0.0 2019b, 2020b broadwell, skylake, epyc iris, aion PAPI provides the tool designer and application engineer with a consistent interface and methodology for use of the performance counter hardware found in most major microprocessors. PAPI enables software engineers to see, in near real time, the relation between software performance and processor events. In addition Component PAPI provides access to a collection of components that expose performance measurement opportunites across the hardware and software stack. PDT 3.25 2019b broadwell, skylake iris Program Database Toolkit (PDT) is a framework for analyzing source code written in several programming languages and for making rich program knowledge accessible to developers of static and dynamic analysis tools. PDT implements a standard program representation, the program database (PDB), that can be accessed in a uniform way through a class library supporting common PDB operations. Scalasca 2.5 2019b broadwell, skylake iris Scalasca is a software tool that supports the performance optimization of parallel programs by measuring and analyzing their runtime behavior. The analysis identifies potential performance bottlenecks -- in particular those concerning communication and synchronization -- and offers guidance in exploring their causes. Score-P 6.0 2019b broadwell, skylake iris The Score-P measurement infrastructure is a highly scalable and easy-to-use tool suite for profiling, event tracing, and online analysis of HPC applications."},{"location":"software/swsets/phys/","title":"Physics","text":"<p>Alphabetical list of available ULHPC software belonging to the 'phys' category. To load a software of this category, use: <code>module load phys/&lt;software&gt;[/&lt;version&gt;]</code></p> Software Versions Swsets Architectures Clusters Description Elk 6.3.2, 7.0.12 2019b, 2020b broadwell, skylake, epyc iris, aion An all-electron full-potential linearised augmented-plane wave (FP-LAPW) code with many advanced features. Written originally at Karl-Franzens-Universit\u00e4t Graz as a milestone of the EXCITING EU Research and Training Network, the code is designed to be as simple as possible so that new developments in the field of density functional theory (DFT) can be added quickly and reliably. FDS 6.7.1, 6.7.6 2019b, 2020b broadwell, skylake, epyc iris, aion Fire Dynamics Simulator (FDS) is a large-eddy simulation (LES) code for low-speed flows, with an emphasis on smoke and heat transport from fires. Meep 1.4.3 2019b broadwell, skylake iris Meep (or MEEP) is a free finite-difference time-domain (FDTD) simulation software package developed at MIT to model electromagnetic systems. UDUNITS 2.2.26 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion UDUNITS supports conversion of unit specifications between formatted and binary forms, arithmetic manipulation of units, and conversion of values between compatible scales of measurement. VASP 5.4.4, 6.2.1 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion The Vienna Ab initio Simulation Package (VASP) is a computer program for atomic scale materials modelling, e.g. electronic structure calculations and quantum-mechanical molecular dynamics, from first principles."},{"location":"software/swsets/system/","title":"System-level software","text":"<p>Alphabetical list of available ULHPC software belonging to the 'system' category. To load a software of this category, use: <code>module load system/&lt;software&gt;[/&lt;version&gt;]</code></p> Software Versions Swsets Architectures Clusters Description CUDA 10.1.243, 11.1.1 2019b, 2020b gpu iris CUDA (formerly Compute Unified Device Architecture) is a parallel computing platform and programming model created by NVIDIA and implemented by the graphics processing units (GPUs) that they produce. CUDA gives developers access to the virtual instruction set and memory of the parallel computational elements in CUDA GPUs. CUDAcore 11.1.1 2020b gpu iris CUDA (formerly Compute Unified Device Architecture) is a parallel computing platform and programming model created by NVIDIA and implemented by the graphics processing units (GPUs) that they produce. CUDA gives developers access to the virtual instruction set and memory of the parallel computational elements in CUDA GPUs. ULHPC-bd 2020b 2020b broadwell, epyc, skylake aion, iris Generic Module bundle for BigData Analytics software in use on the UL HPC Facility ULHPC-bio 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion Generic Module bundle for Bioinformatics, biology and biomedical software in use on the UL HPC Facility, especially at LCSB ULHPC-cs 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion Generic Module bundle for Computational science software in use on the UL HPC Facility, including: - Computer Aided Engineering, incl. CFD - Chemistry, Computational Chemistry and Quantum Chemistry - Data management &amp; processing tools - Earth Sciences - Quantum Computing - Physics and physical systems simulations ULHPC-dl 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion Generic Module bundle for (CPU-version) of AI / Deep Learning / Machine Learning software in use on the UL HPC Facility ULHPC-gpu 2019b, 2020b 2019b, 2020b gpu iris Generic Module bundle for GPU accelerated User Software in use on the UL HPC Facility ULHPC-math 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion Generic Module bundle for  High-level mathematical software and Linear Algrebra libraries in use on the UL HPC Facility ULHPC-toolchains 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion Generic Module bundle that contains all the dependencies required to enable toolchains and building tools/programming language in use on the UL HPC Facility ULHPC-tools 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion Misc tools, incl. - perf:      Performance tools - tools:     General purpose tools hwloc 1.11.12, 2.2.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion The Portable Hardware Locality (hwloc) software package provides a portable abstraction (across OS, versions, architectures, ...) of the hierarchical topology of modern architectures, including NUMA memory nodes, sockets, shared caches, cores and simultaneous multithreading. It also gathers various system attributes such as cache and memory information as well as the locality of I/O devices such as network interfaces, InfiniBand HCAs or GPUs. It primarily aims at helping applications with gathering information about modern computing hardware so as to exploit it accordingly and efficiently. libpciaccess 0.14, 0.16 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Generic PCI access library."},{"location":"software/swsets/toolchain/","title":"Toolchains (software stacks)","text":"<p>Alphabetical list of available ULHPC software belonging to the 'toolchain' category. To load a software of this category, use: <code>module load toolchain/&lt;software&gt;[/&lt;version&gt;]</code></p> Software Versions Swsets Architectures Clusters Description foss 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion GNU Compiler Collection (GCC) based compiler toolchain, including OpenMPI for MPI support, OpenBLAS (BLAS and LAPACK support), FFTW and ScaLAPACK. fosscuda 2019b, 2020b 2019b, 2020b gpu iris GCC based compiler toolchain with CUDA support, and including OpenMPI for MPI support, OpenBLAS (BLAS and LAPACK support), FFTW and ScaLAPACK. gcccuda 2019b, 2020b 2019b, 2020b gpu iris GNU Compiler Collection (GCC) based compiler toolchain, along with CUDA toolkit. gompi 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion GNU Compiler Collection (GCC) based compiler toolchain, including OpenMPI for MPI support. gompic 2019b, 2020b 2019b, 2020b gpu iris GNU Compiler Collection (GCC) based compiler toolchain along with CUDA toolkit, including OpenMPI for MPI support with CUDA features enabled. iccifortcuda 2019b, 2020b 2019b, 2020b gpu iris Intel C, C++ &amp; Fortran compilers with CUDA toolkit iimpi 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Intel C/C++ and Fortran compilers, alongside Intel MPI. iimpic 2019b, 2020b 2019b, 2020b gpu iris Intel C/C++ and Fortran compilers, alongside Intel MPI and CUDA. intel 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Compiler toolchain including Intel compilers, Intel MPI and Intel Math Kernel Library (MKL). intelcuda 2019b, 2020b 2019b, 2020b gpu iris Intel Cluster Toolkit Compiler Edition provides Intel C/C++ and Fortran compilers, Intel MPI &amp; Intel MKL, with CUDA toolkit"},{"location":"software/swsets/tools/","title":"Utilities","text":"<p>Alphabetical list of available ULHPC software belonging to the 'tools' category. To load a software of this category, use: <code>module load tools/&lt;software&gt;[/&lt;version&gt;]</code></p> Software Versions Swsets Architectures Clusters Description ANSYS 19.4, 21.1 2019b, 2020b broadwell, skylake, epyc iris, aion ANSYS simulation software enables organizations to confidently predict how their products will operate in the real world. We believe that every product is a promise of something greater. ArmForge 20.0.3 2019b, 2020b broadwell, skylake, epyc iris, aion The industry standard development package for C, C++ and Fortran high performance code on Linux. Forge is designed to handle the complex software projects - including parallel, multiprocess and multithreaded code. Arm Forge combines an industry-leading debugger, Arm DDT, and an out-of-the-box-ready profiler, Arm MAP. ArmReports 20.0.3 2019b, 2020b broadwell, skylake, epyc iris, aion Arm Performance Reports - a low-overhead tool that produces one-page text and HTML reports summarizing and characterizing both scalar and MPI application performance. Arm Performance Reports runs transparently on optimized production-ready codes by adding a single command to your scripts, and provides the most effective way to characterize and understand the performance of HPC application runs. Aspera-CLI 3.9.1, 3.9.6 2019b, 2020b broadwell, skylake, epyc iris, aion IBM Aspera Command-Line Interface (the Aspera CLI) is a collection of Aspera tools for performing high-speed, secure data transfers from the command line. The Aspera CLI is for users and organizations who want to automate their transfer workflows. DB 18.1.32, 18.1.40 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Berkeley DB enables the development of custom data management solutions, without the overhead traditionally associated with such custom projects. DMTCP 2.5.2 2019b broadwell, skylake iris DMTCP is a tool to transparently checkpoint the state of multiple simultaneous applications, including multi-threaded and distributed applications. It operates directly on the user binary executable, without any Linux kernel modules or other kernel modifications. EasyBuild 4.3.0, 4.3.3, 4.4.1, 4.4.2, 4.5.4 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion EasyBuild is a software build and installation framework written in Python that allows you to install software in a structured, repeatable and robust way. GLPK 4.65 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion The GLPK (GNU Linear Programming Kit) package is intended for solving large-scale linear programming (LP), mixed integer programming (MIP), and other related problems. It is a set of routines written in ANSI C and organized in the form of a callable library. Ghostscript 9.50, 9.53.3 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Ghostscript is a versatile processor for PostScript data with the ability to render PostScript to different targets. It used to be part of the cups printing stack, but is no longer used for that. Hadoop 2.10.0 2020b broadwell, epyc, skylake aion, iris Hadoop MapReduce by Cloudera Horovod 0.19.1, 0.22.0 2019b, 2020b broadwell, skylake, gpu iris Horovod is a distributed training framework for TensorFlow. Inspector 2019_update5 2019b broadwell, skylake iris Intel Inspector XE is an easy to use memory error checker and thread checker for serial and parallel applications Meson 0.51.2, 0.55.3 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Meson is a cross-platform build system designed to be both as fast and as user friendly as possible. Ninja 1.10.1, 1.9.0 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Ninja is a small build system with a focus on speed. Singularity 3.6.0, 3.8.1 2019b, 2020b broadwell, skylake, epyc iris, aion SingularityCE is an open source container platform designed to be simple, fast, and secure.  Singularity is optimized for EPC and HPC workloads, allowing untrusted users to run untrusted containers in a trusted way. Sumo 1.3.1 2019b broadwell, skylake iris Sumo is an open source, highly portable, microscopic and continuous traffic simulation package designed to handle large road networks. Szip 2.1.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Szip compression software, providing lossless compression of scientific data UnZip 6.0 2020b broadwell, epyc, skylake, gpu aion, iris UnZip is an extraction utility for archives compressed in .zip format (also called \"zipfiles\"). Although highly compatible both with PKWARE's PKZIP and PKUNZIP utilities for MS-DOS and with Info-ZIP's own Zip program, our primary objectives have been portability and non-MSDOS functionality. VTune 2019_update8, 2020_update3 2019b, 2020b broadwell, skylake, epyc iris, aion Intel VTune Amplifier XE is the premier performance profiler for C, C++, C#, Fortran, Assembly and Java. XZ 5.2.4, 5.2.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion xz: XZ utilities Z3 4.8.10 2020b broadwell, epyc, skylake, gpu aion, iris Z3 is a theorem prover from Microsoft Research. Zip 3.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Zip is a compression and file packaging/archive utility. Although highly compatible both with PKWARE's PKZIP and PKUNZIP utilities for MS-DOS and with Info-ZIP's own UnZip, our primary objectives have been portability and other-than-MSDOS functionality archspec 0.1.0 2019b broadwell, skylake iris A library for detecting, labeling, and reasoning about microarchitectures binutils 2.32, 2.35 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion binutils: GNU binary utilities bokeh 2.2.3 2020b broadwell, epyc, skylake, gpu aion, iris Statistical and novel interactive HTML plots for Python bzip2 1.0.8 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion bzip2 is a freely available, patent free, high-quality data compressor. It typically compresses files to within 10% to 15% of the best available techniques (the PPM family of statistical compressors), whilst being around twice as fast at compression and six times faster at decompression. cURL 7.66.0, 7.72.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion libcurl is a free and easy-to-use client-side URL transfer library, supporting DICT, FILE, FTP, FTPS, Gopher, HTTP, HTTPS, IMAP, IMAPS, LDAP, LDAPS, POP3, POP3S, RTMP, RTSP, SCP, SFTP, SMTP, SMTPS, Telnet and TFTP. libcurl supports SSL certificates, HTTP POST, HTTP PUT, FTP uploading, HTTP form based upload, proxies, cookies, user+password authentication (Basic, Digest, NTLM, Negotiate, Kerberos), file transfer resume, http proxy tunneling and more. expat 2.2.7, 2.2.9 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Expat is an XML parser library written in C. It is a stream-oriented parser in which an application registers handlers for things the parser might find in the XML document (like start tags) gettext 0.19.8.1, 0.20.1, 0.21 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion GNU 'gettext' is an important step for the GNU Translation Project, as it is an asset on which we may build many other steps. This package offers to programmers, translators, and even users, a well integrated set of tools and documentation git 2.23.0, 2.28.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. gocryptfs 1.7.1, 2.0.1 2019b, 2020b broadwell, skylake, epyc iris, aion Encrypted overlay filesystem written in Go. gocryptfs uses file-based encryption that is implemented as a mountable FUSE filesystem. Each file in gocryptfs is stored as one corresponding encrypted file on the hard disk. groff 1.22.4 2020b broadwell, epyc, skylake, gpu aion, iris Groff (GNU troff) is a typesetting system that reads plain text mixed with formatting commands and produces formatted output. gzip 1.10 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion gzip (GNU zip) is a popular data compression program as a replacement for compress help2man 1.47.16, 1.47.4, 1.47.8 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris help2man produces simple manual pages from the '--help' and '--version' output of other commands. hypothesis 4.44.2, 5.41.2, 5.41.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Hypothesis is an advanced testing library for Python. It lets you write tests which are parametrized by a source of examples, and then generates simple and comprehensible examples that make your tests fail. This lets you find more bugs in your code with less work. itac 2019.4.036 2019b broadwell, skylake iris The Intel Trace Collector is a low-overhead tracing library that performs event-based tracing in applications. The Intel Trace Analyzer provides a convenient way to monitor application activities gathered by the Intel Trace Collector through graphical displays. libarchive 3.4.3 2020b broadwell, epyc, skylake, gpu aion, iris Multi-format archive and compression library networkx 2.5 2020b broadwell, epyc, skylake, gpu aion, iris NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks. numactl 2.0.12, 2.0.13 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion The numactl program allows you to run your application program on specific cpu's and memory nodes. It does this by supplying a NUMA memory policy to the operating system before running your program. The libnuma library provides convenient ways for you to add NUMA memory policies into your own program. re2c 1.2.1, 2.0.3 2019b, 2020b broadwell, skylake, epyc iris, aion re2c is a free and open-source lexer generator for C and C++. Its main goal is generating fast lexers: at least as fast as their reasonably optimized hand-coded counterparts. Instead of using traditional table-driven approach, re2c encodes the generated finite state automata directly in the form of conditional jumps and comparisons. util-linux 2.34, 2.36 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Set of Linux utilities"},{"location":"software/swsets/vis/","title":"Visualisation","text":"<p>Alphabetical list of available ULHPC software belonging to the 'vis' category. To load a software of this category, use: <code>module load vis/&lt;software&gt;[/&lt;version&gt;]</code></p> Software Versions Swsets Architectures Clusters Description ATK 2.34.1, 2.36.0 2019b, 2020b broadwell, skylake, epyc iris, aion ATK provides the set of accessibility interfaces that are implemented by other toolkits and applications. Using the ATK interfaces, accessibility tools have full access to view and control running applications. FFmpeg 4.2.1, 4.3.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion A complete, cross-platform solution to record, convert and stream audio and video. FLTK 1.3.5 2019b, 2020b broadwell, skylake, epyc iris, aion FLTK is a cross-platform C++ GUI toolkit for UNIX/Linux (X11), Microsoft Windows, and MacOS X. FLTK provides modern GUI functionality without the bloat and supports 3D graphics via OpenGL and its built-in GLUT emulation. FreeImage 3.18.0 2020b broadwell, epyc, skylake aion, iris FreeImage is an Open Source library project for developers who would like to support popular graphics image formats like PNG, BMP, JPEG, TIFF and others as needed by today's multimedia applications. FreeImage is easy to use, fast, multithreading safe. GLib 2.62.0, 2.66.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion GLib is one of the base libraries of the GTK+ project GTK+ 3.24.13, 3.24.23 2019b, 2020b broadwell, skylake, epyc iris, aion GTK+ is the primary library used to construct user interfaces in GNOME. It provides all the user interface controls, or widgets, used in a common graphical application. Its object-oriented API allows you to construct user interfaces without dealing with the low-level details of drawing and device interaction. Gdk-Pixbuf 2.38.2, 2.40.0 2019b, 2020b broadwell, skylake, epyc iris, aion The Gdk Pixbuf is a toolkit for image loading and pixel buffer manipulation. It is used by GTK+ 2 and GTK+ 3 to load and manipulate images. In the past it was distributed as part of GTK+ 2 but it was split off into a separate package in preparation for the change to GTK+ 3. HarfBuzz 2.6.4, 2.6.7 2019b, 2020b broadwell, skylake, epyc iris, aion HarfBuzz is an OpenType text shaping engine. ImageMagick 7.0.10-35, 7.0.9-5 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris ImageMagick is a software suite to create, edit, compose, or convert bitmap images JasPer 2.0.14, 2.0.24 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion The JasPer Project is an open-source initiative to provide a free software-based reference implementation of the codec specified in the JPEG-2000 Part-1 standard. LittleCMS 2.11, 2.9 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Little CMS intends to be an OPEN SOURCE small-footprint color management engine, with special focus on accuracy and performance. Mesa 19.1.7, 19.2.1, 20.2.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Mesa is an open-source implementation of the OpenGL specification - a system for rendering interactive 3D graphics. OpenCV 4.2.0, 4.5.1 2019b, 2020b broadwell, skylake, epyc iris, aion OpenCV (Open Source Computer Vision Library) is an open source computer vision and machine learning software library. OpenCV was built to provide a common infrastructure for computer vision applications and to accelerate the use of machine perception in the commercial products. Includes extra modules for OpenCV from the contrib repository. OpenEXR 2.5.5 2020b broadwell, epyc, skylake aion, iris OpenEXR is a high dynamic-range (HDR) image file format developed by Industrial Light &amp; Magic for use in computer imaging applications POV-Ray 3.7.0.8 2020b broadwell, epyc, skylake aion, iris The Persistence of Vision Raytracer, or POV-Ray, is a ray tracing program which generates images from a text-based scene description, and is available for a variety of computer platforms. POV-Ray is a high-quality, Free Software tool for creating stunning three-dimensional graphics. The source code is available for those wanting to do their own ports. Pango 1.44.7, 1.47.0 2019b, 2020b broadwell, skylake, epyc iris, aion Pango is a library for laying out and rendering of text, with an emphasis on internationalization. Pango can be used anywhere that text layout is needed, though most of the work on Pango so far has been done in the context of the GTK+ widget toolkit. Pango forms the core of text and font handling for GTK+-2.x. ParaView 5.6.2, 5.8.1 2019b, 2020b broadwell, skylake, epyc iris, aion ParaView is a scientific parallel visualizer. Pillow 6.2.1, 8.0.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Pillow is the 'friendly PIL fork' by Alex Clark and Contributors. PIL is the Python Imaging Library by Fredrik Lundh and Contributors. PyOpenGL 3.1.5 2020b broadwell, epyc, skylake aion, iris PyOpenGL is the most common cross platform Python binding to OpenGL and related APIs. PyQt5 5.15.1 2020b broadwell, epyc, skylake aion, iris PyQt5 is a set of Python bindings for v5 of the Qt application framework from The Qt Company. This bundle includes PyQtWebEngine, a set of Python bindings for The Qt Company\u2019s Qt WebEngine framework. PyQtGraph 0.11.1 2020b broadwell, epyc, skylake aion, iris PyQtGraph is a pure-python graphics and GUI library built on PyQt5/PySide2 and numpy. Tk 8.6.10, 8.6.9 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Tk is an open source, cross-platform widget toolchain that provides a library of basic elements for building a graphical user interface (GUI) in many different programming languages. VMD 1.9.4a51 2020b broadwell, epyc, skylake aion, iris VMD is a molecular visualization program for displaying, animating, and analyzing large biomolecular systems using 3-D graphics and built-in scripting. VTK 8.2.0, 9.0.1 2019b, 2020b broadwell, skylake, epyc iris, aion The Visualization Toolkit (VTK) is an open-source, freely available software system for 3D computer graphics, image processing and visualization. VTK consists of a C++ class library and several interpreted interface layers including Tcl/Tk, Java, and Python. VTK supports a wide variety of visualization algorithms including: scalar, vector, tensor, texture, and volumetric methods; and advanced modeling techniques such as: implicit modeling, polygon reduction, mesh smoothing, cutting, contouring, and Delaunay triangulation. VirtualGL 2.6.2 2019b broadwell, skylake iris VirtualGL is an open source toolkit that gives any Linux or Unix remote display software the ability to run OpenGL applications with full hardware acceleration. X11 20190717, 20201008 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion The X Window System (X11) is a windowing system for bitmap displays Xvfb 1.20.9 2020b broadwell, epyc, skylake, gpu aion, iris Xvfb is an X server that can run on machines with no display hardware and no physical input devices. It emulates a dumb framebuffer using virtual memory. at-spi2-atk 2.34.1, 2.38.0 2019b, 2020b broadwell, skylake, epyc iris, aion AT-SPI 2 toolkit bridge at-spi2-core 2.34.0, 2.38.0 2019b, 2020b broadwell, skylake, epyc iris, aion Assistive Technology Service Provider Interface. cairo 1.16.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Cairo is a 2D graphics library with support for multiple output devices. Currently supported output targets include the X Window System (via both Xlib and XCB), Quartz, Win32, image buffers, PostScript, PDF, and SVG file output. Experimental backends include OpenGL, BeOS, OS/2, and DirectFB fontconfig 2.13.1, 2.13.92 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Fontconfig is a library designed to provide system-wide font configuration, customization and application access. freetype 2.10.1, 2.10.3 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion FreeType 2 is a software font engine that is designed to be small, efficient, highly customizable, and portable while capable of producing high-quality output (glyph images). It can be used in graphics libraries, display servers, font conversion tools, text image generation tools, and many other products as well. gnuplot 5.2.8, 5.4.1 2019b, 2020b broadwell, skylake, epyc iris, aion Portable interactive, function plotting utility libGLU 9.0.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion The OpenGL Utility Library (GLU) is a computer graphics library for OpenGL. matplotlib 3.1.1, 3.3.3 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion matplotlib is a python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms. matplotlib can be used in python scripts, the python and ipython shell, web application servers, and six graphical user interface toolkits. pixman 0.38.4, 0.40.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Pixman is a low-level software library for pixel manipulation, providing features such as image compositing and trapezoid rasterization. Important users of pixman are the cairo graphics library and the X server. scikit-image 0.18.1 2020b broadwell, epyc, skylake, gpu aion, iris scikit-image is a collection of algorithms for image processing. x264 20190925, 20201026 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion x264 is a free software library and application for encoding video streams into the H.264/MPEG-4 AVC compression format, and is released under the terms of the GNU GPL. x265 3.2, 3.3 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion x265 is a free software library and application for encoding video streams into the H.265 AVC compression format, and is released under the terms of the GNU GPL. xprop 1.2.4, 1.2.5 2019b, 2020b broadwell, skylake, epyc iris, aion The xprop utility is for displaying window and font properties in an X server. One window or font is selected using the command line arguments or possibly in the case of a window, by clicking on the desired window. A list of properties is then given, possibly with formatting information."},{"location":"software/visu/paraview/","title":"ParaView","text":"<p>ParaView is an open-source, multi-platform data analysis and visualization application. ParaView users can quickly build visualizations to analyze their data using qualitative and quantitative techniques. The data exploration can be done interactively in 3D or programmatically using ParaView\u2019s batch processing capabilities.</p> <p>ParaView was developed to analyse extremely large datasets using distributed memory computing resources. It can be run on supercomputers to analyse datasets of petascale size as well as on laptops for smaller data, has become an integral tool in many national laboratories, universities and industry, and has won several awards related to high performance computation.</p> <p>ParaView ia an open-source, interactive, scalable, data analysis and scientific visualization tools. It can be used to visualize the simulation data or processing the data by using GUI or non-interactive mode by using the Python scripting. Using non-interacting mode, that is using the python scripting is much faster than using the interactive mode, when the data set is larger in both ParaView and VisIt.</p>"},{"location":"software/visu/paraview/#available-versions-of-paraview-in-ulhpc","title":"Available versions of ParaView in ULHPC","text":"<p>To check available versions of ParaView at ULHPC type <code>module spider paraview</code>. The following list shows the available versions of ParaView in ULHPC. <pre><code>vis/ParaView/5.5.0-intel-2018a-mpi\nvis/ParaView/5.6.2-foss-2019a-mpi\nvis/ParaView/5.6.2-intel-2019a-mpi\n</code></pre></p>"},{"location":"software/visu/paraview/#interactive-mode","title":"Interactive mode","text":"<p>To open an ParaView in the interactive mode, please follow the following steps:</p> <pre><code># From your local computer\n$ ssh -X iris-cluster\n\n# Reserve the node for interactive computation\n$ salloc -p interactive --time=00:30:00 --ntasks 1 -c 4 --x11  # OR si --x11 [...]\n\n# Load the module abinit and needed environment\n$ module purge \n$ module load swenv/default-env/latest\n$ module load vis/ParaView/5.6.2-intel-2019a-mpi\n\n$ paraview &amp;\n</code></pre>"},{"location":"software/visu/paraview/#batch-mode","title":"Batch mode","text":"<pre><code>#!/bin/bash -l\n#SBATCH -J ParaView\n###SBATCH -A &lt;project name&gt;\n#SBATCH -N 2\n#SBATCH --ntasks-per-node=28\n#SBATCH --time=00:30:00\n#SBATCH -p batch\n\n# Load the module Paraview and needed environment\nmodule purge \nmodule load swenv/default-env/latest\nmodule load vis/ParaView/5.6.2-intel-2019a-mpi\n\nsrun -n ${SLURM_NTASKS} pvbatch python-script.py\n</code></pre>"},{"location":"software/visu/paraview/#additional-information","title":"Additional information","text":"<p>ParaView's User Manual has a detail instructions about visualization and processing data in ParaView. There are two ways of getting or writing the python script for the ParaView:</p> <ol> <li>Reading the ParaView's python scripting wiki and ParaView's Python Scripting Manual.</li> <li>Record the commands that we do in ParaView GUI. Later this commands put into python script and it can be run as python scripting in ParaView.</li> </ol> <p>Tip</p> <p>If you find some issues with the instructions above, please report it to us using support ticket.</p>"},{"location":"support/","title":"Support","text":"<p>ULHPC strives to support in a user friendly way your [super]computing needs. Note however that we are not here to make your PhD at your place ;)</p> <p> Service Now HPC Support Portal</p>"},{"location":"support/#faqtroubleshooting","title":"FAQ/Troubleshooting","text":"<ul> <li>Password reset</li> <li>Connection issues</li> <li>File Permissions<ul> <li>Access rights to project directory</li> <li>Quotas</li> </ul> </li> </ul>"},{"location":"support/#read-the-friendly-manual","title":"Read the Friendly Manual","text":"<p>We have always maintained an extensive documentation and tutorials available online, which aims at being the most up-to-date and comprehensive.</p> <p>So please, read the documentation first if you have a question of problem -- we probably provide detailed instructions here</p>"},{"location":"support/#help-desk","title":"Help Desk","text":"<p>The online help desk Service is the preferred method for contacting ULHPC.</p> <p>Tips</p> <p>Before reporting a problem or and issue, kindly remember that:</p> <ol> <li>Your issue is probably documented here on the ULHPC Technical documentation</li> <li>An event may be on-going:<ul> <li>Planned maintenance are announced at least 2 weeks in advance - -- see Maintenance and Downtime Policy</li> <li>The proper SSH banner is displayed during planned downtime</li> </ul> </li> <li>check the state of your nodes and jobs<ul> <li>Joining/monitoring running jobs</li> <li>Monitoring post-mortem Job status and efficiency</li> </ul> </li> </ol> <p> Service Now HPC Support Portal</p> <p>You can make code snippets, shell outputs, etc in your ticket much more readable by inserting a line with: <pre><code>[code]&lt;pre&gt;\n</code></pre> before the snippet, and another line with: <pre><code>&lt;/pre&gt;[/code]\n</code></pre> after it. For a full list of formatting options, see this ServiceNow article.</p> <p>Be as precise and complete as possible</p> <p>ULHPC team handle thousands of support requests per year. In order to ensure efficient timely resolution of issues, ensure that:</p> <ol> <li>you select the appropriate category (left menu)</li> <li>you include as much of the following as possible when making a request:<ul> <li>Who?  - Name and user id (login), eventually project name</li> <li>When? - When did the problem occur?</li> <li>Where? - Which cluster ? Which node ? Which job ?<ul> <li>Really include Job IDs</li> <li>Location of relevant files<ul> <li>input/output, job launcher scripts, source code, executables etc.</li> </ul> </li> </ul> </li> <li>What? - What happened? What exactly were you doing or trying to do ?<ul> <li>include Error messages - kindly report system or software messages literally and exactly.</li> <li>output of <code>module list</code></li> <li>any steps you have tried</li> <li>Steps to reproduce</li> </ul> </li> <li>Any part of this technical documentation you checked before opening the ticket</li> </ul> </li> </ol> <p>Access to the online help system requires logging in with your Uni.lu username, password, and eventually one-time password. If you are an existing user unable to log in, you can send us an email.</p> <p>Availability and Response Time</p> <p>HPC support is provided on a volunteer basis by UL HPC staff and associated UL experts working at normal business hours. We offer no guarantee on response time except with paid support contracts.</p>"},{"location":"support/#email-support","title":"Email support","text":"<p>You can contact us by mail to the ULHPC Team Email (ONLY if you cannot login/access the HPC Support helpdesk portal : <code>hpc-team@uni.lu</code></p> <p>You may also ask the help of other ULHPC users using the HPC User community mailing list: (moderated): <code>hpc-users@uni.lu</code></p>"},{"location":"systems/","title":"HPC @ Uni.lu","text":"<p>For more details, see the reference ULHPC Article:</p> <p>ACM Reference Format | ORBilu entry | slides: Sebastien Varrette, Hyacinthe Cartiaux, Sarah Peter, Emmanuel Kieffer, Teddy Valette, and Abatcha Olloh. 2022. Management of an Academic HPC &amp; Research Computing Facility: The ULHPC Experience 2.0. In 6<sup>th</sup> High Performance Computing and Cluster Technologies Conference (HPCCT 2022), July 08-10, 2022, Fuzhou, China. ACM, New York, NY, USA, 14 pages. https://doi.org/10.1145/3560442.3560445</p>"},{"location":"systems/#chronological-evolution","title":"Chronological Evolution","text":"<p>With the advent of the technological revolution and the digital transformation that made all scientific disciplines becoming computational nowadays, High-Performance Computing (HPC) is increasingly identified as a strategic asset and enabler to accelerate the research performed in all areas requiring intensive computing and large-scale Big Data analytic capabilities.</p> <p>The University of Luxembourg (UL) operates since 2007 a large academic HPC facility that remained the reference HPC implementation within the country until 2021, offering a cutting-edge research infrastructure to Luxembourg public research while serving as edge access to the Euro-HPC Luxembourg supercomputer operated by LuxProvide and more focused at serving the private sector. Special focus was laid for the ULHPC facility on the development of large computing power combined with huge data storage capacity to accelerate the research performed in intensive computing and large-scale data analytic (Big Data). This was made possible through an ambitious funding strategy enabled from the early stage of the HPC developments, which was supported at the rectorate level to establish the HPC strategy as transversal to all research domains.</p> <p>For more details:  hpc.uni.lu</p>"},{"location":"systems/#capacity-evolution","title":"Capacity evolution","text":"<p>The historically first production system installed in 2007 has been Chaos with a final theoretical peak performance of 14.5 TFlop/s. Gaia was then launched in 2011 as a replacement to reach a theoretical peak performance of 145.5 TFlops. It was the first computing cluster introducing GPU accelerators to our users. Both systems were kept running until their decommissioning in 2019.</p> <p>Info</p> <p>Currently, Iris (R_\\text{peak}=1071 TFlops) and Aion (R_\\text{peak}=1693 TFlops) are our production systems sharing the same High Performance Storage solutions.</p> <p>The below figures illustrates the evolution of the computing and storage capacity of the ULHPC facility over the last years.</p> <p></p> <p></p>"},{"location":"systems/#experimental-systems","title":"Experimental systems","text":"<p>We maintain (or used to maintain) several experimental systems in parallel (<code>nyx</code>, a testing cluster, <code>pyro</code>, an OpenStack-based cluster, <code>viridis</code>, a low-power ARM-based cluster). As of now, only our experimental Grid'5000 clusters are still maintained.</p>"},{"location":"systems/#usage","title":"Usage","text":"<p>The below figure outline the cumulative usage (in CPU Years) of the production clusters within the ULHPC facility for the time period 2015-2019.</p> <ul> <li>During their lifetime, Gaia and Chaos processed respectively 4.5 million and 1.7 million jobs, cumulating 13835 Years of CPU Time usage.</li> </ul> <p></p>"},{"location":"systems/#naming-conventions","title":"Naming conventions","text":"<p>Our clusters and supercomputers are named from Greek primordial deities or Greek mythology while keeping a name as short as possible.</p> <ul> <li>chaos was, according to Hesiod's Theogony, the first thing to exist and thus looked as appropriate.  \"Hesiod's Chaos has been interpreted as either \"the gaping void above the Earth created when Earth and Sky are separated from their primordial unity\"</li> <li>gaia is the personification of the Earth and the ancestral mother of all life. It sounded pertinent for our first system installed in Belval to serve the growing life-science community and the newly created LCSB system bio-medicine Interdisciplinary center.</li> <li>iris is the personification and goddess of the rainbow and messenger of the gods.</li> <li>aion is a Hellenistic deity associated with time, the orb or circle encompassing the universe, and the zodiac.</li> </ul>"},{"location":"systems/aion/","title":"Aion Overview","text":"<p>Aion is a Atos/Bull/AMD supercomputer which consists of 354 compute nodes, totaling 45312 compute cores and 90624 GB RAM, with a peak performance of about 1,88 PetaFLOP/s.</p> <p>All nodes are interconnected through a Fast InfiniBand (IB) HDR100 network<sup>1</sup>, configured over a ** Fat-Tree Topology** (blocking factor 1:2). Aion nodes are equipped with AMD Epyc ROME 7H12 processors.</p> <p>Two global high-performance clustered file systems are available on all ULHPC computational systems: one based on GPFS/SpectrumScale, one on Lustre.</p> <p> Aion Compute  Aion Interconnect  Global Storage</p> <p>The cluster runs a Red Hat Linux operating system. The ULHPC Team supplies on all clusters a large variety of HPC utilities, scientific applications and programming libraries to its user community. The user software environment is generated using Easybuild (EB) and is made available as environment modules from the compute nodes only.</p> <p>Slurm is the Resource and Job Management Systems (RJMS) which provides computing resources allocations and job execution. For more information: see ULHPC slurm docs.</p>"},{"location":"systems/aion/#cluster-organization","title":"Cluster Organization","text":""},{"location":"systems/aion/#data-center-configuration","title":"Data Center Configuration","text":"<p>The Aion cluster is based on a cell made of 4 BullSequana XH2000 adjacent racks installed in the CDC (Centre de Calcul) data center of the University within one of the DLC-enabled server room (CDC S-02-004) adjacent to the room hosting the Iris cluster and the global storage.</p> <p>Each rack has the following dimensions: HxWxD (mm) = 2030x750x1270 (Depth is 1350mm with aesthetic doors). The full solution with 4 racks (total dimension: dimensions: HxWxD (mm) = 2030x3000x1270) with the following characteristics:</p> Rack 1 Rack 2 Rack 3 Rack 4 TOTAL Weight [kg] 1872,4 1830,2 1830,2 1824,2 7357 kg #X2410 Rome Blade 30 29 29 30 118 #Compute Nodes 90 87 87 90 354 #Compute Cores 11520 11136 11136 11520 45312 R_\\text{peak} [TFlops] 479,23 TF 463,25 TF 463,25 TF 479,23 TF 1884.96 TF <p>For more details:  BullSequana XH2000 SpecSheet (PDF)</p>"},{"location":"systems/aion/#cooling","title":"Cooling","text":"<p>The BullSequana XH2000 is a fan less innovative cooling solution which is ultra-energy-efficient (targeting a PUE very close to 1) using an enhanced version of the Bull Direct Liquid Cooling (DLC) technology. A separate hot-water circuit minimizes the total energy consumption of a system. For more information: see [Direct] Liquid Cooling.</p> <p>The illustration on the right shows an exploded view of a compute blade with the cold plate and heat spreaders.  The DLC<sup>1</sup> components in the rack are:</p> <ul> <li>Compute nodes (CPU, Memory, Drives, GPU)</li> <li>High Speed Interconnect: HDR</li> <li>Management network: Ethernet management switches</li> <li>Power Supply Unit: DLC shelves</li> </ul> <p>The cooling area in the rack is composed of:</p> <ul> <li>3 Hydraulic chassis (HYCs) for 2+1 redundancy at the bottom of the cabinet, 10.5U height.</li> <li>Each HYCs dissipates at a maximum of 240W in the air.</li> <li>A primary manifold system connects the University hot-water loop to the HYCs primary water inlets</li> <li>A secondary manifold system connects HYCs outlets to each blade in the compute cabinet</li> </ul>"},{"location":"systems/aion/#loginaccess-servers","title":"Login/Access servers","text":"<ul> <li>Aion has 2 access servers (256 GB of memory each, general access) <code>access[1-2]</code></li> <li>Each login node has two sockets, each socket is populated with an AMD EPYC 7452 processor (2.2 GHz, 32 cores)</li> </ul> <p>Access servers are not meant for compute!</p> <ul> <li>The <code>module</code> command is not available on the access servers, only on the compute nodes</li> <li>you MUST NOT run any computing process on the access servers.</li> </ul>"},{"location":"systems/aion/#rack-cabinets","title":"Rack Cabinets","text":"<p>The Aion cluster (management compute and interconnect) is installed across the two adjacent server rooms in the premises of the Centre de Calcul (CDC), in the CDC-S02-005 server room.</p> Server Room Rack ID Purpose Type Description CDC-S02-005 D02 Network Interconnect equipment CDC-S02-005 A04 Management Management servers, Interconnect CDC-S02-004 A01 Compute regular <code>aion-[0001-0084,0319-0324]</code>, interconnect CDC-S02-004 A02 Compute regular <code>aion-[0085-0162,0325-0333]</code>, interconnect CDC-S02-004 A03 Compute regular <code>aion-[0163-0240,0334-0342]</code>, interconnect CDC-S02-004 A04 Compute regular <code>aion-[0241-0318,0343-0354]</code>, interconnect <p>In addition, the global storage equipment (GPFS/SpectrumScale and Lustre, common to both Iris and Aion clusters) is installed in another row of cabinets of the same server room.</p> <ol> <li> <p>All DLC components are built on a cold plate which cools all components by direct contact, except DIMMS for which custom heat spreaders evacuate the heat to the cold plate.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"systems/aion/compute/","title":"Aion Compute Nodes","text":"<p>Aion is a cluster of x86-64 AMD-based compute nodes. More precisely, Aion consists of 354 \"regular\" computational nodes named <code>aion-[0001-0354]</code> as follows:</p> Hostname           (#Nodes) #cores type Processors RAM R_\\text{peak}[TFlops] <code>aion-[0001-0354]</code>     (354) 45312 Regular Epyc 2 AMD Epyc ROME 7H12 @ 2.6 GHz [64c/280W] 256 GB 5.32 TF <p>Aion compute nodes compute nodes MUST be seen as 8 (virtual) processors of 16 cores each, even if physically the nodes are hosting 2 physical sockets of AMD Epyc ROME 7H12 processors having 64 cores each (total: 128 cores per node).</p> <ul> <li>As will be highlighted in the slurm resource allocation, that means that targetting a full node utilization assumes that you use the following format attributes to your jobs: <code>{sbatch|srun|si|salloc} [-N &lt;N&gt;] --ntasks-per-node &lt;8n&gt; --ntasks-per-socket &lt;n&gt; -c &lt;thread&gt;</code> where <ul> <li>you want to ensure that <code>&lt;n&gt;</code>\\times<code>&lt;thread&gt;</code>= 16 on aion</li> <li>this will bring a total of <code>&lt;N&gt;</code>\\times 8\\times<code>&lt;n&gt;</code> tasks, each on <code>&lt;thread&gt;</code> threads</li> <li>Ex: <code>-N 2 --ntasks-per-node 32 --ntasks-per-socket 4 -c 4</code> (Total: 64 tasks)</li> </ul> </li> </ul>"},{"location":"systems/aion/compute/#processor-performance","title":"Processor Performance","text":"<p>Each Aion node rely on an AMD Epyc Rome processor architecture which is binary compatible with the x86-64 architecture. Each processor has the following performance:</p> Processor Model #cores TDP(*) CPU Freq. R_\\text{peak}[TFlops] R_\\text{max}[TFlops] AMD Epyc ROME 7H12 64 280W 2.6 GHz 2.66 TF 2.13 TF <p>(*) The Thermal Design Power (TDP) represents the average power, in watts, the processor dissipates when operating at Base Frequency with all cores active under an Intel-defined, high-complexity workload.</p> Theoretical R_\\text{peak} vs. Maximum R_\\text{max} Performance for AMD Epyc <p>The AMD Epyc processors carry on 16 Double Precision (DP) ops/cycle. Thus the reported R_\\text{peak} performance is computed as follows: R_\\text{peak} = ops/cycle \\times Freq. \\times \\#Cores</p> <p>With regards the estimation of the Maximum Performance R_\\text{max}, an efficiency factor of 80% is applied. It is computed from the expected performance runs during the HPL benchmark workload.</p>"},{"location":"systems/aion/compute/#regular-dual-cpu-nodes","title":"Regular Dual-CPU Nodes","text":"<p>These nodes are packaged within BullSequana X2410 AMD compute blades.</p> <p></p> <p>Each blade contains 3 dual-socket AMD Rome nodes side-by-side, connected to the BullSequana XH2000 local interconnect network through HDR100 ports which is done through a mezzanine board. The BullSequana AMD blade is built upon a cold plate which cools all components by direct contact, except DIMMS for which custom heat spreaders evacuate the heat to the cold plate -- see exploded view Characteristics of each blade and associated compute nodes are depicted in the below table</p> BullSequana X2410 AMD blade Form Factor 1U blade comprising 3 compute nodes side-by-side #Nodes per blade 3 Processors per node 2x AMD Epyc ROME 7H12 @ 2.6 GHz [64c/280W] Architecture AMD SP3 Platform: 3x1 motherboard Memory per node 256 GB DDR4 3200MT/s (8x16 GB DIMMs per socket, 16 DIMMs per node) Network (per node) InfiniBand HDR100 single port mezzanine Storage (per node) 1x 480 GB SSD Power supply PSU shelves on top of XH2000 cabinet Cooling Cooling by direct contact Physical specs. (HxWxD) 44.45 x 600 x 540 mm <p>The four compute racks of Aion (one XH2000 Cell) holds a total of 118 blades i.e., 354 AMD Epyc compute nodes, totalling 45312 computing core  -- see Aion configuration.</p>"},{"location":"systems/aion/interconnect/","title":"Fast Local Interconnect Network","text":"<p>The Fast local interconnect network implemented within Aion relies on the Mellanox Infiniband (IB) HDR<sup>1</sup> technology. For more details, see Introduction to High-Speed InfiniBand Interconnect.</p>"},{"location":"systems/aion/interconnect/#ib-network-topology","title":"IB Network Topology","text":"<p>One of the most significant differentiators between HPC systems and lesser performing systems is, apart from the interconnect technology deployed, the supporting topology. There are several topologies commonly used in large-scale HPC deployments (Fat-Tree, 3D-Torus, Dragonfly+ etc.).</p> <p> Aion (like Iris) is part of an Island which employs a \"Fat-Tree\" Topology<sup>2</sup> which remains the widely used topology in HPC clusters due to its versatility, high bisection bandwidth and well understood routing.</p> <p>Aion IB HDR switched fabric relies on Mellanox WH40 DLC Quantum Switches located at the back of each BullSequana XH2000 racks. Each DLC cooled (see splitted view on the right) HDR switch has the following characteristics:</p> <p></p> <ul> <li>80 X HDR100 100Gb/s ports in a 1U switch (40 X HDR 200Gb/s ports if used in full HDR mode)</li> <li>16Tb/s aggregate switch throughput</li> <li>Up to 15.8 billion messages-per-second</li> <li>90ns switch latency</li> </ul> <p>Aion 2-Level 1:2 Fat-Tree  is composed of:</p> <ul> <li>12x Infiniband HDR<sup>1</sup> switches (40 HDR ports / 80 HDR100 ports)<ul> <li>8x Leaf  IB (LIB) switches (L1), each with 12 HDR L1-L2 interlinks (2 on each rack)</li> <li>4x Spine IB (SIB) switches (L2), with up to 16 HDR100 uplinks (12 used, total: 48 links) used for the interconnexion with the Iris Cluster</li> </ul> </li> <li>Up to 48 compute nodes HDR100 connection per L1 switch using 24 HDR ports with Y-cable<ul> <li>4 available HDR connections for Service, Access or Gateway node per L1 switch</li> </ul> </li> </ul> <p>The following illustration show HDRtopology within the BullSequana XH2000 cell schematically:</p> <p></p> <p>For more details:  ULHPC Fast IB Interconnect</p>"},{"location":"systems/aion/interconnect/#routing-algorithm","title":"Routing Algorithm","text":"<p>The IB Subnet Managers in Aion are configured with the Up/Down InfiniBand Routing Algorithm Up-Down is a super-set of Fat-Tree with a tracker mode that allow each node to have dedicated route. This is well adapted to IO traffic patterns, and would be used within Aion for Gateway nodes, Lustre OSS, and GPFS/SpectrumScale NSD servers.</p> <p>For more details:  Understanding Up/Down InfiniBand Routing Algorithm</p> <ol> <li> <p>High Data Rate (HDR) \u2013 200 Gb/s throughput with a very low latency, typically below 0,6\\mus. The HDR100 technology allows one 200Gbps HDR port (aggregation 4x 50Gbps) to be divided into 2 HDR100 ports with 100Gbps (2x 50Gbps) bandwidth using an [optical] \"splitter\" cable.\u00a0\u21a9\u21a9</p> </li> <li> <p>with blocking factor 1:2.\u00a0\u21a9</p> </li> </ol>"},{"location":"systems/aion/timeline/","title":"Aion Timeline","text":"<p>This page records a brief timeline of significant events and user environment changes on Aion.</p> <p></p> <p></p> <p>Details are provided below.</p>"},{"location":"systems/aion/timeline/#2019","title":"2019","text":""},{"location":"systems/aion/timeline/#september-2019","title":"September 2019","text":"<ul> <li>Official Public release of Aion  cluster tenders on TED European tender and PMP Portal (Portail des March\u00e9s Publics) on Sept 11, 2019<ul> <li>RFP 190027: Tender for the acquisition of a Complementary High Performance Computing (HPC) cluster 'Aion' for the University of Luxembourg.<ul> <li>TED Reference: TED72/2019-608787</li> <li>PMP Reference: 1901442</li> </ul> </li> </ul> </li> </ul> <p>The RFP is composed of the following Lots:</p> <ul> <li>Lot 1:   DLC Computing cluster <code>aion</code>.<ul> <li>includes cabinets, cooling units (together with any and all required piping and cooling fluids), compute node enclosures and compute nodes blades (dual- socket, relying on x86_64 processor architecture), interconnect elements (Ethernet and InfiniBand) and management servers for the HPC services (including but not limited to operational lifecycle management, Operating System and services deployment, monitoring) tied to this new cluster</li> <li>implementation within one of the University Computer Centre (CDC) server rooms (CDC S-02-004), adjacent to the server room hosting the Iris cluster and its associated storage system), specialized for hosting compute equipment supporting direct liquid cooling through a separate high temperature water circuit, thus guaranteeing unprecedented energy efficiency and equipment density. In the first phase of operation, the system will be connected to the existing cold-water circuit and must be able to operate under these conditions</li> <li>[...]</li> </ul> </li> <li>Lot 2: Adaptation and extension of the existing High-Performance Storage systems<ul> <li>includes extension of the existing primary high-performance storage solution featuring a SpectrumScale/GPFS filesystem (based on a DDN GridScaler solution installed as per attribution of the RFP 160019) hosting the user home and project directories, to enable the utilisation of the GPFS filesystem from both existing Iris and new Aion clusters while</li> <li>enabling access to the Lustre-based SCRATCH filesystem (based on a DDN ExaScaler solution installed as per attribution of the RFP 170035) from the new compute cluster is considered a plus. Enhancing and adapting the InfiniBand interconnection to guarantee current performance characteristics while under load from all clients (existing and new compute clusters) is considered a plus.</li> <li>[...]</li> </ul> </li> <li>Lot 3: Adaptation of the network (Ethernet and IB)<ul> <li>integration of the new cluster within the existing Ethernet-based data and management networks, which involves the extension and consolidation of the actual Ethernet topology</li> <li>adaptation and extension of the existing InfiniBand (IB) topology to allow for bridging the two networks (Iris \"island\" and Aion \"island\")</li> <li>[...]</li> </ul> </li> </ul>"},{"location":"systems/aion/timeline/#october-november-2019","title":"October-November 2019","text":"<ul> <li>Bids Opening for both RFPs on October 29, 2019.<ul> <li>Starting offers analysis by the ULHPC team, together with the procurement and legal department of the University</li> </ul> </li> </ul>"},{"location":"systems/aion/timeline/#december-2019","title":"December 2019","text":"<ul> <li>Awarding notification to the vendors<ul> <li>RFP 190027 attributed to the Atos to provide:</li> <li>Lot 1: the new DLC <code>aion</code> supercomputer, composed by 318 AMD compute nodes hosted within a compute cell made of 4 BullSequana XH2000 adjacent racks<ul> <li>Fast Interconnect: HDR Infiniband Fabric in a Fat tree topology (2:1 blocking)</li> <li>Associated servers and management stack</li> </ul> </li> <li>Lot 2: Adaptation and extension of the existing High-Performance Storage systems.<ul> <li>In particular, the usable storage capacity of the existing primary high-performance storage solution (SpectrumScale/GPFS filesystem) will be extended by 1720TB/1560TiB to reach a total of 4.41 PB</li> </ul> </li> <li>Lot 3:Adaptation of the network (Ethernet and IB)</li> </ul> </li> </ul> <p>See also  Atos Press Release  Aion Supercomputer Overview</p>"},{"location":"systems/aion/timeline/#2020","title":"2020","text":""},{"location":"systems/aion/timeline/#january-2020","title":"January 2020","text":"<ul> <li>Kickoff meeting -- see UL Newsletter<ul> <li>planning for a production release of the new cluster in May 2020</li> </ul> </li> </ul>"},{"location":"systems/aion/timeline/#february-march-2020-start-of-global-covid-19-crisis","title":"February-March 2020: Start of global COVID-19 crisis","text":"<ul> <li>COVID-19 Impact on HPC Activities<ul> <li>all operations tied to the preparation and installation of the new <code>aion</code> cluster are postponed</li> <li>ULHPC systems remain operational, technical and non-technical staff are working remotely from home</li> <li>Global worldwide delays on hardware equipment production and shipment</li> </ul> </li> </ul>"},{"location":"systems/aion/timeline/#july-2020","title":"July 2020","text":"<ul> <li> <p>Start Phase 3 of the deconfinement as per UL policy</p> <ul> <li>Preparation work within the CDC server room by the UL external partners slowly restarted<ul> <li>target finalization of the CDC-S02-004 server room by end of September</li> </ul> </li> <li>Assembly and factory Burn tests completed    *Lot 1: DLC ready for shipment to University<ul> <li>Target date: Sept 14, 2020 in practice postponed above Oct 19, 2020 to allow for the CDC preparation work to be completed by the University and its patners.</li> </ul> </li> <li> <p>ULHPC maintenance with physical intervention of external expert support team by DDN</p> <ul> <li>preparation work for iris storage (HW upgrade, GPFS/SpectrumScale Metadata pool extension, Lustre upgrade)</li> </ul> </li> <li> <p>Start and complete the first Statement of Work for DDN Lot 2 installation</p> </li> </ul> </li> </ul>"},{"location":"systems/aion/timeline/#aug-2020","title":"Aug 2020","text":"<ul> <li> <p>Consolidated work by ULHPC team on Slurm configuration</p> <ul> <li>Updated model for Fairshare, Account Hierarchy and limits</li> </ul> </li> <li> <p>Pre-shipment of [Part of] Ethernet network equipment (Lot 3)</p> </li> </ul>"},{"location":"systems/aion/timeline/#sept-oct-2020","title":"Sept - Oct 2020","text":"<ul> <li> <p>Delivery Lot 1 (Aion DLC) and Lot 3 (Ethernet) equipment</p> <ul> <li>Ethernet network installation done by ULHPC between Sept 3 - 24, 2020</li> </ul> </li> <li> <p>CDC S02-004 preparation work (hydraulic part)</p> <ul> <li>supposed to be completed by Sept 15, 2020 has been delayed and was finally completed on Oct 19, 2020</li> </ul> </li> </ul>"},{"location":"systems/aion/timeline/#nov-2020","title":"Nov 2020","text":"<ul> <li>Partial Delivery of equipment (servers, core switches)<ul> <li>Service servers and remaining network equipments were racked by ULHPC team</li> </ul> </li> </ul>"},{"location":"systems/aion/timeline/#dec-2020","title":"Dec 2020","text":"<ul> <li>Confinement restriction lifted in France, allowing for a french team from Atos to come onsite</li> <li>Delivery of remaining equipment (incl. Lot 1 sequana racks and compute nodes)</li> <li>Compute rack (Lot 1 DLC) installation start</li> </ul>"},{"location":"systems/aion/timeline/#2021","title":"2021","text":""},{"location":"systems/aion/timeline/#jan-feb-2021","title":"Jan - Feb 2021","text":"<ul> <li> <p>The 4 DDN expansion enclosure shipped with the lifting tools and pressure tools</p> <ul> <li> <p>Lot 1: Sequana racks and compute nodes finally postionned and internal Infiniband cabling done</p> </li> <li> <p>Lot 2: DDN disk enclosure racked</p> <ul> <li>the rack was adapted to be able to close the rear door</li> </ul> </li> <li> <p>Lot 3: Ethernet and IB Network</p> <ul> <li>ULHPC cables were used to cable service servers to make progress on the software configuration</li> </ul> </li> </ul> </li> <li> <p>Service servers and compute nodes deployment start remotely</p> </li> </ul>"},{"location":"systems/aion/timeline/#mar-apr-2021","title":"Mar - Apr 2021","text":"<ul> <li>Start GS7990 and NAS server installation (Lot 2)</li> <li>Start installtion of Lot 3 (ethernet side)</li> </ul>"},{"location":"systems/aion/timeline/#may-june-2021","title":"May - June 2021","text":"<ul> <li>IB EDR cables delivered and installed</li> <li>Merge of the Iris/Aion Infiniband island</li> </ul>"},{"location":"systems/aion/timeline/#jul-aug-sept-2021","title":"Jul - Aug - Sept 2021","text":"<ul> <li>Slurm Federation between both clusters <code>Iris</code> and <code>Aion</code></li> <li>Benchmark performance results submitted (HPL, HPCG, Green500, Graph500, IOR, IO500)</li> <li>Pre-Acceptance validated and release of the Aion supercomputer for beta testers</li> </ul>"},{"location":"systems/aion/timeline/#oct-nov-2021","title":"Oct - Nov 2021","text":"<ul> <li>Official production release of Aion supercomputer</li> <li>Inauguration of Aion Supercomputer</li> </ul> <ul> <li>11<sup>th</sup> ULHPC School 2021<ul> <li>relies in Aion for its practical sessions</li> </ul> </li> </ul>"},{"location":"systems/iris/","title":"Iris Overview","text":"<p>Iris is a Dell/Intel supercomputer which consists of 196 compute nodes, totaling 5824 compute cores and 52224 GB RAM, with a peak performance of about 1,072 PetaFLOP/s.</p> <p>All nodes are interconnected through a Fast InfiniBand (IB) EDR network<sup>1</sup>, configured over a ** Fat-Tree Topology (blocking factor 1:1.5). Iris nodes are equipped with Intel Broadwell or Skylake  processors. Several nodes are equipped with 4 Nvidia Tesla V100 SXM2 GPU accelerators. In total, Iris features 96 Nvidia V100** GPU-AI accelerators allowing for high speedup of GPU-enabled applications and AI/Deep Learning-oriented workflows. Finally,  a few large-memory (fat) computing nodes offer multiple high-core density CPUs and a large live memory capacity of 3 TB RAM/node,  meant for in-memory processing of huge data sets.</p> <p>Two global high-performance clustered file systems are available on all ULHPC computational systems: one based on GPFS/SpectrumScale, one on Lustre.</p> <p> Iris Compute  Iris Interconnect  Global Storage</p> <p>The cluster runs a Red Hat Linux Family operating system. The ULHPC Team supplies on all clusters a large variety of HPC utilities, scientific applications and programming libraries to its user community. The user software environment is generated using Easybuild (EB) and is made available as environment modules from the compute nodes only.</p> <p>Slurm is the Resource and Job Management Systems (RJMS) which provides computing resources allocations and job execution. For more information: see ULHPC slurm docs.</p>"},{"location":"systems/iris/#cluster-organization","title":"Cluster Organization","text":""},{"location":"systems/iris/#loginaccess-servers","title":"Login/Access servers","text":"<ul> <li>Iris has 2 access servers (128 GB of memory each, general access) <code>access[1-2]</code></li> <li>Each login node has two sockets, each socket is populated with an Intel Xeon E5-2697A v4 processor (2.6 GHz, 16 core)</li> </ul> <p>Access servers are not meant for compute!</p> <ul> <li>The <code>module</code> command is not available on the access servers, only on the compute nodes</li> <li>you MUST NOT run any computing process on the access servers.</li> </ul>"},{"location":"systems/iris/#rack-cabinets","title":"Rack Cabinets","text":"<p>The Iris cluster (management, compute and interconnect) is installed across 7 racks within a row of cabinets in the premises of the Centre de Calcul (CDC), in the CDC-S02-005 server room.</p> Server Room Rack ID Purpose Type Description CDC-S02-005 D02 Network Interconnect equipment CDC-S02-005 D04 Management Management servers, Interconnect CDC-S02-005 D05 Compute regular <code>iris-[001-056]</code>, interconnect CDC-S02-005 D07 Compute regular <code>iris-[057-112]</code>, interconnect CDC-S02-005 D09 Compute regular <code>iris-[113-168]</code>, interconnect CDC-S02-005 D11 Compute gpu, bigmem <code>iris-[169-177,191-193]</code>(gpu), <code>iris-[187-188]</code>(bigmem) CDC-S02-005 D12 Compute gpu, bigmem <code>iris-[178-186,194-196]</code>(gpu), <code>iris-[189-190]</code>(bigmem) <p>In addition, the global storage equipment (GPFS/SpectrumScale and Lustre, common to both Iris and Aion clusters) is installed in another row of cabinets of the same server room.</p> <ol> <li> <p>Infiniband (IB) EDR networks offer a 100 Gb/s throughput with a very low latency (0,6\\mus).\u00a0\u21a9</p> </li> </ol>"},{"location":"systems/iris/compute/","title":"Iris Compute Nodes","text":"<p>Iris is a cluster of x86-64 Intel-based compute nodes. More precisely, Iris consists of 196 computational nodes named <code>iris-[001-196]</code> and features 3 types of computing resources:</p> <ul> <li>168 \"regular\" nodes, Dual Intel Xeon Broadwell or Skylake CPU (28 cores), 128 GB of RAM</li> <li>24 \"gpu\" nodes, Dual Intel Xeon Skylake CPU (28 cores), 4 Nvidia Tesla V100 SXM2 GPU accelerators (16 or 32 GB), 768 GB RAM</li> <li>4 \"bigmem\" nodes:  Quad-Intel Xeon Skylake CPU (112 cores), 3072 GB RAM</li> </ul> Hostname        (#Nodes) Node type Processor RAM <code>iris-[001-108]</code> (108) Regular Broadwell 2 Xeon E5-2680v4 @ 2.4GHz [14c/120W] 128 GB <code>iris-[109-168]</code> (60) Regular Skylake 2 Xeon Gold 6132 @ 2.6GHz [14c/140W] 128 GB <code>iris-[169-186]</code> (18) Multi-GPUSkylake 2 Xeon Gold 6132 @ 2.6GHz [14c/140W]  4x Tesla V100 SXM2 16G 768 GB <code>iris-[191-196]</code> (6) Multi-GPUSkylake 2 Xeon Gold 6132 @ 2.6GHz [14c/140W]  4x Tesla V100 SXM2 32G 768 GB <code>iris-[187-190]</code> (4) Large MemorySkylake 4 Xeon Platinum 8180M @ 2.5GHz [28c/205W] 3072 GB"},{"location":"systems/iris/compute/#processors-performance","title":"Processors Performance","text":"<p>Each Iris node rely on an Intel x86_64 processor architecture with the following performance:</p> Processor Model #core TDP(*) CPU Freq.(AVX-512 T.Freq.) R_\\text{peak}[TFlops] R_\\text{max}[TFlops] Xeon E5-2680v4 (Broadwell) 14 120W 2.4GHz (n/a) 0.538 TF 0.46 TF Xeon Gold 6132 (Skylake) 14 140W 2.6GHz (2.3GHz) 1.03 TF 0.88 TF Xeon Platinum 8180M (Skylake) 28 205W 2.5GHz (2.3GHz) 2.06 TF 1.75 TF <p>(*) The Thermal Design Power (TDP) represents the average power, in watts, the processor dissipates when operating at Base Frequency with all cores active under an Intel-defined, high-complexity workload.</p> Theoretical R_\\text{peak} vs. Maximum R_\\text{max} Performance for Intel Broadwell/Skylake <p>The reported R_\\text{peak} performance is computed as follows for the above processors:</p> <ul> <li>The Broadwell processors carry on 16 Double Precision (DP) ops/cycle and support AVX2/FMA3.</li> <li>The selected Skylake Gold processors have two AVX512 units, thus they are capable of performing 32 DP ops/cycle YET only upon AVX-512 Turbo Frequency (i.e., the maximum all-core frequency in turbo mode) in place of the base non-AVX core frequency. The reported values are extracted from the Reference Intel Specification documentation.</li> </ul> <p>Then R_\\text{peak} = ops/cycle \\times Freq. \\times \\#Cores with the appropriate frequency (2.3 GHz instead of 2.6 for our Skylake processors).</p> <p>With regards the estimation of the Maximum Performance R_\\text{max}, an efficiency factor of 85% is applied. It is computed from the expected performance runs during the HPL benchmark workload.</p>"},{"location":"systems/iris/compute/#accelerators-performance","title":"Accelerators Performance","text":"<p>Iris is equipped with 96 NVIDIA Tesla V100-SXM2 GPU Accelerators with 16 or 32 GB of GPU memory, interconnected within each node through NVLink which provides  higher bandwidth and improved scalability for multi-GPU system configurations.</p> <p></p> NVidia GPU Model #CUDA core #Tensor core Power InterconnectBandwidth GPU Memory R_\\text{peak}[TFlops] V100-SXM2 5120 640 300W 300 GB/s 16GB 7.8 TF V100-SXM2 5120 640 300W 300 GB/s 32GB 7.8 TF"},{"location":"systems/iris/compute/#regular-dual-cpu-nodes","title":"Regular Dual-CPU Nodes","text":"<p>These nodes are packaged within Dell PowerEdge C6300 chassis, each hosting 4 PowerEdge C6320 blade servers.</p> <p></p>"},{"location":"systems/iris/compute/#broadwell-compute-nodes","title":"Broadwell Compute Nodes","text":"<p>Iris comprises 108 Dell C6320 \"regular\" compute nodes <code>iris-001-108</code> relying on Broadwell Xeon processor generation, totalling 3024 computing cores.</p> <ul> <li>Each node is configured as follows:<ul> <li>2 Intel Xeon E5-2680v4 @ 2.4GHz [14c/120W]</li> <li>RAM: 128 GB DDR4 2400MT/s  (4x16 GB DIMMs per socket, 8 DIMMs per node)</li> <li>SSD 120GB</li> <li>InfiniBand (IB) EDR ConnectX-4 Single Port</li> <li>Theoretical Peak Performance per Node: R_\\text{peak} 1.075 TF (see processor performance)</li> </ul> </li> </ul> <p>Reserving a Broadwell node</p> <p>If you want to specifically reserve a broadwell node (<code>iris-[001-108]</code>), you should use the feature <code>-C broadwell</code> on the <code>batch</code> partition: <code>{sbatch|srun|salloc} -p batch -C broadwell [...]</code></p>"},{"location":"systems/iris/compute/#skylake-compute-nodes","title":"Skylake Compute Nodes","text":"<p>Iris also features 60 Dell C6320 \"regular\" compute nodes <code>iris-109-168</code> relying on Skylake Xeon processor generation, totalling 1680 computing cores.</p> <ul> <li>Each node is configured as follows:<ul> <li>2 Intel Xeon Gold 6132 @ 2.6GHz [14c/140W]</li> <li>RAM: 128 GB DDR4 2400MT/s  (4x16 GB DIMMs per socket, 8 DIMMs per node)</li> <li>SSD 120GB</li> <li>InfiniBand (IB) EDR ConnectX-4 Single Port</li> <li>Theoretical Peak Performance per Node: R_\\text{peak} 2.061 TF (see processor performance)</li> </ul> </li> </ul> <p>Reserving a Regular Skylake node</p> <p>If you want to specifically reserve a regular skylake node (<code>iris-[109-168]</code>), you should use the feature <code>-C skylake</code> on the <code>batch</code> partition: <code>{sbatch|srun|salloc} -p batch -C skylake [...]</code></p>"},{"location":"systems/iris/compute/#multi-gpu-compute-nodes","title":"Multi-GPU Compute Nodes","text":"<p>Iris includes 24 Dell PowerEdge C4140 \"gpu\" compute nodes embedding on total 96  NVIDIA Tesla V100-SXM2 GPU Accelerators.</p> <ul> <li>Each node is configured as follows:<ul> <li>2 Intel Xeon Gold 6132 @ 2.6GHz [14c/140W]</li> <li>RAM: 768 GB DDR4 2666MT/s  (12x 32 GB DIMMs per socket, 24 DIMMs per node)</li> <li>1 Dell NVMe 1.6TB</li> <li>InfiniBand (IB) EDR ConnectX-4 Dual Port</li> <li>4x NVIDIA Tesla V100-SXM2 GPU Accelerators over NVLink<ul> <li><code>iris-[169-186]</code> feature 16G GPU memory - use <code>-C volta</code>   as slurm feature</li> <li><code>iris-[191-196]</code> feature 32G GPU memory - use <code>-C volta32</code> as slurm feature</li> </ul> </li> <li>Theoretical Peak Performance per Node: R_\\text{peak} 33.26 TF (see processor performance and accelerators performance)</li> </ul> </li> </ul> <p>Reserving a GPU node</p> <p>Multi-GPU Compute Nodes can be reserved using the <code>gpu</code> partition. Use the <code>-G [&lt;type&gt;:]&lt;number&gt;</code> to specify  the  total number of GPUs required for the job</p> <pre><code># Interactive job on 1 GPU nodes with 1 GPU\nsi-gpu -G 1\nnvidia-smi      # Check allocated GPU\n\n# Interactive job with 4 GPUs on the same node, one task per gpu, 7 cores per task\nsi-gpu -N 1 -G 4 --ntasks-per-node 4 --ntasks-per-socket 2 -c 7\n\n# Job submission on 2 nodes, 4 GPUs/node and 4 tasks/node:\nsbatch -p gpu -N 2 -G 4 --ntasks-per-node 4 --ntasks-per-socket 2 -c 7 launcher.sh\n</code></pre> <p>Do NOT reserve a GPU node if you don't need a GPU!</p> <p>Multi-GPU nodes are scarce (and very expansive) resources and should be dedicated to GPU-enabled workflows.</p> 16 GB vs. 32 GB Onboard GPU Memory <ul> <li> <p>Compute nodes with Nvidia V100-SMX2 16GB accelerators are registrered with the <code>-C volta</code> feature.</p> <ul> <li>it corresponds to the 18 Multi-GPU compute nodes <code>iris-[169-186]</code></li> </ul> </li> <li> <p>If you want to reserve GPUs with more memory (i.e. 32GB on-board HBM2), you should use <code>-C volta32</code></p> <ul> <li>you would then end on one of the 6 Multi-GPU compute nodes <code>iris-[191-196]</code></li> </ul> </li> </ul>"},{"location":"systems/iris/compute/#large-memory-compute-nodes","title":"Large-Memory Compute Nodes","text":"<p>Iris holds 4 Dell PowerEdge R840 Large-Memory (\"bibmem\") compute nodes <code>iris-[187-190]</code>, totalling 448 computing cores.</p> <ul> <li>Each node is configured as follows:<ul> <li>4 Xeon Platinum 8180M @ 2.5GHz [28c/205W]</li> <li>RAM: 3072 GB DDR4 2666MT/s  (12x64 GB DIMMs per socket, 48 DIMMs per node)</li> <li>1 Dell NVMe 1.6TB</li> <li>InfiniBand (IB) EDR ConnectX-4 Dual Port</li> <li>Theoretical Peak Performance per Node: R_\\text{peak} 8.24 TF (see processor performance)</li> </ul> </li> </ul> <p>Reserving a Large-Memory node</p> <p>These nodes can be reserved using the <code>bigmem</code> partition: <code>{sbatch|srun|salloc} -p bigmem [...]</code></p> <p>DO NOT use bigmem nodes...</p> <p>... Unless you know what you are doing. We have too few large-memory compute nodes so kindly keep them for workloads that truly need these kind of expansive resources.</p> <ul> <li>In short: carefully check your workflow and memory usage before considering using these node!<ul> <li>use <code>seff &lt;jobid&gt;</code> or <code>sacct -j &lt;jobid&gt; [...]</code> for instance</li> </ul> </li> </ul>"},{"location":"systems/iris/interconnect/","title":"Fast Local Interconnect Network","text":"<p>The Fast local interconnect network implemented within Iris relies on the Mellanox Infiniband (IB) EDR<sup>1</sup> technology. For more details, see Introduction to High-Speed InfiniBand Interconnect.</p> <p>One of the most significant differentiators between HPC systems and lesser performing systems is, apart from the interconnect technology deployed, the supporting topology. There are several topologies commonly used in large-scale HPC deployments (Fat-Tree, 3D-Torus, Dragonfly+ etc.).</p> <p> Iris (like Aion) is part of an Island which employs a \"Fat-Tree\" Topology<sup>2</sup> which remains the widely used topology in HPC clusters due to its versatility, high bisection bandwidth and well understood routing.</p> <p>Iris 2-Level 1:1.5 Fat-Tree is composed of:</p> <ul> <li>18x Infiniband EDR<sup>1</sup> Mellanox SB7800 switches (36 ports)<ul> <li>12x Leaf IB (LIB) switches (L1), each with 12 EDR L1-L2 interlinks</li> <li>6x Spine IB (SIB) switches (L2), with 8 EDR downlinks (total: 48 links) used for the interconnexion with the Aion Cluster</li> </ul> </li> <li>Up to 24 Iris compute nodes and servers EDR connection per L1 switch using 24 EDR ports</li> </ul> <p>For more details:  ULHPC Fast IB Interconnect</p> <p>Illustration of Iris network cabling (IB and Ethernet) within one of the rack hosting the compute nodes:</p> <p></p> <ol> <li> <p>Enhanced Data Rate (EDR) \u2013 100 Gb/s throughput with a very low latency, typically below 0,6\\mus.\u00a0\u21a9\u21a9</p> </li> <li> <p>with blocking factor 1:1.5.\u00a0\u21a9</p> </li> </ol>"},{"location":"systems/iris/timeline/","title":"Iris Timeline","text":"<p>This page records a brief timeline of significant events and user environment changes on Iris. The Iris cluster exists since the beginning of 2017 as the flagship HPC supercomputer within the University of Luxembourg until 2020 and the release of the Aion supercomputer.</p>"},{"location":"systems/iris/timeline/#2016","title":"2016","text":""},{"location":"systems/iris/timeline/#september-2016","title":"September 2016","text":"<ul> <li>Official Public release of Iris cluster tenders on TED European tender and Portail des March\u00e9s Publiques (PMP)<ul> <li>RFP 160019: High Performance Storage System for the High Performance Computing Facility of the University of Luxembourg.</li> <li>RFP 160020: High Performance Computing Facility (incl. Interconnect) for the University of Luxembourg.</li> </ul> </li> </ul>"},{"location":"systems/iris/timeline/#october-2016","title":"October 2016","text":"<ul> <li>Bids Opening for both RFPs on October 12, 2016.<ul> <li>Starting offers analysis by the ULHPC team, together with the procurement and legal departments of the University</li> </ul> </li> </ul>"},{"location":"systems/iris/timeline/#november-2016","title":"November 2016","text":"<ul> <li>Awarding notification to the vendors<ul> <li>RFP 160019 attributed to the Telindus/HPE/DDN consortium to provide High Performance Storage solution of capacity 1.44 PB (raw) (over <code>GPFS/SpectrumScale</code> Filesystem), with a RW performance above 10GB/s</li> <li>RFP 160020 attributed to the Post/DELL consortium to provide a High Performance Computing (HPC) cluster of effective capacity R_\\text{max} = 94.08 TFlops (raw capacity R_\\text{peak} = 107.52 TFlops)</li> </ul> </li> </ul>"},{"location":"systems/iris/timeline/#2017","title":"2017","text":""},{"location":"systems/iris/timeline/#march-april-2017","title":"March-April 2017","text":"<p>Delivery and installation of the <code>iris</code> cluster composed of:</p> <ul> <li><code>iris-[1-100]</code>, Dell PowerEdge C6320, 100 nodes, 2800 cores, 12.8 TB RAM</li> <li>10/40GB Ethernet network, high-speed Infiniband EDR 100Gb/s interconnect</li> <li>SpectrumScale (GPFS) core storage, 1.44 PB</li> <li>Redundant / load-balanced services with:<ul> <li>2x adminfront servers (cluster management)</li> <li>2x access servers (user frontend)</li> </ul> </li> </ul>"},{"location":"systems/iris/timeline/#may-june-2017","title":"May-June 2017","text":"<ul> <li>End of cluster validation</li> <li>8 new regular nodes added<ul> <li><code>iris-[101-108]</code>, Dell PowerEdge C6320, 8 nodes, 224 cores, 1.024 TB RAM</li> </ul> </li> <li>Official release of the <code>iris</code> cluster for production on June 12, 2017 at the occasion of the UL HPC School 2017.</li> </ul>"},{"location":"systems/iris/timeline/#october-2017","title":"October 2017","text":"<ul> <li>Official Public release of Iris Lustre Storage acquisition tenders on TED European tender and Portail des March\u00e9s Publiques (PMP)<ul> <li>RFP 170035: Complementary Lustre High Performance Storage System for the High Performance Computing Facility of the University of Luxembourg.</li> </ul> </li> </ul>"},{"location":"systems/iris/timeline/#november-2017","title":"November 2017","text":"<ul> <li>Bids Opening for Lustre RFP on November 28, 2017.<ul> <li>Starting offers analysis by the ULHPC team, together with the procurement and legal departments of the University</li> </ul> </li> </ul>"},{"location":"systems/iris/timeline/#december-2017","title":"December 2017","text":"<ul> <li>Awarding notification to the vendors<ul> <li>Lustre RFP 170035 attributed to the Fujitsu/DDN consortium to provide High Performance Storage solution of capacity 1.28 PB (raw)</li> </ul> </li> <li>60 new regular nodes added yet based on Skylake processors<ul> <li><code>iris-[109-168]</code>, Dell PowerEdge C6420, 60 nodes, 1680 cores, 7.68 TB RAM</li> </ul> </li> </ul>"},{"location":"systems/iris/timeline/#2018","title":"2018","text":""},{"location":"systems/iris/timeline/#february-2018","title":"February 2018","text":"<ul> <li><code>iris</code> cluster moved from CDC S-01 to CDC S-02</li> </ul>"},{"location":"systems/iris/timeline/#april-2018","title":"April 2018","text":"<ul> <li>SpectrumScale (GPFS) DDN GridScaler extension to reach 2284TB raw capacity<ul> <li>new expansion unit and provisioning of enough complementary disks to feed the system.</li> </ul> </li> <li>Delivery and installation of the complementary Lustre storage, with 1280 TB raw capacity</li> </ul>"},{"location":"systems/iris/timeline/#july-2018","title":"July 2018","text":"<ul> <li>Official Public release of tenders on TED European tender and Portail des March\u00e9s Publiques (PMP)<ul> <li>RFP 180027: Complementary Multi-GPU and Large-Memory Computer Nodes for the High Performance Computing Facility of the University of Luxembourg.</li> </ul> </li> </ul>"},{"location":"systems/iris/timeline/#september-2018","title":"September 2018","text":"<ul> <li>Bids Opening for Multi-GPU and Large-Memory nodes RFP on September 10, 2018.<ul> <li>Starting offers analysis by the ULHPC team, together with the procurement and legal departments of the University</li> </ul> </li> </ul>"},{"location":"systems/iris/timeline/#october-2018","title":"October 2018","text":"<ul> <li>Awarding notification to the vendors<ul> <li>RFP 180027 attributed to the Dimension Data/Dell consortium</li> </ul> </li> </ul>"},{"location":"systems/iris/timeline/#dec-2018","title":"Dec 2018","text":"<ul> <li>New Multi-GPU and Bigmem compute nodes added<ul> <li><code>iris-[169-186]</code>: Dell C4140, 18 GPU nodes x 4 Nvidia V100 SXM2 16GB, part of the <code>gpu</code> partition</li> <li><code>iris-[187-190]</code>: Dell R840, 4 Bigmem nodes 4x28c i.e. 112 cores per node, part of the <code>bigmem</code> partition</li> </ul> </li> </ul>"},{"location":"systems/iris/timeline/#2019","title":"2019","text":""},{"location":"systems/iris/timeline/#may-2019","title":"May 2019","text":"<ul> <li>6 new Multi-GPU nodes added<ul> <li><code>iris-[191-196]</code>: Dell C4140, 6 GPU nodes x 4 Nvidia V100 SXM2 32GB, part of the <code>gpu</code> partition</li> </ul> </li> </ul>"},{"location":"systems/iris/timeline/#october-2019","title":"October 2019","text":"<ul> <li>SpectrumScale (GPFS) extension to allow 1Bn files capacity<ul> <li>replacement of 2 data pools (HDD-based) with new metadata pools (SSD-based)</li> </ul> </li> </ul>"}]}